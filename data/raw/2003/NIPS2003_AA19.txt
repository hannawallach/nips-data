Linear Dependent Dimensionality Reduction

Nathan Srebro Tommi Jaakkola

Department of Electrical Engineering and Computer Science Massachusetts Institute of Technology Cambridge, MA 02139 nati@mit.edu,tommi@ai.mit.edu

Abstract

We formulate linear dimensionality reduction as a semi-parametric estimation problem, enabling us to study its asymptotic behavior. We generalize the problem beyond additive Gaussian noise to (unknown) nonGaussian additive noise, and to unbiased non-additive models.

1 Introduction

Factor models are often natural in the analysis of multi-dimensional data. The underlying premise of such models is that the important aspects of the data can be captured via a low-dimensional representation ("factor space"). The low-dimensional representation may be useful for lossy compression as in typical applications of PCA, for signal reconstruction as in factor analysis or non-negative matrix factorization [1], for understanding the signal structure [2], or for prediction as in applying SVD for collaborative filtering [3]. In many situations, including collaborative filtering and structure exploration, the "important" aspects of the data are the dependencies between different attributes. For example, in collaborative filtering we rely on a representation that summarizes the dependencies among user preferences. More generally, we seek to identify a low-dimensional space that captures the dependent aspects of the data, and separate them from independent variations. Our goal is to relax restrictions on the form of each of these components, such as Gaussianity, additivity and linearity, while maintaining a principled rigorous framework that allows analysis of the methods. We begin by studying the probabilistic formulations of the problem, focusing on the assumptions that are made about the dependent, low-rank "signal" and independent "noise" distributions. We consider a general semi-parametric formulation that emphasizes what is being estimated and allows us to discuss asymptotic behavior (Section 2). We then study the standard (PCA) approach, show that it is appropriate for additive i.i.d. noise (Section 3), and present a generic estimator that is appropriate also for unbiased non-additive models (Section 4). In Section 5 we confront the non-Gaussianity directly, develop maximumlikelihood estimators in the presence of Gaussian mixture additive noise, and show that the consistency of such maximum-likelihood estimators should not be taken for granted.


2 Dependent Dimensionality Reduction Our starting point is the problem of identifying linear dependencies in the presence of independent identically distributed Gaussian noise. In this formulation, we observe a data pendent component Z (the "noise") is i.i.d. zero-mean Gaussian with variance 2. We can or sum-squared, norm) and conclude that, regardless of the variance 2, the maximumlikelihood estimator of X is the rank-k matrix minimizing the Frobenius distance. It is given by the leading components of the singular value decomposition of Y .1 Although the above formulation is perfectly valid, there is something displeasing about it. We view the entire matrix X as parameters, and estimate them according to a single observation Y . The number of parameters is linear in the data, and even with more data, we cannot hope to estimate the parameters (entries in X ) beyond a fixed precision. What we can estimate with more data rows is the rank-k row-space of X . Consider the factorization

matrix Y  n×d which we assume was generated as Y = X + Z, where the dependent,

low-dimensional component X 

n×d

(the "signal") is a matrix of rank k and the inde-

write down the log-likelihood of X as -1 2 |Y -X|Fro+Const (where ||Fro is the Frobenius,

X = UV , where V 

k×d

spans this "signal space".

The dependencies of each row y of Y are captured by a row u of U, which, through the parameters V and  specifies how each entry yi is generated independently given u.2 u

y 1 y 2 . . . y d

A standard parametric analysis of the model would view u as a random vector (rather than parameters) and impose some, possibly parametric, distribution over it (interestingly, if u is Gaussian, the maximum-likelihood reconstruction is the same Frobenius low-rank approximation [4]). However, in the analysis we started with, we did not make any assumptions about the distribution of u, beyond its dimensionality. The model class is then non-parametric, yet we still desire, and are able, to estimate a parametric aspect of the model: The estimator can be seen as a ML estimator for the signal subspace, where the distribution over u is unconstrained nuisance. Although we did not impose any form on the distribution u, we did impose a strict form on the conditional distributions yi|u: we required them to be Gaussian with fixed variance 2 and mean uVi . We would like to relax these requirements, and require only that y|u be a product distribution, i.e. that its coordinates yi|u be (conditionally) independent. Since u is continuous, we cannot expect to forego all restrictions on yi|ui, but we can expect to set up a semi-parametric problem in which y|u may lie in an infinite dimensional family of distributions, and is not strictly parameterized. Relaxing the Gaussianity leads to linear additive models y = uV + z, with z independent of u, but not necessarily Gaussian. Further relaxing the additivity is appropriate, e.g., when the noise has a multiplicative component, or when the features of y are not real numbers. These types of models, with a known distribution yi|xi, have been suggested for classification using logistic loss [5], when yi|xi forms an exponential family [6], and in a more abstract framework [7]. Relaxing the linearity assumption x = uV is also appropriate in many situations. Fitting a non-linear manifold by minimizing the sum-squared distance can specifies some smooth manifold. Combining these ideas leads us to discuss the conditional distributions yi|gi(u), or yi|u directly. In this paper we take our first steps is studying this problem, and relaxing restrictions on

be seen as a ML estimator for y|u = g(u) + z, where z is i.i.d. Gaussian and g : k  d

1 A mean term is also usually allowed. Incorporating a non-zero mean is straight forward, and in

order to simplify derivations, we do not account for it in most of our presentation. to indicate random quantities.

2 We use uppercase letters to denote matrices, and lowercase letters for vectors, and use bold type


y|u. We continue to assume a linear model x = uV and limit ourselves to additive noise models and unbiased models in which E [y|x] = x. We study the estimation of the rank-k signal space in which x resides, based on a sample of n independent observations of y (forming the rows of Y), where the distribution on u is unconstrained nuisance. In order to study estimators for a subspace, we must be able to compare two subspaces. A natural way of doing so is through the canonical angles between them [8]. Define the angle between a vector v1 and a subspace V2 to be the minimal angle between v1 and any v2  V2. The largest canonical angle between two subspaces is then the maximal angle between a vector in v1  V1 and the subspace V2. The second largest angle is the maximum over all vectors orthogonal to the v1, and so on. It is convenient to think of a subspace in terms of the matrix whose columns span it. Computationally, if the columns of V1 and V2 form orthonormal bases of V1 and V2, then the cosines of the canonical angles between V1 and V2 are given by the singular values of V1V2. Throughout the presentation, we will slightly overload notation and use a matrix to denote also its column subspace. In particular, we will denote by V0 the true signal subspace, i.e. such that x = uV0 .

3 The L2 Estimator We first consider the "standard" approach to low-rank approximation--minimizing the sum squared error.3 This is the ML estimator when the noise is i.i.d. Gaussian. But the L2 estimator is appropriate also in a more general setting. We will show that the L2 estimator is consistent for any i.i.d. additive noise with finite variance (as we will see later on, this is more than can be said for some ML estimators). The L2 estimator of the signal subspace is the subspace spanned by the leading eigenvectors of the empirical covariance matrix n of y, which is a consistent estimator of the true ^ covariance matrix Y , which in turn is the sum of the covariance matrices of x and z, where X is of rank exactly4 k, and if z is i.i.d., Z = 2I. Let s1  s2  ···  sk > 0 be the non-zero eigenvalues of x. Since z has variance exactly 2 in any direction, the principal directions of variation are not affected by it, and the eigenvalues of Y are exactly s1 + 2, . . . , sk + 2, 2, . . . , 2, with the leading k eigenvectors being the eigenvectors of X. This ensures an eigenvalue gap of sk > 0 between the invariant subspace of Y spanned by the eigenvectors of X and its complement, and we can bound the norm of the canonical sines between V0 and the leading k eigenvectors of

^ |n-Y | n by ^ [8]. Since |n-Y |  0 a.s., we conclude that the estimator is consistent. ^

sk

4 The Variance-Ignoring Estimator We turn to additive noise with independent, but not identically distributed, coordinates. If the noise variances are known, the ML estimator corresponds to minimizing the columnweighted (inversely proportional to the variances) Frobenius norm of Y - X , and can be calculated from the leading eigenvectors of a scaled empirical covariance matrix [9]. If the variances are not known, e.g. when the scale of different coordinates is not known, there is no ML estimator: at least k coordinates of each y can always be exactly matched, and so the likelihood is unbounded when up to k variances approach zero.

3

We call this an L2 estimator not because it minimizes the matrix L2-norm |Y - X |2, which it

does, but because it minimizes the vector L2-norms |y - x|22. with any rank-k subspace containing the support of x, but for simplicity of presentation we assume this does not happen and x is of full rank k.

4 We should also be careful about signals that occupy only a proper subspace of V0, and be satisfied


1 1.1

0.9

L2 variance-ignored ML, known variances

1 1

0.8 0.9

|

0.6 2 0.5

|

0.7 2 0.6

L2 variance-ignored ML, known variances

0.8

0.7 0.8

)| 0.6 2

|sin |sin |sin(

0.4 0.5

0.4

0.3 0.4

0.2 0.3 0.2

0.1 0.2

full L2 variance-ignored

0 0.1 0

10 100

1000 sample size 10000 1 2 3 4 5 6 7 8 9 10 102

spread of noise scale (max/min ratio)

103 sample size (number of observed rows)

104

Figure 1: Norm of sines of canonical angles to correct subspace: (a) Random rank-2 subspaces

in

10

. Gaussian noise of different scales in different coordinates-- between 0.17 and 1.7 signal

strength. (b) Random rank-2 subspaces in 10 , 500 sample rows, and Gaussian noise with varying

distortion (mean over 200 simulations, bars are one standard deviations tall) (c) Observations are

exponentially distributed with means in rank-2 subspace ( 11 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 ) .

The L2 estimator is not satisfactory in this scenario. The covariance matrix Z is still diagonal, but is no longer a scaled identity. The additional variance introduced by the noise is different in different directions, and these differences may overwhelm the "signal" variance along V0, biasing the leading eigenvectors of Y , and thus the limit of the L2 estimator, toward axes with high "noise" variance. The fact that this variability is independent of the variability in other coordinates is ignored, and the L2 estimator is asymptotically biased. Instead of recovering the directions of greatest variability, we recover the covariance structure directly. In the limit, n  Y = X + Z, a sum of a rank-k matrix and a diagonal ^ matrix. In particular, the non-diagonal entries of n approach those of X. We can thus ^ seek a rank-k matrix X approximating n, e.g. in a sum-squared sense, except on the di^ ^ agonal. This is a (zero-one) weighted low-rank approximation problem. We optimize X ^ by iteratively seeking a rank-k approximation of n with diagonal entries filled in from ^ the last iterate of X (this can be viewed as an EM procedure [5]). The row-space of the ^ resulting X is then an estimator for the signal subspace. Note that the L2 estimator is the ^ row-space of the rank-k matrix minimizing the unweighted sum-squared distance to n. ^ Figures 1(a,b) demonstrate this variance-ignoring estimator on simulated data with nonidentical Gaussian noise. The estimator reconstructs the signal-space almost as well as the ML estimator, even though it does not have access to the true noise variance. Discussing consistency in the presence of non-identical noise with unknown variances is problematic, since the signal subspace is not necessarily identifiable. For example, the combined covariance matrix Y = ( 21 ) can arise from a rank-one signal covariance

1 2

 a  2, each corresponding to a different signal subspace.

Counting the number of parameters and constraints suggests identifiability when k < d -



8d+1-1 2

, but this is by no means a precise guarantee. Anderson and Rubin [10] present

several conditions on X which are sufficient for identifiability but require k < other weaker conditions which are necessary.

d 2 , and

X =

a 1 1 1/a

for any

1 2

Non-Additive Noise The above estimation method is also useful in a less straightforward situation. Until now we have considered only additive noise, in which the distribution of yi - xi was independent of xi. We will now relax this restriction and allow more general conditional distributions yi|xi, requiring only that E [yi|xi] = xi. With this requirement, together with the structural constraint (yi independent given x), for any i = j: Cov [yi, yj] = E [yiyj] - E [yi]E [yj] = E [E [yiyj|x]] - E [E [yi|x]]E [E [yj|x]] = E [E [yi|x]E [yj|x]] - E [xi]E [xj] = E [xixj] - E [xi]E [xj] = Cov [xi, xj].


As in the non-identical additive noise case, Y agrees with X except on the diagonal. Even if yi|xi is identically conditionally distributed for all i, the difference Y - X is not in general a scaled identity: Var [yi] = E E yi |xi - E [yi|xi]2 + E E [yi|xi]2 -

2

E [yi]2 = E [Var [yi|xi]] + Var [xi]. Unlike the additive noise case, the variance of yi|xi depends on xi, and so its expectation depends on the distribution of xi. These observations suggest using the variance-ignoring estimator. Figure 1(c) demonstrates how such an estimator succeeds in reconstruction when yi|xi is exponentially distributed with mean xi, even though the standard L2 estimator is not applicable. We cannot guarantee consistency because the decomposition of the covariance matrix might not be unique, y|x is known, even if the decomposition is not unique, the correct signal covariance might be identifiable based on the relationship between the signal marginals and the expected conditional variance of of y|x, but this is not captured by the variance-ignoring estimator. 5 Low Rank Approximation with a Gaussian Mixture Noise Model We return to additive noise, but seeking better estimation with limited data, we confront non-Gaussian noise distributions directly: we would like to find the maximum-likelihood X when Y = X + Z, and Zij are distributed according to a Gaussian mixture: pZ(zij) = To do so, we introduce latent variables Cij specifying the mixture component of the noise at Yij, and solve the problem using EM. In the Expectation step, we compute the posterior probabilities Pr (Cij|Yij; X ) based on the current low-rank parameter matrix X . In the Maximization step we need to find the low-rank matrix X that maximizes the posterior expected log-likelihood:

but when k < d 2 this is not likely to happen. Note that if the conditional distribution

m c=1 2 1/2 pc(2c) exp((zij - µc)2/(2c)). 2

EC [log Pr (Y = X + Z|C; X)] = - |Y

= -12

ij Pr(Cij=c)|Yij

Pr(Cij=c)|Yij 2c

2 (Xij-(Yij+µc))2

ij c

Wij (Xij - Aij)2 + Const

Aij = Yij +

+ Const (1)

where Wij = c 2 Pr(Cij=c)|Yijµc cWij 2

c c

This is a weighted Frobenius low-rank approximation (WLRA) problem. Equipped with a WLRA optimization method [5], we can now perform EM iteration in order to find the matrix X maximizing the likelihood of the observed matrix Y . At each M step it is enough to perform a single WLRA optimization iteration, which is guaranteed to improve the WLRA objective, and so also the likelihood. The method can be augmented to handle an unknown Gaussian mixture, by introducing an optimization of the mixture parameters at each M iteration. Experiments with GSMs We report here initial experiments with ML estimation using bounded Gaussian scale mixtures [11], i.e. a mixture of Gaussians with zero mean, and variance bounded from bellow. Gaussian scale mixtures (GSMs) are a rich class of symmetric distributions, which include non-log-concave, and heavy tailed distributions. We investigated two noise distributions: a 'Gaussian with outliers' distribution formed as a mixture of two zero-mean Gaussians with widely varying variances; and a Laplace distribution p(z)  e-|z , which is an infinite scale mixture of Gaussians. Figures 2(a,b)

|

show the quality of reconstruction of the L2 estimator and the ML bounded GSM estimator, for these two noise distributions, for a fixed sample size of 300 rows, under varying


0.45 1.4

0.4

0.35

0.3

)| 2

|sin(

0.25

0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 00 L2 ML, known noise model ML, nuisance noise model

0.5

1.2 ML L2

0.45

1 0.4

0.35

)|

0.8 2 0.3

|sin(

)

0.25

0.6 sin(

0.1 0.2 0.3 0.4

0.2

0.2 0.4 0.15

L2 ML 0.1

0.15 0.2

0.05

0.10.2 0.4 0.6 0.8 1 1.2 1.4 1.6 00 0.2 0.4 0.6 0.8 1 1.2 1.4 0

10 100 1000 10000

signal variance / noise variance signal variance / noise variance Sample size (number of observed rows)

Figure 2: Norm of sines of canonical angles to correct subspace: (a) Random rank-3 subspace in

10

with Laplace noise. Insert: sine norm of ML est. plotted against sine norm of L2 est. (b)

Random rank-2 subspace in

10

with 0.99N(0, 1) + 0.01N(0, 100) noise. (c) span(2, 1, 1) 

3

with 0.9N(0, 1) + 0.1N(0, 25) noise. The ML estimator converges to (2.34, 1, 1). Bars are one standard deviation tall.

signal strengths. We allowed ten Gaussian components, and did not observe any significant change in the estimator when the number of components increases. The ML estimator is overall more accurate than the L2 estimator--it succeeds in reliably reconstructing the low-rank signal for signals which are approximately three times weaker than those necessary for reliable reconstruction using the L2 estimator. The improvement in performance is not as dramatic, but still noticeable, for Laplace noise. Comparison with Newton's Methods Confronted with a general additive noise distribution, the approach presented here would be to rewrite, or approximate, it as a Gaussian mixture and use WLRA in order to learn X using EM. A different approach is to considering the second order Taylor expansions of the log-likelihood, with respect to the entries of X, and iteratively maximize them using WLRA [5, 7]. Such an approach requires calculating the first and second derivatives of the density. If the density is not specified analytically, or is unknown, these quantities need to be estimated. But beyond these issues, which can be overcome, lies the major problem of Newton's method: the noise density must be strictly log-concave and differentiable. If the distribution is not log-concave, the quadratic expansion of the log-likelihood will be unbounded and will not admit an optimum. Attempting to ignore this fact, and for example "optimizing" U given V using the equations derived for non-negative weights would actually drive us towards a saddle-point rather then a local optimum. The non-concavity does not only mean that we are not guaranteed a global optimum (which we are not guaranteed in any case, due to the non-convexity of the low-rank requirement)-- it does not yield even local improvements. On the other hand, approximating the distribution as a Gaussians mixture and using the EM method, might still get stuck in local minima, but is at least guaranteed local improvement. Limiting ourselves to only log-concave distributions is a rather strong limitation, as it precludes, for example, all heavy-tailed distributions. Consider even the "balanced tail" Laplace distribution p(z)  e-|z . Since the log-density is piecewise linear, a quadratic

|

approximation of it is a line, which of course does not attain a minimum value. Consistency Despite the gains in reconstruction presented above, the ML estimator may suffer from an asymptotic bias, making it inferior to the L2 estimator on large samples. We study the asymptotic limit of the ML estimator, for a known product distribution p. We first establish a necessary and sufficient condition for consistency of the estimator. The ML estimator is the minimizer of the empirical mean of the random function (V ) = minu(- log p(y-uV )). When the number of samples increase, the empirical means converge to the true means, and if E [(V1)] < E [(V2)], then with probability approaching


one V2 will not minimize E [(V )]. For the ML estimator to be consistent, E [(V )] must ^ be minimized by V0, establishing a necessary condition for consistency. The sufficiency of this condition rests on the uniform convergence of {E [(V )]}, which ^ does not generally exist, or at least on uniform divergence from E [(V0)]. It should be noted that the issue here is whether the ML estimator at all converges, since if it does converge, it must converge to the minimizer of E [(V )]. Such convergence can be demonstrated at least in the special case when the marginal noise density p(zi) is continuous, strictly positive, and has finite variance and differential entropy. Under these conditions, the ML estimator is consistent if and only if V0 is the unique minimizer of E [(V )]. When discussing E [(V )], the expectation is with respect to the noise distribution and the signal distribution. This is not quite satisfactory, as we would like results which are independent of the signal distribution, beyond the rank of its support. To do so, we must ensure the expectation of (V ) is minimized on V0 for all possible signals (and not only in consider (V ; x) = Ez [(x + z; V )], where the expectation is only over the additive noise z. Under the previous conditions guaranteeing the ML estimator converges, it is consistent

expectation). Denote the objective (y; V ) = minu(- log p(y - uV )). For any x  d ,

for any signal distribution if and only if, for all x  to V exactly when x  spanV . d , (V ; x) is minimized with respect

It will be instructive to first revisit the ML estimator in the presence of i.i.d. Gaussian noise, i.e. the L2 estimator which we already showed is consistent. We will consider the decomposition y = y + y of vectors into their projection onto the subspace V , and the residual . Any rotation of p is an isotropic Gaussian, and so z and z are independent, and p(y) = p (y )p(y). We can now analyze: (V ; y) = min(- log p (y + uV ) - log p(y)) = - log p (0) + |y|2 + Const yielding (V ; x)  Ez [|x + z|2] + Const, which is minimized when x = 0, i.e. x is spanned by V . We thus re-derived the consistency of the L2 estimator directly, for the special case in which the noise is indeed Gaussian. This consistency proof employed a key property of the isotropic Gaussian: rotations of an isotropic Gaussian random variable remain i.i.d. As this property is unique to Gaussian random variables, other ML estimators might not be consistent. In fact, we will shortly see that the ML estimator for a known Laplace noise model is not consistent. To do so, we will note that a necessary condition for consistency, if the density function p is continuous, is that (V ; 0) = E [(z; V )] is constant over all V . Otherwise we have (V1; 0) < (V2; 0) for some V1, V2, and for small enough x  V2, (V1; x) < (V2; x). A non-constant (V ; 0) indicates an a-priori bias towards certain sub-spaces.

u

1 2

The negative log-likelihood of a Laplace distribution, p(zi) =

1 -|zi|

e

2

, is essentially the

L1 norm. Consider a rank-one approximation in a two-dimensional space with Laplace noise. For any V = (1, ), 0    1, and (z1, z2), the L1 norm |z + uV |1 is minimized when z1 + u = 0 yielding (V ; z) = |z2 - z1|, ignoring a constant term, and (V ; 0) =

1 -|z1|-|z2|

e

4

|z2 - z1|dz1dz2 = 2++1 +1 , which is monotonic increasing in  in the

valid range [0, 1]. In particular, 1 = ((1, 0); 0) < ((1, 1); 0) = biased towards being axis-aligned. 3 2 and the estimator is

Figure 2(c) demonstrates such an asymptotic bias empirically. Two-component Gaussian using an ML estimator with known noise model, and an L2 estimator. For small data sets, the ML estimator is more accurate, but as the number of samples increase, the error of the L2 estimator vanishes, while the ML estimator converges to the wrong subspace.

mixture noise was added to rank-one signal in 3 , and the signal subspace was estimated


6 Discussion In many applications few assumptions beyond independence can be made. We formulate the problem of dimensionality reduction as semi-parametric estimation of the lowdimensional signal, or "factor" space, treating the signal distribution as unconstrained nuisance and the noise distribution as constrained nuisance. We present an estimator which is appropriate when the conditional means E [y|u] lie in a low-dimensional linear space, and a maximum-likelihood estimator for additive Gaussian mixture noise. The variance-ignoring estimator is also applicable when y can be transformed such that E [g(y)|u] lie in a low-rank linear space, e.g. in log-normal models. If the conditional distribution y|x is known, this amount to an unbiased estimator for xi. When such a transformation is not known, we may wish to consider it as nuisance. We draw attention to the fact the maximum-likelihood low-rank estimation cannot be taken for granted, and demonstrate that it might not be consistent even for known noise models. The approach employed here can also be used to investigate the consistency of ML estimators with non-additive noise models. Of particular interest are distributions yi|xi that form exponential families where xi are the natural parameters [6]. When the mean parameters form a low-rank linear subspace, the variance-ignoring estimator is applicable, but when the natural parameters form a linear subspace, the means are in general curved, and there is no unbiased estimator for the natural parameters. Initial investigation reveals that, for example, the ML estimator for a Bernoulli (logistic) conditional distribution is not consistent. The problem of finding a consistent estimator for the linear-subspace of natural parameters when yi|xi forms an exponential family remains open. We also leave open the efficiency of the various estimators, and the problem of finding asymptotically efficient estimators, and consistent estimators exhibiting the finite-sample gains of the ML estimator for additive Gaussian mixture noise. References

[1] Daniel D. Lee and H. Sebastian Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 401:788­791, 1999. [2] Orly Alter, Patrick O. Brown, and David Botstein. Singular value decomposition for genomewide expression data processing and modeling. PNAS, 97(18):10101­10106, 2000. [3] Yossi Azar, Amos Fiat, Anna R. Karlin, Frank McSherry, and Jared Saia. Spectral analysis of data. In 33rd ACM Symposium on Theory of Computing, 2001. [4] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society, Series B, 21(3):611­622, 1999. [5] Nathan Srebro and Tommi Jaakkola. Weighted low rank approximation. In 20th International Conference on Machine Learning, 2003. [6] M. Collins, S. Dasgupta, and R. E. Schapire. A generalization of principal components analysis to the exponential family. In Advances in Neural Information Processing Systems 14, 2002. [7] Geoffrey J. Gordon. Generalized2 linear2 models. In Advances in Neural Information Processing Systems 15, 2003. [8] G. W. Stewart and Ji-guang Sun. Matrix Perturbation Theory. Academic Press, Inc, 1990. [9] Michal Irani and P Anandan. Factorization with uncertainty. In 6th European Conference on Computer Vision, 2000. [10] T. W. Anderson and Herman Rubin. Statistical inference in factor analysis. In Third Berleley Symposium on Mathematical Statistics and Probability, volume V, pages 111­150, 1956. [11] M J Wainwright and E P Simoncelli. Scale mixtures of Gaussians and the statistics of natural images. In Advances in Neural Information Processing Systems 12, 2000.


