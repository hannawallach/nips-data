Near-Minimax Optimal Classification with Dyadic Classification Trees

Clayton Scott Electrical and Computer Engineering Rice University Houston, TX 77005 cscott@rice.edu Robert Nowak Electrical and Computer Engineering University of Wisconsin Madison, WI 53706 nowak@engr.wisc.edu

Abstract This paper reports on a family of computationally practical classifiers that converge to the Bayes error at near-minimax optimal rates for a variety of distributions. The classifiers are based on dyadic classification trees (DCTs), which involve adaptively pruned partitions of the feature space. A key aspect of DCTs is their spatial adaptivity, which enables local (rather than global) fitting of the decision boundary. Our risk analysis involves a spatial decomposition of the usual concentration inequalities, leading to a spatially adaptive, data-dependent pruning criterion. For any distribution on (X, Y ) whose Bayes decision boundary behaves locally like a Lipschitz smooth function, we show that the DCT error converges to the Bayes error at a rate within a logarithmic factor of the minimax optimal rate. We also study DCTs equipped with polynomial classification rules at each leaf, and show that as the smoothness of the boundary increases their errors converge to the Bayes error at a rate approaching n- , the parametric rate. We are not aware of any other practical classi-

1/2

fiers that provide similar rate of convergence guarantees. Fast algorithms for tree pruning are discussed.

1 Introduction We previously studied dyadic classification trees, equipped with simple binary decision rules at each leaf, in [1]. There we applied standard structural risk minimization to derive a pruning rule that minimizes the empirical error plus a complexity penalty proportional to the square root of the size of the tree. Our main result concerned the rate of convergence of the expected error probability of our pruned dyadic classification tree to the Bayes error for a certain class of problems. This class, which essentially requires the Bayes decision boundary to be locally Lipschitz, had previously been studied by Mammen and Tsybakov [2]. They showed the minimax rate of convergence for this class to be n- , where n is

1/d

the number of labeled training samples, and d is the dimension of each sample. They also demonstrated a classification rule achieving this rate, but the rule requires minimization of the empirical error over the entire class of decision boundaries, an infeasible task in practice. In contrast, DCTs are computationally efficient, but converge at a slower rate of

n- 1/(d+1) .


In this paper we exhibit a new pruning strategy that is both computationally efficient and realizes the minimax rate to within a log factor. Our approach is motivated by recent results from Kearns and Mansour [3] and Mansour and McAllester [4]. Those works develop a theory of local uniform convergence, which allows the error to be decomposed in a spatially adaptive way (unlike conventional structural risk minimization). In essence, the associated pruning rules allow a more refined partition in a region where the classification problem is harder (i.e., near the decision boundary). Heuristic arguments and anecdotal evidence in both [3] and [4] suggest that spatially adaptive penalties lead to improved performance compared to "global" penalties. In this work, we give theoretical support to this claim (for a specific kind of classification tree, the DCT) by showing a superior rate of convergence for DCTs pruned according to spatially adaptive penalties. We go on to study DCTs equipped with polynomial classification rules at each leaf. This provides more flexible classifiers that can take advantage of additional smoothness in the Bayes decision boundary. We call such a classifier a polynomial-decorated DCT (PDCT). PDCTs can be practically implemented by employing polynomial kernel SVMs at each leaf node of a pruned DCT. For any distribution whose Bayes decision boundary behaves locally like a Holder- smooth function, we show that the PDCT error converges to the ¨

Bayes error at a rate no slower than O((log n/n)/(

within a log factor of the parametric rate, n-1 /2 .

d+2-2)

). As    the rate tends to

Perceptron trees, tree classifiers having linear splits at each node, have been investigated by many authors and in particular we point to the works [5,6]. Those works consider optimization methods and generalization errors associated with perceptron trees, but do not address rates of approximation and convergence. A key aspect of PDCTs is their spatial adaptivity, which enables local (rather than global) polynomial fitting of the decision boundary. Traditional polynomial kernel-based methods are not capable of achieving such rates of convergence due to their lack of spatial adaptivity, and it is unlikely that other kernels can solve this problem for the same reason. Consider approximating a Holder- smooth func¨ tion on a bounded domain with a single polynomial. Then the error in approximation is O(1), a constant, which is the best one could hope for in learning a Holder smooth bound¨ ary with a traditional polynomial kernel scheme. On the other hand, if we partition the domain into hypercubes of side length O(1/m) and fit an individual polynomial on each hypercube, then the approximation error decays like O(m- ). Letting m grow with the



sample size n guarantees that the approximation error will tend to zero. On the other hand, pruning back the partition helps to avoid overfitting. This is precisely the idea behind the PDCT.

2 Dyadic Classification Trees In this section we review our earlier results on dyadic classification trees. Let X be a d-dimensional observation, and Y  {0, 1} its class label. Assume X  [0, 1]d. This is a realistic assumption for real-world data, provided appropriate translation and scaling has been applied. DCTs are based on the concept of a cyclic dyadic partition (CDP). Let P = {R1,...,Rk} be a tree-structured partition of the input space, where each Ri is a hyperrectangle with sides parallel to the coordinate axes. Given an integer , let [ ]d denote

j in the tree, let Ri

the element of {1, .(1). ,and that isbe . d} congruent to

(2)

Ri

modulo d. If Ri  P is a cell at depth

the rectangles formed by splitting Ri at its midpoint

along coordinate [j + 1]d. A CDP is a partition P constructed according to the rules: (i) The trivial partition P = [0, 1]d is a CDP; (ii) If {R1, . . . , Rk} is a CDP, then so is how the splits cycle through the coordinates of the input space as one traverses a path down the tree. We define a dyadic classification tree (DCT) to be a cyclic dyadic partition with

(1) (2)

, Ri {R1,...,Ri- ,Ri 1 , Ri+1, . . . , Rk}, where 1  i  d. The term "cyclic" refers to


(a) (b) (c)

Figure 1: Example of a dyadic classification tree when d = 2. (a) Training samples from two classes, and Bayes decision boundary. (b) Initial dyadic partition. (c) Pruned dyadic classification tree. Polynomial-decorated DCTs, discussed in Section 4, are similar in structure, but a polynomial decision rule is employed at each leaf of the pruned tree, instead of a simple binary label.

a class label (0 or 1) assigned to each node in the tree. We use the notation T to denote a DCT. Figure 1 (c) shows an example of a DCT in the two-dimensional case. Previously we presented a rule for pruning DCTs with consistency and rate of convergence properties. In this section we review those results, setting the stage for our main result in the next section. Let m = 2J be a dyadic integer, and define T0 to be the DCT that has every leaf node at depth dJ. Then each leaf of T0 corresponds to a cube of side length 1/m, and T0 has md total leaf nodes. Assume a training sample of size n is given, and each node of T0 is labeled according to a majority vote with respect to the training data reaching that node. A subtree T of T0 is referred to as a pruned subtree, denoted T  T0, if T includes the root of T0, if every internal node of T has both its children in T, and if the nodes of T inherit their labels from T0. The size of a tree T, denoted |T|, is the number of leaf nodes. We defined the complexity penalized dyadic classification tree Tn to be the solution of

Tn = arg min ^(T ) + n

T T0

|T|, (1)

where n = 32 log(en)/n, and ^(T ) is the empirical error, i.e., the fraction of training

data misclassified by T. (The solution to this pruning problem can be computed efficiently [7].) We showed that if X  [0, 1]d with probability one, and md = o(n/ log n), then probability over all classifiers (not just trees). We also demonstrated a rate of convergence result for Tn, under certain assumptions on the distribution of (X, Y ). Let us recall the definition of this class of distributions. Again, let X  [0, 1]d with probability one. Definition 1 Let c1, c2 > 0, and let m0 be a dyadic integer. Define F = F(c1, c2, m0) to be the collection of all distributions on (X, Y ) such that A1 (Bounded density): For any measurable set A, P{X  A}  c1(A), where  denotes the Lebesgue measure. A2 (Regularity): For all dyadic integers m  m0, if we subdivide the unit cube into cubes1 of side length 1/m, The Bayes decision boundary passes through at most c2md- of the resulting md cubes. These assumptions are satisfied when the density of X is essentially bounded with respect to Lebesgue measure, and when the Bayes decision boundary for the distribution on (X, Y ) behaves locally like a Lipschitz function. See, for example, the boundary fragment class of [2] with  = 1 therein.

Y } is the true error probability for T , and

E{ (Tn)} 



with probability one (i.e., Tn is consistent). Here, (T) = P{T(X) =

 is the Bayes error, i.e., the minimum error


In [1], we showed that if the distribution of (X, Y ) belongs to F, and m

(n/ log n)1 /(d+1)  /(d+1)



). However, this upper

bound on the rate of convergence is not tight. The results of Mammen and Tsybakov [2] , then E{ (Tn)} - = O((log n/n)1 show that the minimax rate of convergence, infn supF E{ (n)} - , is on the order of troduce a new strategy for pruning DCTs, which leads to an improved rate of convergence other practically implementable classifiers that can achieve this rate. 3 Improved Tree Pruning with Spatially Adaptive Penalties An improved rate of convergence is achieved by pruning the initial tree T0 using a new complexity penalty. Given a node v in a tree T, let Tv denote the subtree of T rooted at v. Let S denote the training data, and let nv denote the number of training samples reaching node v. Let R denote a pruned subtree of T. In the language of [4], R is called a root fragment. Let L(R) denote the set of leaf nodes of R. Consider the pruning rule that selects Tn = arg min ^(T ) + min (T, S, R) , (2)



(here n ranges over all possible discrimination rules). In the next section, we inn- 1/d

of (log n/n)1 /d (i.e., within a logarithmic factor of the minimax rate). We are not aware of

T T0

1 n

RT

where

(T, S, R) = 48nv|Tv| log(2n) + 48nvd log(m) .

vL(R)

Observe that the penalty is data-dependent (since nv depends on S) and spatially adaptive (choosing R  T to minimize ). Thepvpenalty can be interpreted as follows. The first can be viewed as an empirical average of the complexity penalties for each of the subtrees Tv, which depend on the local data associated with each subtree. The second term can be interpreted as the "cost" of spatially decomposing the bound on the generalization error. The penalty has the following property. Consider pruning one of two subtrees, both with the same size, and assume that both options result in the same increase in the empirical error. Then the subtree with more data is selected for pruning. Since deeper nodes typically have less data, this shows that the penalty favors unbalanced trees, which may promote higher resolution (deeper leaf nodes) in the vicinity of the decision boundary. In contrast, the pruning rule (1) penalizes balanced and unbalanced trees (with the same size) equally. The following theorem bounds the expected error of Tn. This kind of bound is known as an index of resolvability result [3,8]. Recall that m specifies the depth of the initial tree T0.

term in the penalty is written vL(R) 48|Tv| log(2n)/nv, where pv = nv/n. This

Theorem 1 If m  (n/ log n)1

E{ (Tn) - }  min T T0 

/d , then



( (T ) - ) + E min (T, S, R) RT + O log n n .

The first term in braces on the right is the approximation error. The remaining terms on the right-hand side bound the estimation error. Since the bound holds for all T, one feature of the pruning rule (2) is that Tn performs at least as well as the subtree T  T0 that minimizes the bound. This theorem may be applied to give us our desired rate of convergence result.

Theorem 2 Assume the density of (X, Y ) belongs to F. If m  (n/ log n)1 E{ (Tn)} - = O((log n/n)1/d).



/d , then


In other words, the pruning rule (2) comes within a log factor of the minimax rate. These theorems are proved in the last section. 4 Faster Rates for Smoother Boundaries In this section we extend Theorem 2 to the case of smoother decision boundaries. Define F(, c1, c2, m0)  F(c1, c2, m0) to be those distributions on (X, Y ) satisfying the following additional assumption. Here   1 is fixed. A3 (-regularity): Subdivide [0, 1]d into cubes of side length 1/m, m  m0. Within each cube the Bayes decision boundary is described by a function (one coordinate is a function of the others) with Holder regularity . ¨ like the graph of a function with Holder regularity . The "boundary fragments" class of The collection G contains all distributions whose Bayes decision boundaries behave locally ¨ Mammen and Tsybakov is a special case of boundaries satisfying A1 and A3. We propose a classifier, called a polynomial-decorated dyadic classification tree (PDCT), that achieves fast rates of convergence for distributions satisfying A3. Given a positive integer r, a PDCT of degree r is a DCT, with class labels at each leaf node assigned by a degree r polynomial classifier. Consider the pruning rule that selects Tn,r = arg min ^(T ) + min r(T, S, R) , (3) where

T T0 RT

r(T, S, R) =

Here Vd,r =

d+r r

1 n 48nvVd,r|Tv| log(2n) + 48nv(d + ) log(m) .

v

is the V C dimension of the collection of degree r polynomial classifiers

in d dimensions. Also, the notation T  T0 in (3) is rough. We actually consider a search over all pruned subtrees of T0, and with all possible configurations of degree r polynomial classifiers at the leaf nodes. An index of resolvability result analgous to Theorem 1 for Tn,r can be derived. Moreover, If r = ¨ a PDCT of degree r. In this case, Tn,r converges to the Bayes risk at rates bounded by the  - 1, then a decision boundary with Holder regularity  is well approximated by next theorem. 

(n/ log n)1/(

Theorem 3 Assume the density of (X, Y ) belongs to G and that r =

d+2-2)

, then E{ (Tn,r)} - = O((log n/n)/(

 d+2-2) ).

- 1. If m 

Note that in the case  = 1 this result coincides with the near-minimax rate in Theorem 2. the parametric rate n- . The proof is discussed in the final section. Also notice that as   , the rate of convergence comes within a logarithmic factor of 5 Efficient Algorithms The optimally pruned subtree Tn of rule (2) can be computed exactly in O(|T0|2) operations. This follows from a simple bottom-up dynamic programming algorithm, which we

1/2


describe below, and uses a method for "square-root" pruning studied in [7]. In the context of Theorem 2, we have |T0| = md  n, so the algorithm runs in time O(n2). an algorithm for finding both the optimal T  T0 and R  T solving (2). Given a node Note that an algorithm for finding the optimal R  T was provided in [4]. We now describe



and let Rv be the associated subtree that minimizes (Tv , S, R). The problem is solved v  T0, let Tv be the subtree of T0 rooted at v that minimizes the objective function of (2), by finding Troot and Rroot using a bottom-up procedure.

 

 

the children of v by u and w. There are three cases for Tv and Rv: (i) |T | = |R | = 1, If v is a leaf node of T0, then clearly Tv = Rv = {v}. If v is an internal node,denote computed by merging Tu with Tw and Ru with Rw, respectively; (iii) |Tv | > |Rv| = 1, in in which case Tv = Rv = {v};(ii) |Tv|  |Rv| > 1, in which case T and R can be v

   

v v



 

like the one in (1). At each node, these three candidates are determined, and Tv and Rv which case Rv = {v}, and Tv is determined by solving a square root pruning problem, just are the candidates minimizing the objective function (empirical error plus penalty) at each node. Using the first algorithm in [7], the overall pruning procedure may be accomplished in (|T0|2) operations. Determining the optimally pruned degree r PDCT is more challenging. The problem requires the construction, at each node of T0, a polynomial classifier of degree r having minimum empirical error. Unfortunately, this task is computationally infeasible for large sample sizes. As an alternative, we recommend the use of polynomial support vector machines. SVMs are well known for their good generalization ability in practical problems. Moreover, linear SVMs in perceptron trees have been shown to work well [6]. 6 Conclusions A key aspect of DCTs is their spatial adaptivity, which enables local (rather than global) fitting of the decision boundary. Our risk analysis involves a spatial decomposition of the usual concentration inequalities, leading to a spatially adaptive, data-dependent pruning criterion that promotes unbalanced trees that focus on the decision boundary. For distributions on (X, Y ) whose Bayes decision boundary behave locally like a Holder- smooth ¨ function, we show that the PDCT error converges to the Bayes error at a rate no slower such rates due to their lack of spatial adaptivity. When  = 1, the DCT convergence rate is a log factor of n- , the parametric rate. However, the rates for  > 1 are not within a within a logarithmic factor of the minimax optimal rate. As    the rate tends to within

than O((log n/n)/( d+2-2) ). Polynomial kernel methods are not capable of achieving

1/2

logarithmic factor of the minimax rate [2]. It may be possible to tighten the bounds further. On the other hand, near-minimax rates might not be achievable using rectangular partitions, and more flexible partitioning schemes, such as adaptive triangulations, may be required. 7 Proof Sketches The key to proving Theorem 1 is the following result, which is a modified version of a theorem of Mansour and McAllester [4]. Lemma 1 Let   (0, 1). With probability at least 1 - , every T  T0 satisfies (T )  ^(T ) + min f(T, S, R, ),

RT

where f(T, S, R, )



   v



= 1 n 48nv|Tv| log(2n)

vL(R)


+ 24nv[d log(m) + log(3/)] + 2[d log(m) + log(3/)]

Our primary modification to the lemma is to replace one local uniform deviation inequality (which holds for countable collections of classifiers [4, Lemma 4]) with another (which holds for infinite collections of classifiers [3, Lemma 2]). This eases our extension to polynomial-decorated DCTs in Section 4, by allowing us to avoid tedious quantization arguments. To prove Theorem 1, define the event m to be the collection of all training samples S such that for all T  T0, the bound of Lemma 1 holds, with  = 3/md. By that lemma, P(m)  1 - 3/md. Let T  T0 be arbitrary. We have

E{=(TnP)(m()TE){} -

(Tn) - (T ) | 3 } + P (cm)E{ (Tn) - (T ) | cm}

m

md .

 E{ (Tn) - (T)|m} + Given S  m, we know

(Tn)  ^(Tn) + min f(Tn,S,R,3m- )

RTn

^(Tn) + min (Tn, S, R) + RTn = 4d log(m) n

d

 ^(T) + min (T,S,R) + RT 4d log(m) , n

where the last inequality comes from the definition of Tn. From Chernoff's inequality, we know P{^(T)  (T) + t}  e-

bound, and the fact E{Z}  7.1 Proof of Theorem 2  0

2nt2

P{Z > t} dt, the theorem is proved.

. By applying this 2

By Theorem 1, it suffices to find a tree T  T0 such that

E min (T , S, R)

RT 

+ ( (T ) - ) = O  1 d log n n

.

Define T to be the tree obtained by pruning back T0 at every node (thought of as a region of space) that does not intersect the Bayes decision boundary. It can be shown without

much difficulty that (T) - = O((log n/n)1

 /d ) [9, Lemma 1]. It remains to bound the

nv Now

estimation error. Recall that T0 (and hence T) has depth Jd, where J = log2(m). Define R to be the pruned subtree of T consisting of all nodes in T up to depth j0d, where j0 = J - ) (1/d) log2(Jnp(truncated if necessary). Let v be the set of all training samples such that

v  L(R). 2 v . Let  be the set of all training samples S such that S  v for all

E min (T , S, R)

RT 

 P()E min (T , S, R)|

RT 

+ P(c)E min (T , S, R)|c

RT 

.


It can be shown, by applying the union bound, A2, and a theorem of Okamoto [10], that be O(1) by considering the root fragment consisting of only the root node. Hence it remains to bound the first term on the right-hand side. We use P()  1, and focus on bounding).the suffices to bound the first term of (T, S, R), which clearly dominates the second term. The first term, consisting of a sum of terms over the leaf nodes of R, is dominated by the sum of those terms over the leaf nodes of R at depth j0d. The number of such nodes may be bounded by assumption A2. The remaining expression is bounded using assumptions A1 and A2, as well as the definitions of T, R, and . 7.2 Proof of Theorem 3

P(c) = O((log n/n)1 /d ). Moreover, the second expectation on the right is easily seen to

expectation. It can be shown, assuming S  , that (T, S, R) = O((log n/n)1 /d It

unchanged. The only significant change is in the analysis of the approximation error. The The estimation error is increased by a constant  Vd,r, so its asymptotic analysis remains tree T is defined as in the previous proof. Recall the leaf nodes of T at maximum depth are cells of side length 1/m. By a simple Taylor series argument, the approximation error proof of Theorem 2. (T ) - Acknowledgments This work was partially supported by the National Science Foundation, grant nos. MIP­ 9701692 and ANI-0099148, the Army Research Office, grant no. DAAD19-99-1-0349, and the Office of Naval Research, grant no. N00014-00-1-0390. References [1] C. Scott and R. Nowak, "Dyadic classification trees via structural risk minimization," in Advances in Neural Information Processing Systems 14, S. Becker, S. Thrun, and K. Obermayer, Eds., Cambridge, MA, 2002, MIT Press. [2] E. Mammen and A. B. Tsybakov, "Smooth discrimination analysis," Annals of Statistics, vol. 27, pp. 1808­1829, 1999. [3] M. Kearns and Y. Mansour, "A fast, bottom-up decision tree pruning algorithm with nearoptimal generalization," in International Conference on Machine Learning, 1998, pp. 269­277. [4] Y. Mansour and D. McAllester, "Generalization bounds for decision trees," in Proceedings of the Thirteenth Annual Conference on Computational Learning Theory, Palo Alto, California, Nicol`o Cesa-Bianchi and Sally A. Goldman, Eds., 2000, pp. 69­74. [5] K. Bennett and J. Blue, "A support vector machine approach to decision trees," in Proceedings of the IEEE International Joint Conference on Neural Networks, Anchorage, Alaska, 1998, vol. 41, pp. 2396­2401. [6] K. Bennett, N. Cristianini, J. Shawe-Taylor, and D. Wu, "Enlarging the margins in perceptron decision trees," Machine Learning, vol. 41, pp. 295­313, 2000. [7] C. Scott, "Tree pruning using a non-additive penalty," Tech. Rep. TREE 0301, Rice University, 2003, available at http://www.dsp.rice.edu/cscott/pubs.html. [8] A. Barron, "Complexity regularization with application to artificial neural networks," in Nonparametric functional estimation and related topics, G. Roussas, Ed., pp. 561­576. NATO ASI series, Kluwer Academic Publishers, Dordrecht, 1991. [9] C. Scott and R. Nowak, "Complexity-regularized dyadic classification trees: Efficient pruning and rates of convergence," Tech. Rep. TREE0201, Rice University, 2002, available at http://www.dsp.rice.edu/cscott/pubs.html. [10] M. Okamoto, "Some inequalities relating to the partial sum of binomial probabilites," Annals of the Institute of Statistical Mathematics, vol. 10, pp. 29­35, 1958.

 behaves like m- . The remainder of the proof is essentially the same as the 


