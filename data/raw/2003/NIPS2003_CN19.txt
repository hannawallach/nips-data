Linear Program Approximations for Factored Continuous-State Markov Decision Processes

Milos Hauskrecht and Branislav Kveton Department of Computer Science and Intelligent Systems Program

 

University of Pittsburgh milos,bkveton¡ @cs.pitt.edu

Abstract Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with finite state spaces. In this work we show that ALP solutions are not limited only to MDPs with finite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. The point is supported by experiments on three CMDP problems with 24-25 continuous state factors. 1 Introduction Markov decisionprocesses (MDPs) offer an elegant mathematical frameworkfor representing and solving decision problems in the presence of uncertainty. While standard solution techniques, such as value and policy iteration, scale-up well in terms of the number of states, the state space of more realistic MDP problems is factorized and thus becomes exponentialin the number of state components. Much of the recent workin the AI community has focused on factored structured representations of finite-state MDPs and their efficient solutions. Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with discrete state components. The approach uses a linear combination of local feature functions to model the value function. The coefficients of the model are fit using linear program methods. A number of refinements of the ALP approach have been developed over past few years. These include the work by Guestrin et al [8], de Farias and Van Roy [6, 5], Schuurmans and Patrascu [15], and others [11]. In this work we show how the same set of linear programming (LP) methods can be extended also to solutions of factored continuous-state MDPs.1 The optimal solution of the continuous-state MDP (CMDP) may not (and typically does not) have a finite support. To address this problem, CMDPs and their solutions are usually approximated and solved either through state space discretization or by fitting a surrogate and (often much simpler) parametric value function model. The two methods come with

different advantages and limitations.

1 2

2 The disadvantage of discretizations is their accu-

We assumethat action spacesstay finite. Rust [14] calls suchmodels discrete decisionprocesses. The two methods are described in more depth in Section 3.


racy and the fact that higher accuracy solutions are paid for by the exponential increase in the complexity of discretizations. On the other hand, parametric value-function approximations may become unstable when combined with the dynamic programming methods and least squares error [1]. The ALP solution that is developed in this work eliminates the disadvantages of discretization and function approximation approaches while preserving their good properties. It extends the approach of Trick and Zin [17] to factored multidimensional continuous state spaces. Its main benefits are good runningtime performance, stability of the solution, and good quality policies. Factored models offer a more natural and compact way of parameterizing complex decision processes. However, not all CMDP models and related factorizations are equally suitable also for the purpose of optimization. In this work we study factored CMDPs with state by an ALP with infinite number of constraints that decompose locally. In addition, we show that by choosing transition models based on beta densities (or their mixtures) and basis functions defined by products of polynomials one obtains an ALP in which both the objective function and constraints are in closed form. In order to alleviate the problem of infinite number of constraints, we develop and study approximation based on constraint sampling [5, 6]. We show that even under a relatively simple random constraint sampling we are able to very quicklycalculate solutionsof a highqualitythat are comparable to other existing CMDP solution methods. The text of the paper is organized as follows. First we review finite-state MDPs and approximate linear programming (ALP) methods developed for their factored refinements. Next we show how to extend the LP approximations to factored continuous-state MDPs and discuss assumptions underlying the model. Finally, we test the new method on a continuousstate version of the computer network problem [8, 15] and compare its performance to alternative CMDP methods. 2 Finite-state MDPs

spaces restricted to ¢¤£¦¥¨§© . We show that the solutionfor such a model can be approximated

A finite state MDP defines a stochastic control process with components

where



is a finite set of states,



is a finite set of actions,

¥¥ !¥ "$# ,

&%')(01(2435¢ £6¥7§8©

defines a probabilistic transition model mapping a state to the next states given an action, Given an MDP our objective is to find the policy maximizing the infinite-

and "9%@A(BC3 IR defines a reward model for choosing an action in a specific state.

WTc

DFEG%HI3P

#

, where

aed

horizon discounted reward criterion:

c

and

W

QRTSVUWX`Yba

W

¢¤£6¥7§f#

is a discount factor

is a reward obtained in step . The value of the optimal policy satisfies the Bellman

g

fixed point equation [12]:

h

ib#qpsrBtvu w

x

h

"yi¥# av Ri i¥# i#R¥ (1)

where

i d 

h

is the value of the optimal policy and h h

the equation can be written as

i



denotes the next state. For all states

the value function , the optimal policy

h

pV D E ib#

, where



is the Bellman operator. Given

is defined by the action optimizing Eqn 1.

Methods for solving an MDP include value iteration, policy iteration, and linear programming [12, 2]. In the linear program (LP) formulation we solve the following problem:

minimize subject to:

where values of h ib#

h  

ib# (2)

h h

ib#F a   Ri i¥# i#F0"yi¥6#edf£¦¥ ghi¥

for every state are treated as variables. i


Factorizations and LP approximations In factored MDPs, the state space

 

Bif¥Rjf¥fklkfk¥ ¡



is defined in terms of state variables

. As a result, the state space becomes exponential in the number of

variables. Compact parameterizations of MDPs based on dynamic belief networks [7] and decomposable reward functionsare routinelyused torepresent such MDPs more efficiently. However, the presence of a compact model does not implythe existence of efficient optimal solutions. To address this problem Koller and Parr [9] and Guestrin at al [8] propose to use

a linear model [13]:

m m

ib#!p  W W i W #



WCn

. Here to approximate the value function

m h

ib# W

are the linear coefficients to be found (fit)

and s denote feature functions defined over subsets

W

n

i W

of state variables.

Given a factored binary-state MDP, the coefficients of the linear model can be found by solving the surrogate of the LP in Equation 2 [8]:

psft¨uv w¨xyv p|}~p

 minimizeo `prq (3)

w¨x{z

p p |} p  |}~pT } p @T  p |}`p 2|} y 2 ¨ } 

subject to:  p q   z w x z

where

i W w

are the parents of state variables in

X

i

"  w  i  w  ¥#

, such that

"  w  i  w 

i W under action , and

 "yi¥#

decom-

poses to

S ¥6#

is a local reward function defined

over a subset of state variables. Note that while the objective function can be computed efficiently, the number of constraints one has to satisfy remains exponential in the number of random variables. However, only a subset of these constraints becomes active and affect the solution. Guestrin et al [8] showed how to find active constraints by solving a cost network problem. Unfortunately, the cost network formulation is NP-hard. An alternative approach for finding active constraints was devised by Schuurmans and Patrascu [15]. The approach implements a constraint generation method [17] and appears to give a very good performance on average. The idea is to greedily search for maximally violated constraints which can be done efficiently by solving a linear optimization problem. These constraints are included in the linear program and the process is repeated until no violated constraints are found. De Farias and Van Roy [5] analyzed a Monte Carlo approach with randomly sampled constraints. 3 Factored continuous-state MDPs Many stochastic controlled processes are more naturally defined using continuous state variables. In this work we focus on continuous-state MDPs (CMDPs) where state spaces

are restricted to

3

¢¤£6¥7§8©

. We assume factored representations where transition probabil-

ities are defined in terms of densities over state variable subspaces:



¢¤£¦¥¨§©

lX 

i

F    i¥#

where

i 

and

i

bi



 i¥#p

denote the current and previous states. Rewards are rep-

resented compactly over subsets of state variables, similarly to factored finite-state MDPs. 3.1 Solving continuous-state MDP The optimal value function for a continuous-state MDP satisfies the Bellman fixed point equation:

h v h

ib#'prt@u w¢¡ "yi¥# a¤£

Fi   i¥# t i  #¥¦i §¦ k

 ©«ª t

3 We note that in general any bounded subspaceof IR can be transformed to ¨ .


The problem with CMDPs is that in most cases the optimal value function does not have a finite support and cannot be computed. The solutions attempt to replace the value function or the optimal policy with a finite approximation. Grid-based MDP (GMDP) discretizations. A typical solution is to discretize the state space to a set of grid points and approximate value functions over such points. Unfortunately, classic grid algorithms scale up exponentially with the number of state variables

[4]. Let

  i j

¬p i ¥i ¥lkfkfk¥i®¡

be a set of grid points over the state space

¢¤£¦¥¨§© 

. Then

the Bellman operator



can be approximated with an operator

G¯

that is restricted to grid

points . One such operator has been studied by Rust [14] and is defined as:

¬



h W W ®  W h  

¯i #'psrBtvu w "yi ¥6#` a  lX H¯i  i ¥# ¯i # ¥ (4)

i

where

that

±

 W W  W

w

 ¯ i W i #  i ¥#°p9±

w

i #bi  i ¥# defines a normalized transition probability such

is a normalizing constant. Equationh 4 appliedh to,grid points

¬

defines a fi-

nite state MDP with

 ¬B

states. The solution,

¯hp²¯ ¯

approximates the original

continuous-state MDP. Convergence properties of the approximation scheme in Equation 4 for random or pseudo-random samples were analyzed by Rust [14]. Parametric function approximations. An alternative way to solve a continuous-state function model [3]. The parameters of the model are fitted iteratively by applying one step Bellman backups to a finite set of state points arranged on a fixed grid or obtained through Monte Carlo sampling. Least squares criterion is used to fit the parameters of the model. In addition to parallel updates and optimizations, on-line update schemes based on gradient descent [3, 16] are very popular and can be used to optimize the parameters. The disadvantage of the methods is their instabilityand possible divergence [1]. 3.2 LP approximations of CMDPs Our objective is to develop an alternative to the above solutions that is based on ALP techniques and that takes advantage of model factorizations. It is easy to see that for a general continuous-state model the exact solutioncannot be formulated as a linear program as was done in Equation 2 since the number of states is infinite. However, using linear representations of the value functions we need to optimize only over a finite number of weights combining feature functions. So adopting the ALP approach from factored MDPs (Section 2), the CMDP problem can be formulated as:

MDP is to approximate the optimal value function h ib# with an appropriate parametric

p p |} p ´¨} p

£ w³x minimizeo  p q

z

p p|}~p

subject to:  pfq £  w xµ¶s·

¸ ¹»º



|½   } ¾   T ¿À py|} p ´¨} p  |}  2 R } «

¾

z w x¼ z

The above formulation of the ALP builds upon our observation that linear models in com(or any bounded space) and nicely decompose along state-variable subsets defining feature functions similarly to Equation 3. This simplification is a consequence of the following variable elimination transformation:

bination with factored transitionsare well-behaved when integrated over ¢¤£¦¥¨§©T state space

i m m i m

£ £Ã

YÂÁ Äv#T¥Ä³Å¥Æp £¦Ã £¦Ã ¡ Äv#¥6Ä³ÅÇÆ7¦ p  Ä@#T¥Ä¦k

Á Y

Despite the decomposition, the ALP formulation of the factored CMDP comes with two concerns. First, the integrals may be improper and not computable. Second, we need to


satisfy infinite number of constraints (for all values of solutions to both issues. i and ). In the following we give 

Closed form solutions Integrals in the objective function and constraints depend on the choice of transition models and basis functions. We want all these integrals to be proper Riemannian integrals. We prefer integrals with closed-form expressions. To this point, we have identified conjugate classes of transition models and basis functions leading to closed form expressions.

Beta transitions. To parameterize the transition model over densities or their mixtures. The beta transition is defined as:

F È  i  w ¥#'prÉÊ»Ë È  Ì  i w i  w #¥«Ì j 

w

¢¤£6¥7§8© we propose to use beta

i  w #«#¥

where

for i 

i

 d w

is the parent set of a variable

¹Ð Ñ

¢¤£¦¥¨§©TÏ Ï

  under action , and  Ì i  w

i  w #8¥Ì j  w

i  w #ÎÍC£

w define the parameters of the beta model.

Feature functions. A feature function form that is particularly suitable for the ALP and matches beta transitions is a product of power functions:

m x

¹ Ð W

i W #'p 

· Ò ¹Ó x    k

It is easy to show that for such a case the integrals in the objective function simplify to:

m x x

£  ¹Ð ¹Ð

x W i W #T¥¦i W §

p £ 

x   £ 

· Ò ¹»Ó x    ¥Ôi W p

· Ò ¹Ó x    ¥  p k

Ò ¹ · Ò ¹Ó x Õ  W V§

Similarly, using our conjugate transition and basis models the integrals in constraints simplify to:

|Ø³Ù

|Ø ¾

   

  ~ÚØ7Û

  ÚGØ ¾

    T |Ø³Ù     TÚÝÜ

|Ø Ù   |}

 ¨T ¿À p |} p ´¨} p~Ö ¾ ¾ ¾ 

Ù

|} |}

Û

|} |}

¾

|}  p   p    T ¾

£ 

w

¸

· ¹»º 

|½   } ¾

¾

¸

· ¹º   

x µ¶ w x ¼ z w x× ¾ ¾ ¾ ¾

  `ÚÜ ×

¾ ¾ ¾

× ×

where

Þ'kß#m

is the gammafunction. For example, assuming features with products of state



i W

x

#p Ò ¹ Ó

pà

© sá

 

, the ALP formulation becomes:

v w¨xTv

variables: minimizeo W

 prq (5)

p ½ 

¾ Ø Ù

º ¸

· ¹ º   |}

Ø Ù ¾

  |}   

Û    ¨ÚGØ¾

 |} « 2  } 

subject to: |}  f  p q 

· ¸ ¹ w¨x w x ¾ ¾ ¾ ¾

ALP solution. Although the ALP uses infinitely many constraints, only a finite subset of constraints, active constraints, is necessary to define the optimal solution. Existing ALP methods for factored finite-state MDPs search for this subset more efficiently by taking advantage of local constraint decompositions and various heuristics. However, at the end these methods always rely on the fact the decompositions are defined on a finite state subspace. Unfortunately, constraints in our model decompose over smaller but still continuous subspaces, so the existing solutions for the finite-state MDPs cannot be applied directly. Sampling constraints. To avoid the problem of continuous state spaces we approximate the ALP solutionusing a finite set of constraints defined by a finite set of state space points and actions in . These state space points can be defined by regular grids on state sub-



spaces or via random sampling of states

i dÝâ

. In this work we focus on and experiment


a  j, xj = 1 a = j

5

a)},

x,

1-j

4

j

{x|

j 3

p(x'

2 model

1

Transition

xj xj xj

00

- 1 - 1 - 1 0.5 x'j = 0.0 = 0.5 = 1.0

1

8 7 6 5 4 3 2 1 00

(b)

0.5 x'j 1

(a)

Figure 1: a. Topologies of computer networks used in experiments. b. Transition densities for the th computer and different previous-state/action combinations.

ã

with the random sampling approach. For the finite state spaces such a technique has been devised and analyzed by de Farias and Van Roy [5]. We note that the blind sampling approach can be improved via various heuristics.4 However, despite many possible heuristic improvements, we believe that the crucial benefit comes from the ALP formulation that "fits" the linear model and subsequent constraint and subspace decompositions. 4 Experiments To test the ALP method we use a continuous-state modification of the computer network example proposed by Guestrin et al [8]. Figure 1a illustrates three different network structures used in experiments. Nodes in graphs represent computers. The state of a machine is represented by a number between 0 and 1 reflecting its processing capacity (the ability to process tasks). The network performance can be controlled through activities of a human operator: the operator can attend a machine (one at time) or do nothing. Thus, processing capacity of a machine fluctuates randomly and is determined by: (1) a random event (e.g., a software bug), (2) machines connected to it and (3) the presence of the operator at the machine console. The transition model represents the dynamics of the computer network. The model is factorized and defined in terms of beta densities:

there is a total of äå§ actions where ä is the number of computers in the network. The

b



 i  w ¥#ypæÉÊ»Ë 



 Ì

i 

w i  w #¥«Ì

j 

w i  w ##

, where





is the current state of the th

ã

computer, and

i vçX

w w i   w

i  w describes the previous-step state of the computers affecting . We use:

ã

for transitions

w w  i  w #pVè

Ì #HpVè`§lév  {ê@   ¨ë i

and Ì j vçX

w w i   w #'på§l£{èv

lX i w w  i  w

 {ìv   ³ë

when thehuman does notattend thecomputer, and Ì

#'pVèv£

and Ì

i lX j

when the operator is present at the computer. Figure  1b illustrates transition densities for maintain the processing ability of the network at the highest possible level over time. The

the th computer given different values of its parents ã   ¥  ³ë i¡ and actions. The goal is to

preferences are expressed in the reward function:

the server. The discount factor is a £¦k§í@ê .

j

"yi¥#'p9è@ i  S lX  j 

j , where

 i

is

To define the ALP approximation, we used a linear combination of linear (for every node) and quadratic (for every link) feature functions. To demonstrate the practical benefit of the approach we have compared it to the grid-based approximation (Equation 4) and leastsquare value iteration approach (with the same linear value function model as in the ALP). The constraints in the ALP were sampled randomly. To make the comparison fair the same sets of samples were shared by all three methods. The full comparison study was run on

4 Various constraint sampling heuristics are analyzed and reported in a separate work [10].


24-ring 25-star 24-ring-of-rings

145 140 135 1300 400 300 200 100 00

reward

(a)

Expected

(b) (sec)

Time

random GMDP LS ALP

155 150 145 140 500 1350 400 300 200 100 145 140 135 500 1300 400 300 200 100

500

500 Number of samples 00 500 Number of samples 00 500 Number of samples

Figure 2: (a) Average values of control policies for ALP, least-squares (LS), and grid (GMDP) approaches for different sample sizes. Random policy is used as a baseline. (b) Average running times.

problems with three network structures from Figure 1a, each with 24 or 25 nodes. Figure 2a illustrates the average quality (value) of a policy obtained by different approximation methods while varying the number of samples. The average is computed over 30 solutions obtainedfor 30 differentsample sets and 100 different(random) start states. The simulation trajectories of length50 are used. Figure 2b illustrates the scale-up potentialof the methods in terms of running times. Results are averaged over 30 solutions. Overall, the results of experiments clearly demonstrate the benefit of the ALP with "local" feature functions. For the sample size range tested, our ALP method came close to the least-squares (LS) approach in terms of the quality. Both used the same value function model and both managed to fit well the parameters, hence we got comparable quality results. However, the ALP was much better in terms of running time. Oscillations and poor convergence behavior of the iterative LS method is responsible for the difference. The ALP outperformed the grid-based approach (GMDP) in both the policy qualityand running times. The gap in the policy quality was more pronounced for smaller sample sizes. This can be explained by the ability of the model to "cover" complete state space as opposed to individual grid points. Better running times for the ALP can be explained by the fact that the number of free variables to be optimized is fixed (they are equal to weights ), while

î

in grid methods free variables correspond to grid samples and their number grows linearly. 5 Conclusions We have extended the application of linear program approximation methods and their benmodel based on beta densitiesand identifiedfeature functionsthatmatch well such a model. Our ALP solution offers numerous advantages over standard grid and function approximation approaches: (1) it takes advantage of the structure of the process; (2) it allows one to define non-linear value function models and avoids the instabilities associated with leastsquared approximations; (3) it gives a more robust solution for small sample sizes when

efits to factored MDPs with continuous states. 5 We have proposed a factored transition

5 We note that our CMDP solution pavesthe road to ALP solutions for factoredhybrid state MDPs.


compared to grid methods and provides a better way of "smoothing" value function to unseen examples; (4) its running time scales up better than grid methods. These has been demonstrated experimentally on three large problems. Many interestingissues related to thenew method remain tobe addressed. First, therandom sampling of constraints can be improved using various heuristics. We report results of some heuristic solutions in a separate work [10]. Second, we did not give any complexity bounds for the random constraint sampling approach. However, we expect that the proofs by de Farias and Van Roy [5] can be adapted to cover the CMDP case. Finally, our ALP method assumes a bounded subspace of IR . The important open question is how to extend the ALP method to IR spaces. References [1] D.P. Bertsekas. A counter-example to temporal differences learning. Neural Computation, 7:270­279, 1994. [2] D.P. Bertsekas. Dynamic programming and optimal control. Athena Scientific, 1995. [3] D.P. Bertsekas and J.N. Tsitsiklis. Neuro-dynamic Programming. Athena Sc., 1996. [4] C.S. Chow and J.N. Tsitsiklis. An optimal one-way multigrid algorithm for discretetime stochastic control. IEEE Transactions on Automatic Control, 36:898­914, 1991. [5] D. P. de Farias and B. Van Roy. On constraintsamplingfor thelinear programmingapproach to approximate dynamic programming. Mathematics of Operations Research, submitted, 2001. [6] D.P. de Farias and B. Van Roy. The Linear Programming Approach to Approximate Dynamic Programming. In Operations Research, 51:6, 2003. [7] T. Dean and K. Kanazawa. A model for reasoning about persistence and causation. ComputationalIntelligence, 5:142­150, 1989. [8] C. Guestrin, D. Koller, and R. Parr. Max-norm projectionsfor factored MDPs. In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence, pages 673­682, 2001. [9] D. Koller and R. Parr. Computing factored value functions for policies in structured MDPs. In Proceedings of the 16th International Joint Conference on Artificial Intelligence, pages 1332­1339, 1999. [10] B. Kveton and M. Hauskrecht. Heuristics refinements of approximate linear programming for factored continuous-state Markov decision processes. In 14Th International Conference on Automated Planning and Scheduling, to appear, 2004. [11] P. Poupart, C. Boutilier, R. Patrascu, and D. Schuurmans. Piecewise linear value functionapproximationfor factoredMDPs. In Proceedings ofthe EighteenthNational Conference on AI, pages 292­299, 2002. [12] M.L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley, New York, 1994. [13] B. Van Roy. Learning and value function approximation in complex decision problems. PhD thesis, Massachussetts Institute of Technology, 1998. [14] J. Rust. Using randomization to break the course of dimensionality. Econometrica, 65:487­516, 1997. [15] D. Schuurmans and R.Patrascu. Direct value-approximation for factored MDPs. In Advances in Neural Information Processing Systems 14, MIT Press, 2002. [16] R. S. Sutton and A. G. Barto. Reinforcement Learning: An introduction. 1998. [17] M. Trick and E.S Zin. A linear programming approach to solving stochastic dynamic programs, TR, 1993.


