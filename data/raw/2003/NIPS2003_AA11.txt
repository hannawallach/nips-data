Kernel Dimensionality Reduction for Supervised Learning

Kenji Fukumizu Institute of Statistical Mathematics Tokyo 106-8569 Japan fukumizu@ism.ac.jp Francis R. Bach CS Division University of California Berkeley, CA 94720, USA fbach@cs.berkeley.edu Michael I. Jordan CS Division and Statistics University of California Berkeley, CA 94720, USA jordan@cs.berkeley.edu

Abstract We propose a novel method of dimensionality reduction for supervised learning. Given a regression or classification problem in which we wish to predict a variable Y from an explanatory vector X, we treat the problem of dimensionality reduction as that of finding a low-dimensional "effective subspace" of X which retains the statistical relationship between X and Y . We show that this problem can be formulated in terms of conditional independence. To turn this formulation into an optimization problem, we characterize the notion of conditional independence using covariance operators on reproducing kernel Hilbert spaces; this allows us to derive a contrast function for estimation of the effective subspace. Unlike many conventional methods, the proposed method requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y . 1 Introduction Many statistical learning problems involve some form of dimensionality reduction. The goal may be one of feature selection, in which we aim to find linear or nonlinear combinations of the original set of variables, or one of variable selection, in which we wish to select a subset of variables from the original set. Motivations for such dimensionality reduction include providing a simplified explanation and visualization for a human, suppressing noise so as to make a better prediction or decision, or reducing the computational burden. We study dimensionality reduction for supervised learning, in which the data consists of (X, Y ) pairs, where X is an m-dimensional explanatory variable and Y is an -dimensional response. The variable Y may be either continuous or discrete. We refer to these problems generically as "regression," which indicates our focus on the conditional probability density We wish to solve a problem of feature selection in which the features are linear combinations of the components of X. In particular, we assume that there is an r-dimensional subspace S  Rm such that the following equality holds for all x and y: where S is the orthogonal projection of Rm onto S. The subspace S is called the effective subspace for regression. Based on observations of (X, Y ) pairs, we wish to re-

pY |X (y|x). Thus, our framework includes classification problems, where Y is discrete.

pY |X (y|x) = pY |SX (y|Sx), (1)


cover a matrix whose columns span S. We approach the problem within a semiparametric statistical framework--we make no assumptions regarding the conditional distribution may then proceed to build a parametric or nonparametric regression model on that subspace. Thus our approach is an explicit dimensionality reduction method for supervised learning that does not require any particular form of regression model; it can be used as a preprocessor for any supervised learner. Most conventional approaches to dimensionality reduction make specific assumptions reboth. For example, classical two-layer neural networks can be seen as attempting to estimate an effective subspace in their first layer, using a specific model for the regressor. Similar comments apply to projection pursuit regression [1] and ACE [2], which assume an additive model E[Y |X] = g1(1 X) + ··· + gK(KX). While canonical correlation

pY |SX (y|Sx) or the distribution pX(x) of X. Having found an effective subspace, we

garding the conditional distribution pY |SX (y|Sx), the marginal distribution pX(x), or

T T

analysis (CCA) and partial least squares (PLS, [3]) can be used for dimensionality reduction in regression, they make a linearity assumption and place strong restrictions on the allowed dimensionality. The line of research that is closest to our work is sliced inverse regression (SIR, [4]) and related methods including principal Hessian directions (pHd, [5]). SIR is a semiparametric method that can find effective subspaces, but only under strong assumptions of ellipticity for the marginal distribution pX(x). pHd also places strong restrictions on pX(x). If these assumptions do not hold, there is no guarantee of finding the effective subspace. In this paper we present a novel semiparametric method for dimensionality reduction that we refer to as Kernel Dimensionality Reduction (KDR). KDR is based on a particular class of operators on reproducing kernel Hilbert spaces (RKHS, [6]). In distinction to algorithms such as the support vector machine and kernel PCA [7, 8], KDR cannot be viewed as a "kernelization" of an underlying linear algorithm. Rather, we relate dimensionality reduction to conditional independence of variables, and use RKHSs to provide characterizations of conditional independence and thereby design objective functions for optimization. This builds on the earlier work of [9], who used RKHSs to characterize marginal independence of variables. Our characterization of conditional independence is a significant extension, requiring rather different mathematical tools--the covariance operators on RKHSs that we present in Section 2.2. 2 Kernel method of dimensionality reduction for regression 2.1 Dimensionality reduction and conditional independence The problem discussed in this paper is to find the effective subspace S defined by Eq. (1), . The crux of the problem is that we have no a priori

given an i.i.d. sample {(Xi, Yi)}ni a marginal probability pX for X=1,

sampled from the conditional probability Eq. (1) and

knowledge of the regressor, and place no assumptions on the conditional probability pY or the marginal probability pX.

|X

We do not address the problem of choosing the dimensionality r in this paper--in practical applications of KDR any of a variety of model selection methods such as cross-validation can be reasonably considered. Rather our focus is on the problem of finding the effective subspace for a given choice of dimensionality. The notion of effective subspace can be formulated in terms of conditional independence. Let Q = (B, C) be an m-dimensional orthogonal matrix such that the column vectors of B span the subspace S (thus B is m × r and C is m × (m - r)), and define U = BT X and V = CT X. Because Q is an orthogonal matrix, we have pX(x) = pU,V (u, v) and pX,Y (x, y) = pU,V,Y (u, v, y). Thus, Eq. (1) is equivalent to

pY |U,V (y|u, v) = pY |U (y|u). (2)


Y Y

Y

V U

V | U X = (U,V)

X X

Figure 1: Graphical representation of dimensionality reduction for regression.

This shows that the effective subspace S is the one which makes Y and V conditionally independent given U (see Figure 1). Mutual information provides another viewpoint on the equivalence between conditional independence and the effective subspace. It is well known that I(Y, X) = I(Y, U) + EU I(Y |U, V |U) , (3) where I(Z, W) is the mutual information between Z and W. Because Eq. (1) implies I(Y, X) = I(Y, U), the effective subspace S is characterized as the subspace which retains the entire mutual information between X and Y , or equivalently, such that I(Y |U, V |U) = 0. This is again the conditional independence of Y and V given U. 2.2 Covariance operators on kernel Hilbert spaces and conditional independence We use cross-covariance operators [10] on RKHSs to characterize the conditional independence of random variables. Let (H, k) be a (real) reproducing kernel Hilbert space of functions on a set  with a positive definite kernel k :  ×   R and an inner product ·,· . The most important aspect of a RKHS is the reproducing property:

H

f, k(·, x) H = f(x) for all x   and f  H. (4)

In this paper we focus on the Gaussian kernel k(x1, x2) = exp - x1 - x2 2 /22 .

Let (H1, k1) and (H2, k2) be RKHSs over measurable spaces (1, B1) and (2, B2), respectively, with k1 and k2 measurable. For a random vector (X, Y ) on 1 × 2, the

cross-covariance operator Y

g, Y X f H2

X

from H1 to H2 is defined by the relation

= EXY [f(X)g(Y )] - EX[f(X)]EY [g(Y )] (= Cov[f(X), g(Y )]) (5)

for all f  H1 and g  H2. Eq. (5) implies that the covariance of f(X) and g(Y ) is given EX[k1(X, X)] and EY [k2(Y, Y )] are finite, by using Riesz's representation theorem, it is

by the action of the linear operator Y X and the inner product. Under the assumption that

not difficult to see that a bounded operator Y

Y

X

X is uniquely defined by Eq. (5). We have

= XY , where A denotes the adjoint of A. From Eq. (5), we see that Y

X captures

all of the nonlinear correlations defined by the functions in HX and HY .

Cross-covariance operators provide a useful framework for discussing conditional probability and conditional independence, as shown by the following theorem and its corollary1: Theorem 1. Let (H1, k1) and (H2, k2) be RKHSs on measurable spaces 1 and 2, respectively, with k1 and k2 measurable, and (X, Y ) be a random vector on 1×2. Assume that EX[k1(X, X)] and EY [k2(Y, Y )] are finite, and for all g  H2 the conditional expec-

tation EY

|X

[g(Y ) | X = ·] is an element of H1. Then, for all g  H2 we have

XXEY |X [g(Y ) | X = ·] = XY g. (6)

1 Full proofs of all theorems can be found in [11].


Corollary 2. Let XX be the right inverse of XX on (KerXX). Under the same assumptions as Theorem 1, we have, for all f  (KerXX) and g  H2,

f, XXXY g ~- 1 H1 = f, EY |X [g(Y ) | X = ·] H1 . (7)

~- 1

Sketch of the proof. XY can be decomposed as XY = 1XXV 1Y

/2 /2

Y

for a bounded oper-

ator V (Theorem 1, [10]). Thus, we see XXXY is well-defined, because RangeXY  RangeXX = (KerXX). Then, Eq. (7) is a direct consequence of Theorem 1. Given that XX is invertible, Eq. (7) implies

EY |X [g(Y ) | X = ·] = - XY g XX 1 for all g  H2. (8)

~- 1

This can be understood by analogy to the conditional expectation of Gaussian random variables. If X and Y are Gaussian random variables, it is well-known that the conditional where XX and XY are the variance-covariance matrices in the ordinary sense. Using cross-covariance operators, we derive an objective function for characterizing conditional independence. Let (H1, k1) and (H2, k2) be RKHSs on measurable spaces 1 and 2, respectively, with k1 and k2 measurable, and suppose we have random variables

expectation is given by EY |X [aT Y | X = x] = xT - XY a for an arbitrary vector a, XX 1

U  H1 and Y  H2. We define the conditional covariance operator Y

Y ~- 1 Y |U := Y Y - Y U UU UY .

Y |U

on H1 by (9)

Corollary 2 easily yields the following result on the conditional covariance of variables: Theorem 3. Assume that EX[k1(X, X)] and EY [k2(Y, Y )] are finite, and that

EY

|X

[f(Y )|X] is an element of H1 for all f  H2. Then, for all f, g  H2, we have

g, Y Y |U f H2 = EY [f(Y )g(Y )] - EU EY

= EU CovY |U

|U [f(Y )|U]EY |U [g(Y )|U]

f(Y ), g(Y ) | U . (10)

As in the case of Eq. (8), Eqs. (9) and (10) can be viewed as the analogs of the well-known

equality for Gaussian variables: Cov[aT Y, bT Y |U] = aT (Y

From Theorem 3, it is natural to use minimization of Y

Y

- Y

U

- UY )b.

UU

1

Y |U

as a basis of a method for

finding the most informative U, which gives the least VarY

|U

[f(Y )|U]. The following

definition is needed to justify this intuition. Let (, B) be a measurable space, let (H, k) be a RKHS over  with k measurable and bounded, and let M be the set of all the probability measures on (, B). The RKHS H is called probability-determining, if the map is one-to-one, where H is the dual space of H. The following theorem can be proved using a argument similar to that used in the proof of Theorem 2 in [9]. Theorem 4. For an arbitrary  > 0, the RKHS with Gaussian kernel k(x, y) = exp(- x- Recall that for two RKHSs H1 and H2 on 1 and 2, respectively, the direct product H1H2 istheRKHSon1×2 withthekernelk1k2 [6]. Therelationbetweenconditional independence and the conditional covariance operator is given by the following theorem: Theorem 5. Let (H11, k11), (H12, k12), and (H2, k2) be RKHSs on measurable spaces 11, 12, and 2, respectively, with continuous and bounded kernels. Let (X, Y ) = (U, V, Y ) be a random vector on 11 × 12 × 2, where X = (U, V ), and let H1 =

M P  (f  EX P [f(X)])  H (11)

y 2 /22) on Rm is probability-determining.

H11  H12 be the direct product. It is assumed that EY

EY

|X

[g(Y )|X = ·]  H1 for all g  H2. Then, we have

Y Y |U  Y Y |X ,

|U

[g(Y )|U = ·]  H11 and (12)


where the inequality refers to the order of self-adjoint operators. If further H2 is

probability-determining, in particular, for Gaussian kernels, we have the equivalence:

Y Y |X = Y Y |U  Y V | U.

Sketch of the proof. Taking the expectation of the well-known equality VarY

EV

|U VarY |U,V

[g(Y )|U, V ] + VarV

|U

EY

|U,V

[g(Y )|U, V ] |U

(13) [g(Y )|U] =

with respect to U, we ob-

tain EU VarY

|U

[g(Y )|U] -EX VarY

|X

[g(Y )|X] = EU VarV |U [EY |X [g(Y )|X]]  0,

which implies Eq. (12). The equality holds iff EY

|X

[g(Y )|X] = EY

|X

= PY

|U

|U

[g(Y )|U] for a.e. X.

Since H2 is probability-determining, this means PY , that is, Y V | U.

From Theorem 5, for probability-determining kernel spaces, the effective subspace S can be characterized in terms of the solution to the following minimization problem: min Y U = SX. (14) 2.3 Kernel generalized variance for dimensionality reduction To derive a sampled-based objective function from Eq. (14) for a finite sample, we have to estimate the conditional covariance operator with given data, and choose a specific way to evaluate the size of self-adjoint operators. Hereafter, we consider only Gaussian kernels, which are appropriate for both continuous and discrete variables. For the estimation of the operator, we follow the procedure in [9] (see also [11] for further details), and use the centralized Gram matrix [9, 8], which is defined as:

S Y |U , subject to

KY = In- n1n1Tn GY In- n1n1Tn , ^ 1 1 KU = In- n1n1Tn GU In- n1n1Tn ^ 1 1 (15)

where 1n = (1, . . . , 1)T , (GY )ij = k1(Yi, Yj) is the Gram matrix of the samples of Y , and (GU)ij = k2(Ui, Uj) is given by the projection Ui = BT Xi. With a regularization

constant  > 0, the empirical conditional covariance matrix Y ^

Y ^ ^- ^ 1 ^ ^ ^ Y |U ^ := Y Y ^ -Y U

Y |U is then defined by

UU UY = (KY +In)2 -KY KU (KU +In)- KU KY . (16) ^ ^ ^ 2

The size of Y ^ Y |U in the ordered set of positive definite matrices can be evaluated by its

determinant. Although there are other choices for measuring the size of Y ^ Y |U

, such as

the trace and the largest eigenvalue, we focus on the determinant in this paper. Using the

Schur decomposition, det(A - BC- BT ) = det

det Y ^ Y |U ^ = det [

1 A B

BT C

/detC, we have ^

Y U][Y U]

^

/ det UU, (17)

. Y U][Y U] ^ is defined by [ = Y Y Y

^

where [ ^ Y U][Y U] U

UY UU ^ ^

= ^ (KY +In)2 KU KY ^ ^ KY KU (KU +In)2 ^ ^ ^

We symmetrize the objective function by dividing by the constant det Y ^

det [ ^

det Y ^

min BRm ×r

Y U][Y U]

, where U = BT X.

Y

, which yields (18)

Y ^ det UU

We refer to this minimization problem with respect to the choice of subspace S or matrix B as Kernel Dimensionality Reduction (KDR). Eq. (18) has been termed the "kernel generalized variance" (KGV) by Bach and Jordan [9]. They used it as a contrast function for independent component analysis, in which the goal is to minimize a mutual information. They showed that KGV is in fact an approximation of the mutual information among the recovered sources around the factorized distributions. In the current setting, on the other hand, our goal is to maximize the mutual information


R(b1) R(b2) SIR(10) 0.987 0.421 SIR(15) 0.993 0.705 SIR(20) 0.988 0.480 SIR(25) 0.990 0.526 pHd 0.110 0.859 KDR 0.999 0.984

Table 1: Correlation coefficients. SIR(m) indicates the SIR method with m slices.

I(Y, U), and with an entirely different argument, we have shown that KGV is an appropriate objective function for the dimensionality reduction problem, and that minimizing Eq. (18) can be viewed as maximizing the mutual information I(Y, U). Given that the numerical task that must be solved in KDR is the same as the one to be solved in kernel ICA, we can import all of the computational techniques developed in [9] for minimizing KGV. In particular, the optimization routine that we use is gradient descent with a line search, where we exploit incomplete Cholesky decomposition to reduce the n × n matrices to low-rank approximations. To cope with local optima, we make use of an annealing technique, in which the scale parameter  for the Gaussian kernel is decreased gradually during the iterations of optimization. For a larger , the contrast function has fewer local optima, and the search becomes more accurate as  is decreased.

3 Experimental results We illustrate the effectiveness of the proposed KDR method through experiments, comparing it with several conventional methods: SIR, pHd, CCA, and PLS. The first data set is a synthesized one with 300 samples of 17 dimensional X and one dimensional Y , which are generated by Y  0.9X1 + 0.2/(1 + X17) + Z, where Z  N(0, 0.012) and X follows a uniform distribution on [0, 1]17. The effective subspace is given by b1 = (1, 0, . . . , 0) and b2 = (0, . . . , 0, 1). We compare the KDR method with SIR and pHd only--CCA and PLS cannot find a 2-dimensional subspace, because Y is onedimensional. To evaluate estimation accuracy, we use the multiple correlation coefficient in Table 1, KDR outperforms the others in finding the weak contribution of b2. Next, we apply the KDR method to classification problems, for which many conventional methods of dimensionality reduction are not suitable. In particular, SIR requires the dimensionality of the effective subspace to be less than the number of classes, because SIR uses the average of X in slices along the variable Y . CCA and PLS have a similar limitation on the dimensionality of the effective subspace. Thus we compare the result of KDR only with pHd, which is applicable to general binary classification problems. We show the visualization capability of the dimensionality reduction methods for the Wine dataset from the UCI repository to see how the projection onto a low-dimensional space realizes an effective description of data. The Wine data consists of 178 samples with 13 variables and a label with three classes. Figure 2 shows the projection onto the 2-dimensional subspace estimated by each method. KDR separates the data into three classes most completely. We can see that the data are nonlinearly separable in the two-dimensional space. In the third experiment, we investigate how much information on the classification is preserved in the estimated subspace. After reducing the dimensionality, we use the support vector machine (SVM) method to build a classifier in the reduced space, and compare its accuracy with an SVM trained using the full-dimensional vector X. We use three data sets from the UCI repository. Figure 3 shows the classification rates for the test set for subspaces of various dimensionality. We can see that KDR yields good classification even in low-dimensional subspaces, while pHd is much worse in small dimensionality. It is noteworthy that in the Ionosphere data set the classifier in dimensions 5, 10, and 20 outperforms

R(b) = max T XXb/(T XX · bT XXb)1 S /2 , which is used in [4]. As shown


20 20 20

KDR CCA PLS

15 15 15

10 10 10

5 5 5

0 0 0

-5 -5 -5

-10 -10 -10

-15 -15 -15

-20 -20 -20

-20 -15 -10 -5 0 5 10 15 20 -20 -15 -10 -5 0 5 10 15 20 -20 -15 -10 -5 0 5 10 15 20

20 20

SIR pHd

15 15

10 10

5 5

0 0

-5 -5

-10 -10

-15 -15

-20 -20

-20 -15 -10 -5 0 5 10 15 20 -20 -15 -10 -5 0 5 10 15 20

Figure 2: Projections of Wine data: "+", "·", and gray " " represent the three classes.

the classifier in the full-dimensional space. This is caused by suppressing noise irrelevant to explain Y . These results show that KDR successfully finds an effective subspace which preserves the class information even when the dimensionality is reduced significantly. 4 Extension to variable selection The KDR method can be extended to variable selection, in which a subset of given explanatory variables {X1, . . . , Xm} is selected. Extension of the KGV objective function to variable selection is straightforward. We have only to compare the KGV values for all the subspaces spanned by combinations of a fixed number of selected variables. We of course do not avoid the combinatorial problem of variable selection; the total number of combinations may be intractably large for a large number of explanatory variables m, and greedy or random search procedures are needed. We first apply this kernel method to the Boston Housing data (506 samples with 13 dimensional X), which has been used as a typical example of variable selection. We select four variables that attain the smallest KGV value among all the combinations. The selected variables are exactly the same as the ones selected by ACE [2]. Next, we apply the method to the leukemia microarray data of 7129 dimensions ([12]). We select 50 effective genes to classify two types of leukemia using 38 training samples. For optimization of the KGV value, we use a greedy algorithm, in which new variables are selected one by one, and subsequently a variant of genetic algorithm is used. Half of the 50 genes accord with 50 genes selected by [12]. With the genes selected by our method, the same classifier as that used in [12] classifies correctly 32 of the 34 test samples, for which, with their 50 genes, Golub et al. ([12]) report a result of classifying 29 of the 34 samples correctly. 5 Conclusion We have presented KDR, a novel method of dimensionality reduction for supervised learning. One of the striking properties of this method is its generality. We do not place any strong assumptions on either the conditional or the marginal distribution, in distinction to


(a) Heart-disease (b) Ionosphere

Kernel PHD All variables

(c) Wisconsin Breast Cancer

100

95

100

85 80 75 70 65 60 Classification 55 50

(%) 98 (%) (%)

rate 96 90 rate rate

94 85

92 80

Kernel PHD All variables

Kernel PHD All variables 7 9 11

90 75 Classification Classification

88 3 5 13 3 5 10 15 20 34 70 0 10 20 30

Dimensionality Dimensionality Dimensionality

Figure 3: Classification accuracy of the SVM for test data after dimensionality reduction.

essentially all existing methods for dimensionality reduction in regression, including SIR, pHd, CCA, and PPR. We have demonstrating promising empirical performance of KDR, showing its practical utility in data visualization and feature selection for prediction. We have also discussed an extension of KDR method to variable selection. The theoretical basis of KDR lies in the nonparametric characterization of conditional independence that we have presented in this paper. Extending earlier work on the kernel-based characterization of marginal independence [9], we have shown that conditional independence can be characterized in terms of covariance operators on a kernel Hilbert space. While our focus has been on the problem of dimensionality reduction, it is also worth noting that there are many possible other applications of this result. In particular, conditional independence plays an important role in the structural definition of graphical models, and our result may have implications for model selection and inference in graphical models. References [1] Friedman, J.H. and Stuetzle, W. Projection pursuit regression. J. Amer. Stat. Assoc., 76:817­ 823, 1981. [2] Breiman, L. and Friedman, J.H. Estimating optimal transformations for multiple regression and correlation. J. Amer. Stat. Assoc., 80:580­598, 1985. [3] Wold, H. Partial least squares. in S. Kotz and N.L. Johnson (Eds.), Encyclopedia of Statistical Sciences, Vol. 6, Wiley, New York. pp.581­591. 1985. [4] Li, K.-C. Sliced inverse regression for dimension reduction (with discussion). J. Amer. Stat. Assoc., 86:316­342, 1991. [5] Li, K.-C. On principal Hessian directions for data visualization and dimension reduction: Another application of Stein's lemma. J. Amer. Stat. Assoc., 87:1025­1039, 1992. [6] Aronszajn, N. Theory of reproducing kernels. Trans. Amer. Math. Soc., 69(3):337­404, 1950. [7] Sch¨olkopf, B., Burges, C.J.C., and Smola, A. (eds.) Advances in Kernel Methods: Support Vector Learning. MIT Press. 1999. [8] Sch¨olkopf, B., Smola, A and M¨uller, K.-R. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation, 10:1299­1319, 1998. [9] Bach, F.R. and Jordan, M.I. Kernel independent component analysis. JMLR, 3:1­48, 2002. [10] Baker, C.R. Joint measures and cross-covariance operators. Trans. Amer. Math. Soc., 186:273­ 289, 1973. [11] Fukumizu, K., Bach, F.R. and Jordan, M.I. Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces. JMLR, 5:73­99, 2004. [12] Golub T.R. et al. Molecular classification of cancer: Class discovery and class prediction by gene expression monitoring. Science, 286:531­537, 1999.


