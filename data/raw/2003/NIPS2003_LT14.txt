Large margin classifiers: convex loss, low noise, and convergence rates

Peter L. Bartlett, Michael I. Jordan and Jon D. McAuliffe Division of Computer Science and Department of Statistics University of California, Berkeley Berkeley, CA 94720 {bartlett,jordan,jon}@stat.berkeley.edu

Abstract Many classification algorithms, including the support vector machine, boosting and logistic regression, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. We characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function--that it satisfy a pointwise form of Fisher consistency for classification. The relationship is based on a variational transformation of the loss function that is easy to compute in many applications. We also present a refined version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a finite-dimensional base class. 1 Introduction Convexity has played an increasingly important role in machine learning in recent years, echoing its growing prominence throughout applied mathematics (Boyd and Vandenberghe, 2003). In particular, a wide variety of two-class classification methods choose a real-valued classifier f based on the minimization of a convex surrogate (yf(x)) in the place of an intractable loss function 1(sign(f(x)) = y). Examples of this tactic include the support vector machine, AdaBoost, and logistic regression, which are based on the exponential loss, the hinge loss and the logistic loss, respectively. What are the statistical consequences of choosing models and estimation procedures so as to exploit the computational advantages of convexity? In the setting of 0-1 loss, some basic answers have begun to emerge. In particular, it is possible to demonstrate the Bayes-risk consistency of methods based on minimizing convex surrogates for 0-1 loss, with appropriate regularization. Lugosi and Vayatis (2003) have provided such a result for any differentiable, monotone, strictly convex loss function  that satisfies (0) = 1. This handles many common cases although it does not handle the SVM. Steinwart (2002) has demonstrated consistency for the SVM as well, where F is a reproducing kernel Hilbert space and  is continuous. Other results on Bayes-risk consistency have been presented by Jiang (2003), Zhang (2003), and Mannor et al. (2002).


To carry this agenda further, it is necessary to find general quantitative relationships between the approximation and estimation errors associated with , and those associated with 0-1 loss. This point has been emphasized by Zhang (2003), who has presented several examples of such relationships. We simplify and extend Zhang's results, developing a general methodology for finding quantitative relationships between the risk associated with  and the risk associated with 0-1 loss. In particular, let R(f) denote the risk based on 0-1 loss and let R = inff R(f) denote the Bayes risk. Similarly, let us refer to R(f) = E(Y f(X)) as the "-risk," and let R = inff R(f) denote the "optimal -risk." We show that, for all measurable f,



(R(f) - R)  R(f) - R,  (1)

assessments of error in terms of "excess risk" R(f) - R. Although our principal goal is to understand the implications of convexity in classification, we do not impose a convexity assumption on  at the outset. Indeed, while conditions such as convexity, continuity, and differentiability of  are easy to verify and have natural relationships to optimization procedures, it is not immediately obvious how to relate such conditions to their statistical consequences. Thus, in Section 2 we consider the weakest possible condition on --that it is "classification-calibrated," which is essentially a pointwise form of Fisher consistency for classification. We show that minimizing -risk leads to minimal risk precisely when  is classification-calibrated. Building on (1), in Section 3 we study the low noise setting, in which the posterior probability (X) is not too close to 1/2. We show that in this setting we are able to obtain an improvement in the relationship between excess -risk and excess risk. Section 4 turns to the estimation of convergence rates for empirical -risk minimization in the low noise setting. We find that for convex  satisfying a certain uniform convexity condition, empirical -risk minimization yields convergence of misclassification risk to that of the best-performing classifier in F, and the rate of convergence can be strictly faster than the classical parametric rate of n- . 2 Relating excess risk to excess -risk There are three sources of error to be considered in a statistical analysis of classification problems: the classical estimation error due to finite sample size, the classical approximation error due to the size of the function space F, and an additional source of approximation error due to the use of a surrogate in place of the 0-1 loss function. It is this last source of error that is our focus in this section. We give estimates for this error that are valid for any measurable function. Since the error is defined in terms of the probability distribution, we work with population expectations in this section. Fix an input space X and let (X, Y ), (X1, Y1), . . . , (Xn, Yn)  X × {±1} be i.i.d., with distribution P. Define  : X  [0, 1] as (x) = P(Y = 1|X = x). Define the {0, 1}-risk, or just risk, of f as R(f) = P(sign(f(X)) = Y ), where sign() = 1 for  > 0 and -1 otherwise. Based on the sample Dn = ((X1, Y1), . . . , (Xn, Yn)), we want to choose a function fn with small risk. Define the Bayes risk R = inff R(f), where 1/2) a.s. on {(X) = 1/2} has R(f ) = R. the infimum is over all measurable f. Then any f satisfying sign(f(X)) = sign((X) -

1/2

for a nondecreasing function  : [0, 1]  [0, ), and that no better bound is possible. Moreover, we present a general variational representation of  in terms of , and show how this representation allows us to infer various properties of . This result suggests that if  is well-behaved then minimization of R(f) may provide a reasonable surrogate for minimization of R(f). Moreover, the result provides a quantitative way to transfer assessments of statistical error in terms of "excess -risk" R(f)-R into




Fix a function  :

view  as specifying acontrast function that is minimized in determining a discriminant f. Define C() = () + (1 - )(-), so that the conditional -risk at x  X is As a useful illustration for the definitions that follow, consider a singleton domain X =

 

E((Y f(X))|X = x) = C( x) (f (x)) = (x)(f (x)) + (1 - (x))(-f (x)).

{x0}. Minimizing -risk corresponds to choosing f(x0) to minimize C( For   [0, 1], define the optimal conditional -risk H() = inf C() = inf (() + (1 - )(-)). x0)

  ¡ ¡

(f (x0)).

[0, ). Define the -risk of f as R(f ) = E(Y f (X)). We can

Then the optimal -risk satisfies R := inff R(f) = EH((X)), where the infimum is over measurable functions. For   [0, 1], define

H-() = inf C() = inf

:(2-1)0 :(2-1)0

(() + (1 - )(-)).



This is the optimal value of the conditional -risk, under the constraint that the sign of the argument  disagrees with that of 2 - 1. We now turn to the basic condition we impose on . This condition generalizes the requirement that the minimizer of C() (if it exists) has the correct sign. This is a minimal condition that can be viewed as a form of Fisher consistency for classification (Lin, 2001). Definition 1. We say that  is classification-calibrated if, for any  = 1/2, H-() > H(). The following functional transform of the loss function will be useful in our main result.

Definition 2. We define the -transform of a loss function as follows. Given  : [0, ), define the function  : [0, 1]  [0, ) by  = ~, where

~() = H-

1 +  2

- H

1 +  2

,

  

   

the epigraph of g is the closure of the convex hull of the epigraph of g. (Recall that the and g : [0, 1]  is the Fenchel-Legendre biconjugate of g : [0, 1]  . Equivalently, epigraph of a function g is the set {(x, t) : x  [0, 1], g(x)  t}.) It is immediate from the definitions that ~ and  are nonnegative and that they are also continuous on [0, 1]. We calculate the -transform for exponential loss, logistic loss, quadratic loss and truncated quadratic loss, tabulating the results in Table 1. All of these loss functions can be verified to be classification-calibrated. (The other parameters listed in the table will be referred to later.) The importance of the -transform is shown by the following theorem. () ( )



e- ln(1 + e- ) (1 - )2 (max{0, 1 - })2

2

B

exponential logistic quadratic truncated quadratic

() 1 - 1 - 2   2 2 LB eB 2 2(B + 1) 2(B + 1)

e- /8

e- 2B

2 2

/4

2 2

/4 /4

Table 1: Four convex loss functions and the corresponding -transform. On the interval [-B, B], each loss function has the indicated Lipschitz constant LB and modulus of convexity ( ) with respect to d. All have a quadratic modulus of convexity.


1. For any nonnegative loss function , any measurable f : X 

Theorem 3. and any probability distribution on X × {±1},

(R(f) - R)  R(f) - R. 2. Suppose |X|  2. For any nonnegative loss function , any 

 

> 0 and any  

 

such that R(f) - R =  and ()  R(f) - R  () + . [0, 1], there is a probability distribution on X × {±1} and a function f : X  3. The following conditions are equivalent. (a)  is classification-calibrated. (b) For any sequence (i) in [0, 1], (i)  0 if and only if i  0. bility distribution on X × {±1}, R(fi)  R implies R(fi)  R. Remark: It can be shown that classification-calibration implies  is invertible on [0, 1], in which case it is meaningful to write the upper bound on excess risk as - (R(f) - R).

(c) For every sequence of measurable functions fi : X    and every proba-



1 

Remark: Zhang (2003) has given a comparison theorem like Part 1, for convex  that satisfy certain conditions. Lugosi and Vayatis (2003) and Steinwart (2002) have shown limiting results like Part 3c under other conditions on . All of these conditions are stronger than the ones we assume here. The following lemma summarizes various useful properties of H, H- and . Lemma 4. The functions H, H- and  have the following properties, for all   [0, 1]: 1. H and H- are symmetric about 1/2: H() = H(1 - ), H-() = H-(1 - ). 2. H is concave and satisfies H()  H(1/2) = H-(1/2). 3. If  is classification-calibrated, then H() < H(1/2) for  = 1/2. 4. H- is concave on [0, 1/2] and [1/2, 1], and satisfies H-()  H(). 5. H, H- and ~ are continuous on [0, 1]. 6.  is continuous on [0, 1],  is nonnegative and minimal at 0, and (0) = 0. 7.  is classification-calibrated iff () > 0 for all   (0, 1]. Proof. (Of Theorem 3). For Part 1, it is straightforward to show that R(f) - R = E (1 [sign(f(X)) = sign((X) - 1/2)] |2(X) - 1|) , where 1 [] is 1 if the predicate  is true and 0 otherwise. From the definition,  is convex, so we can apply Jensen's inequality, the fact that (0) = 0 (Lemma 4, part 6) and the fact that ()  ~(), to show that (R(f) - R)  E (1[sign(f(X)) = sign((X) - 1/2)]|2(X) - 1|) = E (1 [sign(f (X)) = sign((X) - 1/2)]  (|2(X) - 1|))  E 1 [sign(f(X)) = sign((X) - 1/2)] ~ (|2(X) - 1|) = E 1 [sign(f (X)) = sign((X) - 1/2)] H-((X)) - H((X))

= E 1 [sign(f(X)) = sign((X) - 1/2)] inf C( X) :(2(X)-1)0

 E C(

X)

(f (X)) - H((X))

() - H((X))

= R(f ) - R, 


where the last inequality used the fact that for any x, and in particular when sign(f(x)) =

sign((x) - 1/2), we have C( x) (f (x))  H((x)).

the definition of , we can choose , 1, 2  [0, 1] for which  = 1 + (1 - )2 and For Part 2, the first inequality is from Part 1. For the second, fix > 0 and   [0, 1]. From

1

that PX{x1} = , PX{x2} = 1 - , (x1) = (1 + such2that (x2) = (1 + 2)/2. ()  ~(1) + (1 - )~(2) - /2. Choose distinct x), x2  X , and choose PX such easy to verify that R(f)-R  ~(1)+(1-)~(2)+ /2  ()+ . Furthermore, since sign(f(xi)) = -1 but (xi)  1/2, we have R(f) - R = E|2(X) - 1| = . For Part 3, first note that, for any ,  is continuous on [0, 1] and (0) = 0 by Lemma 4, part 6, and hence i  0 implies (i)  0. Thus, we can replace condition (3b) by (3b') For any sequence (i) in [0, 1], (i)  0 implies i  0 . To see that (3a) implies (3b'), let  be classification-calibrated, and let (i) be a sequence that does not converge to 0. Define c = lim sup i > 0, and pass to a subsequence with lim i = c. Then lim (i) = (c) by continuity, and (c) > 0 by classification-calibration (Lemma 4, part 7). Thus, for the original sequence (i), we see lim sup (i) > 0, so we cannot have (i)  0. Part 1 implies that (3b') implies (3c). The proof that (3c) implies (3a) is straightforward; see Bartlett et al. (2003). The following observation is easy to verify. It shows that if  is convex, the classificationcalibration condition is easy to verify and the  transform is a little easier to compute. Lemma 5. Suppose  is convex. Then we have 1.  is classification-calibrated if and only if it is differentiable at 0 and  (0) < 0. 2. If  is classification-calibrated, then ~ is convex, hence  = ~. All of the classification procedures mentioned in earlier sections utilize surrogate loss functions which are either upper bounds on 0-1 loss or can be transformed into upper bounds via a positive scaling factor. It is easy to verify that this is necessary. ()  1 [  0] for all   . Lemma 6. If  :  [0,) is classification-calibrated, then there is a  > 0 such that 3 Tighter bounds under low noise conditions In a study of the convergence rate of empirical risk minimization, Tsybakov (2001) provided a useful condition on the behavior of the posterior probability near the optimal deciwe show in this section, it allows us to obtain a refinement of Theorem 3. sion boundary {x : (x) = 1/2}. Tsybakov's condition is useful in our setting as well; as Recall that

1

From the definition of H-, we can choose f : X( 

C( x1)

 

(f (x1))  H-((x1)) + /2 and C( x2) f(x2))  H-((x2)) + /2. Then it is

 

 

R(f) - R = E (1 [sign(f(X)) = sign((X) - 1/2)] |2(X) - 1|)  PX (sign(f(X)) = sign((X) - 1/2)),

(2)

/ , and f(x1)  0, f(x2)  0,

with equality provided that (X) is almost surely either 1 or 0. We say that P has noise

exponent   0 if there is a c > 0 such that every measurable f : X  has PX (sign(f(X)) = sign((X) - 1/2))  c (R(f) - R) .

 

(3)

Notice that we must have   1, in view of (2). If  = 0, this imposes no constraint on the noise: take c = 1 to see that every probability measure P satisfies (3). On the other hand, it is easy to verify that  = 1 if and only if |2(X) - 1|  1/c a.s. [PX].


Theorem 7. Suppose P has noise exponent 0 <   1, and  is classification-calibrated. Then there is a c > 0 such that for any f : X  ,

 

c (R(f) - R)  (R(f ) - R)1- 2c    R(f) - R.

Furthermore, this never gives a worse rate than the result of Theorem 3, since

(R(f ) - R)  (R(f ) - R)1- 2c    R(f) - R 2c .

The proof follows closely that of Theorem 3(1), with the modification that we approximate the error integral separately over subsets of the input space with low and high noise. 4 Estimation rates Large margin algorithms choose f^from a class F to minimize empirical -risk,

n

R(f) = E(Y f(X)) = ^ ^ 1 n (Yif(Xi)).

i=1

We have seen how the excess risk depends on the excess -risk. In this section, we examine the convergence of f^'s excess -risk, R(f^) - R. We can split this excess risk into an



estimation error term and an approximation error term: R(f^) - R = (R(f^) - inf R(f)) + (finf R(f) - R).

  f F F

We focus on the first term, the estimation error term. For simplicity, we assume throughout that some f  F achieves the infimum, R(f) = inffF R(f). ^ uniformly over F. This approach can give the wrong rate. For example,for a nontrivial The simplest way to bound R(f^) - R(f) is to show that R(f) and R(f) are close, class F, the resulting estimation error bound can decrease no faster than 1/ n. However, if as log n/n. Lee et al. (1996) showed that fast rates are also possible for the quadratic F is a small class (for instance, a VC-class) and R(f) = 0, then R(f^) should decrease loss () = (1 - )2 if F is convex, even if R(f) > 0. In particular, because the quadratic loss function is strictly convex, it is possible to bound the variance of the excess loss (difference between the loss of a function f and that of the optimal f) in terms of its expectation. Since the variance decreases as we approach the optimal f, the risk of the empirical minimizer converges more quickly to the optimal risk than the simple uniform convergence results would suggest. Mendelson (2002) improved this result, and extended it from prediction in L2(PX) to prediction in Lp(PX) for other values of p. The proof used the idea of the modulus of convexity of a norm. This idea can be used to give a simpler proof of a more general bound when the loss function satisfies a strict convexity condition, and we obtain risk bounds. The modulus of convexity of an arbitrary strictly convex function (rather than a norm) is a key notion in formulating our results. Definition 8 (Modulus of convexity). Given a pseudometric d defined on a vector space S, and a convex function f : S  , the modulus of convexity of f with respect to d is the function  : [0, )  [0, ] satisfying

 

( ) = inf f(x1) + f(x2) 2 - f x1 + x2 2 : x1, x2  S, d(x1, x2)  .

If ( ) > 0 for all > 0, we say that f is strictly convex with respect to d.


We consider loss functions  that also satisfy a Lipschitz condition with respect to a pseudometric d on : we say that  : is Lipschitz with respect to d, with constant L, then  necessarily satisfies a Lipschitz·condition on any compact subset of .) if for all a, b  , |(a) - (b)|  Ld(a, b). (Note that if d is a metric and  is convex, We consider four loss functions that satisfy these conditions: the exponential loss function used in AdaBoost, the deviance function for logistic regression, the quadratic loss function, and the truncated quadratic loss function; see Table 1. We use the pseudometric d(a, b) = inf {|a - | + | - b| :  constant on (min{, }, max{, })} . For all except the truncated quadratic loss function, this corresponds to the standard metric ignores differences to the right of 1. It is easy to calculate the Lipschitz constant and modon , d(a, b) = |a-b|. In all cases, d(a, b)  |a-b|, but for the truncated quadratic, d ulus of convexity for each of these loss functions. These parameters are given in Table 1. In the following result, we consider the function class used by algorithms such as AdaBoost: the class of linear combinations of classifiers from a fixed base class. We assume that this base class has finite Vapnik-Chervonenkis dimension, and we constrain the we write F = B absconv(G), for some constant B, where norm of the linear parameters. If G is the VC-class, B absconv(G) = igi : m  , i  , gi  G,  = B .

     

 

 

 

size of the class by restricting the 1

m

 

1  

i=1

Theorem 9. Let  : 

   

be a convex loss function. Suppose that, on the interval

[-B, B],  is Lipschitz with constant LB and has modulus of convexity ( ) = aB with respect to the pseudometric d). 2 (both

a constant c for which the following is true. For i.i.d. data (X1, Y1), . . . , (Xn, Yn), let For any probability distribution P on X × Y that has noise exponent  = 1, there is f^  F be the minimizer of the empirical -risk,( R(f) = E(Y f(X)). Suppose that ^

F = B absconv(G), where G  {±1}X has dV

C

G) = d, and

1/(d+1)



 BLB max LBaB B , 1 n- (d+2)/(2d+2)

Then with probability at least 1 - e- ,

R(f^)  R + c  +

LB(LB/aB + B)x n

x



+ inf R(f ) - R f F .

Notice that the rate obtained here is strictly faster than the classical n- 1/2 parametric rate,

even though the class is infinite dimensional and the optimal element of F can have risk larger than the Bayes risk. The key idea in the proof is similar to ideas from Lee et al. (1996), Mendelson (2002), but simpler. Let f be the minimizer of -risk in a function class F. If the class F is convex and the loss function  is strictly convex and Lipschitz, then the variance of the excess loss, gf(x, y) = (yf(x)) - (yf(x)), decreases with (Y f^(X)) and (Y f (X)) become strongly correlated. This leads to the faster rates. its expectation. Thus, as a function f  F approaches the optimum, f, the two losses

More formally, suppose that  is L-Lipschitz and has modulus of convexity ( )  c

with r  2. Then it is straightforward to show that Egf  L2 (Egf/(2c))2

2 /r

r

. For the

details, see Bartlett et al. (2003). 5 Conclusions We have studied the relationship between properties of a nonnegative margin-based loss function  and the statistical performance of the classifier which, based on an i.i.d. training


set, minimizes empirical -risk over a class of functions. We first derived a universal upper bound on the population misclassification risk of any thresholded measurable classifier in terms of its corresponding population -risk. The bound is governed by the -transform, a convexified variational transform of . It is the tightest possible upper bound uniform over all probability distributions and measurable functions in this setting. Using this upper bound, we characterized the class of loss functions which guarantee that every -risk consistent classifier sequence is also Bayes-risk consistent, under any population distribution. Here -risk consistency denotes sequential convergence of population -risks to the smallest possible -risk of any measurable classifier. The characteristic property of such a , which we term classification-calibration, is a kind of pointwise Fisher conis apparent; the sufficiency underscores its fundamental importance in elaborating the stasistency for the conditional -risk at each x  X. The necessity of classification-calibration tistical behavior of large-margin classifiers. Under the low noise assumption of Tsybakov (2001), we sharpened our original upper bound and studied the Bayes-risk consistency of f^, the minimizer of empirical -risk over a convex, bounded class of functions F which is not too complex. We found that, for convex  satisfying a certain uniform strict convexity condition, empirical -risk minimization yields convergence of misclassification risk to that of the best-performing classifier in F, as the sample size grows. Furthermore, the rate of convergence can be strictly faster than the classical n- , depending on the strictness of convexity of  and the complexity of F. Acknowledgments We would like to thank Gilles Blanchard, Olivier Bousquet, Pascal Massart, Ron Meir, Shahar Mendelson, Martin Wainwright and Bin Yu for helpful discussions. References Bartlett, P. L., Jordan, M. I., and McAuliffe, J. M. (2003). Convexity, classification and risk bounds. Technical Report 638, Dept. of Statistics, UC Berkeley. [www.stat.berkeley.edu/tech-reports]. Boyd, S. and Vandenberghe, L. (2003). Convex Optimization. [www.stanford.edu/boyd]. Jiang, W. (2003). Process consistency for Adaboost. Annals of Statistics, in press. Lee, W. S., Bartlett, P. L., and Williamson, R. C. (1996). Efficient agnostic learning of neural networks with bounded fan-in. IEEE Transactions on Information Theory, 42(6):2118­2132. Lin, Y. (2001). A note on margin-based loss functions in classification. Technical Report 1044r, Department of Statistics, University of Wisconsin. Lugosi, G. and Vayatis, N. (2003). On the Bayes risk consistency of regularized boosting methods. Annals of Statistics, in press. Mannor, S., Meir, R., and Zhang, T. (2002). The consistency of greedy algorithms for classification. In Proceedings of the Annual Conference on Computational Learning Theory, pages 319­333. Mendelson, S. (2002). Improving the sample complexity using global data. IEEE Transactions on Information Theory, 48(7):1977­1991. Steinwart, I. (2002). Consistency of support vector machines and other regularized classifiers. Technical Report 02-03, University of Jena, Department of Mathematics and Computer Science. Tsybakov, A. (2001). Optimal aggregation of classifiers in statistical learning. Technical Report PMA-682, Universite´ Paris VI. Zhang, T. (2003). Statistical behavior and consistency of classification methods based on convex risk minimization. Annals of Statistics, in press.

1/2


