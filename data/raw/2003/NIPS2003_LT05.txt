On the Dynamics of Boosting

Cynthia Rudin Ingrid Daubechies

Princeton University Progr. Appl. & Comp. Math. Fine Hall Washington Road Princeton, NJ 08544-1000 {crudin,ingrid}@math.princeton.edu

Robert E. Schapire Princeton University Department of Computer Science 35 Olden St. Princeton, NJ 08544 schapire@cs.princeton.edu

Abstract In order to understand AdaBoost's dynamics, especially its ability to maximize margins, we derive an associated simplified nonlinear iterated map and analyze its behavior in low-dimensional cases. We find stable cycles for these cases, which can explicitly be used to solve for AdaBoost's output. By considering AdaBoost as a dynamical system, we are able to prove Ratsch and Warmuth's conjecture that AdaBoost may fail ¨ to converge to a maximal-margin combined classifier when given a `nonoptimal' weak learning algorithm. AdaBoost is known to be a coordinate descent method, but other known algorithms that explicitly aim to maximize the margin (such as AdaBoost and arc-gv) are not. We consider a differentiable function for which coordinate ascent will yield a maximum margin solution. We then make a simple approximation to derive a new boosting algorithm whose updates are slightly more aggressive than those of arc-gv.

1 Introduction AdaBoost is an algorithm for constructing a "strong" classifier using only a training set and a "weak" learning algorithm. A "weak" classifier produced by the weak learning algorithm has a probability of misclassification that is slightly below 50%. A "strong" classifier has a much smaller probability of error on test data. Hence, AdaBoost "boosts" the weak learning algorithm to achieve a stronger classifier. AdaBoost was the first practical boosting algorithm, and due to its success, a number of similar boosting algorithms have since been introduced (see [1] for an introduction). AdaBoost maintains a distribution (set of weights) over the training examples, and requests a weak classifier from the weak learning algorithm at each iteration. Training examples that were misclassified by the weak classifier at the current iteration then receive higher weights at the following iteration. The end result is a final combined classifier, given by a thresholded linear combination of the weak classifiers. Often, AdaBoost does not empirically seem to suffer badly from overfitting, even after a large number of iterations. This lack of overfitting has been attributed to AdaBoost's

 This research was partially supported by NSF Grants IIS-0325500, CCR-0325463, ANI-

0085984 and AFOSR Grant F49620-01-1-0099.


ability to generate a large margin, leading to a better guarantee on the generalization performance. When it is possible to achieve a positive margin, AdaBoost has been shown to approximately maximize the margin [2]. In particular, it is known that AdaBoost achieves a margin of at least , where  is the largest margin that can possibly be attained by a com-

1 2

bined classifier (other bounds appear in [3]). Many of the subsequent boosting algorithms that have emerged (such as AdaBoost [4], and arc-gv [5]) have the same main outline as AdaBoost but attempt more explicitly to maximize the margin at the expense of lowering the convergence rate; the trick seems to be to design an update for the combined classifier that maximizes the margin, has a fast rate of convergence, and is robust. For all the extensive theoretical and empirical study of AdaBoost, it is still unknown if AdaBoost achieves a maximal margin solution, and thus the best upper bound on the probability of error (for margin-based bounds). While the limiting dynamics of the linearly inseparable case (i.e.,  = 0) are fully understood [6], other basic questions about the dynamics of AdaBoost in the more common case  > 0 are unknown. For instance, we do not know, in the limit of a large number of rounds, if AdaBoost eventually cycles among the base classifiers, or if its behavior is more chaotic. In this paper, we study the dynamics of AdaBoost. First we simplify the algorithm to reveal a nonlinear iterated map for AdaBoost's weight vector. This iterated map gives a direct relation between the weights at time t and the weights at time t + 1, including renormalization, thus providing a much more concise mapping than the original algorithm. We then provide a specific set of examples in which trajectories of this iterated map converge to a limit cycle, allowing us to calculate AdaBoost's output vector directly. There are two interesting cases governing the dynamics: the case where the optimal weak classifiers are chosen at each iteration (the `optimal' case), and the case where permissible non-optimal weak classifiers may be chosen (the `non-optimal' case). In the optimal case, the weak learning algorithm is required to choose a weak classifier which has the largest edge at every iteration. In the non-optimal case, the weak learning algorithm may choose any weak classifier as long as its edge exceeds , the maximum margin achievable by a combined classifier. This is a natural notion of non-optimality for boosting; thus it provides a natural sense in which to measure robustness. Based on large scale experiments and a gap in theoretical bounds, Ratsch and Warmuth [3] ¨ conjectured that AdaBoost does not necessarily converge to a maximum margin classifier in the non-optimal case, i.e., that AdaBoost is not robust in this sense. In practice, the weak classifiers are generated by CART or another heuristic weak learning algorithm, implying that the choice need not always be optimal. In Section 3, we show this conjecture to be true using a low-dimensional example. Thus, our low-dimensional study provides insight into AdaBoost's large scale dynamical behavior. AdaBoost, as shown by Breiman [5] and others, is actually a coordinate descent algorithm on a particular exponential loss function. However, minimizing this function in other ways does not necessarily achieve large margins; the process of coordinate descent must be somehow responsible. In Section 4, we introduce a differentiable function that can be maximized to achieve maximal margins; performing coordinate ascent on this function yields a new boosting algorithm that directly maximizes margins. This new algorithm and AdaBoost use the same formula to choose a direction of ascent/descent at each iteration; thus AdaBoost chooses the optimal direction for this new setting. We approximate the update rule for coordinate ascent on this function and derive an algorithm with updates that are slightly more aggressive than those of arc-gv. We proceed as follows: in Section 2 we introduce some notation and state the AdaBoost algorithm. Then we decouple the dynamics for AdaBoost in the binary case to reveal a nonlinear iterated map. In Section 3, we analyze these dynamics for a simple case: the case where each hypothesis has one misclassified point. In a 3 × 3 example, we find 2 stable


cycles. We use these cycles to show that AdaBoost produces a maximal margin solution in the optimal case; this result generalizes to m×m. Then, we produce the example promised above to show that AdaBoost does not necessarily converge to a maximal margin solution in the non-optimal case. In Section 4 we introduce a differentiable function that can be used to maximize the margin via coordinate ascent, and then approximate the coordinate ascent update step to derive a new algorithm. 2 Simplified Dynamics of AdaBoost

The training set consists of {(xi,yi)}i =1..m , where each example (xi,yi)  X ×{-1,1}.

expressed as a column vector. (Denote dTt as its transpose.) Denote by n the total number Denote by dt  Rm the distribution (weights) over the training examples at iteration t, of classifiers that can be produced by the weak learning algorithm. Since our classifiers are binary, n is finite (at most 2m), but may be very large. The weak classifiers are denoted appears. We construct a matrix M so that Mij = yihj(xi), i.e., Mij = +1 if training h1, ..., hn, with hj : X  {1, -1}; we assume that for every hj on this list, -hj also coefficient of classifier hj for the final combined hypothesis is denoted j, so that the final example i is classified correctly by hypothesis hj, and -1 otherwise. The (unnormalized) this paper, either hj or -hj remains unused.) The simplex of n-dimensional vectors with positive entries that sum to 1 will be denoted n. The margin of training example i is defined by yifAda(xi), or equivalently (M)i/  , and the edge of hypothesis j with

combined hypothesis is fAda(x) = n j=1 (j/  )h (x) where  = 1 j 1 n j=1 j. (In

1

respect to the training data (weighted by d) is (dTM)j, or 1 - 2×(probability~ of error of hj on the training set weighted by d). Our goal is to find a normalized vector   n that maximizes mini(M)i. We call this minimum margin over training examples the margin ~ of classifier . Here is the AdaBoost algorithm and our reduction to an iterated map. AdaBoost (`optimal' case): 1. Input: Matrix M, Number of iterations tmax 2. Initialize: 1,j = 0 for j = 1,...,n 3. Loop for t = 1,...,tmax (b) jt = argmaxj(dTt M)j (c) rt = (dTt M)jt

(a) dt,i = e( -Mt)i / m i=1 e( -Mt)i for i = 1,...,m

(d) t = ln 1 2

1+rt 1-rt

(e) t = t + tejt, where ejt is 1 in position jt and 0 elsewhere. Thus at each iteration, the distribution dt is computed (Step 3a), classifier jt with maximum edge is selected (Step 3b), and the weight of that classifier is updated (Step 3c, 3d, 3e). (Note that wlog one can omit from M all the unused columns.) AdaBoost can be reduced to the following iterated map for the dt's. This map gives a direct relationship between dt and dt , taking the normalization of Step 3a into account

+1

4. Output: combined,j = tmax+1,j/ tmax+1 1

+1

automatically. Initialize d1,i = 1/m for i = 1,...,m as in the first iteration of AdaBoost. Reduced Iterated Map: 1. jt = argmaxj(dTt M)j 2. rt = (dTt M)jt

3. dt +1,i = dt,i 1+Mijtrt for i = 1,...,m


To derive this map, consider the iteration defined by AdaBoost and reduce as follows.

e-(M

m i=1

t)i e-(Mijt t)

dt +1,i = , so

e-(M

1-rt 1+rt 1 2

t)i Mijt

e-(Mijt

= dt,i

1-Mijtrt

t)

1 where t = ln 2

1+rt 1-rt

1-Mijtrt 1+Mijtrt

1 2

1 2

e-(Mijt dt

t)

= =

, thus

. +1,i

m

i=1 t,i 1+Mijtrt

d

{i:Mijt=1} t,i

1+Mijtrt 1-Mijtrt 1 2

d

Define d+ =

and d- = 1 - d+. Thus, d+ = 1+rt 2 and d- = 1-rt 2 . For

each i such that Mijt = 1, we find:

dt +1,i =

dt,i d+ + d- =

1+rt 1-rt

dt,i 1+rt

Likewise, for each i such that Mijt = -1, we findmdt

+1,i

i=1

dt +1,i complete. To check that

d+ 2d+ + d- 2d- = 1.

m i=1 dt +1,i = 1, we see

dt,i

=

=

1-rt1

1+rt

. Our reduction is

d+ + 1 1-rt d- =

3 The Dynamics of Low-Dimensional AdaBoost First we will introduce a simple 3×3 input matrix and analyze the convergence of AdaBoost in the optimal case. Then we will consider a larger matrix and show that AdaBoost fails to converge to a maximum margin solution in the non-optimal case.

Consider the following input matrix M = -1 1 1 1 -1 1 1 1 -1 corresponding to the case of

three training examples, where each weak classifier misclassifies one example. (We could add additional hypotheses to M, but these would never be chosen by AdaBoost.) The maximum value of the margin for M is 1/3. How will AdaBoost achieve this result? We are in the optimal case, where jt = argmaxj(dTt M)j. Consider the dynamical system on the simwith vertices (0,0,1),(1/3,1/3,1/3),(0,1,0), jt will be 1. Similarly, we have regions for jt = 2 and jt = 3 (see Figure 1(a)). Since dt will always satisfy (dTt+1M)jt = 0, the

plex 3 i=1 dt,i = 1, dt,i > 0 i defined by our reduced map above. In the triangular region

+1

dynamics are restricted to the edges of a triangle with vertices (0, , ),(12,0, ),(12, ,0)

after the first iteration (see Figure 1(b)).

1 1 2 2 1 2 1 2

1 1

d_t d_t

of of

1/2

(dTM)2 =0 (dTM)1 =0

jt=1 jt=3 component component

1/3

second jt=2

1/3 first component of d_t

second

1

(dTM)3 =0 first component of d_t

1/2 1

Figure 1: (a-Left) Regions of dt-space where classifiers jt = 1,2,3 will respectively be selected. (b-Right) All weight vectors d2, ..., dtmax are restricted to lie on the edges of the inner triangle.


(0,.5,.5) (0,.5,.5)

(.5,.0,.5) triangle triangle(.5,0,.5)

along along

(.5,.5,0) (.5,.5,0)

position position

(0,.5,.5) (0,.5,.5) (0,.5,.5) (0,.5,.5)

(.5,.5,0) (.5,.0,.5) (0,.5,.5)

(.5,.5,0) (.5,.0,.5) (0,.5,.5)

position along triangle position along triangle

0.6 vector weight of 0.5 vector weight of

component component

second 0.15 0.15

first component of weight vector

second 0

0.55 0 first component of weight vector 0.5

Figure 2: (a-Upper Left) The iterated map on the unfolded triangle. Both axes give coordinates on

the edges of the inner triangle in Figure 1(b). The plot shows where dt +1

Right) The map from (a) iterated twice, showing where dt +3

will be, given dt. (b-Upper

will be, given dt. There are 6 stable

fixed points, 3 for each cycle. (c-Lower Left) 50 timesteps of AdaBoost showing convergence of dt's to a cycle. Small rings indicate earlier timesteps of AdaBoost, while larger rings indicate later timesteps. There are many concentric rings at positions dcyc, d(2) , and d(3) . (d-Lower Right) 500

(1)

timesteps of AdaBoost on a random 11x21 matrix. The axes are dt, 1

cyc cyc

vs dt, . 2

On this reduced 1-dimensional phase space, the iterated map has no stable fixed points or

orbits of length 2.However, consider the following periodic orbit oflength 3:

d(1)T = ( 3 , cyc

 -4 5 5-1 1 4 2 , ), d(2)T = (12,

cyc

 3-4 5

,

5 4-1

), d(3)T = ( cyc

5-1 1 3-4 5

, ,

2 4



). This

In this way, AdaBoost will cycle between hypotheses j = 1,2,3,51,2,,33-4etc.), There is visiting each hypothesis in turn, and used the reduced equations from Section 2 to solve for the cycle coordinates. We give the following outline of the proof for global stability: This map is a contraction, so any small perturbation from the cycle will diminish, yielding local stability of the cycles. One only needs to consider the one-dimensional map defined on the unfolded triangle, since within one iteration every trajectory lands on the triangle. This map and its iterates are piecewise continuous and monotonic in each piece, so one can find exactly where each interval will be mapped (see Figure 2(a)). Consider the second iteration of this map (Figure 2(b)). One can break the unfolded triangle into intervals and find the region of attraction of each fixed cycle; in fact the whole triangle is the union of both regions of attraction. The convergence to one of these two 3cycles is very fast; Figure 2(b) shows that the absolute slope of the second iterated map at the fixed points is much less than 1. The combined classifier AdaBoost will output is: combined = ((d(1)TM)1,(d(2)TM)2,(dcyc M)3)/normaliz. = (1/3,1/3/1/3), and since mini(Mcombined)i = 1/3 AdaBoost produces a maximal margin solution.

in fact another 3-cycle, d(1)T = (3 , , ), d(2)T = (2, 1

(54-1,

3-4 5 1 , ). To find these cycles, we hypothesized only that a cycle of length 3 exists,

2



cyc 2 cyc

 -4 5 1 5 4-1

4-1

5

d(3)T = cyc

(3)T cyc cyc

is clearly a cycle, since starting from d(1) , AdaBoost will choose jt = 1. Then r1 = (d(1)TM)1 = (5 - 1)/2. Now, computing d(1) /(1 + Mi, r ) for each, i yields d(2).

cyc

cyc

cyc,i

1 1 cyc


This 3 × 3 case can be generalized to m classifiers, each having one misclassified training example; in this case there will be periodic cycles of length m, and the contraction will also persist (the cycles will be stable). We note that for every low-dimensional case we tried, periodic cycles of larger lengths seem to exist (such as in Figure 2(d)), but that the contraction at each iteration does not, so it is harder to show stability. Now, we give an example to show that non-optimal AdaBoost does not necessarily converge to a maximal margin solution. Consider the following input matrix (again, omitting

unused columns): M = -1 1 1 1 1 -1 1 1 1 1 -1 1 1 1 1 -1 -1 -1 1 1 . For this matrix, the maximal mar-

gin  is 1/2. In the optimal case, AdaBoost will produce this value by cycling among the first four columns of M. Recall that in the non-optimal case jt -8{j5 :3(-dt5M2)j 54-1}).. Consider the following initial condition for the dynamics: dT1 = (3 , , , Since (d1 M)5 > , we are justified in choosingj5 =, 53, although here it is not the optimal which we choose j2 = 4. At the third iteration, we choose j3 = 3, and at the fourth iteration we find d4 = d1. This cycle is the same cycle as in our previous example (although there is one extra dimension). There is actually a whole manifold of 3-cycles in this non-optimal

T

1

8 T

choice. Another iteration yields dT2 = (4, , 1 1 4

1 4-1  -4 5

), satisfying (dT1M)4 >  for

case, since d~1 := ( , T case, the value of the margin- 3-4 5 

, ,

1 2

5 4-1

produced by this cycle is 1/3, not 1/2.

)liesonacycleforany ,0  

 3-4 5

. In any

We have thus established that AdaBoost is not robust in the sense we described; if the weak learner is not required to choose the optimal hypothesis at each iteration, but is only required to choose a sufficiently good weak classifier jt  {j : (dTt M)j  }, then a maximum margin solution will not necessarily be attained. In practice, it may be possible for AdaBoost to converge to a maximum margin solution when hypotheses are chosen to be only slightly non-optimal; however the notion of non-optimal we are using is a very natural notion, and we have shown that AdaBoost may not converge to  here. Note that for some matrices M, a maximum margin solution may still be attained in the non-optimal case (for shown by our example. We are not saying that the only way for AdaBoost to converge to example the simple 3×3 matrix we analyzed above), but it is not attained in general as a non-optimal solution is to fall into the wrong cycle; there may be many other non-cyclic ways for the algorithm to fail to converge to a maximum margin solution. Also note that for the other algorithms mentioned in Section 1 and for the new algorithms in Section 4, there are fixed points rather than periodic orbits. 4 Coordinate Ascent for Maximum Margins AdaBoost can be interpreted as an algorithm based on coordinate descent. There are other algorithms such as AdaBoost and arc-gv that attempt to maximize the margin explicitly, but these are not based on coordinate descent. We now suggest a boosting algorithm that aims to maximize the margin explicitly (like arc-gv and AdaBoost) yet is based on coordinate ascent. An important note is that AdaBoost and our new algorithm choose the direction of descent/ascent (value of jt) using the same formula, jt = argmaxj(dTt M)j. This lends further credence to the conjecture that AdaBoost maximizes the margin in the optimal case, since the direction AdaBoost chooses is the same direction one would choose to maximize the margin directly via coordinate ascent.

The function that AdaBoost minimizes via coordinate descent is F() =

Consider any  such that (M)i > 0 i. Then lima 

m i=1

e-(M . )i

a will minimize F, yet the origi-

nal normalized  might not yield a maximum margin. So it must be the process of coordinate descent which awards AdaBoost its ability to increase margins, not simply AdaBoost's ability to minimize F. Now consider a different function (which bears a resemblance to an


-Boosting objective in [7]):

G() = - 1 

m n

1

lnF() = - 1  ln e-(M )i where  :=

1 j .

1 i=1 j=1

It can be verified that G has many nice properties, e.g., G is a concave function for each

fixed value of  , whose maximum only occurs in the limit as 

1 1 1 , and more

1 i

the margin of . That is, importantly, as    we have G()  µ(), where µ() = (min(M)i)/  ,

me-µ () 

(1) (2)

1



m i=1 e-(M )i

> e-µ < µ() ()  1

-(lnm)/  +µ()  1 G()

For (1), the first inequality becomes equality only when all m examples achieve the same minimal margin, and the second inequality holds since we took only one term. Rather than performing coordinate descent on F as in AdaBoost, let us perform coordinate ascent on G. The choice of direction jt at iteration t is:

e-(Mt)iMij

argmax = argmax

j

=0

j

dG(t + ej) d

m i=1

+ ln(F(t)).

F(t) t 1

1 t

2 1

Of these two terms, the second term does not depend on j, and the first term is proportional to (dTt M)j. Thus the same direction will be chosen here as for AdaBoost. Now consider the distance to travel along this direction. Ideally, we would like to maximize G(t + ejt) with respect to , i.e., we would like:

e-(Mt)ie-Mijt Mijt F(t + ejt)  0 = dG(t + ejt) d m i=1 t

+1 1 = -G(t +ejt)

There is not an analytical solution for , but maximization of G(t+ejt) is 1-dimensional so it can be performed quickly. An approximate coordinate ascent algorithm which avoids this line search is the following approximation to this maximization problem:

e-(Mt)ie-Mijt Mijt F(t + ejt)  0  m i=1 -G(t).

We can solve for t analytically:

1 t = ln 2

1+rt 1-rt

1 - ln 2

1+gt 1-gt

, where gt = max{0,G(t)}. (3)

Consider some properties of this iteration scheme. The update for t is strictly positive (in the case of positive margins) due to the Von Neumann min-max theorem and equation (2),

that is: rt   = mind m maxj (dTM)j = max n mini (M)i  mini (Mt)i/ t 1

at each iteration of our approximate coordinate ascent algorithm, and that our algorithms > G(t), and thus t > 0 t. We have preliminary proofs that the value of G increases converge to a maximum margin solution, even in the non-optimal case. Our new update (3) is less aggressive than AdaBoost's, but slightly more aggressive than arc-gv's. The other algorithm we mention, AdaBoost, has a different sort of update. It converges to a combined classifier attaining a margin inside the interval [ - ,] within 2(log2m)/2 steps, but does not guarantee asymptotic convergence to  for a fixed . There are many other boosting algorithms, but some of them require minimization over non-convex functions; here, we choose to compare with the simple updates of AdaBoost (due to its fast convergence rate), AdaBoost, and arc-gv. AdaBoost, arc-gv, and our algorithm have initially large updates, based on a conservative estimate of the margin. AdaBoost's updates are initially small based on an estimate of the edge.


0.65 0.16

AdaBoost

arc-gv, approximate coord ascent, and coord ascent AdaBoost

Margin 0.5

arc-gv approximate coord. ascent and coord. ascent Margin 0.13

arc-gv

approximate coordinate ascent

AdaBoost

AdaBoost*

0.4 0

Iterations 20 150 1100 0.1 90 400 1800 10000

Iterations

Figure 3: (a-Left) Performance of all algorithms in the optimal case on a random 11 × 21 input matrix (b-Right) AdaBoost, arc-gv, and approximate coordinate ascent on synthetic data. Figure 3(a) shows the performance of AdaBoost, arc-gv, AdaBoost (parameter  set to .001), approximate coordinate ascent, and coordinate ascent on G (with a line search for t at every iteration) on a reduced randomly generated 11 × 21 matrix, in the optimal case. AdaBoost settles into a cycle (as shown in Figure2(d)), so its updates remain consistently large, causing t to grow faster, thus converge faster with respect to G. The values

1

of rt in the cycle happen to produce an optimal margin solution, so AdaBoost quickly converges to this solution. The approximate coordinate ascent algorithm has slightly less aggressive updates than AdaBoost, and is very closely aligned with coordinate ascent; arcgv is slower. AdaBoost has a more methodical convergence rate; convergence is initially slower but speeds up later. Artificial test data for Figure 3(b) was designed as follows: 50 example points were constructed randomly such that each xi lies on a corner of the component of xi. The jth weak learner is hj(x) = x(j), thus Mij = yixi(j). As expected, the convergence rate of approximate coordinate ascent falls between AdaBoost and arc-gv. 5 Conclusions We have used the nonlinear iterated map defined by AdaBoost to understand its update rule in low-dimensional cases and uncover cyclic dynamics. We produced an example to show that AdaBoost does not necessarily maximize the margin in the non-optimal case. Then, we introduced a coordinate ascent algorithm and an approximate coordinate ascent algorithm that aim to maximize the margin directly. Here, the direction of ascent agrees with the direction chosen by AdaBoost and other algorithms. It is an open problem to understand these dynamics in other cases. References [1] Robert E. Schapire. A brief introduction to boosting. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, 1999. [2] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. The Annals of Statistics, 26(5):1651­1686, October 1998. [3] Gunnar R¨atsch and Manfred Warmuth. Maximizing the margin with boosting. In Proceedings of the 15th Annual Conference on Computational Learning Theory, pages 334­350, 2002. [4] Gunnar R¨atsch and Manfred Warmuth. Efficient margin maximizing with boosting. Journal of Machine Learning Research, submitted 2002. [5] Leo Breiman. Prediction games and arcing classifiers. Neural Computation, 11(7):1493­1517, 1999. [6] Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, AdaBoost and Bregman distances. Machine Learning, 48(1/2/3), 2002. [7] Saharon Rosset, Ji Zhu, and Trevor Hastie. Boosting as a regularized path to a maximum margin classifier. Technical report, Department of Statistics, Stanford University, 2003.

11 k=1 xi(k)), where xi(k) indicates the kth hypercube {-1,1}100. We set yi = sign(


