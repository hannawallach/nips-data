Sparseness of Support Vector Machines--Some Asymptotically Sharp Bounds

Ingo Steinwart Modeling, Algorithms, and Informatics Group, CCS-3, Mail Stop B256 Los Alamos National Laboratory Los Alamos, NM 87545, USA ingo@lanl.gov

Abstract The decision functions constructed by support vector machines (SVM's) usually depend only on a subset of the training set--the so-called support vectors. We derive asymptotically sharp lower and upper bounds on the number of support vectors for several standard types of SVM's. In particular, we show for the Gaussian RBF kernel that the fraction of support vectors tends to twice the Bayes risk for the L1-SVM, to the probability of noise for the L2-SVM, and to 1 for the LS-SVM.

1 Introduction Given a training set T = ((x1, y1), . . . , (xn, yn)) with xi  X, yi  Y

:= {-1, 1}

standard support vector machines (SVM's) for classification (cf. [1], [2]) solve

n

arg min  f

f H bR

2 H + 1 n L yi(f(xi) + b) ,

(1)

i=1

where H is a reproducing kernel Hilbert space (RKHS) of a kernel k : X ×X  R (cf. [3], [4]),  > 0 is a free regularization parameter and L : R  [0, ) is a convex loss function. Common choices for L are the hinge loss function L(t) := max{0, 1-t}, the squared hinge loss function L(t) := (max{0, 1-t})2 and the least square loss function L(t) := (1-t)2. The corresponding classifiers are called L1-SVM, L2-SVM and LS-SVM, respectively. x, x  Rd and fixed  > 0 and polynomial kernels k(x, x ) = ( x, x +c)m for x, x  Rd and fixed c  0, m  N. If (fT,, bT,)  H × R denotes a solution of (1) we have

Common choices of kernels are the Gaussian RBF k(x, x ) = exp(-2 x - x 2 2 ) for

n

fT, = 1 2 yii k(xi, .)  (2)

i=1

for suitable coefficients 1, . . . , n  R (cf. [5]). Obviously, only the samples xi with i = 0 have an impact on fT,. These samples are called support vectors. The fewer



support vectors fT, has the faster it can be evaluated. Moreover, it is well known that

 


the number of support vectors #SV (fT,) of the representation of fT, (cf. Section 3 for a brief discusssion) also has a large impact on the time needed to solve (1) using the dual problem. Therefore, it is of high interest to know how many support vectors one can expect for a given classification problem. In this work we address this question by establishing asymptotically lower and upper bounds on the number of support vectors for typical situations. The rest of the paper is organized as follows: in Section 2 we introduce some technical notions and recall recent results in the direction of the paper. In Section 3 our results are presented and discussed, and finally, in Section 4 their proofs can be found. 2 Notations and known results The standard assumption in classification is that the training set T consists of i.i.d. pairs drawn from an unknown distribution P on X×Y . For technical reason we assume throughout this paper that X is a compact metric space, e.g. a bounded, closed subset of Rd. A Bayes decision function (cf. [6]) fP : X  Y is a function that PX-a.s. equals 1 and -1 on C1 := {x  X : P(1|x) > 1/2} and C- := {x  X : P(-1|x) > 1/2}, respectively.

1

The corresponding classification error RP of such a function is called the Bayes risk of P. Recall, that the Bayes risk is the smallest possible classification error. C(X). The best known example of a universal kernel is the Gaussian RBF kernel (cf. [7]). Let us recall some results of the recent paper [8]. To simplify the statements, let us assume that P has no discrete components, i.e. PX({x}) = 0 for all x  X. Furthermore, let L be a continuous convex loss function satisfying some minor regularity conditions. Then it was shown for universal RKHS's and stritly positive nullsequences (n) satisfying a regularity condition that the following statements hold for all  > 0 and n  : P n T  (X × Y )n : #SV (fT,n)  (RP - )n  1. (3) In particular, this result holds for L1-SVM's. Furthermore, for L being also differentiable (e.g. L2-SVM's and LS-SVM's) it was proved P n T  (X × Y )n : #SV (fT,n)  (SP - )n  1, (4) where SP := PX({x  X : 0 < P(1|x) < 1}) denotes the probability of the set of points where noise occurs. Obviously, we always have SP  2RP and for noisy non-degenerate P , that is for P with PX x  X : P (1|x)  {0, 1/2, 1} > 0 this relation becomes a strict inequality. We shall prove in the next section that (3) can be significantly improved for the L1-SVM. We shall also show that this new lower bound is also an upper bound under moderate conditions on P and H. Furthermore, we prove that (4) is asymptotically optimal for the L2-SVM and show that it can be significantly improved for the LS-SVM. 3 New bounds We begin with lower and upper bounds for the L1-SVM. Recall, that the problem (1) for this classifier can be reformulated as

A RKHS H is called universal if H is .  -dense in the space of continuous functions

minimize subject to

 f, f + 1 n n i=1 i

(5)

yi f(xi) + b  1 - i, i  0,

for f  H, b  R,   Rn i = 1, . . . , n i = 1, . . . , n .


Instead of solving (5) directly, one usually solves the dual optimization problem (cf. [4])

maximize subject to n i=1 n i=1

i - 1 4 n i,j=1 yiyjijk(xi, xj) for   Rn

(6)

yii = 0, 0  i  ,

1 n i = 1, . . . , n .

If (1, . . . , n)  R denotes a solution of (6) then fT, can be computed by (2). Note that the representaion of fT, is not unique in general, i.e. using other algorithms for solving (5) can lead to possibly sparser representations. However, in contrast to the general case the representation (2) of fT, is Pn-almost surely (a.s.) unique if the kernel is universal and P has no discrete components (cf. [8]). Since our results for the L1-SVM hold for general kernels we always assume that fT, is found by (6). Finally, for a loss function L and a RKHS H we write RL,P,H := inf RL,P (f + b) ,

f H bR

where RL,P (f) := E(x,y

 

)P L yf(x) . Note, that fT,n +bT,n cannot achieve an L-risk

better than RL,P,H, if H is the RKHS used in (1). Now, our first result is: Theorem 3.1 Let k be a continuous kernel on X and P be a probability measure on X×Y with no discrete components. Then for the L1-SVM using a regularization sequence (n) with n  0 and n2n/ log n   and all  > 0 we have P n T  (X × Y )n : #SV (fT,n)  (RL,P,H - )n  1 . Remark 3.2 If k is a universal kernel we have RL,P,H = 2RP (cf. Ste7) and thus Theorem 3.1 yields the announced improvement of (3). For non-universal kernels we even have RL,P,H > 2RP in general. Remark 3.3 For specific kernels the regularity condition n2n/ log n   can be weakened. Namely, for the Gaussian RBF kernel on X  Rd it can be substituted by The interested reader can prove such conditions by establishing (9) using the results of [9]. Remark 3.4 If H is finite dimensional and n > dim H the representation (2) of fT,n can be simplified such that only at most dim H kernel evaluations are neccessary. However, this simplification has no impact on the time needed for solving (6). In order to formulate an upper bound on #SV (fT,n) recall that a function is called analytic if it can be locally represented by its Taylor series. Let L be a loss function, H be a RKHS over X and P be a probability measure on X × Y . We call the pair (H, P) non-trivial (with respect to L) if RL,P,H < inf RL,P(b),

nn |log n|-d -1  . Only slightly stronger conditions are sufficient for C-kernels.

bR

i.e. the incorporation of H has a non-trivial effect on the L-risk of P. If H is universal we have RL,P,H = inf{RL,P (f) f : X  R} (cf. [9]) and therefore (H, P) is non-trivial if P has two non-vanishing classes, i.e. PX(C1) > 0 and PX(C- ) > 0. Furthermore, we

1

denote the open unit ball of Rd by BRd. Now our upper bound is:

Theorem 3.5 Let H be the RKHS of an analytic kernel on BRd. Furthermore, let X  BRd be a closed ball and P be a noisy non-degenerate probability measure on X × Y such that


PX has a density with respect to the Lebesgue measure on X and (H, P ) is non-trivial. Then for the L1-SVM using a regularization sequence (n) which tends sufficiently slowly

to 0 we have

#SV (fT,n) n  RL,P,H

in probability. Probably the most restricting condition on P in the above theorem is that PX has to have a density with respect to the Lebesgue measure. Considering the proof this condition can be slightly weakened to the assumption that every d-1-dimensional subset of X has measure zero. Although it would be desirable to exclude only probability measures with discrete components it is almost obvious that such a condition cannot be sufficient for d > 1 (cf. [10, p.32]). The assumption that P is noisy and non-degenerate is far more less restrictive since neither completely noise-free P nor noisy problems with only "coin-flipping" noise often occur in practice. Finally, the condition that (H, P) is non-trivial is more or less implicitly assumed whenever one uses nontrivial classifiers. Example 3.6 Theorem 3.5 directly applies to polynomial kernels. Note, that the limit RL,P,H depends on both P and the choice of the kernel. Example 3.7 Let k be a Gaussian RBF kernel with RKHS H and X be a closed ball of Rd. Moreover, let P and (n) be according to Theorem 3.5. Recall, that k is universal and hence (H, P) is non-trivial iff P has two non-vanishing classes. Since k is also analytic on

Rd we find

#SV (fT,n) n  2RP .

Therefore, (4) shows that in general this L1-SVM produces sparser decision functions than the L2-SVM and the LS-SVM based on a Gaussian RBF kernel (cf. also Theorem 3.11). Remark 3.8 A variant of the L1-SVM that is often considered in theoretical papers is based on the optimization problem (5) with a-priori fixed b := 0. Besides the constraint (6). Hence it is easily seen that Theorem 3.1 also holds for this classifier. Moreover, for this modification Theorem 3.5 can be simplified. Namely, the assumption that P is noisy and non-degenerate is superfluous (cf. [8, Prop. 33] to guarantee (14)). In particular, for a Gaussian RBF kernel and noise-free problems P we then obtain

n i=1 yii = 0, which no longer appears, the corresponding dual problem is identical to

#SV (fT,n) n  0, (7)

i.e. the number of support vectors increases more slowly than linearly. This motivates the often claimed sparseness of SVM's. The following theorem shows that the lower bound (4) on #SV (fT,n) for the L2-SVM is often asymptotically optimal. This result is independent of the used optimization algorithm since we only consider universal kernels and measures with no discrete components. Theorem 3.9 Let H be the RKHS of an analytic and universal kernel on BRd. Furthermore, let X  BRd be a closed ball and P be a probability measure on X × Y with RP > 0 such that PX has a density with respect to the Lebesgue measure on X and (H, P ) is non-trivial. Then for the L2-SVM using using a regularization sequence (n) which tends sufficiently slowly to 0 we have

#SV (fT,n) n  SP

in probability.


Remark 3.10 For the L2-SVM with fixed offset b := 0 the assumption RP > 0 in the above theorem is superfluous (cf. proof of Theorem 3.9 and proof of [8, Prop. 33]). In particular, for a Gaussian RBF kernel and noise-free problems P we obtain (7), i.e. for noise-free problems this classifier also tends to produce sparse solutions in the sense of Remark 3.8. Our last result shows that LS-SVM's often tend to use almost every sample as a support vector: Theorem 3.11 Let H be the RKHS of an analytic and universal kernel on BRd. Furthermore, let X  BRd be a closed ball and P be a probability measure on X × Y such that PX has a density with respect to the Lebesgue measure on X and (H, P ) is non-trivial. Then for the LS-SVM using a regularization sequence (n) which tends sufficiently slowly

to 0 we have

#SV (fT,n) n  1

in probability. Remark 3.12 Note, that unlike the L1-SVM and the L2-SVM (with fixed offset) the LSSVM does not tend to produce sparse decision functions for noise-free P. This still holds if one fixes the offset for L2-SVM's, i.e. one considers regularization networks (cf. [11]). The reason for the different behaviours is the margin as already observed in [12]: the assumptions on H and P ensure that only a very small fraction of samples xi can be mapped to ±1 by fT,n (cf. also Remark 4.1). For the L2-SVM this asymptotically ensures that most of the samples are mapped to values outside the margin, i.e. yifT,n(xi) > 1, (cf. the properties of Bn \ A in the proof of Theorem 3.9) and it is well-known that such samples cannot be support vectors. In contrast to this the LS-SVM has the property that every point not lying on the margin is a support vector. Using the techniques of our proofs it is fairly easy to see that the same reasoning holds for the hinge loss function compared to "modified hinge loss functions with no margin". 4 Proofs Let L be a loss function and T be a training set. For a function f : X  R we denote the empirical L-risk of f by

n

RL,T(f + b) := 1 n L yi(f(xi) + b) .

i=1

Proof of Theorem 3.1: Let (fT,n, bT,n, )  H × R × Rn and   Rn be solutions of (5) and (6) for the regulariztion parameter n, respectively. Since there is no duality gap between (5) and (6) we have (cf. [4]):

n n n

n fT,n, fT,n By (2) this yields

n

1 n i 

+ 1 n i =  i -  1 4n yiyji j k(xi, xj)   (8)

i=1 i=1 i,j=1

n n

 2n fT,n,fT,n + 1 n i  = i . 

i=1 i=1 i=1

Furthermore, recall that n  0 and n2n/ log n   implies

n

1 n i = RL,T (fT,n + bT,n)  RL,P,H  (9)

i=1


in probability for n   (cf. [9]) and hence for all  > 0 the probability of i  RL,P,H - 

n



i=1

tends to 1 for n  . Now let us assume that our training set satisfies (10). Since i  1/n we then find 

n

RL,P,H -   which finishes the proof. i   1 n = 1 # n SV (fT,n)

(10)

i=1 =0 i

For our further considerations we need to consider the optimization problem (1) with respect to P, i.e. we treat the (solvable, see [8]) problem

(fP,, bP,) := arg min  f

f H bR

2 H + RL,P (f + b) . (11)

Proof of Theorem 3.5: Since H is the RKHS of an analytic kernel every function f  H is analytic. Using the holomorphic extension of a non-constant f  H we see (after a suitable

complex linear coordinate change, cf. [10, p. 31f]) that for c  R and x1, . . . , xd -1

equation f(x1, . . . , xd respect to x1, . . . , xd

-1

-1

 R the

, xd) = c has at most j solutions xd, where j  0 is locally (with

 R) constant . By a simple compactness argument we hence find

PX {x  X : f(x) = c} > 0  f(x) = c PX-a.s. (12)

for all f  H and all c  R. Now, let us suppose that PX {x  X : fP,(x) + bP, = fP (x)} > 0

(13)

for some  > 0, where fP denotes the Bayes decision function. Then we may assume without loss of generality that PX {x  X : fP,(x)+bP, = 1} > 0 holds. By (12) this leads to fP,(x) + bP, = 1 PX-a.s. However, since RL,P (fP, + bP,)  RL,P,H for   0 (cf. [9]) we see that fP, cannot be constant for small  since (H, P ) was assumed to be non-trivial. Therefore (13) cannot hold for small  > 0 and hence we may assume without loss of generality that PX {x  X : |fP,(x) + bP, - fP (x)| = 0} = 0 holds for all  > 0. We define A() := x  X : |fP,(x) + bP, - fP (x)|   for ,  > 0. Our above considerations show that for all  > 0 there exists a  > 0 that there exists no sequence n   = 0 with n  0. Let us assume the converse. Then there exists a subsequence with (fP,n , bP,n )  (fP,, bP,) weakly and we have

with PX(A())  . We write  := 1 2 sup{ > 0 : PX(A())  }. We first show

j j

A3n (nj )  A0(). By the construction we have PX(A3n (nj ))   j j lim supj 

and hence PX(lim supj  A3n (nj ))   by the Lemma of Fatou. This gives the j

contradiction PX(A0())  . Thus, the increasing function   m() := inf{~ :   ~ } satisfies m() > 0 for all  > 0. We fix a T = ((x1, y1), . . . , (xn, yn)) with

fT,n + bT,n - fP,n - bP,n 

RL,T(fT,n + bT,n) - RL,P(fP,n + bP,n)

 n ,   (14) (15)

and {i : xi  An(n))}  2n. If m4(n)3nn   the results of [9] and [8] ensure, that the probability of such a T converges to 1 for n  . Moreover, by (8) we find

n

2n fT,n, fT,n + RL,T (fT,n + bT,n) = i .

i=1

 (16)


Since fT,n + bT,n and fP,n + bP,n minimize the regularized risks, (15) implies

n fT,n 2 H + RL,T (fT,n + bT,n) - n fP,n 2 H -RL,P(fP,n +bP,n)  . (17)

Furthermore, if n   we have

n fP,n 2 H + RL,P (fP,n + bP,n)  RL,P,H

(cf. [9]) and therefore we obtain n fT,n 2 H + RL,T (fT,n + bT,n) - RL,P,H

(18)  2

for large n. Now, (15), (17) and (18) implies n fT,n, fT,n (16) yields RL,P,H + 5  i

n



i=1

if n is sufficiently large. Now let us suppose that we have a sample (xi, yi) of T with xi  An(n). Then we have |fP,n(xi) + bP,n - fP (xi)| > n and hence fT,n(xi) + bT,n = ±1 by (14). By [4, p. 107] this means either i = 0 or i = 1/n. Therefore, by

 

(19) we find RL,P,H + 5 

n n  i=1 xiAn (n)

i 

 3 for large n. Hence

(19)

i =  1 n {i : xi  An(n) and i = 0} 

Since we have at most 2n samples in An(n) we finally obtain

1 # n SV (fT,n)  RL,P,H + 7 .

Now the assertion follows by Theorem 3.1. Remark 4.1 The proof of Theorem 3.5 is based on a kind of paradox: recall that it was shown in [8] that fT,n + bT,n  fP on x  X : P(1|x)  {0, 1/2, 1} in probability. However, the assumption on both H and P ensures that for typical T the sets x  X : |fT,n(x) + bT,n - fP (x)|   become arbitrarily small for   0. We will apply these seemingly contradicting properties in the following proofs, too. Proof of Theorem 3.9: Let N := {x  X : 0 < P(1|x) < 1} be the subset of X where P is noisy. Furthermore, let A(n) be defined as in the proof of Theorem 3.5. We write

B(n) :=

x  C1 \ N : fP,n(x) + bP,n  1 -   x  C- \ N : fP,n(x) + bP,n  -1 + 

1 .

By [8, Thm. 22]) for all n  1 there exists a  > 0 with PX(B(n))  PX(X \ N) - . us fix a training set T = ((x1, y1), . . . , (xn, yn)) with fT,n + bT,n - fP,n - bP,n  n , {i : xi  B(n) \ An(n)}  n PX(X \ N) - 3 . Again, the probability of such T converges to 1 for n   whenever (n) converges sufficiently slowly to 0. In view of (4) it suffices to show that every sample xi  B(n) \ An(n) cannot be a support vector. Given an xi  B(n)\An(n) we may assume without loss of generality that xi  C1. Then xi  B(n) implies fP,n(xi)+bP,n  1-n while xi  An(n) yields |fP,n(xi)+bP,n -1| > n. Hence we find fP,n(xi)+bP,n > 1+n and thus fT,n(xi)+bT,n > 1. By the Karush-Kuhn-Tucker conditions of the primal/dual optimization problem of the L2-SVM (cf. [4, p. 105]) this shows that xi is not a support vector.

We define n := 1 2 sup{ > 0 : PX(A(n))   and PX(B(n))  PX(X \ N ) - }. Let




Proof of Theorem 3.11: Let A(n) and n be defined as in the proof of Theorem 3.5. Without loss of generality we may assume n  (0, 1/2). Let us define C0 := {x  X : P (1|x) = 1/2} and Dn = x  C0 : |fP,n(x) + bP,n|  1/2 . By [8, Thm. 22] we may assume without loss of generality that PX(Dn)  PX(C0) -  for all n  1. Now, let us fix a training set T = ((x1, y1), . . . , (xn, yn)) with fT,n + bT,n - fP,n - bP,n  n {i : xi  An(n)}  2n {i : xi  Dn}  n PX(C0) - 2 . Again, the probability of such T converges to 1 for n   whenever (n) converges sufficiently slowly to 0. Now let us consider a sample xi  (X \ An(n))  C1 of T. Then we have |fP,n(xi) + bP,n - 1| > n and hence fT,n(xi) + bT,n = 1. By [8, Cor. 32] this shows that xi is a support vector. Obviously, the same holds true for samples xi  (X \ An(n))  C- . Finally, for samples xi  Dn we have |fT,n(xi) + bT,n| 



1

1/2 + n < 1 and hence these samples are always support vectors. Acknowledgments I would like to thank D. Hush and C. Scovel for helpful comments. References [1] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:1995, 273­297. [2] J.A.K. Suykens and J. Vandewalle. Least squares support vector machine classifiers. Neural Processing Letters, 9:293­300, 1999. [3] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68:337­404, 1950. [4] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University Press, 2000. [5] B. Scholkopf, R. Herbrich, and A.J. Smola. A generalized representer theorem. ¨ In Proceedings of the 14th Annual Conference on Computational Learning Theory, pages 416­426. Lecture Notes in Artificial Intelligence 2111, 2001. [6] L. Devroye, L. Gyorfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. ¨ Springer, New York, 1997. [7] I. Steinwart. On the influence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research, 2:67­93, 2001. [8] I. Steinwart. Sparseness of support vector machines. Journal of Machine Learning Research, 4:1071­1105, 2003. [9] I. Steinwart. Consistency of support vector machines and other regularized kernel machine. IEEE Transactions on Information Theory, to appear. [10] R.M. Range. Holomorphic Functions and Integral Representations in Several Complex Variables. Springer, 1986. [11] F. Girosi, M. Jones, and T. Poggio. Regularization theory and neural networks architectures. Neural Computation, 7:219­269, 1995. [12] A. Kowalczyk. Sparsity of data representation of optimal kernel machine and leaveone-out estimator. In T.K. Leen, T.G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 252­258. MIT Press, 2001.


