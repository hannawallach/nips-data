Online Learning of Non-stationary Sequences

Claire Monteleoni and Tommi Jaakkola MIT Computer Science and Artificial Intelligence Laboratory 200 Technology Square Cambridge, MA 02139 {cmontel,tommi}@ai.mit.edu

Abstract We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts. On the basis of the performance bounds we provide the optimal a priori discretization for learning the parameter that governs the switching dynamics. We demonstrate the new algorithm in the context of wireless networks. 1 Introduction We focus on the online learning framework in which the learner has access to a set of experts but possesses no other a priori information relating to the observation sequence. In such a scenario the learner may choose to quickly identify a single best expert to rely on [12], or switch from one expert to another in response to perceived changes in the observation sequence [8], thus making assumptions about the switching dynamics. The ability to shift emphasis from one "expert" to another, in response to changes in the observations, is valuable in many applications, including energy management in wireless networks. Many algorithms developed for universal prediction on the basis of a set of experts have clear performance guarantees (e.g., [12, 6, 8, 14]). The performance bounds characterize the regret relative to the best expert, or best sequence of experts, chosen in hindsight. Algorithms with such relative loss guarantees have also been developed for adaptive game playing [5], online portfolio management [7], paging [3] and the k-armed bandit problem [1]. Other relative performance measures for universal prediction involve comparing across systematic variations in the sequence [4]. Here we extend the class of algorithms considered in [8], by learning the switching-rate parameter online, at the optimal resolution. Our goal of removing the switching-rate as a parameter is similar to Vovk's in [14], though the approach and the comparison class for the bounds differ. We provide upper and lower performance bounds, and demonstrate the utility of these algorithms in the context of wireless networks. 2 Algorithms and performance guarantees The learner has access to n experts, a1, . . . , an, and each expert makes a prediction at each time-step over a finite (known) time period t = 1, . . . , T. We denote the ith expert at


time t as ai,t to suppress any details about how the experts arrive at their predictions and what information is available to facilitate the predictions. These details may vary from one expert to another and may change over time. We denote the non-negative prediction loss of expert i at time t as L(i, t), where the loss, a function of t, naturally depends on the observation yt  Y at time t. We consider here algorithms that provide a distribution pt(i), i = 1, . . . , n, over the experts at each time point. The prediction loss of such an algorithm is denoted by L(pt, t). For the purpose of deriving learning algorithms such as Static-expert and Fixedshare described in [8], we associate the loss of each expert with a predictiveprobabilityso that - log p(yt|yt- , . . . , y1, i) = L(i, t). We define the loss of any probabilistic prediction

1

to be the log-loss: L(pt, t) = - log

n n

pt(i) p(yt|i, y1, . . . , yt -1 ) = - log pt(i)e-L (i,t) (1)

i=1 i=1

Many other definitions of the loss corresponding to pt(·) can be bounded by a scaled logloss [6, 8]. We omit such modifications here as they do not change the essential nature of the algorithms nor their analysis. The algorithms combining expert predictions can be now derived as simple Bayesian estithe basis of the observations seen so far. p1(i) = 1/n for any such method as any other initial bias could be detrimental in terms of relative performance guarantees. Updating pt(·) involves assumptions about how the optimal choice of expert can change with time. For parameterizes the one-step transition probabilities. Allowing switches at rate , we define1

mation methods calculating the distribution pt(i) = P(i|y1, . . . , yt -1 ) over the experts on

simplicity, we consider here only a Markov dynamics, defined by p(it|it -1 ; ), where 

p(it|it -1 ; ) = (1 - )(it, it -1 ) +  n - 1 [1 - (it,it

-1 )] (2)

which corresponds to the Fixed-share algorithm, and yields the Static-expert forward propagation in generalized HMMs (allowing observation dependence on past): algorithm when  = 0. The Bayesian algorithm updating pt(·) is defined analogously to

n

pt(i; ) = 1 Zt pt

-1 (j; )e-L (j,t-1) p(i|j; ) (3)

j=1

where Zt normalizes the distribution. While we have made various probabilistic assumptions (e.g., conditional independence of expert predictions) in deriving the algorithm, the algorithms can be used in a context where no such statistical assumptions about the observation sequence or the experts are warranted. The performance guarantees we provide below for these algorithms do not require these assumptions. 2.1 Relative loss bounds The existing upper bound on the relative loss of the Fixed-share algorithm [8] is expressed in terms of the loss of the algorithm relative to the loss of the best k-partition of the observation sequence, where the best expert is assigned to each segment. We start by providing here a similar guarantee but characterizing the regret relative to the best Fixedshare algorithm, parameterized by , where  is chosen in hindsight after having seen the observation sequence. Our proof technique is different from [8] and gives rise to simple guarantees for a wider class of prediction methods, along with a lower bound on this regret.

1 where (·, ·) is the Kronecker delta.


Lemma 1 Let LT () = T t=1 L(pt ;

share algorithm on an arbitrary sequence of observations. Then for any ,:

LT () - LT () = - log E ^Q e(T -1)[D( )-D( )] ^ ^

, t),   [0, 1], be the cumulative loss of the Fixed-

(4)

Proof: The cumulative log-loss of the Bayesian algorithm can be expressed in terms of negative log-probability of all the observations: LT () = - log[ (s)p(s; )] (5)

where s = {i1, . . . , iT }, (s) = Consequently, LT () - LT ()

= s r

T t=1 e-L

s (it,t)

, and p(s; ) = p1(i1) T t=2 p(it|it -1 ; ).

=

=

-log -log -log

(s)p(s; ) (r)p(r; )

(s)p(s; )

r

(r)p(r; ) Q(s; )elog

= - log

s

p(s; ) Q(s; ) p (s; )

Q(s; )e(T

= - log

p(s; ) p(s; )

p(s;) p(s;)

s s

-1) (s)log (^   +(1-(s)) log ^ 1- 1- )

s

where Q(s; ) is the posterior probability over the choices of experts along the sequence, induced by the hindsight-optimal switching-rate , and (s) is the empirical fraction of ^ non-self-transitions in the selection sequence s. This can be rewritten as the expected value of  ^ under distribution Q. 2 We obtain upper and lower bounds on regret by optimizing Q in Q, the set of all distributions over   [0, 1], of the expression for regret. ^ 2.1.1 Upper bound

The upper bound follows from solving: maxQQ - log E^Q e(T

-1)[D( )-D( )] ^ ^

subject to the constraint that  has to be the hindsight-optimal switching-rate, i.e. that:

(C1)

d d (LT () - LT ())|

= = 0

Theorem 1 Let LT () = min LT() be the loss of the best Fixed-share algorithm chosen in hindsight. Then for any   [0, 1], LT()-LT ()  (T -1) D( ), where D( ) is the relative entropy between Bernoulli distributions defined by  and . The bound vanishes when  =  and does not depend directly on the number of experts. The dependence on n may appear indirectly through , however. While the regret appears proportional to T, this dependence vanishes for any reasonable learning algorithm that is guaranteed to find  such that D( )  O(1/T), as we will show in Section 3. Theorem 1 follows, as a special case, from an analogous result for algorithms based on arbitrary first-order Markov transition dynamics. In the general case, the regret bound D(· ·) is the relative entropy between discrete distributions. For brevity, we provide only is: (T - 1) maxi D(P(j|i, ) P (j|i, )), where ,  are now transition matrices, and

the proof of the scalar case of Theorem 1. Proof: Constraint (C1) can be expressed simply as

alent to E ^Q ^

d d LT ()| = = 0, which is equiv-

results in the upper bound. 2 {} = . Taking the expectation outside the logarithm, in Equation 4,


2.1.2 Lower bound The relative losses obviously satisfy LT()-LT ()  0 providing a trivial lower bound. Any non-trivial lower bound on the regret cannot be expressed only in terms of  and , but needs to incorporate some additional information about the losses along the observation sequence. We express the lower bound on the regret as a function of the relative quality  of the minimum :

 = (1 - ) d2 d2 LT ( )|

= (6)

where the normalization guarantees that   1.   0 for any  that minimizes LT (). T - 1

The lower bound is found by solving: minQ

Q d2

d2

subject to both constraint (C1) and (C2)

-log E

^Q

e(T -1)[D( )-D( )] ^

= (LT () - LT ())| =

^ (T -1) (1-)

Theorem 2 Let  and  be defined as above based on an arbitrary observation se-

quence, and q1 = [1 +

T -1 1- -1

]

1- 

and q0 = [1 +

T -1  -1

] . Then

1- 1-

LT () - LT ()  - log E ^Q e(T -1)[D( )-D( )] ^ ^ (7)

where Q(1) = q1 and Q(( - q1)/(1 - q1)) = 1 - q1 whenever   ; Q(0) = q0 and Q(/(1 - q0)) = 1 - q0 otherwise. Proof omitted due to space constraints. The upper and lower bounds agree for all ,   (0, 1) when   1. Thus there may exist observation sequences on which Fixedshare, using  = , must incur regret linear in T. 2.2 Algorithm Learn- We now give an algorithm to learn the switching-rate simultaneously to updating the probability weighting over the experts. Since the cumulative loss Lt() of each Fixedshare algorithm running with switching parameter  can be interpreted as a negative log-probability, the posterior distribution over the switching-rate becomes

pt() = P (|yt -1 , . . . , y1)  e-Lt-1( ) (8)

include the observation at the same time point. We can view this algorithm as finding assuming a uniform prior over   [0, 1]. As a predictive distribution pt() does not the single best "-expert," where the collection of -experts is given by Fixed-share algorithms running with different switching-rates, . We will consider a finite resolution version of this algorithm, allowing only m possible chosen values {j}, we expect to be able to always find j   and suffer only a minimal choices for the switching-rate, {1, . . . , m}. For a sufficiently large m and appropriately additional loss due to not being able to represent the hindsight-optimal value exactly. Let pt,j(i) be the distribution over experts defined by the jth Fixed-share algorithm corresponding to j, and let ptop(j) be the top-level algorithm producing a weighting over

t

such Fixed-share experts. The top-level algorithm is given by

ptop(j) = t 1 Zt ptop (j)e-L (pt-1,j ,t-1)

t-1 (9)

where ptop(j) = 1/m, and the loss per time-step becomes 1

Ltop(ptop, t) = - log t

m m

ptop(j)e-L (pt,j ,t) t = - log

n

ptop(j)pt,j(i)e-L (i,t) t (10)

j=1 j=1 i=1

as is appropriate for a hierarchical Bayesian method.


3 Relative loss and optimal discretization We derive here the optimal choice of the discrete set {1, . . . , m} on the basis of the upper bound on relative loss. We begin by extending Theorem 1 to provide an analogous guarantee for the Learn- algorithm. Corollary to Theorem 1 Let Ltop be the cumulative loss of the hierarchical Learn-

T

algorithm using {1, . . . , m}. Then Ltop - LT ()  log(m) + (T - 1)

T min

j=1,...,m

D( j) (11)

The hierarchical algorithm involves two competing goals that manifest themselves in the regret: 1) the ability to identify the best Fixed-share expert, which degrades for larger m, and 2) the ability to find j whose loss is close to the optimal  for that sequence, which improves for larger m. The additional regret arising from having to consider a number of non-optimal values of the parameter, in the search, comes from the relative loss bound for the Static-Expert algorithm, i.e. the relative loss associated with tracking the best single expert [8, 12]. This regret is simply log(m) in our context. More precisely, the corollary follows directly from successive application of that single expert relative loss bound, and then our Fixed-share relative loss bound (Theorem 1):

Ltop - LT () min  log(m) + (T - 1) T  log(m) +

j=1,...,m

LT (j) min

j=1,...,m

D( j)

(12) (13)

3.1 Optimal discretization We start by finding the smallest discrete set of switching-rate parameters so that any additional regret due to discretization does not exceed (T - 1), for some threshold . In other

words, we find m = m() values 1, . . . , m ()

max min [0,1] j=1,...,m()

such that

D( j) =  (14)

The resulting discretization, a function of , can be found algorithmically as follows. First,

we set 1 so that max [0,1]

Each subsequent j is found conditionally on j

max [j-1,j ] min{D( j -1

D( 1) = D(0 1) =  implying that 1 = 1 - e-.

-1 so that

), D( j)} =  (15)

The maximizing  can be solved explicitly by equating the two relative entropies giving

 = log( 11--

which lies within [j

-1

j -1 -1 j

log( j

-1

) 1 - j

-1

) (16)

j

1 - j

, j] and is an increasing function of the new point j. Substituting

this  back into one of the relative entropies we can set j so that D( j

-1 ) = . The

relative entropy is an increasing function of j (through ) and the solution is obtained easily via, e.g., bisection search. The iterative procedure of generating new values j can be stopped after the new point exceeds 1/2; the remaining levels can be filled-in by symmetry so long as we also include 1/2. The resulting discretization is not uniform but denser towards the edges; the spacing around the edges is O(), and O() around 1/2. For small values of , the logarithm of the number of resulting discretization levels, or log m(), closely approximates -1/2 log . We can then optimize Tthe regret bound (11): uniform discretization would not, however, possess the same regret guarantee, resulting in

-12Tlog + (T - 1), yielding  = 1/(2T), and m() = / 2 . Thus we will need

O( ) settings of , as in the case of choosing the levels uniformly with spacing a higher than necessary loss due to discretization.

.

The


3.1.1 Optimized regret bound for Learn- The optimized regret bound for Learn-() is thus (approximately)

1 2

log T +c, which is

comparable to analysis of universal coding for word-length T [11]. The optimal discretization for learning the parameter is not affected by n, the number of original experts. Unlike regret bounds for Fixed-share, the value of the bound does not depend on the observation sequence. And notably, in comparison to the lower bound on Fixed-share's performance, Learn-'s regret is at most logarithmic in T. 4 Application to wireless networks We applied the Learn- algorithm to an open problem in computer networks: managing the tradeoff between energy consumption and performance in wireless nodes of the IEEE 802.11 standard [9]. Since a node cannot receive packets while asleep, yet maintaining the awake state drains energy, the existing standard uses a fixed polling time at which a node should wake from the sleep state to poll its neighbors for buffered packets. Polling at fixed intervals however, does not respond optimally to current network activity. This problem is clearly an appropriate application for an online learning algorithm, such as Fixed-share due to [8]. Since we are concerned with wireless, mobile nodes, there is no principled way to set the switching-rate parameter a priori, as network activity varies not only over time, but across location, and the location of the mobile node is allowed to change. We can therefore expect an additional benefit from learning the switching-rate. Previous work includes Krashinsky and Balakrishnan's [10] Bounded Slowdown algorithm which uses an adaptive control loop to change polling time based on network conditions. This algorithm uses parameterized exploration intervals, and the tradeoff is not managed optimally. Steinbach applied reinforcement learning [13] to this problem, yet required an unrealistic assumption: that network activity possesses the Markov property. We instantiate the experts as deterministic algorithms assuming constant polling times. Thus we use n experts, each corresponding to a different but fixed polling time in milliseconds (ms): Ti : i  {1 . . .n} The experts form a discretization over the range of possible polling times. We then apply the Learn- algorithm exactly as in our previous exposition, using the discretization defined by , and thus running m() sub-algorithms, each running Fixed-share with a different j. In this application, the learning algorithm can only receive observations, and perform learning updates, when it is awake. So our subscript t here signifies only wake times, not every time epoch at which bytes might arrive. We define the loss function, L, to reflect the tradeoff inherent in the conflicting goals of minimizing both the energy usage of the node, and the network latency it introduces by sleeping. We propose a loss function that is one of many functions proportional to this tradeoff. We define loss per expert i as:

Loss(i, t) =  I2 t

Ti 2

+ (17)

Tt

1 Ti

where It is the observation the node receives, of how many bytes arrived upon awakening at time t, and Tt is the length of time that the node just slept. The first term models the average latency introduced into the network by buffering those bytes, and scales It to the number of bytes that would have arrived had the node slept for time Ti instead of Tt, under the assumption that the bytes arrived at a uniform rate. The second term models the energy consumption of the node, based on the design that the node wakes only after an interval Tt to poll for buffered bytes, and the fact that it consumes less energy when asleep than awake. The objective function is a sum of convex functions and thus admits a unique minimum.  > 0 allows for scaling between the units of information and time, and the ability to encode a preference for the ratio between energy and latency that the user favors.


12000 1150

arbitrary expert (500ms) 1100

10000

1050 (n=10)

8000 alg

1000 loss Fixed-share(alpha) alg

6000 Learn-alpha

of 950

Cumulative loss

4000 best expert (100ms) IEE 802.11 Protocol alg

900

Cumulative

2000

Static-expert alg Learn-alpha(delta*) 850

0 800 0 0.1 0.2 0.3 0.4

a) 0.5 alpha 0.6 0.7 0.8 0.9 1 0 2 4 6 8 10 12

14

4

x 10

c) 1/delta

3500 1280

best expert (100ms) IEE 802.11 Protocol alg

1260

3000

1240

(n=5) 1220

2500 alg

1200

loss

2000 Learn-alpha 1180 Fixed-share(alpha) alg

of

Cumulative Static-expert alg loss 1160

1500

1140

Cumulative

1120

1000

Learn-alpha(delta*)

1100

500 1080 0 0.2 0.4 0.6 0.8

b) 1 alpha 1.2 1.4 1.6 1.8

2

-3

x 10

0 2 4 6 8 10 12 14

4

x 10

d) 1/delta

Figure 1: a) Cumulative loss of Fixed-share() as a function of , compared to the cumulative loss on the same trace of the 802.11 protocol, Static-expert, and Learn(). Figure b) zooms in on the first 0.002 of the  range. c) Cumulative loss of Learn(), as a function of 1/, when n = 10, and b) n = 5. Circles at 1/ = 2T .

4.0.2 Experiments We used traces of real network activity from [2], a UC Berkeley home dial-up server that monitored users accessing HTTP files from home. Multiple overlapping connections, passing through a collection node over several days, were recorded by start and end times, and number of bytes transferred. Per connection we smoothed the total number of bytes uniattain polling times within the range of the existing protocol. formly over 10ms intervals spanning its duration. We set  = 1.0 × 10-7, calibrated to Figure 1a) and b) compare cumulative loss of the various algorithms on a 4 hour trace, with observation epochs every 10ms. This corresponds to approximately 26,100 training of learning iterations, i.e. the time horizen parameter to the loss bounds, is just the number iterations for the learning algorithms. In the typical online learning scenario, T , the number of observation epochs. In this application, the number of training epochs need not match the number of observation epochs, since the application involves sleeping during many observation epochs, and learning is only done upon awakening. Since in these experiments the performance of the three learning algorithms are compared by each algorithm using n experts spanning the range of 1000ms at regularly spaced intervals of 100ms, to obtain a prior estimate of T , we assume a mean sleep interval of 550ms, the mean of the experts. The Static-expert algorithm achieved lower cumulative loss than the best expert, since it can attain the optimal smoothed value over the desired range of polling times, whereas the expert values just form a discretization. On this trace, the optimal  for Fixed-share turns out to be extremely low. So for most settings of , one would be better off using a Static-expert model, yet as the second graph shows, there is a value of  below which it is beneficial to use Fixed-share. This lends validity to our fundamental goal of being able to quantify the level of non-stationarity of a process, in order


to better model it. Moreover there is a clear advantage to using Learn-, since without prior knowledge of the stochastic process to be observed, there is no optimal way to set . Figure 1c) and d) show the cumulative loss of Learn- as a function of 1/. We see that reduce cumulative loss by decreasing . As expected, the performance of the algorithm levels off after the optimal  that we can compute a priori. Our results also verify that the optimal  is not significantly affected by the number of experts n. 5 Conclusion We proved upper and lower bounds on the regret for a class of online learning algorithms, applicable to any sequence of observations. The bounds extend to richer models of nonstationary sequences, allowing the switching dynamics to be governed by an arbitrary transition matrix. We derived the regret-optimal discretization (including the overall resolution) for learning the switching-rate parameter in a simple switching dynamics, yielding an algorithm with stronger guarantees than previous algorithms. We exemplified the approach in the context of energy management in wireless networks. In future work, we hope to extend the online estimation of  and the optimal discretization to learning a full transition matrix. References [1] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. Gambling in a rigged casino: the adversarial multi-armed bandit problem. In Proc. of the 36th Annual Symposium on Foundations of Computer Science, pages 322­331, 1995. [2] Berkeley. UC Berkeley home IP web traces. In http://ita.ee.lbl.gov/html/contrib/UCB.homeIP-HTTP.html, 1996. [3] A. Blum, C. Burch, and A. Kalai. Finely-competitive paging. In IEEE 40th Annual Symposium on Foundations of Computer Science, page 450, New York, New York, October 1999. [4] D. P. Foster and R. Vohra. Regret in the on-line decision problem. Games and Economic Behavior, 29:7­35, 1999. [5] Y. Freund and R. Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29:79­103, 1999. [6] D. Haussler, J. Kivinen, and M. K. Warmuth. Sequential prediction of individual sequences under general loss functions. IEEE Trans. on Information Theory, 44(5):1906­1925, 1998. [7] D. P. Helmbold, R. E. Schapire, Y. Singer, and M. K. Warmuth. On-line portfolio selection using multiplicative updates. In International Conference on Machine Learning, pages 243­ 251, 1996. [8] M. Herbster and M. K. Warmuth. Tracking the best expert. Machine Learning, 32:151­178, 1998. [9] IEEE. Computer society LAN MAN standards committee. In IEEE Std 802.11: Wireless LAN Medium Access Control and Physical Layer Specifications, August 1999. [10] R. Krashinsky and H. Balakrishnan. Minimizing energy for wireless web access with bounded slowdown. In MobiCom 2002, Atlanta, GA, September 2002. [11] R. Krichevsky and V. Trofimov. The performance of universal encoding. IEEE Trans. on Information Theory, 27(2):199­207, 1981. [12] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. In IEEE Symposium on Foundations of Computer Science, pages 256­261, 1989. [13] C. Steinbach. A reinforcement-learning approach to power management. In AI Technical Report, M.Eng Thesis, Artificial Intelligence Laboratory, MIT, May 2002. [14] V. Vovk. Derandomizing stochastic prediction strategies. Machine Learning, 35:247­282, 1999.

choosing  = 1 2T , matches the point in the curve beyond which one cannot significantly


