Minimax embeddings

Matthew Brand Mitsubishi Electric Research Labs Cambridge MA 02139 USA Abstract Spectral methods for nonlinear dimensionality reduction (NLDR) impose a neighborhood graph on point data and compute eigenfunctions of a quadratic form generated from the graph. We introduce a more general and more robust formulation of NLDR based on the singular value decomposition (SVD). In this framework, most spectral NLDR principles can be recovered by taking a subset of the constraints in a quadratic form built from local nullspaces on the manifold. The minimax formulation also opens up an interesting class of methods in which the graph is "decorated" with information at the vertices, offering discrete or continuous maps, reduced computational complexity, and immunity to some solution instabilities of eigenfunction approaches. Apropos, we show almost all NLDR methods based on eigenvalue decompositions (EVD) have a solution instability that increases faster than problem size. This pathology can be observed (and corrected via the minimax formulation) in problems as small as N < 100 points.

1 Nonlinear dimensionality reduction (NLDR) Spectral NLDR methods are graph embedding problems where a set of N points X = . reparameterized by imposing a neighborhood graph G on X and embedding the graph with minimal distortion in a "parameterization" space R , d < D. Typically the graph is sparse

[x1,··· ,xN]  R D×N sampled from a low-dimensional manifold in a ambient space R is D

d

and local, with edges connecting points to their immediate neighbors. The embedding must keep these edges short or preserve their length (for isometry) or angles (for conformality). The graph-embedding problem was first introduced as a least-squares problem by Tutte [1], and as an eigenvalue problem by Fiedler [2]. The use of sparse graphs to generate metrics for least-squares problems has been studied intensely in the following three decades (see [3]). Modern NLDR methods use graph constraints to generate a metric in a space of embeddings R . Eigenvalue decomposition (EVD) gives the directions of least or greatest variance

N

under this metric. Typically a subset of d extremal eigenvectors gives the embedding of N points in R parameterization space. This includes the IsoMap family [4], the locally linear d

embedding (LLE) family [5,6], and Laplacian methods [7,8]. Using similar methods, the Automatic Alignment [6] and Charting [9] algorithms embed local subspaces instead of

points, and by combining subspace projections thus obtain continuous maps between R and R . d

This paper introduces a general algebraic framework for computing optimal embeddings directly from graph constraints. The aforementioned methods can can be recovered as special cases. The framework also suggests some new methods with very attractive properties, including continuous maps, reduced computational complexity, and control over the degree

D


of conformality/isometry in the desired map. It also eliminates a solution instability that is intrinsic to EVD-based approaches. A perturbational analysis quantifies the instability. 2 Minimax theorem for graph embeddings We begin with neighborhood graph specified by a nondiagonal weighted adjacency matrix X in practice). The graph-embedding and NLDR literatures offer various constructions of M, each appropriate to different sets of assumptions about the original embedding and its sampling X (e.g., isometry, local linearity, noiseless samples, regular sampling, etc.). Typically Mij = 0 if points i, j are nearby on the intrinsic manifold and |Mij| is small or zero otherwise. Each point is taken to be a linear or convex combination of its neighbors, and thus M specifies manifold connectivity in the sense that any nondegenerate embedding and the structure of local neighborhoods. For example, in barycentric embeddings, each point is the average of its neighbors and thus Mij = 1/k if vertex i is connected to vertex j (of degree k). We will also consider three optional constraints on the embedding : 1. A null-space restriction, where the solution must be outside to the column-space be centered, i.e., YC = 0 for C = 1, the constant vector. 2. A basis restriction, where the solution must be a linear combination of the rows vertices of the graph that serves as example inputs for a target NLDR function. We will use this to construct dimension-reducing radial basis function networks. example, it might be important that boundary points have less error. We assume that  is symmetric positive definite and has factorization  = AA (e.g., A could be a Cholesky factor of ). In most settings, the optional matrices will default to the identity matrix. In this context, we define the per-dimension embedding error of row-vector yi  rows(Y) to be

M  R N×N that has the data-reproducing property XM = X (this can be relaxed to XM 

Y that satisfies YM  Y with small residual YM - Y F will preserve this connectivity

of C  R N×M , M < N. For example, it is common to stipulate that the solution Y

of basis Z  R K×N , K  N. This can be thought of as information placed at the

3. A metric   R N×N that determines how error is distributed over the points. For

. EM(yi) = max

yirange(Z),,KRM×N

(yi(M+CD)-yi)A yiA (1)

where D is a matrix constructed by an adversary to maximize the error. The optimizing yi is a vector inside the subspace spanned by the rows of Z and outside the subspace spanned by the columns of C, for which the reconstruction residual yiM-yi has smallest norm w.r.t. the metric . The following theorem identifies the optimal embedding Y for any choice of M,Z,C,:

Minimax solution: Let Q  S

K×P be a column-orthonormal basis of the null-space of the

rows of ZC, with P = K - rank(C). Let B  R

P×P

be a square factor satisfying B B =

Q ZZ Q, e.g., a Cholesky factor (or the "R" factor in QR-decomposition of (Q ZA) ). singular values s = [s1,··· ,sP] ordered s1  s2  ···  sp. Using the leading columns U1:d of U, set Y = U1:dB- Q Z. Theorem 1. Y is the optimal (minimax) embedding in R with error [s1,··· ,sd] :

Compute the left singular vectors U  S . N×N of Udiag(s)V = B- Q Z(I - M)A, with

d 2

EM(yi)2 with EM(yi) = si. Y = U1:dB- Q Z = arg min . YRd×N 

yirows(Y)

(2)


Appendix A develops the proof and other error measures that are minimized. Local NLDR techniques are easily expressed in this framework. When Z = A = I, C = [], and M reproduces X through linear combinations with M 1 = 1, we recover LLE [5]. When Z = I, C = [], I-M is the normalized graph Laplacian, and A is a diagonal matrix of vertex degrees, we recover Laplacian eigenmaps [7]. When further Z = X we recover locally preserving projections [8]. 3 Analysis and generalization of charting The minimax construction of charting [9] takes some development, but offers an interesting insight into the above-mentioned methods. Recall that charting first solves for a set the data and vary smoothly over the manifold. Each subspace offers a chart--a local parameterization of the data by projection onto the local axes. Charting then constructs a weighted mixture of affine projections that merges the charts into a global parameterization. If the data manifold is curved, each projection will assign a point a slightly different embedding, so the error is measured as the variance of these proposed embeddings about their mean. This maximizes consistency and tends to produce isometric embeddings; [9] discusses ways to explicitly optimize the isometry of the embedding. Under the assumption of isometry, the charting error is equivalent to the sumsquared displacements of an embedded point relative to its immediate neighbors (summed over all neighborhoods). To construct the same error criteria in the minimax setting, let xi-k,··· ,xi,··· ,xi+k denote points in the ith neighborhood and let rameterization Si [xi-k,··· ,xi,··· ,xi+k]. Then a nonzero reparameterization will satisfy [yi-k,··· ,yi,··· ,yi+k]ViVi = [yi-k,··· ,yi,··· ,yi+k] if and only if it preserves the relative position of the points in the local parameterization. Conversely, any relative displacements of the points are isolated by the formula [yi-k,··· ,yi,··· ,yi+k](I - ViVi ). Minimizing the Frobenius norm of this expression is thus equivalent to minimizing the local error in charting. We sum these constraints over all neighborhoods to obtain the constraint matrix M = I-i Fi(I-ViVi )Fi , where (Fi)k = 1 iff the jth point of the ith neighborhood is

of local affine subspace axes S1  R D×d ,S2,··· at offsets µ1  R ,µ2,··· that best cover D

the columns of Vi  R (2k+1)×d be an orthonormal basis of rows of the local pa-

j

the kth point of the dataset. Because ViVi and (I - ViVi ) are complementary, it follows that the error criterion of any local NLDR method (e.g., LLE, Laplacian eigenmaps, etc.) must measure the projection of the embedding onto some subspace of (I-ViVi ). To construct a continuous map, charting uses an overcomplete radial basis function (RBF) representation Z = [z(x1),z(x2),···z(xN)], where z(x) is a vector that stacks z1(x), z2(x), etc., and

. zm(x) = Km(x-µm) 1 pm(x) m pm(x) ,

-1(x-µm)/2 m pm(x) = N (x|µm,m)  e-(x-µm) .

(3) (4)

and Km is any local linear dimensionality reducer, typically Sm itself. Each column of Z contains many "views" of the same point that are combined to give its low-dimensional embedding. Finally, we set C = 1, which forces the embedding of the full data to be centered. Applying the minimax solution to these constraints yields the RBF network mixing matrix, f(x) = U1:dB- Q z(x). Theorem 1 guarantees that the resulting embedding is least. squares optimal w.r.t. Z,M,C,A at the datapoints f(xi), and because f(·) is an affine transform of z(·) it smoothly interpolates the embedding between points. There are some interesting variants:


Kernel embeddings of the twisted swiss roll generalized EVD minimax SVD

UR corner detail

LL corner detail

Fig. 1. Minimax and generalized EVD solution for kernel eigenmap of a non-developable swiss roll. Points are connected into a grid which ideally should be regular. The EVD solution shows substantial degradation. Insets detail corners where the EVD solution crosses itself repeatedly. The border compression is characteristic of Laplacian constraints. One-shot charting: If we set the local dimensionality reducers to the identity matrix (all Km = I), then the minimax method jointly optimizes the local dimensionality reduction to charts and the global coordination of the charts (under any choice of M). This requires that rows(Z)  N for a fully determined solution. Discrete isometric charting: If Z = I then we directly obtain a discrete isometric embedding of the data, rather than a continuous map, making this a local equivalent of IsoMap. Reduced basis charting: Let Z be constructed using just a small number of kernels ran-

domly placed on the data manifold, such that rows(Z) problem is substantially reduced. 4 Numerical advantage of minimax method N. Then the size of the SVD

Note that the minimax method projects the constraint matrix M into a subspace derived from C and Z and decomposes it there. This suppresses unwanted degrees of freedom (DOFs) admitted by the problem constraints, for example the trivial R embedding where

0

all points are mapped to a single point yi = N-1/2. The R embedding serves as a trans-

0

lational DOF in the solution. LLE- and eigenmap-based methods construct M to have a constant null-space so that the translational DOF will be isolated in the EVD as null eigenvalue paired to a constant eigenvector, which is then discarded. However, section 4.1 shows that this construction makes the EVD increasingly unstable as problem size grows and/or the data becomes increasing amenable to low-residual embeddings, ultimately causing solution collapse. As the next paragraph demonstrates, the problem is exacerbated when embedding w.r.t. a basis Z (via the equivalent generalized eigenproblem), partly because the eigenvector associated with the unwanted DOF can have arbitrary structure. In all cases the problem can be averted by using the minimax formulation with C = 1 to suppress the DOF. A 2D plane was embedded in 3D with a curl, a twist, and 2.5% Gaussian noise, then regularly sampled at 900 points. We computed a kernelized Laplacian eigenmap using 70 random points as RBF centers, i.e., a continous map using M derived from the graph Laplacian and Z constructed as above. The map was computed both via the minimax (SVD) method and via the equivalent generalized eigenproblem, where the translational degree of freedom must be removed by discarding an eigenvector from the solution. The two solutions are algebraically equivalent in every other regard. A variety of eigensolvers were tried; we took


x 10-5 Eigen spectrum compared to minimax spectrum

energy

excess

15 10 5 0

100 eigenvalue 200

x 10-5 Eigen spectrum compared to minimax spectrum x 10-5

Error in null embedding

energy

excess

15 10 5 0

deviation

2 0 -2 -4 -6 -8

100 200 100 200 300 400 500 600 700 800 900

eigenvalue point

Fig. 2.x Excess energy in the eigenspectrum indicates that the translational DOF has contaminated many eigenvectors. If the EVD had successfully isolated the unwanted DOF, then its

2 0

remaining eigenvalues should be identical to those derived from the minimax solution. The

-2

graph at left shows the difference in the eigenspectra. The graph at right shows the EVD solution's deviation from the translational vector y0 = 1 · N-1/2  .03333. If the numer(roughly 1% of the diameter of the embedding) to noticably perturb points in figure 1. the best result. Figure 1 shows that the EVD solution exhibits many defects, particularly a folding-over of the manifold at the top and bottom edges and at the corners. Figure 2 shows that the noisiness of the EVD solution is due largely to mutual contamination of numerically unstable eigenvectors. 4.1 Numerical instability of eigen-methods The following theorem uses tools of matrix perturbation theory to show that as the problem size increases, the desired and unwanted eigenvectors become increasingly wobbly and gradually contaminate each other, leading to degraded solutions. More precisely, the low-order eigenvalues are ill-conditioned and exhibit multiplicities that may be true (due to noiseless samples from low-curvature manifolds) or false (due to numerical noise). Although in many cases some post-hoc algebra can "filter" the unwanted components out of the contaminated eigensolution, it is not hard to construct cases where the eigenvectors cannot be cleanly separated. The minimax formulation is immune to this problem because it explicitly suppresses the gratuitous component(s) before matrix decomposition. Theorem 2. For any finite numerical precision, as the number of points N increases, the Frobenius norm of numerical noise in the null eigenvector v0 can grow as O(N3/2), and the eigenvalue problem can approach a false multiplicity at a rate as fast as O(N3/2), at which point the eigenvectors of interest--embedding and translational--are mutually contaminated and/or have an indeterminate eigenvalue ordering. Please see appendix B for the proof. This theorem essentially lower-bounds an upperbound on error; examples can be constructed in which the problem is worse. For example, it can be shown analytically that when embedding points drawn from the simple curve xi = [a,cosa] , a  [0,1] with K = 2 neighbors, instabilities cannot be bounded better than O(N5/2); empirically we see eigenvector mixing with N < 100 points and we see it grow at the rate  O(N4)--in many different eigensolvers. At very large scales, more pernicious instabilities set in. E.g., by N = 20000 points, the solution begins to fold over. Although algebraic multiplicity and instability of the eigenproblem is conceptually a minor oversight in the algorithmic realizations of eigenfunction embeddings, as theorem 2 shows, the consequences are eventually fatal. 5 Summary One of the most appealing aspects of the spectral NLDR literature is that algorithms are usually motivated from analyses of linear operators on smooth differentiable manifolds, e.g., [7]. Understandably, these analysis rely on assumptions (e.g., smoothness or isometry

-4

deviation

-6 -8

ics were100 perfect the line would be700flat,800but900in practice the deviation is significant enough 200 300 400 500 600 point

10-5

Error in null embedding


or noiseless sampling) that make it difficult to predict what algorithmic realizations will do when real, noisy data violates these assumptions. The minimax embedding theorem provides a complete algebraic characterization of this discrete NLDR problem, and provides a solution that recovers numerically robustified versions of almost all known algorithms. It offers a principled way of constructing new algorithms with clear optimality properties and good numerical conditioning--notably the construction of a continuous NLDR map (an RBF network) in a one-shot optimization (SVD). We have also shown how to cast several local NLDR principles in this framework, and upgrade these methods to give continuous maps. Working in the opposite direction, we sketched the minimax formulation of isometric charting and showed that its constraint matrix contains a superset of all the algebraic constraints used in local NLDR techniques. References 1. W.T. Tutte. How to draw a graph. Proc. London Mathematical Society, 13:743­768, 1963. 2. Miroslav Fiedler. A property of eigenvectors of nonnegative symmetric matrices and its application to graph theory. Czech. Math. Journal, 25:619­633, 1975. 3. Fan R.K. Chung. Spectral graph theory, volume 92 of CBMS Regional Conference Series in Mathematics. American Mathematical Society, 1997. 4. Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290:2319­2323, December 22 2000. 5. Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323­2326, December 22 2000. 6. Yee Whye Teh and Sam T. Roweis. Automatic alignment of hidden representations. In Proc. NIPS-15, 2003. 7. Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. volume 14 of Advances in Neural Information Processing Systems, 2002. 8. Xiafei He and Partha Niyogi. Locality preserving projections. Technical Report TR-2002-09, University of Chicago Computer Science, October 2002. 9. Matthew Brand. Charting a manifold. volume 15 of Advances in Neural Information Processing Systems, 2003. 10. G.W. Stewart and Ji-Guang Sun. Matrix perturbation theory. Academic Press, 1990. A Proof of minimax embedding theorem (1) The burden of this proof is carried by supporting lemmas, below. To emphasize the proof strategy, we give the proof first; supporting lemmas follow. Proof. Setting yi = li Z, we will solve for li  columns(L). Writing the error in terms of li,

EM(li) = max KRM×N li Z(I-M-CK)A li ZA = max

KRM×N

li Z(I-M)A-li ZCKA li ZA . (5)

The term li ZCKA produces infinite error unless li ZC = 0, so we accept this as a constraint and seek min . (6) By lemma 1, that orthogonality is satisfied by solving the.problem in the space orthogonal to ZC; the basis for this space is given by columns of Q = null((ZC) ). By lemma 2, the denominator of the error specifies the metric in solution space to be ZAA Z ; when the problem is projected into the space orthogonal to ZC it becomes Q (ZAA Z )Q. Nesting the "orthogonally-constrained-SVD" construction of lemma 1

li ZC=0

li Z(I-M)A li ZA


inside the "SVD-under-a-metric" lemma 2, we obtain a solution that uses the correct metric in the orthogonal space:

B B = Q ZAA Z Q Udiag(s)V = B- {Q(Z(I-M)A)} L = QB-1U (7) (8) (9)

where braces indicate the nesting of lemmas. By the "best-projection" lemma (#3), if we order the singular values by ascending magnitude,

L1:d = arg min jicols(J)( j Z(I-M)A / j

JRN×d ZZ )2

The proof is completed by making the substitutions L Z  Y and x A  x  = AA ), and leaving off the final square root operation to obtain

2

(Y )1:d = arg min jicols(J) j (I-M)

JRN×d  / j  .



(10) (for

(11)

Lemma 1. Orthogonally constrained SVD: The left singular vectors L of matrix M under the constraint U C = 0 are calculated as Q = null(C ), Udiag(s)V  Q M, L = QU. Proof. First observe that L is orthogonal to C: By definition, the null-space basis satisfies Q C = 0, thus L C = U Q C = 0. Let J be an orthonormal basis for C, with J J = I and Q J = 0. Then Ldiag(s)V = QQ M = (I - JJ )M, the orthogonal projector of C applied to M, proving that the SVD captures the component of M that is orthogonal to C. Lemma 2. SVD with respect to a metric: The vectors li  L, vi  V that diagonalize matrix M with respect to positive definite column-space metric  are calculated as B B  , .

SVD

Udiag(s)V  B- M, L = B-1U satisfy li M / li

SVD

.

 = si and extremize this form for

the extremal singular values smin,smax. Proof. By construction, L and V diagonalize M: L MV = (B-1U) MV = U (B- M)V = diag(s)

(12)

and diag(s)V = B- M. Forming the gram matrices of both sides of the last line, we obtain the identity Vdiag(s)2V = M B-1B- M = M -1M, which demonstrates that si  s are the singular values of M w.r.t. column-space metric . Finally, L is orthonormal

w.r.t. the metric , because L 2  = L L = U B- B BB-1U = I. Consequently,

l M / l  = l M /1 = sivi = si . (13)

and by the Courant-Hilbert theorem, smax = max l M / l ;



l

smin = min l M / l .  l (14)

Lemma 3. Best projection: Taking L and s from lemma 2, let the columns of L and elements of s be sorted so that s1  s2  ···  sN. Then for any dimensionality 1  d  N,

. L1:d = [l1,··· ,ld] = arg max J M (J J)-1

J M F

JRN×d

= arg max JRN×d|J J=I

= arg max JRN×d jicols(J)( j M / j )2

(15) (16) (17)

with the optimum value of all right hand sides being (di=1 s2i )1/2. If the sort order is reversed, the minimum of this form is obtained.


Proof. By the Eckart-Young-Mirsky theorem, if U MV = diag(s) with singular values

.

sorted in descending order, then U1:d = [u1,··· ,ud] = argmaxUS extend this to a non-orthonogonal basis J under a Mahalonobis norm: N×d

maxJR

2 (J J)-1

N×d J M (J J)-1 = maxUS N×d U M F

U M . We first (18) F

because (JJ+)M

J M = trace(M J(J J)-1J M) = trace(M JJ+(JJ+) M) =

2 F = U M 2 F since JJ+ is a (symmetric) orthogonal pro2 F = UU M

jector having binary eigenvalues   {0,1} and therefore it is the gram of an thin orthogonal matrix. We then impose a metric  on the column-space of J to obtain the first criterion (equation 15), which asks what maximizes variance in J M while minimizing the norm of J w.r.t. metric . Here it suffices to substitute in the leading (resp., trailing) columns of L and verify that the norm is maximized (resp., minitrace((L1:dM) I(L1:dM)) = trace((diag(s1:d)V1:d) (diag(s1:d)V1:d)) = s1:d . Again,

mized). Expanding, L1:dM 2 (L1 L1 )-1

:d :d

= trace((L1:dM) (L1:dL1:d)-1(L1:dM)) =

2

by the Eckart-Young-Mirsky theorem, these are the maximal variance-preserving projections, so the first criterion is indeed maximized by setting J to the columns in L corresponding to the largest values in s. Criterion #2 restates the first criterion with the set of candidates for J restricted to (the hyperelliptical manifold of) matrices that reduce the metric on the norm to the identity matrix (thereby recovering the Frobenius norm). Criterion #3 criterion merely expands the above trace by individual singular values. Note that the numerator and denominator can have different metrics because they are norms in different spaces, possibly of different dimension. Finally, that the trailing d eigenvectors minimize these criteria follows directly from the fact that leading N -d singular values account for the maximal part of the variance. B Proof of instability theorem (2) Proof. When generated from a sparse graph with average degree K, weighted connectivity matrix W is sparse and has O(NK) entries. Since the graph vertices represent samples from a smooth manifold, increasing the sampling density N does not change the distribution of magnitudes in W. Consider a perturbation of the nonzero values in W, e.g., W  W +E due to numerical noise E created by finite machine precision. By the weak law of large ever the tth-smallest nonzero eigenvalue t(W) grows as t(W) = vt Wvt  O(N-1), because elements of corresponding eigenvector vt grow as O(N-1/2) and only K of those elements are multiplied by nonzero values to form each element of Wvt. In sum, the perthe eigengap of interest is gap = 1 - 0. The tail eigenvalue 0 = 0 by construction but it is possible that 0 > 0 with numerical error, thus gap  1. Combining these facts, the ratio between the perturbation and the eigengap grows as E /gap  O(N3/2) or

numbers, the Frobenius norm of the sparse perturbation grows as E F   O( N). How-

turbation E F grows while the eigenvalue t(W) shrinks. In linear embedding algorithms, .

F

faster. Now consider the shifted eigenproblem I - W with leading (maximal) eigenvalues 1 - 0  1 - 1  ··· and unchanged eigenvectors. From matrix perturbation the. ory [10, thm. V.2.8], when W is perturbed to W = W + E, the change in the lead-

 ing eigenvalue from 1 - 0 to 1 - 0 is boundedas |0 - 0|  2 E



1 - 1  1 - 1 +

F and similarly

2 E . Thus gap  gap - 2 E . Since E /gap  O(N3/2),

F F F

the right hand side of the gap bound goes negative at a supralinear rate, implying that the eigenvalue ordering eventually becomes unstable with the possibility of the first and second eigenvalue/vector pairs being swapped. Mutual contamination of the eigenvectors happens well before: Under general (dense) conditions, the change in the eigenvector v0 is bounded as a good approximation.) Specializing this to the sparse embedding matrix, we find that

as v0 -v0  4 EF |0-1|- 2 E [10, thm. V.2.8]. (This bound is often tight enough to serve

F

the bound weakens to v0 -1·N-1/2 

 O( N) O(N-1)-O( N)

>

 O( N) O(N-1)

= O(N3/2).


