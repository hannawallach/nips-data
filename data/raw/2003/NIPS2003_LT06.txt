Boosting versus Covering

Kohei Hatano Tokyo Institute of Technology hatano@is.titech.ac.jp Manfred K. Warmuth UC Santa Cruz manfred@cse.ucsc.edu

Abstract We investigate improvements of AdaBoost that can exploit the fact that the weak hypotheses are one-sided, i.e. either all its positive (or negative) predictions are correct. In particular, for any set of m labeled examples consistent with a disjunction of k literals (which are one-sided in this case), AdaBoost constructs a consistent hypothesis by using O(k2 log m) iterations. On the other hand, a greedy set covering algorithm finds a consistent hypothesis of size O(k log m). Our primary question is whether there is a simple boosting algorithm that performs as well as the greedy set covering. We first show that InfoBoost, a modification of AdaBoost proposed by Aslam for a different purpose, does perform as well as the greedy set covering algorithm. We then show that AdaBoost requires (k2 log m) iterations for learning k-literal disjunctions. We achieve this with an adversary construction and as well as in simple experiments based on artificial data. Further we give a variant called SemiBoost that can handle the degenerate case when the given examples all have the same label. We conclude by showing that SemiBoost can be used to produce small conjunctions as well.

1 Introduction The boosting method has become a powerful paradigm of machine learning. In this method a highly accurate hypothesis is built by combining many "weak" hypotheses. AdaBoost [FS97, SS99] is the most common boosting algorithm. The protocol is as follows. We start with m labeled examples labeled with ±1. AdaBoost maintains a distribution over the examples. At each iteration t, the algorithm receives a ±1 valued weak hypothesis ht whose error (weighted by the current distribution on the a linear combination of the received weak hypotheses and it stops when this final hypothesis is consistent with all examples. It is well known [SS99] that if each weak hypothesis has weighted error at most

examples) is slightly smaller than

1 2

. It then updates its distribution so that after

the update, the hypothesis ht has weighted error exactly 1 2 . The final hypothesis is

1 2 - , then the upper bound on the training error reduces by a factor of

2





1 - 2

This research was done while K. Hatano was visiting UC Santa Cruz under the EAP

exchange program.


and after O(2 log m) iterations, the final hypothesis is consistent with all examples. Also, it has been shown that if the final hypotheses are restricted to (unweighted) majority votes of weak hypotheses [Fre95], then this upper bound on the number of iterations cannot be improved by more than a constant factor. However, if there always is a positively one-sided weak hypothesis (i.e. its positive rithm can be used to reduce the training error by a factor2 of 1 -  and O( log m) we show that the improved factor is also achieved by InfoBoost, a modification of AdaBoost developed by Aslam [Asl00] based on a different motivation. In particular, consider the problem of finding a consistent hypothesis for m examples labeled by a k literal disjunction. Assume we use the literals as the pool of weak hypotheses and always choose as the weak hypothesis a literal that is consistent with all negative examples. Then it can be shown that, for any distribution D on the examples, there exists a literal (or a constant hypothesis) h with weighted error

predictions are always correct) that has error1 at most 1 2 - , then a set cover algo2 

1

In this paper weak hypotheses suffice to form a consistent hypothesis [Nat91].

at most

1 2 - 1 4k

(See e.g. [MG92]). Therefore, the upper bound on the training error

of AdaBoost reduces by a factor of 1 - 1 4k2 and O(k2 log m) iterations suffice.

1

However, a trivial greedy set covering algorithm, that follows a strikingly similar protocol as the boosting algorithms, finds a consistent disjunction with O(k log m) literals. We show that InfoBoost mimics the set cover algorithm in this case (and attains the improved factor of 1 - ).

1 k

We first explain the InfoBoost algorithm in terms of constraints on the updated distribution. We then show that (k2 log m) iterations are really required by AdaBoost using both an explicit construction (which requires some assumptions) and artificial experiments. The differences are quite large: For m = 10, 000 random examples and a disjunction of size k = 60, AdaBoost requires 2400 iterations (on the average), whereas Covering and InfoBoost require 60 iterations. We then show that InfoBoost has the improved reduction factor if the weak hypotheses happen to be one-sided. Finally we give a modified version of AdaBoost that exploits the one-sidedness of the weak hypotheses and avoids some technical problems that can occur with InfoBoost. We also discuss how this algorithm can be used to construct small conjunctions. 2 Minimizing relative entropy subject to constraints Assume we are given a set of m examples (x1, y1), . . . , (xm, ym). The instances xi are in some domain X and the labels yi are in {-1, 1}. The boosting algorithms maintain a distribution Dt over the examples. The initial distribution is D1 and is typically uniform. At the t-th iteration, the algorithm chooses a weak3 hypothesis ht : X  {-1, 1} and then updates its distribution. The most popular boosting algorithm does this as follows:

AdaBoost: Dt +1 (i) = Dt(i) exp{-yiht(xi)t} , Zt

1 2 This assumes equal weight on both types of examples. Wipe out the weights of positive examples that are correctly classified and re-balance

both types of examples. weak hypotheses is ±1 valued. Many parts of this paper generalize to the range [-1, 1] [SS99, Asl00]. 3

For the sake of simplicity we focus on the case when the range of the labels and the


Here Zt is a normalization constant and the coefficient t depends on the error

at iteration t: t =

1 2

ln

1-

t

t

and t

t

= PrDt[ht(xi) = yi]. The final hypothesis is

given by the sign of the following linear combination of the chosen weak hypotheses: distributions of boosting algorithms as a constraint minimization of the relative entropy between the new and old distributions:

H(x) = T t=1 tht(x). Following [KW99, Laf99], we motivate the updates on the

AdaBoost: Dt +1 = argminD [0,1]m, (D, Dt), s.t. Pr[ht(xi) = yi] = i D(i)=1

Here the relative entropy is defined as (D, D ) = the updated distribution is constraint to half. i D(i) ln

D D(i) D (i)

1 2.

and error w.r.t.

The constraint can be easily understood using the table of Figure 1. There are two types of misclassified examples: false positive (weight c) and false negative (weight b). The

AdaBoost constraint means b + c =

distribution Dt +1 .

1 2

yi \ ht +1 -1 +1 a c -1 b d

w.r.t. the updated Figure 1: Four types of examples.

The second boosting algorithm we discuss in this paper has the following update:

InfoBoost: Dt +1

where t[±1] = 1 2

1- [±1]

t

[±1] ln ,

(i) = Dt(i) exp{-yiht(xi)t[ht(xi)]} , Zt

t [±1] = PrDt[ht(xi) = yi|ht(xi) = ±1] and Zt is

the normalization factor.

T t=1

t[ht(x)] ht(x).

The final hypothesis is given by the sign of H(x) =

In the original paper [Asl00], the InfoBoost update was motivated by seeking a information zero. Here we motivate InfoBoost as a minimization of the same relative entropy subject to the AdaBoost constraint b + c = and a second simultaneously constraint w.r.t. the constant hypothesis 1. A natural question is why not just do two steps of AdaBoost at each iteration t: One for ht and and then, sequentially, one for 1. We call the latter algorithm AdaBoost with Bias, since the constant hypothesis introduces a bias into the final hypothesis. See Figure 2 for an example of the different updates.

distribution Dt +1 for which the error of ht is half and yi and ht(xi) have mutual

enforced constraint a + b = 1 2

1 2

. Note that the second constraint is the AdaBoost

+1

2 5

0 +1 0 0

-1

2 5 1 5

-1

1 2 1 2

+1

1 3

0 +1

1 5

0

-1

1 2 1 6

-1

3 10 1 2

Dt :

Dt +1 : InfoB.

yi \ ht +1 -1 yi \ ht +1 -1

Dt +1 : AdaB.

Dt +1 : AdaB.w.Bias

yi \ ht +1 -1 yi \ ht +1 -1

Figure 2: Updating based on a positively one-sided hypothesis ht (weight c is 0): The updated distributions on the four types of examples are quite different. We will show in the next section that in the case of learning disjunctions, AdaBoost with Bias (and plain AdaBoost) can require many more iterations than InfoBoost and the trivial covering algorithm. This is surprising because the AdaBoost with Bias and InfoBoost seem so similar to each other (simultaneous versus sequential


enforcement of the same constraints). A natural extension would be to constrain the errors of all past hypotheses to half which is the Totally Corrective Algorithm of [KW99]. However this can lead to subtle convergence problems (See discussion in [RW02]). 3 Lower bounds of AdaBoost for Learning k disjunctions So far we did not specify how the weak hypothesis ht is chosen at iteration t. We assume there is a pool H of weak hypotheses and distinguish two methods: Greedy: Choose a ht  H for which the normalization factor Zt in the update of the algorithm is minimized.

Minimal: Choose ht with error smaller than a given threshold

The greedy method is motivated by the fact that

t

1 2

- .

Zt upper bounds the training

error of the final hypothesis ([SS99, Asl00]) and this method greedily minimizes this upper bound. Note that the Zt factors are different for AdaBoost and InfoBoost. In our lower bounds on the number of iterations the example set is always consistent with a k-literal monotone disjunction over N variables. More precisely the instances xi are in {±1}N and the label yi is xi, xi, . . .xi,k. The pool of weak

1 2

learners consists of the N literals Xj, where Xj(xi) = xij. For the greedy method we show that on random data sets InfoBoost and the covering algorithm use drastically fewer iterations than AdaBoost with Bias. We chose 10, 000 examples as follows: The first k bits of each example are chosen independently at random so that the probability of label +1 is half (i.e. the probability of +1 for each of the first k bits is 1 - 2- ); the remaining N - k ir-

1/k

relevant bits of each example are chosen +1 with probability half. Figure 3 shows the number of iterations as function of the size of disjunction k (averaged over 20 runs) of AdaBoost with Bias until consistency is reached on all 10, 000 exam-

Figure 3: Average # of steps

ples. The number of iteration in this very simple of AdaBoost with Bias for k setting grows quadratically with k. If the num- 10, 20, 30, 40, 50, 60.

=

ber of iterations is divided by k2 then the resulting curve is larger than a constant. In contrast the number of iterations of the greedy covering algorithm and InfoBoost is provably linear in k: For k = 60 and m = 10, 000, the former require 60 iterations on the average, whereas AdaBoost with Bias with the greedy choice of the weak hypothesis requires 1200 even though it never chooses irrelevant variables as weak learners (Plain AdaBoost requires twice as many iterations). The above construction is not theoretical. However we now give an explicit construction for the minimal method of choosing the weak hypothesis for which the number of iterations of greedy covering and InfoBoost grow linearly in k and the number of iterations of AdaBoost with Bias is quadratic in k. For any dimension N we define an example set which is the rows of the following (N + 1) × N dimensional matrix x: All entries on the main diagonal and above are +1 and the remaining entries -1. In particular, the last row is all -1 (See Figure 4). The i-th instance xi is the i-th row of this matrix and the first N examples Clearly the literal XN is consistent with the labels and thus always has error 0 w.r.t. any distribution on the examples. But note that the disjunction of the last k

(rows) are labeled +1 and the label of the last row yN +1 is -1.


literals is also consistent (for any k). We will construct a distribution on the rows that gives high probability to the early rows (See Figure 4) and allows the following "minimal" choice of weak hypotheses: At iteration t, AdaBoost with Bias1is) given Contrary to our construction, the initial distribution for boosting applications is typically uniform. However, using padding this can be avoided but makes the construction more complicated. For any precision parameter  (0,1) and disjunction

the weak hypothesis Xt. This weak hypothesis will have error current distribution of the examples. 1 2 - 1 2k ( = 2k w.r.t.

size k, we define the dimension

N :=

 -ln 

1 2 - ln 1 - 1 k

 ln 1- 2 k(k+1)

   

+ 1  k2 2 ln 1 2 - k 2

The initial distribution D1 is defined as

1 2k ,

D1(xt) :=

        

1 k(k+1)

1 2 1 2

- ,

1 -

N-1 t=1

1 k 1 - 2 k(k+1) t-2

,

D(xt),

for t = 1 for 2  t  N - 1 for t = N for t=N + 1.

.

The example xN has the lowest probability w.r.t. D1 (See Figure 4). However one can show that its probability is at least .

D1(xi)

      

+ +

- +

xi,j + +

- - +

+ + +

- - - + - - - +

+ + + + +

- - - - + - - - - -

             

yi + + + + + + -       

Figure 4: The examples (rows of the matrix), the labels, and the distribution D1. Also for t  N - 1, the probability D1(x ) of the first t examples is

t

. AdaBoost with Bias does two update steps at 1 2

t-1

1 - 1 - 1 k 1 - 2 k(k+1)

iteration t (constrain the error of ht to half and then sequentially the error of 1 to half.)

Dt(i) = Dt(i) exp{-yiht(xi)t} Zt and Dt

+1

ln 1-t t

=

Dt(i) exp{-yit} . Zt

The Z's are normalization factors, t = 1 2 and t = 1 2

Dt(x ) N

Dt(xN ) +1

ln . The final

hypothesis is the sign of the following linear combination: H(x) =

T t=1

t.

T t=1 tht(x) +

Proposition 1. For AdaBoost with Bias and t  N, PrDt[Xt(xi) = yi] = 1 2 - . 2k 1

Proof. (Outline) Since each literal Xt is one-sided, Xt classifies the negative example

xN +1

correctly. Since PrDt[Xt(xi) = yi] = Dt(xN

+1

)+Dt(x ) and Dt(xN

t +1 ) =

1 2


it suffices to show that Dt(x ) = t

1 2k

for t  N. The proof is by induction on t.

For t = 1, the statement follows from the definition of D1. Now assume that the statement holds for any t < t. Then we have

-1

Dt(x ) = Dt(x ) + Dt(xt) = Dt t t-1 -1 (x )eZt t-1

-1

-t -1

et Zt

-1

+ Dt(xt). (1)

Note that the example xt is not covered by any previous hypotheses X1, . . . , Xt and thus we have -1

t-1

Dt(xt) = D1(xt)

ej ej

Zj Zj

.

,

(2)

j=1

Using the inductive assumption that PrDt [Xt (xi) = yi] =

can show that t = ln

Dt (xN +1 ) = 1 2 - 2(k+1)

1 2 1

k+1 k-1

, Zt =

1 2

- , for t < t, one

2k

1

, t =

1 2 ln

1 k k+2 k

(k - 1)(k + 1), Dt (x ) =

N

, and Zt = 1 k+1

1 2 + 1 2(k+1) ,

k(k + 2). Substituting

these values into the formulae (1) and (2), completes the proof. Theorem 2. For the described examples set, initial distribution D1, and minimal choice of weak hypotheses, AdaBoost with Bias needs at least N iterations to construct a final hypothesis whose error with respect to D1 is below . Proof. Let t be any integer smaller than N. At the end of the iteration t, the X1, . . . , Xt. In particular, the final linear combination evaluated at xN is

examples xt +1 , . . . , xN are not correctly classified by the past weak hypotheses

t t t t

jXj(xN )+ j = - j+

j=1 j=1

H(xN ) = t j = - 2 ln k + 1 + t k - 1 2 ln < 0.

j=1 j=1

k + 2 k

Thus sign(H(xN)) = -1 and the final hypothesis has error at least D1(xN)   with respect to D1. To show a similar lower bound for plain AdaBoost we use the same example set and iteration

the following sequence of weak hypotheses X1, 1, X2, 1, . . . XN, 1. For odd1 - 2

numbers t the above proposition shows the error of the weak hypothesis is for even iteration numbers one can show that the hypothesis 1 has error

1

k

12 2

and

1 -

2(k+1)

.

4 InfoBoost and SemiBoost for one-sided weak hypotheses Aslam proved the following upper bound on the training error[Asl00] of InfoBoost: Theorem 3. The training error of the final hypothesis produced by InfoBoost is

bounded by

-1]

T t=1

Zt, where Zt = PrDt[ht(xi) = +1] 1 - t[+1]2 + PrDt[ht(xi) =

1 - t[-1]2 and edge4 t[±1] = 1 - 2t[±1].

Let t = 1 - 2t. If t[+1] = t[-1] = t, then Zt =

1 - t2, asfor AdaBoost.

However, if ht is one-sided, InfoBoost gives the improved factor of

Corollary 4. For t  2, if ht is one-sided w.r.t. Dt, then Zt =

4

1

1 - t: - t.

The edge  and error are related as follows:  = 1-2 and = - 2 ; = 1 2 1  1 2   = 0.


Proof. Wlog. assume ht is always correct when it predicts +1. Then t[+1] = 1 and the first summand in the expression for Zt given in the above theorem disappears. Recall that InfoBoost maintains the distribution Dt over examples so that PrDt[yi =

+1] =

2

1 2 for t  2. So the second summand becomes

Pr[ht(xi) = -1, yi = +1] Pr[ht(xi) = -1, yi = -1] Dt Dt

= 2 =

Pr[yi = +1] Pr[yi = -1] Dt Dt

Pr[ht(xi) = -1|yi = +1]. Dt

Pr[ht(xi) = -1|yi = +1] Pr[ht(xi) = -1|yi = -1] Dt Dt

By the definition of t, we have 1 - t = 2PrDt[ht(xi) = yi] = 2PrDt[ht(xi) = -1, yi = +1]

(because of one-sidedness of ht)

= 2PrDt[yi = +1] Pr[ht(xi) = -1|yi = +1] Dt

= PrDt[ht(xi) = -1|yi = +1] (because PrDt[yi = +1] = 1 2 ) 2

This corollary implies that if a one-sided hypothesis is chosen at each iteration, then InfoBoost constructs a final hypothesis consistent with all m examples within then the trivial greedy covering algorithm (which simply chooses the set that covers the most uncovered positive examples), achieves the improved factor of 1-, which that the factor for InfoBoost can be improved to 1 - , if all weak hypotheses are one-sided. So in this case InfoBoost indeed matches the 1 -  factor of the greedy covering algorithm. A technical problem arises when InfoBoost is given a set of examples that are all labeled +1. Then we have 1[+1] =  and 1[-1] = -. This implies H(x) = 1[h1(xi)]ht(xi) =  for any instance xi. Thus InfoBoost terminates in a single iteration and outputs a hypothesis that predicts +1 for any instance and InfoBoost cannot be used for constructing a cover. We propose a natural way to cope with this subtlety. Recall that the final hyseem to be a linear combination of hypotheses from H since the coefficients vary with the prediction of weak hypotheses. However observe that t[ht(x)] ht(x) = t[+1] h+(x) + t[-1] h-(x), where h± = h(x) if h(x) = ±1 and 0 otherwise.

2  ln m iterations. When the considered weak hypotheses are positively one-sided,

means at most 1  ln m iterations. By a careful analysis (not included), one can show

pothesis of InfoBoost is given by H(x) = T t=1 t[ht(x)] ht(x). This doesn't

t t

We call h+ and h- the semi hypotheses of h.

h-(x) =

h(x)-1 2

Note that h+(x) = h(x)+1 2

and

. So the final hypothesis of InfoBoost and the new algorithm we

will define in a moment is a bias plus a linear combination of the the original weak learners in H. We propose the following variant of AdaBoost (called Semi-Boost): In each iteration execute one step of AdaBoost but the chosen weak hypothesis must be a semi hypothesis of one of the original hypothesis h  H which has a positive edge. SemiBoost avoids the outlined technical problem and can handle equally labeled example sets. Also if all the chosen hypotheses are of the h+ type then the final hypothesis is a disjunction. If hypotheses are chosen by smallest error (largest edge), then the greedy covering algorithm is simulated. Analogously, if all the chosen hypotheses are of the h- type then one can show that the final hypothesis of SemiBoost is a conjunction. Furthermore, two steps of SemiBoost (with hypothesis h+ in the first step followed by the sibling hypothesis h- in the second step) are equivalent to one step of InfoBoost with hypothesis h.


Finally we note that the final hypothesis of InfoBoost (or SemiBoost) is not welldefined when it includes both types of one-sided hypotheses, i.e. positive and negative infinite coefficients may conflict each other. We propose two solutions. First, small  > 0. It can be shown that the new Z increases by at most 2([SS99]). Second, we allow infinite coefficients but interpret the final hypothesis as a version of a decision list [Riv87]: Whenever more than one semi hypotheses with infinite coefficients are non-zero on the current instance, then the semi hypothesis with the lowest iteration number determines the label. Once such a consistent decision list over some set of hypothesis ht and 1 has been found, it is easy the find an alternate linear combination of the same set of hypotheses (using linear programming) that maximizes the margin or minimizes the one-norm of the coefficient vector subject to consistency. Conclusion: We showed that AdaBoost can require significantly more iterations than the simple greedy cover algorithm when the weak hypotheses are one-sided and gave a variant of AdaBoost that can readily exploit one-sidedness. The open question is whether the new SemiBoost algorithm gives improved performance on natural data and can be used for feature selection. Acknowledgment: This research benefited from many discussions with Gunnar R¨atsch. He encouraged us to analyze AdaBoost with Bias and suggested to write the final hypothesis of InfoBoost as a linear combination of semi hypotheses. We also thank anonymous referees for helpful comments. References [Asl00] J. A. Aslam. Improving algorithms for boosting. In Proc. 13th Annu. Conference on Comput. Learning Theory, pages 200­207, 2000. [Fre95] Y. Freund. Boosting a weak learning algorithm by majority. Inform. Comput., 121(2):256­285, September 1995. Also appeared in COLT90. [FS97] Y. Freund and R. E. Schapire:. A decision-theoretic generalization of on-line learning and an application to boosting. J. Comput. Syst. Sci., 55(1):119­139, 1997. [KW99] Jyrki Kivinen and Manfred K. Warmuth. Boosting as entropy projection. In Proc. 12th Annu. Conf. on Comput. Learning Theory, pages 134­144. ACM Press, New York, NY, 1999. [Laf99] J. Lafferty. Additive models, boosting, and inference for generalized divergences. In Proc. 12th Annu. Conf. on Comput. Learning Theory, pages 125­133. ACM, 1999. [MG92] A. A. Razborov M. Goldmann, J. Hastad. Majority gates vs. general weighted threshold gates. Journal of Computation Complexity, 1(4):277­ 300, 1992. [Nat91] B. K. Natarajan. Machine Learning: A Theoretical Approach. Morgan Kaufmann, San Mateo, CA, 1991. [Riv87] R. L. Rivest. Learning decision lists. Machine Learning, 2:229­246, 1987. [RW02] G. R¨atsch and M. K. Warmuth. Maximizing the margin with boosting. In Proceedings of the 15th Annual Conference on Computational Learning Theory, pages 334­350. Springer, July 2002. [SS99] Robert E. Schapire and Yoram Singer. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37(3):297­336, 1999.

following [SS99] one can use the modified coefficients [±1] = 1 2 1-[±1]+ [±1]+  ln for


