Distributed Optimization in Adaptive Networks

Ciamac C. Moallemi Electrical Engineering Stanford University Stanford, CA 94305 ciamac@stanford.edu

Benjamin Van Roy Management Science and Engineering and Electrical Engineering Stanford University Stanford, CA 94305 bvr@stanford.edu

Abstract We develop a protocol for optimizing dynamic behavior of a network of simple electronic components, such as a sensor network, an ad hoc network of mobile devices, or a network of communication switches. This protocol requires only local communication and simple computations which are distributed among devices. The protocol is scalable to large networks. As a motivating example, we discuss a problem involving optimization of power consumption, delay, and buffer overflow in a sensor network. Our approach builds on policy gradient methods for optimization of Markov decision processes. The protocol can be viewed as an extension of policy gradient methods to a context involving a team of agents optimizing aggregate performance through asynchronous distributed communication and computation. We establish that the dynamics of the protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective.

1 Introduction This paper is motivated by the potential of policy gradient methods as a general approach to designing simple scalable distributed optimization protocols for networks of electronic devices. We offer a general framework for such protocols that builds on ideas from the policy gradient literature. We also explore a specific example involving a network of sensors that aggregates data. In this context, we propose a distributed optimization protocol that minimizes power consumption, delay, and buffer overflow. The proposed approach for designing protocols based on policy gradient methods comprises one contribution of this paper. In addition, this paper offers fundamental contributions to the policy gradient literature. In particular, the kind of protocol we propose can be viewed as extending policy gradient methods to a context involving a team of agents optimizing system behavior through asynchronous distributed computation and parsimonious local communication. Our main theoretical contribution is to show that the dynamics of our protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective.


2 A General Formulation Consider a network consisting of a set of components V = {1,...,n}. Associated with this network is a discrete-time dynamical system with a finite state space W. Denote the state of the system at time k by w(k), for k = 0, 1, 2,.... There are n subsets W1,..., Wn of W, each consisting of states associated with events at component i. Note that these subsets need not be mutually exclusive or totally exhaustive. At the kth epoch, there are n control actions a1(k)  A1,...,an(k)  An, where each Ai is a finite set of possible actions that can be taken by component i. We sometimes write these control actions in a vector form a(k)  A = A1 × ··· An. The actions are governed by a set of policies 1,...,n, parameterized by vectors 1  R ,...,n  R . Each ith action pro-

1 n N1 Nn

cess only transitions when the state w(k) transitions to an element of Wi. At the time of transition, the probability that ai(k) becomes any ai  Ai is given by i(ai|w(k)). The state transitions depend on the prior state and action vector. In particular, let P(w ,a ,w) be a transition kernel defining the probability of state w given prior state w and action a . Letting  = (1,...,n), we have Pr {w(k) = w,a(k) = a|w(k - 1) = w ,a(k - 1) = a ,} i

= P(w ,a ,w) i(ai|w)

i:wWi

i 1{a

i =ai} .

i:w/Wi 

Define Fk to be the -algebra generated by {(w( ),a( ))| = 1,...,k}. While the system is in state w  W and action a  A is applied, each component i receives a reward ri(w,a). The average reward received by the network is r(w,a) = Assumption 1. For every , the Markov chain w(k) is ergodic (aperiodic, irreducible). Given Assumption 1, for each fixed , there is a well-defined long-term average reward

1 n n i=1 ri(w,a).

() = limK  1 K E[ K-1 k=0 r(w(k),a(k))].

We will consider a stochastic approximation iteration (1) i(k + 1) = i(k) + i(k). Here, > 0 is a constant step size and i(k) is a noisy estimate of the gradient

i ((k))

computed at component i based on the component's historically observed states, actions, and rewards, in addition to communication with other components. Our goal is to develop an estimator i(k) that can be used in an adaptive, asynchronous, and decentralized context, and to establish the convergence of the resulting stochastic approximation scheme. Our approach builds on policy gradient algorithms that have been proposed in recent years ([5, 7, 8, 3, 4, 2]). As a starting point, consider a gradient estimation method that is a decentralized variation of the OLPOMDP algorithm of [3, 4, 1]. In this algorithm, each component i maintains and updates an eligibility vector zi (t)  R , defined by

 Ni

k



zi (k) = k -

i

(2) i( (ai( )|w( ))1{w

) i

i( (ai( )|w( ))

)

,

i ( )Wi}

=0

for some   (0, 1). The algorithm generates an estimate i(k) = r(w(t),a(t))zi (k) to ing only local information, the gradient estimate i(t) cannot be computed without knowl¯ edge of the global reward r(x(t),a(t)) at each time. In a fully decentralized environment, where components only have knowledge of their local rewards, this algorithm cannot be used.

¯



((k)). Note that while the credit vector zi (t) can be computed usthe local gradient i




In this paper, we present a simple scalable distributed protocol through which rewards occurring locally at each node are communicated over time across the network and gradient estimates are generated at each node based on local information. A fundamental issue this raises is that rewards may incur large delays before being communicated across the network. Moreover, these delays may be random and may correlated with the underlying events that occur in operation of the network. We address this issue and establish conditions for convergence. Another feature of the protocol is that it is completely decentralized ­ there is no central processor that aggregates and disseminates rewards. As such, the protocol is robust to isolated changes or failures in the network. In addition to design of the protocol, a significant contribution is in the protocol's analysis, which we believe requires new ideas beyond what has been employed in the prior policy gradient literature. 3 A General Framework for Protocols We will make the following assumption regarding the policies, which is common in the policy gradient literature ([7, 8, 3, 4, 2]). Assumption 2. For all i and every w  Wi, ai  Ai, i(ai|w) is a continuously differen-

i

tiable function of i. Further, for every i, there exists a bounded function Li(w,ai,) such

that for all w  Wi,ai  Ai, i i(ai|w) = i(ai|w)Li(w,ai,).

i i

The latter part of the assumption is satisfied, for example, if there exists a constant > 0

such that for each i,w  Wi,ai  Ai, either i(ai|w) = 0 for every i or i(ai|w)  , for all i. Consider the following gradient estimator: (3) i(k) = zi (k)n d ( ,k)rj( ), where we use the shorthand rj( ) = rj(w( ),a( )). Here, the random variables {d( ,k)}, with parameter   (0,1), represent an arrival process describing the com-

n k 1



ij

j=1 =0

ij

munication of rewards across the network. Indeed, d ( ,k) is the fraction of the reward ij

rj( ) at component j that is learned by component i at time k  . We will assume the arrival process satisfies the following conditions. Assumption 3. For each i,j, , and   (0, 1), the process {d ( ,k)|k = , + 1, + 2,...} satisfies: 1. d ( ,k) is Fk-measurable.

ji

ji

2. There exists a scalar   (0, 1) and a random variable c such that for all k  ,

d ( ,k) ji

(1 - )k -

- 1 < c k , -

i i

with probability 1. Further, we require that the distribution of c given F depend only on (w( ),a( )), and that there exist a constant c¯ such that E[c |w( ) = w,a( ) = a] < c¯ < , with probability 1 for all initial conditions w  W and a  A. 3. The distribution of {d ( ,k)|k = , + 1,...} given F depends only on w( )

ji

and a( ). The following result, proved in our appendix [9], establishes the convergence of the longterm sample averages of i(t) of the form (3) to an estimate of the gradient. This type of convergence is central to the convergence of the stochastic approximation iteration (1).


Theorem 1. Holding  fixed, the limit

K-1

 i () = lim

K

1 K E i(k)

k=0

exists. Further,

lim lim sup

1

1

 i () - i () = 0.

.

4 Example: A Sensor Network In this section, we present a model of a wireless network of sensors that gathers and communicates data to a central base station. Our example is motivated by issues arising in the development of sensor network technology being carried out by commercial producers of electronic devices. However, we will not take into account the many complexities associated with real sensor networks. Rather, our objective is to pose a simplified model that motivates and provides a context for discussion of our distributed optimization protocol. 4.1 System Description Consider a network of n sensors and a central base station. Each sensor gathers packets of data through observation of its environment, and these packets of data are relayed through the network to the base station via multi-hop wireless communication. Each sensor retains a queue of packets, each obtained either through sensing or via transmission from another sensor. Packets in a queue are indistinguishable ­ each is of equal size and must be transferred to the central base station. We take the state of a sensor to be the number of packets in the queue and denote the state of the ith sensor at time k by xi(k). The number of packets in a queue cannot exceed a finite buffer size, which we denote by x. A number of triggering events occur at any given device. These include (1) packetizing of an observation (2) reception of a packet from another sensor, (3) transmission of a packet to another sensor, (4) awakening from a period of sleep, (5) termination of a period of attempted reception, (6) termination of a period of attempted transmission. At the time of a triggering event, the sensor must decide on its next action. Possible actions include (1) sleep, (2) attempt transmission, (3) attempt reception. When the buffer is full, options are limited to (1) and (2). When the buffer is empty, options are limited (1) and (3). The action taken by the ith sensor at time k is denoted by ai(k). The base station will be thought of as a sensor that has an infinite buffer and perpetually attempts reception. For each i, there is a set N(i) of entities with which the ith sensor can directly communicate. If the ith sensor is attempting transmission of a packet and there is at least one element of N(i) that is simultaneously attempting reception and is closer to the base station than component i, the packet is transferred to the queue of that element. If there are multiple such elements, one of them is chosen randomly. Note that if among the elements of N(i) that are attempting reception, all are further away from the base station than component i, no packet is transmitted. Observations are made and packetized by each sensor at random times. If a sensor's buffer is not full when an observation is packetized, an element is added to the queue. Otherwise, the packet is dropped from the system.


4.2 Control Policies and Objective Every sensor employs a control policy that selects an action based on its queue length each time a triggering event occurs. The action is maintained until occurrence of the next triggering event. Each ith sensor's control policy is parameterized by a vector i  R .

2

Given i, at an event time, if the ith sensor has a non-empty queue, it chooses to transmit with probability i . If the ith sensor does not transmit and its queue is not full, it chooses

1

to receive with probability i . If the sensor does not transmit or receive, then it sleeps. In 2

order to satisfy Assumption 2, we constrain i and i to lie in an interval [ ,h], where 1 2

0 <  < h < 1. Assume that each sensor has a finite power supply. In order to guarantee a minimum lifespan for the network, we will require that each sensor sleeps at least a fraction fs of the time. This is enforced by considering a time window of length Ts. If, at any given time, a sensor has not slept for a total fraction of a least fs of the preceding time Ts, it is forced to sleep and hence not allowed to transmit or receive. The objective is to minimize a weighted sum of the average delay and average number of dropped packets per unit of time. Delay can be thought of as the amount of time a packet spends in the network before arriving at the base station. Hence, the objective is: max lim sup - (xi(k) + Di(k)) , where Di(k) is the number of packets dropped by sensor i at time k, and  is a weight reflecting the relative importance of delay and dropped packets. 5 Distributed Optimization Protocol We now describe a simple protocol by which components a the network can communicate rewards, in a fashion that satisfies the requirements of Theorem 1 and hence will produce good gradient estimates. This protocol communicates the rewards across the network over time using a distributed averaging procedure. In order to motivate our protocol, consider a different problem. Imagine each component i in the network is given a real value Ri. Our goal is to design an asynchronous distributed define the vector Y (0)  R by Yi(0) = Ri for all i. For each edge (i,j), define a function

K-1 n

1,...,n K

1 K 1 n

k=0 i=1

protocol through which each node will obtain the average R =

n i=1 Ri/n. To do this,

n

Q( i,j) : R  R by n n

Q( i,j) (Y ) =

Yi+Yj 2

Y

if  {i,j}, otherwise.

At each time t, choose an edge (i,j), and set Y (k + 1) = Q( i,j) (Y (k)). If the graph

is connected and every edge is sampled infinitely often, then limk

Y = R. To see this, note that the operators Q( i

hence

n i=1

i,j)

 Y (t) = Y , where

preserve the average value of the vector,

Yi(k)/n = R. Further, for any k, either Y (k+1) = Y (k) or Y (k+1)-Y <

Y (k) - Y . Further, Y is the unique vector with average value R that is a fixed point for infinitely often, Yi(k)R as k and the components agree to the common average R. In the context of our distributed optimization protocol, we will assume that each component i maintains a scalar value Yi(k) at time k representing an estimate of the global reward. We will define a structure by which components communicate. Define E to be the set of edges along which communication can occur. For an ordered set of distinct edges S =

all operators Q( i,j) . Hence, as long as the graph is connected and each edge is sampled


(ii,j1),..., (i|S ,j ) , define a set WS  W. Let (E) be the set of all possible ordered | |S|

sets of disjoint edges S, including the empty set. We will assume that the sets {WS|S  (E)} are disjoint and together form a partition of W. If w(k)  WS, for some set S, we will assume that the components along the edges where the terms in the product are taken over the order specified by S. Define R(k) = (r1(k),...,rn(k)) is a vector of rewards occurring at time k. The update rule for the

in S communicate in the order specified by S. Define QS = Q( i|S ,j|S ) | | ···Q( i1,j1) ,

vector Y (k) is given by Y (k + 1) = R(k + 1) + QS

QS (k+1) = 1{w

(k+1)

(k+1)WS}

Y (k), where QS.

S(E)

Let E = {(i,j)|(i,j)  S, WS = }. We will make the following assumption. ^ Assumption 4. The graph (V, E) is connected. ^ Since the process (w(k),a(k)) is aperiodic and irreducible (Assumption 1), this assumption guarantees that every edge on a connected subset of edges is sampled infinitely often. Policy parameters are updated at each component according to the rule: In relation to equations (1) and (3), we have



i(k + 1) = i(k) + zi (k)(1 - )Yi(k). (4)

(5) where Q( ,k) = QS ^

d ( ,k) = n(1 - )k ji

···QS . (k-1) ( )

- Q( ,k) ^ ,

ij

The following theorem, which relies on a general stochastic approximation result from [6] together with custom analysis available in our appendix [9], establishes the convergence of the distributed stochastic iteration method defined by (4). Theorem 2. For each > 0, define { (k)|k = 0, 1,...} as the result of the stochastic approximation iteration (4) with the fixed value of . Assume the set { (k)|k, } is bounded. Define the continuous time interpolation ¯ (t) by setting ¯ (t) =  (k) for t  [k ,k + ). Then, for any sequence of processes {¯ (t)| 0} there exists a subsequence that weakly converges to ¯(t) as 0, where ¯(t) is a solution to the ordinary differential equation

(6) ¯(t) =   (¯(t)).

Further, define L to be the set of limit points of (6), and for a  > 0, N(L) to be a neighborhood of radius  about L. The fraction of time that ¯ (t) spends in N(L) over the time interval [0,T] goes to 1 in probability as 0 and T. Note that since we are using a constant step-size , this type of weak convergence is the strongest one would expect. The parameters will typically oscillate in the neighborhood of an limit point, and only weak convergence to a distribution centered around a limit point can be established. An alternative would be to use a decreasing step size (k)0 in (4). In such instances, probability 1 convergence to a local optimum can often be established. However, with decreasing step sizes, the adaptation of parameters becomes very slow as (n) decays. We expect our protocol to be used in an online fashion, where it is ideal to be adaptive to long-term changes in network topology or dynamics of the environment. Hence, the constant step size case is more appropriate as it provides such adaptivity. Also, a boundedness requirement on the iterates in Theorem 2 is necessary for the mathematical analysis of convergence. In practical numerical implementations, choices of the policy parameters i would be constrained to bounded sets of Hi  R . In such an imple-

Ni

mentation, the iteration (4) would be replaced with an iteration projected onto the set Hi. The conclusions of Theorem 2 would continue to hold, but with the ODE (6) replaced with an appropriate projected ODE. See [6] for further discussion.


5.1 Relation to the Example In the example of Section 4, one approach to implementing our distributed optimization protocol involves passing messages associated with the optimization protocol alongside normal network traffic, as we will now explain. Each ith sensor should maintain and update two vectors: a parameter vector i(k)  R and an eligibility vector zi (k). If a triggering event occurs at sensor i at time k, the eligibility vector is updated according to

2 

zi (k) = z(k - 1) + 

i

i( (ai(k)|w(k)).

k) i

i( (ai(k)|w(k))

k)

Otherwise, zi (k) = zi (k - 1). Furthermore, each sensor maintains an estimate Yi(k) of the global reward. At each time k, each ith sensor observes a reward (negative cost) of ri(k) = -xi(k) - Di(k). If two neighboring sensors are both not asleep at a time k, they communicate their global reward estimates from the previous time. If the ith sensor is not involved in a reward communication event at that time, its global reward estimate is updated according to Yi(k) = Yi(k - 1) + ri(k). On the other hand, at any time k that there is a communication event, its global reward estimate is updated according to Yi(k) = ri(k)+(Yi(k)+Yj(k))/2, where j is the index of the sensor with which communication occurs. If communication occurs with multiple neighbors, the corresponding global reward estimates are averaged pairwise in an arbitrary order. Clearly this update process can be modeled in terms of the sets WS introduced in the previous section. In this context, the graph E contains an edge for each pair of neighbors in the sensor network, ^ where the neighborhood relations are capture by N, as introduced in Section 4. To optimize performance over time, each ith sensor would update its parameter values according to our stochastic approximation iteration (4). To highlight the simplicity of this protocol, note that each sensor need only maintain and update a few numerical values. Furthermore, the only communication required by the optimization protocol is that an extra scalar numerical value be transmitted and an extra scalar numerical value be received during the reception or transmission of any packet. As a numerical example, consider the network topology in Figure 1. Here, at every time step, an observation arrives at a sensor with a 0.02 probability, and each sensor maintains a queue of up to 20 observations. Policy parameters i and i for each sensor i are

1 2

constrained to lie in the interval [0.05, 0.95]. (Note that for this set of parameters, the chance of a buffer overflow is very small, and hence did not occur in our simulations.) A baseline policy is defined by having leaf nodes transmit with maximum probability, and interior nodes splitting their time roughly evenly between transmission and reception, when not forced to sleep by the power constraint. Applying our decentralized optimization method to this example, it is clear in Figure 2 that the performance of the network is quickly and dramatically improved. Over time, the algorithm converges to the neighborhood of a local optimum as expected. Further, the algorithm achieves qualitatively similar performance to gradient optimization using the centralized OLPOMDP method of [3, 4, 1], hence decentralization comes at no cost. 6 Remarks and Further Issues We are encouraged by the simplicity and scalability of the distributed optimization protocol we have presented. We believe that this protocol represents both an interesting direction for practical applications involving networks of electronic devices and a significant step in the policy gradient literature. However, there is an important outstanding issue that needs to be addressed to assess the potential of this approach: whether or not parameters can be adapted fast enough for this protocol to be useful in applications. There are two dimensions

i

 


Figure 1: Example network topology. Figure 2: Convergence of method.

0

OLPOMDP decentralized baseline

-0.05

10 6 9

-0.1

Reward

4 3 5

-0.15 Average

8 2 1

Long-Term -0.2

7 root

-0.25

-0.3 0 1 2 3 4

5 Iteration 6 7 8 9

10

6

x 10

to this issue: (1) variance of gradient estimates and (2) convergence rate of the underlying ODE. Both should be explored through experimentation with models that capture practical contexts. Also, there is room for research that explores how variance can be reduced and the convergence rate of the ODE can be accelerated. Acknowledgements The authors thank Abbas El Gamal, Abtin Keshavarzian, Balaji Prabhakar, and Elif Uysal for stimulating conversations on sensor network models and applications. This research was supported by NSF CAREER Grant ECS-9985229 and by the ONR under grant MURIN00014-00-1-0637. The first author was also supported by a Benchmark Stanford Graduate Fellowship. References [1] P. L. Bartlett and J. Baxter. Stochastic Optimization of Controlled Markov Decision Processes. In IEEE Conference on Decision and Control, pages 124­129, 2000. [2] P. L. Bartlett and J. Baxter. Estimation and Approximation Bounds for Gradient-Based Reinforcement Learning. Journal of Computer and System Sciences, 64:133­150, 2002. [3] J. Baxter and P. L. Bartlett. Infinite-Horizon Gradient-Based Policy Search. Journal of Artificial Intelligence Research, 15:319­350, 2001. [4] J. Baxter, P. L. Bartlett, and L. Weaver. Infinite-Horizon Gradient-Based Policy Search: II. Gradient Ascent Algorithms and Experiments. Journal of Artificial Intelligence Research, 15:351­381, 2001. [5] T. Jaakkola, S. P. Singh, and M. I. Jordan. Reinforcement Learning Algorithms for Partially Observable Markov Decision Problems. In Advances in Neural Information Processing Systems 7, pages 345­352, 1995. [6] H. J. Kushner and G. Yin. Stochastic Approximation Algorithms and Applications. SpringerVerlag, New York, NY, 1997. [7] P. Marbach, O. Mihatsch, and J.N. Tsitsiklis. Call Admission Control and Routing in Integrated Service Networks. In IEEE Conference on Decision and Control, 1998. [8] P. Marbach and J.N. Tsitsiklis. Simulation­Based Optimization of Markov Reward Processes. IEEE Transactions on Automatic Control, 46(2):191­209, 2001. [9] C. C. Moallemi and B. Van Roy. Appendix to NIPS Submission. URL: http://www. moallemi.com/ciamac/papers/nips-2003-appendix.pdf, 2003.


