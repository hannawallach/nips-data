Variational Linear Response

Manfred Opper(1) Ole Winther(2)

(1)

Neural Computing Research Group, School of Engineering and Applied Science, Aston University, Birmingham B4 7ET, United Kingdom

(2)

Informatics and Mathematical Modelling, Technical University of Denmark, R. Petersens Plads, Building 321, DK-2800 Lyngby, Denmark opperm@aston.ac.uk owi@imm.dtu.dk

Abstract A general linear response method for deriving improved estimates of correlations in the variational Bayes framework is presented. Three applications are given and it is discussed how to use linear response as a general principle for improving mean field approximations.

1 Introduction Variational and related mean field techniques have attracted much interest as methods for performing approximate Bayesian inference, see e.g. [1]. The maturity of the field has recently been underpinned by the appearance of the variational Bayes method [2, 3, 4] and associated software making it possible with a window based interface to define and make inference for a diverse range of graphical models [5, 6]. Variational mean field methods have shortcomings as thoroughly discussed by Mackay [7]. The most important is that it based upon the variational assumption of independent variables. In many cases where the effective coupling between the variables are weak this assumption works very well. However, if this is not the case, the variational method can grossly underestimate the width of marginal distributions because variance contributions induced by other variables are ignored as a consequence of the assumed independence. Secondly, the variational approximation may be non-convex which is indicated by the occurrence of multiple solutions for the variational distribution. This is a consequence of the fact that a possibly complicated multi-modal distribution is approximated by a simpler uni-modal distribution. Linear response (LR) is a perturbation technique that gives an improved estimate of the correlations between the stochastic variables by expanding around the solution to variational distribution [8]. This means that we can get non-trivial estimates of correlations from the factorizing variational distribution. In many machine learning models, e.g. Boltzmann machine learning [9] or probabilistic Independent Component Analysis [3, 10], the M-step of the EM algorithm depend upon the covariances of the variables and LR has been applied with success in these cases [9, 10]. Variational calculus is in this paper used to derive a general linear response correction from the variational distribution. It is demonstrated that the variational LR correction can be calculated as systematically the variational distribution in the Variational Bayes framework


(albeit at a somewhat higher computational cost). Three applications are given: a model with a quadratic interactions, a Bayesian model for estimation of mean and variance of a 1D Gaussian and a Variational Bayes mixture of multinomials (i.e. for modeling of histogram data). For the two analytically tractable models (the Gaussian and example two above), it is shown that LR gives the correct analytical result where the variational method does not. The need for structured approximations, see e.g. [5] and references therein, that is performing exact inference for solvable subgraphs, might thus be eliminated by the use of linear response. We define a general probabilistic model M for data y and model parameters s: p(s, y) = p(s, y|M). The objective of a Bayesian analysis are typically the following: to derive

the marginal likelihood p(y|M) =

one-variable pi(si|y) =

1 p(y) k=i,j

1 p(y) k=i

ds p(s, y|M) and marginal distributions e.g. the dskp(s, y) and the two-variable pij(si, sj|y) =

dskp(s, y). In this paper, we will only discuss how to derive linear response

approximations to marginal distributions. Linear response corrected marginal likelihoods can also be calculated, see Ref. [11]. The paper is organized as follows: in section 2 we discuss how to use the marginal likelihood as a generating functions for deriving marginal distributions. In section 4 we use this result to derive the linear response approximation to the two-variable marginals and derive an explict solution of these equations in section 5. In section 6 we discuss why LR in the cases where the variational method gives a reasonable solution will give an even better result. In section 7, we give the three applications and in section we conclude and discuss how to combine the mean field approximation (variational, Bethe,...) with linear response to give more precise mean field approaches. After finishing this paper we have become aware of the work of Welling and Teh [12, 13] which also contains the result eq. (8) and furthermore extend linear response to the Bethe approximation, give several general results for the properties of linear response estimates and derive belief propagation algorithms for computing the linear response estimates. The new contributions of this paper compared to Refs. [12, 13] are the explicit solution of the linear response equations, the discussion of the expected increased quality of linear response estimates, the applications of linear response to concrete examples especially in relation to variational Bayes and the discussion of linear response and mean field methods beyond variational. 2 Generating Marginal Distributions In this section it is shown how exact marginal distributions can be obtained from functional derivatives of a generating function (the log partition function). In the derivation of the variational linear response approximation to the two-variable marginal distribution pij(si, sj|y), we can use result by replacing the exact marginal distribution with the variational approximation. To get marginal distributions we introduce a generating function

Z[a] = ds p(s, y)ePi ai(si) (1)

which is a functional of the arbitrary functions ai(si) and a is shorthand for the vector of functions a = (a1(s1), a2(s2), . . . , aN(sN)). We can now obtain the marginal distribution p(si|y, a) by taking the functional derivative1 with respect to ai(si):

 ai(si) eai( Z[a] si) ln Z[a] =

ds^keak(^

k=i aj (sj ) ai(si)

sk) p(^s, y) = pi(si|y, a) . (2)

1 The functional derivative is defined by = ij (si - sj) and the chain rule.


Setting a = 0 above gives the promised result. The next step is to take the second derivative. This will give us a function that are closely related to the two-variable marginal distribution. A careful derivation gives

Bij(si, sj)

 =

2 ln Z[a] aj(sj)ai(si) = pi(si|y, a) aj(sj) (3)

a=0 a=0

ij(si - sj)pi(si|y) + (1 - ij)pij(si, sj|y) - pi(si|y)pj(sj|y)

Performing an average of sm(sj)n over Bij(si, sj), it is easy to see that Bij(si, sj) gives i

the 'mean-subtracted' marginal distributions. In the two next sections, variational approximations to the single variable and two-variable marginals are derived. 3 Variational Learning In many models of interest, e.g. mixture models, exact inference scales exponentially with the size of the system. It is therefore of interest to come up with polynomial approximations. A prominent method is the variational, where a simpler factorized distribution marginal distributions pi(si|y) and pij(si, sj|y) are now simply qi(si) and qi(si)qj(sj). The purpose of this paper is to show that it is possible within the variational framework to go beyond the factorized distribution for two-variable marginals. For this purpose we need the distribution q(s) which minimizes the KL-divergence or `distance' between q(s) and p(s|y):

q(s) = i qi(si) is used instead of the posterior distribution. Approximations to the

KL(q(s)||p(s|y)) = ds q(s) ln q(s) p(s|y) . (4)

The variational approximation to the Likelihood is obtained from

-lnZv[a] = ds q(s) ln q(s) p(s, y)ePk = - ln Z[a] + KL(q(s)||p(s|y, a)) ,

ak(^k) s

where a has been introduced to be able use qi(si|a) as a generating function. In-

troducing Lagrange multipliers {i} as enforce normalization and minimizing KL +

i i( dsiqi(si) - 1) with respect to qi(si) and i, one finds

eai( ds^ieai(^

si)+R Qk=i{ dskqk(sk|a)} ln p(s,y)

qi(si|a) =

si)+R

. (5)

Qk=i { ds^kqk(^k|a)} ln p(^,y) s s

Note that qi(si|a) depends upon all a through the implicit dependence in the qks appearing on the right hand side. Writing the posterior in terms of `interaction potentials', i.e. as a factor graph p(s, y) = i(si) i,j(si, sj) . . . , (6)

i i>j

it is easy to see that potentials that do not depend upon si will drop out of variational distribution. A similar property will be used below to simplify the variational two-variable marginals. 4 Variational Linear Response Eq. (3) shows that we can obtain the two-variable marginal as the derivative of the marginal distribution. To get the variational linear response approximation we exchange the exact marginal with the variational approximation eq. (5) in eq. (3). In section 6 an argument is


given for why one can expect the variational approach to work in many cases and why the linear response approximation gives improved estimates of correlations in these cases. Defining the variational 'mean subtracted' two-variable marginal as

Cij(si, sj|a)  qi(si|a) aj(sj) , (7)

it is now possible to derive an expression corresponding to eq. (3). What makes the derivation a bit cumbersome is that it necessary to take into account the implicit dependence of aj(sj) in qk(sk|a) and the result will consequently be expressed as a set of linear integral equations in Cij(si, sj|a). These equations can be solved explicitly, see section 5 or can as suggested by Welling and Teh [12, 13] be solved by belief propagation. Taking into account both explicit and implicit a dependence we get the variational linear response theorem: Cij(si, sj|a) = ij (si - sj)qi(si|a) - qi(si|a)qj(sj|a) (8) +qi(si|a) dsk qk(sk|a)Clj (sl, sj|a)

l=i k=i k=i,l

× lnp(s,y) - dsiqi(si|a) ln p(s, y) .

The first term represents the normal variational correlation estimate and the second term is linear response correction which expresses the coupling between the two-variable marginals. Using the factorization of the posterior eq. (6), it is easily seen that potentials that do not depend on both si and sl will drop out in the last term. This property will make the calculations for the most variational Bayes models quite simple since this means that one only has to sum over variables that are directly connected in the graphical model. 5 Explicit Solution The integral equation can be simplified by introducing the symmetric kernel

Kij(s, s ) = (1 - ij) where the brackets . . .

ln p(s, y)

\(i,j) =

\(i,j)

. . .

- lnp(s,y)

q\(i,j)

\j - lnp(s,y) \i + ln p(s, y) ,

denote expectations over q for all vari-

ables, except for si and sj and similarly for . . . ds qi(s) Kij(s, s ) = 0. Writing C in the form

Cij(s, s ) = qi(s)qj(s ) ij (s - s ) qj(s )

\i . One can easily show that

- 1 + Rij(s, s ) , (9)

we obtain an integral equation for the function R Rij(s, s ) = ds~ ql(~)Kil(s, s~)Rlj(~ s ) + Kij(s, s ) .

s s,

l

This result can most easily be obtained by plugging the definition eq. (9) into eq. (8) and using that ds qi(s) Rij(s, s ) = 0. For many applications, kernels can be written in the form of sums of pairwise multiplicative `interactions', i.e.

Kij(s, s ) = Jij



 (s) (s ) i j (11)

(10)


with 

i

q = 0 for all i and  then the solution will be on the form Rij (s, s ) =

 A (s) (s ). The integral equation reduces to a system of linear equations

ij i j

for the coefficients A . ij

We now discuss the simplest case where Kij(s, s ) = Jiji(s)j(s ). This is obtained if the model has only pairwise interactions of the quadratic form ij(s, s ) = eJiji(s)j(s ,

)

. Using Rij(s, s ) = Aiji(s)j(s ) and augmenting the where i(s) = i(s) - i q

matrix of Jij's with the diagonal elements Jii  -

1 2i

yield the solution q

Aij = -JiiJjj D(Jii) - J-1 ij , (12)

where D(Jii) is a diagonal matrix with entries Jii. Using (9), this result immediately leads to the expression for the correlations ij = ij - i j = -(J-1)ij . (13) 6 Why Linear Response Works It may seem paradoxical that an approximation which is based on uncorrelated variables allows us to obtain a nontrivial result for the neglected correlations. To shed more light on this phenomenon, we would like to see how the true partition function, which serves as a generating function for expectations, differs from the mean field one when the approximating mean field distribution q are close. We will introduce into the generating function eq. (1) the parameter :

Z [a] = ds q(s)e (Pi ai(si)+ln p(s|y)-ln q(s)) (14)

which serves as a bookkeepingdevice for collecting relevant terms, when ln p(s|y)-ln q(s)

is assumed to be small. At the end we will set = 1 since Z[a] = Z the partition function to first order in , we get

ln Z [a] = ai(si)

i

=1 [a]. Then expanding

+ ln p(s|y) - ln q(s) + O( 2 q q ) (15)

= ai(si) - KL(q||p) + O( 2 q ) .

i

Keeping only the linear term, setting = 1 and inserting the minimizing mean field distribution for q yields

pi(s|y, a) =  ln Z ai(s) = qi(s|a) + O( 2 ) . (16)

Hence the computation of the correlations via

Bij(s, s ) = 2 ln Z ai(s)aj(s ) = pi(s|a) aj(s ) = qi(s|a) aj(s ) + O( 2 ) = Cij (s, s ) + O( 2 ) (17)

can be assumed to incorporate correctly effects of linear order in ln p(s|a) - ln q(s). On the other hand, one should expect p(si, sj|y) - qi(si)qj(sj) to be order . Although the above does not prove that diagonal correlations are estimated more precisely from Cii(s, s ) than from qi(s)­only that both are correct to linear order in --one often observes this in practice, see below.


7 Applications 7.1 Quadratic Interactions The quadratic interaction model--ln ij(si, sj)

ln p(s, y) = i ln i(si) +

1 2

= siJijsj and arbitrary (si), i.e.

i=j siJijsj + constant--is used in many contexts, e.g.

the Boltzmann machine, independent component analysis and the Gaussian process prior. For this model we can immediately apply the result eq. (13) to get sisj - si = -(J-1)ij (18)

where we have set Jii = -1/( s2i q - si 2 q

sj ).

We can apply this to the Gaussian model ln i(si) = hisi + Ais2i /2, The variational distribution is Gaussian with variance -1/Ai (and covariance zero). Hence, we can set Jii = Ai. The mean is -[J-1h]i. The exact marginals have mean -[J-1h]i and covariance -[J-1]ij. The difference can be quite dramatic, e.g. in two dimensions for

1 J = , we get J-1 =

1 1 1- 2 1 - - 1 . The variance estimates are 1/Jii = 1

for variational and [J-1]ii = 1/(1 - ) for the exact case. The latter diverges for completely correlated variable,  1 illustrating that the variational covariance estimate breaks down when the interaction between the variables are strong. A very important remark should be made at this point: although the covariance eq. (18) comes out correctly, the LR method does not reproduce the exact two variable marginals, i.e. the distribution eq. (9) plus the sum of the product of the one variable marginals is not a Gaussian distribution. 7.2 Mean and Variance of 1D Gaussian A one dimensional Gaussian observation model p(y|µ, ) = /2 exp(-(x - µ)2/2),  = 1/2 with a Gaussian prior over the mean and a Gamma prior over  [7] serves as another example of where linear response--as opposed to variational--gives exact covariance estimates. The N example likelihood can be rewritten as

N 2 

p(y|µ, ) =

where y and 2 = ^

i

 2 exp -N2 ^ - N (µ - y)2

2

, (19)

2

2

(yi - y)2/N are the empirical mean and variance. We immediately

recognize -2 N(µ - y)2 as the interaction term. Choosing non-informative priors--p(µ) flat and p()  1/--the variational distribution qµ(µ) becomes Gaussian with mean y

and variance 1/N  q and q() becomes a Gamma distribution (|b, c)  c -1 -/b e ,

with parameters cq = N/2 and 1/bq =

N 2

(^2 + (µ - y)2  q ). The mean and variance of

Gamma distribution are given by bc and b2c. Solving with respect to (µ - y)2

give 1/bq =

N2 ^ 2 N N-1

q and 

N2 ^ 2

q

. Exact inference gives cexact = (N -1)/2 and 1/bexact = [7].

A comparison shows that the mean bc is the same in both cases whereas variational underestimates the variance b2c. This is a quite generic property of the variational approach. The LR correction to the covariance is easily derived from (13) setting J12 = -N/2 and

1() =  - 

-1/bq  q

q

and 2(µ) = (µ - y)2 - (µ - y)2

q

. This yields J11 = -1/ 21() =

. Using (µ - y)2

= 1/N  q

and (µ - y)4

q

= 3 (µ - y)2 2 q

, we have

J22 = -1/ 22(µ) = -N2 

q 2 q

/2. Inverting the 2 × 2 matrix J, we immediately get

21 = Var() = -(J-1)11 = bq 

Inserting the result for  q

q /(1 - bq/2  q )

, we find that this is in fact the correct result.


7.3 Variational Bayes Mixture of Multinomials As a final example, we take a mixture model of practical interest and show that linear response corrections straightforwardly can be calculated. Here we consider the problem of modeling histogram data ynj consisting of N histograms each with D bins: n = 1, . . . , N and j = 1, . . . , D. We can model this with a mixture of multinomials (Lars Kai Hansen 2003, in preparation):

K D

k

p(yn|, ) = ykj nj

k=1 j=1

where k is the probability of the kth mixture and kj is the probability of observing the Eventually in the variational Bayes treatment we will introduce Dirichlet priors for the variables. But the general linear response expression is independent of this. To rewrite the model such that it is suitable for a variational treatment--i.e. in a product form--we

jth histogram given we are in the kth component, i.e. k k = 1 and j kj = 1.

introduce hidden (Potts) variables xn = {xnk}, xnk = {0, 1} and the joint probability of observed and hidden variables as:

K D

k We can now identify the interaction terms in

k=1

p(yn, xn|, ) =

 ykj

nj

xnk

.

k xnk = 1 and write

, (20)

n

 ln p(yn, xn, , ) as xnk ln k and

(21)

j=1

Summing over all possible xn vectors, we recover the original observation model. ynjxnk ln kj. Generalizing eq. (8) to sets of variables, we can compute the following correlations C(, ), C(, ) and C(k,k ). To get the explicit solution we need to write the coupling matrix for the problem and add diagonal terms and invert. Normally, the complexity will be order cubed in the number of parameters. However, it turns out that the two variable marginal distributions involving the hidden variables--the number of which scales with the number of examples--can be eliminated analytically. The computation of correlation are thus only cubic in the number of parameters, K + K  D, making computation of correlations attractive even for mixture models. The symmetric coupling matrix for this problem can be written as

Jx 

J 

J

Jx 

J J

···

x1 J1

xN

J =

Jxx Jx Jx

with Jx = 

 

J1 ...

JK x1

... ··· JK

xn jk )

xN

where for simplicity the log on  and  are omitted and (Jk

= ynj. The other non-

   ,

(22)

zero sub-matrix is: Jx = [Jx1 ··· JxN] with (Jxn)kk = k,k . To get the covariance

V we introduce diagonal elements into J (which are all tractable in . . . = . . . -(J-1 )kk xnkxnk - xnk xnk xnk

xnxn

-(J)kk -(Jkk)jj

-1

-1 

= = =

ln k ln k ln kj ln kj

- lnk

= kk xnk - xnk ln k

q

):

- lnkj ln kj

(23) (24) (25)

and invert: V = -J-1. Using inversion by partitioning and the Woodbury formula we find the following simple formula

-1 -1

V = J - J - J ^ ^ J - J ^ J ^ , (26)

and J ^ = where we have introduced the `indirect' couplings J ^

=

JxJ-1Jx xx 

JxJ-1Jx . Similar formulas can be obtained for V and V.

xx 


8 Conclusion and Outlook In this paper we have shown that it is possible to extend linear response to completely general variational distributions and solve the linear response equations explicitly. We have given three applications that show 1. that linear response provides approximations of increased quality for two-variable marginals and 2. linear response is practical for variational Bayes models. Together this suggests that building linear response into variational Bayes software such as VIBES [5, 6] would be useful. Welling and Teh [12, 13] have, as mentioned in the introduction, shown how to apply the general linear response methods to the Bethe approximation. However, the usefulness of linear response even goes beyond this: if we can come up with a better tractable approximation to the marginal distribution q(si) with some free parameters, we can tune these parameters by requiring consistency between q(si) and the linear response estimate of the diagonal of the two-variable marginals eq. (8): Cii(si, si) = (si - si)q(si) - q(si)q(si) . (27) This design principle can be generalized to models that give non-trivial estimates of twovariable marginals such as Bethe. It might not be possible to match the entire distribution for a tractable choice of q(si). In that case it is possibly to only require consistency for some statistics. The adaptive TAP approach [11]--so far only studied for quadratic interactions-- can be viewed in this way. Generalizing this idea to general potentials, general mean field approximations, deriving the corresponding marginal likelihoods and deriving guaranteed convergent algorithms for the approximations are under current investigation. References [1] M. Opper and D. Saad, Advanced Mean Field Methods: Theory and Practice, MIT Press, 2001. [2] H. Attias, "A variational Bayesian framework for graphical models," in Advances in Neural Information Processing Systems 12, T. Leen et al., Ed. 2000, MIT Press, Cambridge. [3] J. W. Miskin and D. J. C. MacKay, "Ensemble learning for blind image separation and deconvolution," in Advances in Independent Component Analysis, M Girolami, Ed. 2000, SpringerVerlag Scientific Publishers. [4] Z. Ghahramani and M. J. Beal, "Propagation algorithms for variational Bayesian learning," in Advances in Neural Information Processing Systems 13. 2001, pp. 507­513, MIT Press, Cambridge. [5] C. M. Bishop, D. Spiegelhalter, and J. Winn, "VIBES: A variational inference engine for Bayesian networks," in Advances in Neural Information Processing Systems 15, 2002. [6] C. M. Bishop and J. Winn, "Structured variational distributions in VIBES," in Artificial Intelligence and Statistics, Key West, Florida, 2003. [7] D. J. C. MacKay, Information Theory, Inference, and Learning Algorithms, Cambridge University Press, 2003. [8] G. Parisi, Statistical Field Theory, Addison-Wesley, 1988. [9] H.J. Kappen and F.B. Rodr´iguez, "Efficient learning in Boltzmann machines using ' linear response theory," Neural Computation, vol. 10, pp. 1137­1156, 1998. [10] P. A.d.F.R. Hojen-Sorensen, O. Winther, and L. K. Hansen, "Mean field approaches to independent component analysis," Neural Computation, vol. 14, pp. 889­918, 2002. [11] M. Opper and O. Winther, "Adaptive and self-averaging Thouless-Anderson-Palmer mean field theory for probabilistic modeling," Physical Review E, vol. 64, pp. 056131, 2001. [12] M. Welling and Y. W. Teh, "Linear response algorithms for approximate inference," Artificial Intelligence Journal, 2003. [13] M. Welling and Y. W. Teh, "Propagation rules for linear response estimates of joint pairwise probabilities," preprint, 2003.


