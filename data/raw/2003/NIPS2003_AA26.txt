The IM Algorithm : A variational approach to Information Maximization

David Barber Felix Agakov

Institute for Adaptive and Neural Computation : www.anc.ed.ac.uk Edinburgh University, EH1 2QL, U.K.

Abstract The maximisation of information transmission over noisy channels is a common, albeit generally computationally difficult problem. We approach the difficulty of computing the mutual information for noisy channels by using a variational approximation. The resulting IM algorithm is analagous to the EM algorithm, yet maximises mutual information, as opposed to likelihood. We apply the method to several practical examples, including linear compression, population encoding and CDMA. Introduction

1

The reliable communication of information over noisy channels is a widespread issue, ranging from the construction of good error-correcting codes to feature extraction[3, 12]. In a neural context, maximal information transmission has been extensively studied and proposed as a principal goal of sensory processing[2, 5, 7]. The central quantity in this context is the Mutual Information (MI) which, for source variables (inputs) x and response variables (outputs) y, is

I(x, y)  H(y) - H(y|x),

p(y)

and H(y|x)  - log p(y|x) where H(y)  - log p(y)

p(x,y)

(1) are marginal and

conditional entropies respectively, and angled brackets represent averages. The

goal is to adjust parameters of the mapping p(y|x) to maximise I(x, y). Despite the simplicity of the statement, the MI is generally intractable for all but special cases. The key difficulty lies in the computation of the entropy of p(y) (a mixture). One such tractable special case is if the mapping y = g(x; ) is deterministic and invertible, for which the difficult entropy term trivially becomes

H(y) = log |J| p(y) + const. (2)

Here J = {yi/xj} is the Jacobian of the mapping. For non-Gaussian sources p(x), and special choices of g(x; ), the minimization of (1) with respect to the parameters  leads to the infomax formulation of ICA[4]. Another tractable special case is if the source distribution p(x) is Gaussian and the mapping p(y|x) is Gaussian.


x p(x) source

z p(z)

y p(y|x) encoder x q(x|y,z) decoder

Figure 1: An illustration of the form of a more general mixture decoder. x represents the sources or inputs, which are (stochastically) encoded as y. A receiver decodes y (possibly with the aid of auxiliary variables z).

However, in general, approximations of the MI need to be considered. A variety of methods have been proposed. In neural coding, a popular alternative is to maximise the Fisher `Information'[5]. Other approaches use different objective criteria, such as average reconstruction error. 2 Variational Lower Bound on Mutual Information Since the MI is a measure of information transmission, our central aim is to maximise a lower bound on the MI. Using the symmetric property of the MI, an equivalent formulation of the MI is I(x, y) = H(x) - H(x|y). Since we shall generally be interested in optimising MI with respect to the parameters of p(y|x), and p(x) is simply the data distribution, we need to bound H(x|y) suitably. The KullbackLeibler bound p(x|y) log p(x|y) - p(x|y) log q(x|y)  0 gives

x

I(x, y)  H(x) + log q(x|y)

"entropy "energy

def = I~(x, y).

p(x,y) (3)

where q(x|y) is an arbitrary variational distribution. The bound is exact if q(x|y)  p(x|y). The form of this bound is convenient since it explicitly includes both the encoder p(y|x) and decoder q(x|y), see fig(1). Certainly other well known lower bounds on the MI may be considered [6] and a future comparison of these different approaches would be interesting. However, our current experience suggests that the bound considered above is particularly computationally convenient. Since the bound is based on the KL divergence, it is equivalent to a moment matching approximation of p(x|y) by q(x|y). This fact is highly beneficial in terms of decoding, since mode matching approaches, such as mean-field theory, typically get trapped in the one of many sub-optimal local minima. More successful decoding algorithms approximate the posterior mean[10]. The IM algorithm To maximise the MI with respect to any parameters  of p(y|x, ), we aim to push up the lower bound (3). First one needs to choose a class of variational distributions q(x|y)  Q for which the energy term is tractable. Then a natural recursive procedure for maximising I~(X, Y ) for given p(x), is 1. For fixed q(x|y), find new = arg max I~(X, Y )

2. For fixed , qnew(x|y) = arg maxq of distributions. (x|y)Q I~(X, Y ), where Q is a chosen class

These steps are iterated until convergence. This procedure is analogous to the

(G)EM algorithm which maximises a lower bound on the likelihood[9]. The difference is simply in the form of the "energy" term. Note that if |y| is large, the posterior p(x|y) will typically be sharply peaked around its mode. This would motivate a simple approximation q(x|y) to the posterior,


Figure 2: The MI optimal linear projection of data x (dots) is not always given by PCA. PCA projects data onto the vertical line, for which the entropy conditional on the projection H(x|y) is large. Optimally, we should project onto the horizontal line, for which the conditional entropy is zero.

significantly reducing the computational complexity of optimization. In the case of real-valued x, a natural choice in the large |y| limit is to use a Gaussian. A simple approximation would then be to use a Laplace approximation to p(x|y) with form reminiscent of the Fisher Information[5]. The bound presented here is arguably more general and appropriate than presented in [5] since, whilst it also tends to the exact value of the MI in the limit of a large number of responses, it is a principled bound for any response dimension. Relation to Conditional Likelihood Consider an autoencoder x  y  x and imagine that we wish to maximise the ~ probability that the reconstruction x is in the same s state as x: ~

covariance elements [-1]ij = 2 log p(x|y) xixj . Inserted in the bound, this then gives a

Jensen

 log p(~ = s|x = s) = log x p(~ = s|y)p(y|x = s) x log p(~ = s|y) x p(y|x=s)

y

Averaging this over all the states of x: p(x = s) log p(~ x = s|x = s) 

s s

Hence, maximising I~(X, Y ) (for fixed p(x)) is the same as maximising the lower bound on the probability of a correct reconstruction. This is a reassuring property of the lower bound. Even though we do not directly maximise the MI, we also indirectly maximise the probability of a correct reconstruction ­ a form of autoencoder. Generalisation to Mixture Decoders A straightforward application of Jensen's inequality leads to the more general result: I(X, Y )  H(X) + log q(x|y, z)  I~(X,Y ) where q(x|y, z) and q(z) are variational distributions. The aim is to choose q(x|y, z) such that the bound is tractably computable. The structure is illustrated in fig(1). 3 Linear Gaussian Channel : Improving on PCA A common theme in linear compression and feature extraction is to map a (high dimensional) vector x to a (lower dimensional) vector y = W x such that the information in the vector x is maximally preserved in y. The classical solution to this problem (and minimizes the linear reconstruction error) is given by PCA. However, as demonstrated in fig(2), the optimal setting for W is, in general not given by the widely used PCA. To see how we might improve on the PCA approach, we consider optimising our bound with respect to linear mappings. We take as our projection (encoder) model,

p(y|x)p(x)q(z)

log p(~ = s|y) x p(x=s,y)  logq(x|y) p(x,y)


p(y|x)  N (Wx, s2I), with isotropic Gaussian noise.

tion is simply p(x)  P µ=1

The empirical distribu-

(x - xµ), where P is the number of datapoints.

Without loss of generality, we assume the data is zero mean. For a decoder

q(x|y) = N (m(y), (y)), maximising the bound on MI is equivalent to minimising

P

(x - m(y))T -1(y)(x - m(y)) + log det (y) p(y|xµ)

µ=1

For constant diagonal matrices (y), this reduces to minimal mean square reconstruction error autoencoder training in the limit s2  0. This clarifies why autoencoders (and hence PCA) are a sub-optimal special case of MI maximisation. Linear Gaussian Decoder A simple decoder is given by q(x|y)  N (Uy, 2I), for which I~(x, y)  2tr(UWS) - tr(UMUT ), (4)

where S = xxT = µ

xµ(xµ)T /P is the sample covariance of the data, and M = Is2 + WSWT

(5)

is the covariance of the mixture distribution p(y). Optimization of (4) for U leads to SWT = UM. Eliminating U, this gives I~(x, y)  tr SWT M-1WS (6) In the zero noise limit, optimisation of (6) produces PCA. For noisy channels, una norm-constrained optimisation in general produces a different result to PCA. The simplicity of the linear decoder in this case severely limits any potential improvement over PCA, and certainly would not resolve the issue in fig(2). For this, a non-linear decoder q(x|y) is required, for which the integrals become more complex. Non-linear Encoders and Kernel PCA An alternative to using non-linear decoders to improve on PCA is to use a non-linear encoder. A useful choice is p(y|x) = N (W(x), 2I) where (x) is in general a high dimensional, non-linear embedding function, for which W will be non-square. In the zero-noise limit the optimal solution for the encoder results in non-linear PCA on the covariance (x)(x)T of the transformed data. By Mercer's theorem, the elements of the covariance matrix may be replaced by a Kernel function of the users choice[8]. An advantage of our framework is that our bound enables the principled comparison of embedding functions/kernels. 4 Binary Responses (Neural Coding) In a neurobiological context, a popular issue is how to encode real-valued stimuli in a population of spiking neurons. Here we look briefly at a simple case in which each neuron fires (yi = 1) with increasing probability the further the membrane potential wi x is above threshold -bi. Independent neural firing suggests:

constrained optimization of (6) leads to a divergence of the matrix norm WWT  ;

T

p(y|x) = p(yi|x) =

i

def

(yi(wi x + bi)). T (7)


Figure 3: Top row: a subset of the original real-valued source data. Middle row: after training, 20 samples from each of the 7 output units, for each of the corresponding source inputs. Bottom row: Reconstruction of the source data from 50 samples of the output units. Note that while the 8th and the 10th patterns have closely matching stochastic binary representations, they differ in the firing rates of unit 5. This results in a visibly larger bottom loop of the 8th reconstructed pattern, which agrees with the original source data. Also, the thick vertical 1 (pattern 3) differs from the thin vertical eight (pattern 6) due to the differences in stochastic firings of the third and the seventh units.

Here the response variables y  {-1, +1}|y , and (a) = 1/(1 + e-a). For the decoder, we chose a simple linear Gaussian q(x|y)  N (Uy, ). In this case, exact evaluation of the bound (3) is straightforward, since it only involves computations of the second-order moments of y over the factorized distribution. A reasonable reconstruction of the source x from its representation y will be given

by the mean x = x ~ q(x|y) of the learned approximate posterior. In noisy channels

we need to average over multiple possible representations, i.e. x = ~ x q(x|y) p(y|x ) .

| def

We performed reconstruction of continuous source data from stochastic binary responses for |x| = 196 input and |y| = 7 output units. The bound was optimized with respect to the parameters of p(y|x) and q(x|y) with isotropic norm constraints on W and b for 30 instances of digits 1 and 8 (15 of each class). The source variables were reconstructed from 50 samples of the corresponding binary representations at the mean of the learned q(x|y), see fig(3). 5 Code Division Multiple Access (CDMA) In CDMA[11], a mobile phone user j  1, . . . , M wishes to send a bit sj  {0, 1} of information to a base station. To send sj = 1, she transmits an N dimensional realvalued vector gj, which represents a time-discretised waveform (sj = 0 corresponds to no transmission). The simultaneous transmissions from all users results in a received signal at the base station of

ri = gi sj + i,

j

where i is Gaussian noise. Probabilistically, we can write p(r|s)  exp -(r - Gs)2 /(22)

j i = 1, . . . , N, or r = Gs + 

.

The task for the base station (which knows G) is to decode the received vector r so that s can be recovered reliably. For simplicity, we assume that N = M so that the matrix G is square. Using Bayes' rule, p(s|r)  p(r|s)p(s), and assuming a flat

prior on s,

p(s|r)  exp - -2rT Gs + sT GT Gs /(22) (8)

Computing either the MAP solution arg maxs p(s|r) or the MPM solution arg maxsj p(sj|r), j = 1, . . . , M is, in general, NP-hard.


If GT G is diagonal, optimal decoding is easy, since the posterior factorises, with p(sj|r)  exp 2 riGji - Djj sj/(22)

i

where the diagonal matrix D = GT G (and we used s2i  si for si  {0, 1}). For suitably randomly chosen matrices G, GT G will be approximately diagonal in the limit of large N. However, ideally, one would like to construct decoders that perform near-optimal decoding without recourse to the approximate diagonality of GT G. The MAP decoder solves the problem

min s{0,1}N sT GT Gs - 2sT GT r  min s{0,1}N s - G-1r T GT G s - G-1r

and hence the MAP solution is that s which is closest to the vector G-1r. The difficulty lies in the meaning of `closest' since the space is non-isotropically warped by the matrix GT G. A useful guess for the decoder is that it is the closest in the Euclidean sense to the vector G-1r. This is the so-called decorrelation estimator. Computing the Mutual Information Of prime interest in CDMA is the evaluation of decoders in the case of nonorthogonal matrices G[11]. In this respect, a principled comparison of decoders can be obtained by evaluating the corresponding bound on the MI1, I(r, s)  H(s) - H(s|r)  H(s) + p(s)p(r|s) log q(s|r) (9)

r s

where H(s) is trivially given by M (bits). The bound is exact if q(s|r) = p(s|r). We make the specific assumption in the following that our decoding algorithm takes q(si|r) =  ((2si - 1)fi(r)) (10) for some decoding function fi(r). We restrict interest here to the case of simple linear decoding functions fi(r) = ai + wijrj.

the factorised form q(s|r) = i q(si|r) and, without loss of generality, we may write

j

Since p(r|s) is Gaussian, (2si - 1)fi(r)  xi is also Gaussian, p(xi|s) = N (µi(s), vari), µi(s)  (2si - 1)(ai + wi Gs), where wi is the ith row of the matrix [W ]ij  wij. Hence

T

T

 x=-

-H(s|r)  [log  (x)] e-[

T

i

1 22wi wi

vari  2wi wi T

x-(2si-1)(ai+wi Gs)]2/(22wi wi) T T

p(s)

(11) In general, the average over the factorised distribution p(s) can be evaluated by using the Fourier Transform [1]. However, to retain clarity here, we constrain the decoding matrix W so that wi Gs = bisi, i.e. W G = diag(b), for a parameter

T

vector b. The average over p(s) then gives

-H(s|r)  1 2 log  (x) (1 + e-[ -2xbi-4xai+2aibi+b2i]/(22wi wi) T ,

N(-ai,var=2wi wi) T i

1

(12) Other variational methods may be considered to approximate the normalisation con-

stant of p(s|r)[13], and it would be interesting to look into the possibility of using them in a MI approximation, and also as approximate decoding algorithms.


1

Figure 4: The bound given by the decoder W  G- r plotted against the optimised bound (for the same G) found using 50 updates of conjugate gradients. This was repeated over several trials of randomly chosen matrices G, each of which are square of N = 10 dimensions. For clarity, a small number of poor results (in which the bound is negative) have been omitted. To generate G, form the matrix Aij  N (0, 1), and B = A + AT . From the eigen-decomposition of B, i.e BE = E, form [G]ij = [E]ij + 0.1N(0, 1) (so that GT G has small off diagonal elements). 1

0.9

Decoder 0.8

0.7

Optimal 0.6

0.5

0.4

Constrained 0.3

for

0.2

bound 0.1

MI

0 0 0.2 0.4 0.6 0.8 1

MI bound for Inverse Decoder

a sum of one dimensional integrals, each of which can be evaluated numerically. In the case of an orthogonal matrix GT G = D the decoding function is optimal and the MI bound is exact with the parameters in (12) set to ai = -[GT G]ii/(22) W = GT /2 bi = [GT G]ii/2.

Optimising the linear decoder In the case that GT G is non-diagonal, what is the optimal linear decoder?

A

partial answer is given by numerically optimising the bound from (11). For the constrained case, W G = diag(b), (12) can be used to calculate the bound. Using W = diag(b)G-1,

2wi wi = 2b2i T ([G-1]ij)2,

j

and the bound depends only on a and b. Under this constraint the bound can be As an alternative we can employ the decorrelation decoder, W = G-1/2, with ai = -1/(22). In fig(4) we see that, according to our bound, the decorrelation or (`inverse') decoder is suboptimal versus the linear decoder fi(r) = ai + wi r with

numerically optimised as a function of a and b, given a fixed vector j ([G-1]ij)2.

T

W = diag(b)G-1, optimised over a and b. These initial results are encouraging, and motivate further investigations, for example, using syndrome decoding for CDMA. 6 Posterior Approximations There is an interesting relationship between maximising the bound on the MI and computing an optimal estimate q(s|r) of an intractable posterior p(s|r). The optimal bit error solution sets q(si|r) to the mean of the exact posterior marginal p(si|r). Mean Field Theory approximates the posterior marginal by minimising the KL divergence: KL(q||p) = (q(s|r) log q(s|r) - q(s|r) log p(s|r)), where a neglectable prefactor). However, this form of the KL divergence chooses q(si|r) to be any one of a very large number of local modes of the posterior distribution p(si|r). Since the optimal choice is to choose the posterior marginal mean, this is why using Mean Field decoding is generally suboptimal. Alternatively, consider KL(p||q) = (p(s|r) log p(s|r) - p(s|r) log q(s|r)) = - p(s|r) log q(s|r)+const.

s

q(s|r) = i q(si|r). In this case, the KL divergence is tractably computable (up to

s s

This is the correct KL divergence in the sense that, optimally, q(si|r) = p(si|r), that is, the posterior marginal is correctly calculated. The difficulty lies in performing


averages with respect to p(s|r), which are generally intractable. Since we will have a distribution p(r) it is reasonable to provide an averaged objective function, p(r)p(s|r) log q(s|r) = p(s)p(r|s) log q(s|r). (13)

r s r s

Whilst, for any given r, we cannot calculate the best posterior marginal estimate, we may be able to calculate the best posterior marginal estimate on average. This is precisely the case in, for example, CDMA since the average over p(r|s) is tractable, and the resulting average over p(s) can be well approximated numerically. Wherever an average case objective is desired is of interest to the methods suggested here. 7 Discussion We have described a general theoretically justified approach to information maximization in noisy channels. Whilst the bound is straightforward, it appears to have attracted little previous attention as a practical tool for MI optimisation. We have shown how it naturally generalises linear compression and feature extraction. It is a more direct approach to optimal coding than using the Fisher `Information' in neurobiological population encoding. Our bound enables a principled comparison of different information maximisation algorithms, and may have applications in other areas of machine learning and Information Theory, such as error-correction.

[1] D. Barber, Tractable Approximate Belief Propagation, Advanced Mean Field Methods Theory and Practice (D. Saad and M. Opper, eds.), MIT Press, 2001. [2] H. Barlow, Unsupervised Learning, Neural Computation 1 (1989), 295­311. [3] S Becker, An Information-theoretic unsupervised learning algorithm for neural networks, Ph.D. thesis, University of Toronto, 1992. [4] A.J. Bell and T.J. Sejnowski, An information-maximisation approach to blind separation and blind deconvolution, Neural Computation 7 (1995), no. 6, 1004­1034. [5] N. Brunel and J.-P. Nadal, Mutual Information, Fisher Information and Population Coding, Neural Computation 10 (1998), 1731­1757. [6] T. Jaakkola and M. Jordan., Improving the mean field approximation via the use of mixture distributions, Proceedings of the NATO ASI on Learning in Graphical Models, Kluwer, 1997. [7] R. Linsker, Deriving Receptive Fields Using an Optimal Encoding Criterion, Advances in Neural Information Processing Systems (Lee Giles (eds) Steven Hanson, Jack Cowan, ed.), vol. 5, Morgan-Kaufmann, 1993. [8] S. Mika, B. Schoelkopf, A.J. Smola, K-R. Muller, M. Scholz, and Gunnar Ratsch, Kernel PCA and De-Noising in Feature Spaces, Advances in Neural Information Processing Systems 11 (1999). [9] R. M. Neal and G. E. Hinton, A View of the EM Algorithm That Justifies Incremental, Sparse, and Other Variants, Learning in Graphical Models (M.J. Jordan, ed.), MIT Press, 1998. [10] D. Saad and M. Opper, Advanced Mean Field Methods Theory and Practice, MIT Press, 2001. [11] T. Tanaka, Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators, Advances in Neural Information Processing Systems (T. K. Leen et al. (eds.), ed.), vol. 13, MIT Press, 2001, pp. 315­321. [12] K. Torkkola and W. M. Campbell, Mutual Information in Learning Feature Transformations, Proc. 17th International Conf. on Machine Learning (2000). [13] M. Wainwright, T. Jaakkola, and A. Willsky, A new class of upper bounds on the log partition function, Uncertainty in Artificial Intelligence, 2002.


