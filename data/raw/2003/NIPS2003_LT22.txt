Margin Maximizing Loss Functions

Saharon Rosset Watson Research Center IBM Yorktown, NY, 10598 srosset@us.ibm.com Ji Zhu Department of Statistics University of Michigan Ann Arbor, MI, 48109 jizhu@umich.edu Trevor Hastie Department of Statistics Stanford University Stanford, CA, 94305 hastie@stat.stanford.edu

Abstract

Margin maximizing properties play an important role in the analysis of classi£cation models, such as boosting and support vector machines. Margin maximization is theoretically interesting because it facilitates generalization error analysis, and practically interesting because it presents a clear geometric interpretation of the models being built. We formulate and prove a suf£cient condition for the solutions of regularized loss functions to converge to margin maximizing separators, as the regularization vanishes. This condition covers the hinge loss of SVM, the exponential loss of AdaBoost and logistic regression loss. We also generalize it to multi-class classi£cation problems, and present margin maximizing multiclass versions of logistic regression and support vector machines.

1 Introduction Assume we have a classi£cation "learning" sample {xi, yi}ni

=1

with yi  {-1, +1}. We

wish to build a model F(x) for this data by minimizing (exactly or approximately) a loss criterion C(yi, F (xi)) = C(yiF (xi)) which is a function of the margins yiF (xi) of this model on this data. Most common classi£cation modeling approaches can be cast in this framework: logistic regression, support vector machines, boosting and more. The model F(x) which these methods actually build is a linear combination of dictionary functions coming from a dictionary H which can be large or even in£nite: F (x) = jhj(x)

i i

hj H

and our prediction at point x based on this model is sgnF(x). When |H| is large, as is the case in most boosting or kernel SVM applications, some regularization is needed to control the "complexity" of the model F(x) and the resulting over£tting. Thus, it is common that the quantity actually minimized on the data is a regularized version of the loss function:

(1) ^() = min C(yi h(xi)) +  



i

p p

where the second term penalizes for the lp norm of the coef£cient vector  (p  1 for convexity, and in practice usually p  {1, 2}), and   0 is a tuning regularization parameter. The 1- and 2-norm support vector machine training problems with slack can be cast in this form ([6], chapter 12). In [8] we have shown that boosting approximately follows the


"path" of regularized solutions traced by (1) as the regularization parameter  varies, with the appropriate loss and an l1 penalty. The main question that we answer in this paper is: for what loss functions does ^() converge to an "optimal" separator as   0? The de£nition of "optimal" which we will use depends on the lp norm used for regularization, and we will term it the "lp-margin maximizing separating hyper-plane". More concisely, we will investigate for which loss functions and under which conditions we have: (2) lim = arg max min yi h(xi) This margin maximizing property is interesting for three distinct reasons. First, it gives us a geometric interpretation of the "limiting" model as we relax the regularization. It tells us that this loss seeks to optimally separate the data by maximizing a distance between a separating hyper-plane and the "closest" points. A theorem by Mangasarian [7] allows us to interpret lp margin maximization as lq distance maximization, with 1/p + 1/q = 1, and hence make a clear geometric interpretation. Second, from a learning theory perspective large margins are an important quantity -- generalization error bounds that depend on the margins have been generated for support vector machines ([10] -- using l2 margins) and boosting ( [9] -- using l1 margins). Thus, showing that a loss function is "margin maximizing" in this sense is useful and promising information regarding this loss function's potential for generating good prediction models. Third, practical experience shows that exact or approximate margin maximizaion (such as non-regularized kernel SVM solutions, or "in£nite" boosting) may actually lead to good classi£cation prediction models. This is certainly not always the case, and we return to this hotly debated issue in our discussion. Our main result is a suf£cient condition on the loss function, which guarantees that (2) holds, if the data is separable, i.e. if the maximum on the RHS of (2) is positive. This condition is presented and proven in section 2. It covers the hinge loss of support vector machines, the logistic log-likelihood loss of logistic regression, and the exponential loss, most notably used in boosting. We discuss these and other examples in section 3. Our result generalizes elegantly to multi-class models and loss functions. We present the resulting margin-maximizing versions of SVMs and logistic regression in section 4. 2 Suf£cient condition for margin maximization The following theorem shows that if the loss function vanishes "quickly" enough, then it will be margin-maximizing as the regularization vanishes. It provides us with a uni£ed margin-maximization theory, covering SVMs, logistic regression and boosting.

0

^() ^()

 i p =1

Theorem 2.1 Assume the data {xi, yi}ni =1

is separable, i.e.  s.t. mini yi h(xi) > 0.

Let C(y, f) = C(yf) be a monotone non-increasing loss function depending on the margin only. If T > 0 (possibly T =  ) such that: (3) lim = ,  > 0 Then C is a margin maximizing loss function in the sense that any convergence point of the

t T

C(t · [1 - ]) C(t)

normalized solutions ^() ^() to the regularized problems (1) as   0 is an lp margin-

p

maximizing separating hyper-plane. Consequently, if this margin-maximizing hyper-plane is unique, then the solutions converge to it: (4) lim = arg max min yi h(xi)

0

^() ^()

 i

p p

=1


Proof We prove the result separately for T =  and T < . a. T = :

Lemma 2.2 ^() 0 -  p

Proof Since T =  then C(m) > 0 m > 0, and limm

loss+penalty to vanish as   0, ^()

p

 C(m) = 0. Therefore, for

must diverge, to allow the margins to diverge.

Lemma 2.3 Assume 1, 2 are two separating models, with 1 p = 2 p = 1, and 1

separates the data better, i.e.: 0 < m2 = mini yih(xi) 2 < m1 = mini yih(xi) 1. Then U = U(m1, m2) such that t > U, C(yih(xi) (t1)) < C(yih(xi) (t2))

i i

In words, if 1 separates better than 2 then scaled-up versions of 1 will incur smaller loss than scaled-up versions of 2, if the scaling factor is large enough.

Proof Since condition (3) holds with T = , there exists U such that t > U, n. Thus from C being non-increasing we immediately get:

t > U, C(yih(xi) (t1))  n · C(tm1) < C(tm2) <

i

Proof of case a.: Assume  is a convergence point of

Now assume by contradiction ~ has ~ p

C(tm2) C(tm1) >

C(yih(xi) (t2))

i

as   0, with  = 1. p ^() ^()

p

= 1 and bigger minimal lp margin. Denote the

minimal margins for the two models by m and m ~ , respectively, with m < m.

~

By continuity of the minimal margin in , there exists some open neighborhood of  on

the lp sphere: and an > 0, such that: N = { :

 p = 1,  -  < } 2

min yi h(xi) < m Now by lemma 2.3 we get that exists U = U(m, m - ) such that t~ incurs smaller loss ~ ~ i

than t for any t > U,   N. Therefore  cannot be a convergence point of b. T <  Lemma 2.4 C(T) = 0 and C(T - ) > 0,  > 0.

Proof From condition (3),

Lemma 2.5 lim 0

C(T -T ) C(T )

^() ^() .

p

~ - ,   N

= . Both results follow immediately, with  = T .

mini yi^() h(xi) = T

Proof Assume by contradiction that there is a sequence 1, 2, ... j, mini yi^(j) h(xi)  T - .

Pick any separating normalized model ~ i.e. ~

Then for any  < mp C ~ (T - ) T p we get:

p

0 and > 0 s.t.

= 1 and m := mini yi~ h(xi) > 0. ~

C(yi m~ h(xi)) +  T ~ T m ~ ~ p p < C(T - )

i


since the £rst term (loss) is 0 and the penalty is smaller than C(T - ) by condition on . we assumed mini yi^(j0) h(xi)  T - and thus: C(yi^(j0) h(xi))  C(T - )

But j0 s.t. j0 < m ~ p C (T - ) T p and so we get a contradiction to optimality of ^(j0), since

i

We have thus proven that lim inf

0 mini yi^() h(xi)  T . It remains to prove equality.

Assume by contradiction that for some value of  we have m := mini yi^() h(xi) > T.

Then the re-scaled model

since T m ^() = T m

T m

^()

^() has the same zero loss as ^(), but a smaller penalty, < ^() . So we get a contradiction to optimality of ^().

Proof of case b.: Assume  is a convergence point of

Now assume by contradiction ~ has ~ p

^() ^() as   0, with  = 1.

p p

= 1 and bigger minimal margin. Denote the

minimal margins for the two models by m and m

Let 1, 2, ... 0 be a sequence along which

assumption, ^(j) consequently: p  T m > T m ~

^(j ) ^(j)

~ , respectively, with m < m.

~  . By lemma 2.5 and our

p

. Thus, j0 such that j > j0,

T

^(j) p > T m ~ and

C(yi^(j) h(xi)) +  ^(j) p p > ( T m ~ )p = C(yi mh(xi)) +  ~ ~ T m ~ ~ p p

i i

So we get a contradiction to optimality of ^(j). Thus we conclude for both cases a. and b. that any convergence point of

maximize the lp margin. Since ^() ^() p

^() ^() must

p

= 1, such convergence points obviously exist. p

If the lp-margin-maximizing separating hyper-plane is unique, then we can conclude:

^() ^()

 ^ := arg max minyi h(xi)

 i

p

=1 p

Necessity results A necessity result for margin maximization on any separable data seems to require either additional assumptions on the loss or a relaxation of condition (3). We conjecture that if we (3) is suf£cient and necessary. However this is still a subject for future research. 3 Examples Support vector machines Support vector machines (linear or kernel) can be described as a regularized problem:

also require that the loss is convex and vanishing (i.e. limm  C(m) = 0) then condition

(5) min [1 - yi h(xi)]+ +  



i

p p

where p = 2 for the standard ("2-norm") SVM and p = 1 for the 1-norm SVM. This formulation is equivalent to the better known "norm minimization" SVM formulation in the sense that they have the same set of solutions as the regularization parameter  varies in (5) or the slack bound varies in the norm minimization formulation.


The loss in (5) is termed "hinge loss" since it's linear for margins less than 1, then £xed at 0 (see £gure 1). The theorem obviously holds for T = 1, and it veri£es our knowledge that the non-regularized SVM solution, which is the limit of the regularized solutions, maximizes the appropriate margin (Euclidean for standard SVM, l1 for 1-norm SVM). Note that our theorem indicates that the squared hinge loss (AKA truncated squared loss): C(yi, F (xi)) = [1 - yiF (xi)]2+ is also a margin-maximizing loss. Logistic regression and boosting The two loss functions we consider in this context are:

(6) (7) Exponential : Log likelihood : Ce(m) = exp(-m) Cl(m) = log(1 + exp(-m))

These two loss functions are of great interest in the context of two class classi£cation: Cl is used in logistic regression and more recently for boosting [4], while Ce is the implicit loss function used by AdaBoost - the original and most famous boosting algorithm [3] . In [8] we showed that boosting approximately follows the regularized path of solutions ^() using these loss functions and l1 regularization. We also proved that the two loss functions are very similar for positive margins, and that their regularized solutions converge to margin-maximizing separators. Theorem 2.1 provides a new proof of this result, since the theorem's condition holds with T =  for both loss functions. Some interesting non-examples Commonly used classi£cation loss functions which are not margin-maximizing include any regularized solutions to margin maximizing solutions. Another interesting method in this context is linear discriminant analysis. Although it does not correspond to the loss+penalty formulation we have described, it does £nd a "decision hyper-plane" in the predictor space. For both polynomial loss functions and linear discriminant analysis it is easy to £nd examples which show that they are not necessarily margin maximizing on separable data. 4 A multi-class generalization Our main result can be elegantly extended to versions of multi-class logistic regression and support vector machines, as follows. Assume the response is now multi-class, with K  2 possible values i.e. yi  {c1, ..., cK}. Our model consists of a "prediction" for each class: Fk(x) = hj(x)

polynomial loss function: C(m) = 1 m , C(m) = m2, etc. do not guarantee convergence of

(k)

j

hj H

with the obvious prediction rule at x being arg maxk Fk(x). This gives rise to a K - 1 dimensional "margin" for each observation. For y = ck, de£ne the margin vector as: And our loss is a function of this K - 1 dimensional margin: C(y, f1, ..., fK) = I{y = ck}C(m(ck, f1, ..., fK))

(8) m(ck, f1, ..., fK) = (fk - f1, ..., fk - fk -1 , fk - fk +1 , ..., fk - fK)

k


3

hinge exponential logistic

6

2.5

5

2 4

3

1.5 2

1

1

0 -2

-1 -2

0.5 -1 0

0 1

1

2 2

0 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 3 3

Figure 1: Margin maximizing loss functions for 2-class problems (left) and the SVM 3-class loss function of section 4.1 (right)

The lp-regularized problem is now: (9) ^() = arg min C(yi, h(xi) (1), ..., h(xi) (K ) + 

)

(1),...,(K )

i

Where ^() = (^(1)(), ..., ^(K ())  RK ) ·|H|

(k ) p p

k

.

In this formulation, the concept of margin maximization corresponds to maximizing the minimal of all n · (K - 1) normalized lp-margins generated by the data: (10) max min min h(xi) ((yi) - (k ) Note that this margin maximization problem still has a natural geometric interpretation, as h(xi) ((yi) - (k ) > 0 i, k = yi implies that the hyper-plane h(x) ((j - (k ) = 0

)

(1) p p +...+ (K ) p p =1 i yi=ck

) ) )

successfully separates classes j and k for any two classes. Here is a generalization of the optimal separation theorem 2.1 to multi-class models: Theorem 4.1 Assume C(m) is commutative and decreasing in each coordinate, then if T > 0 (possibly T =  ) such that:

C(t[1 - ], tu1, ...tuK

C(t, tv1, ..., tvK -2

-2 )

(11) limt T

)

 > 0, u1  1,...,uK -2

= ,  1,v1  1,...vK

-2  1

Then C is a margin-maximizing loss function for multi-class models, in the sense that any

convergence point of the normalized solutions to (9), tion as de£ned in (10) ^() ^() , attains the optimal separa-

p

Idea of proof The proof is essentially identical to the two class case, now considering the n · (K - 1) margins on which the loss depends. The condition (11) implies that as the regularization vanishes the model is determined by the minimal margin, and so an optimal model puts the emphasis on maximizing that margin.


Corollary 4.2 In the 2-class case, theorem 4.1 reduces to theorem 2.1.

Proof The loss depends on (1) - (2), the penalty on (1) p p + (2) p p . An optimal

solution to the regularized problem must thus have (1) + (2) = 0, since by transforming:

(1)  (1) - (1) + (2) 2 , (2)  (2) - (1) + (2) 2

we are not changing the loss, but reducing the penalty, by Jensen's inequality:

(1) - (1) + (2) 2 p p + (2) - (1) + (2) 2 p p = 2 (1) - (2) 2 p p  (1) p p + (2) p p

So we can conclude that ^(1)() = -^(2)() and consequently that the two margin maximization tasks (2), (10) are equivalent. 4.1 Margin maximization in multi-class SVM and logistic regression Here we apply theorem 4.1 to versions of multi-class logistic regression and SVM. For logistic regression, we use a slightly different formulation than the "standard" logistic regression models, which uses class K as a "reference" class, i.e. assumes that (K = 0.

)

This is required for non-regularized £tting, since without it the solution is not uniquely de£ned. However, using regularization as in (9) guarantees that the solution will be unique and consequently we can "symmetrize" the model -- which allows us to apply theorem 4.1. So the loss function we use is (assume y = ck belongs to class k):

(12) C(y, f1, ..., fK) = - log efk ef1 + ... + efK =

-1 = log(ef1-fk + ... + efk -fk + 1 + efk +1 -fk + ... + efK-fk)

with the linear model: fj(xi) = h(xi) (j . It is not dif£cult to verify that condition (11) sum of exponentials which results from applying this £rst-order approximation satis£es (11), and as  0, the second order term can be ignored. For support vector machines, consider a multi-class loss which is a natural generalization of the two-class loss: (13) C(m) = [1 - mj]+

holds for this loss function with T = , using the fact that log(1 + ) = + O( 2 ). The

K-1

j=1

Where mj is the j'th component of the multi-margin m as in (8). Figure 1 shows this loss for K = 3 classes as a function of the two margins. The loss+penalty formulation using 13 is equivalent to a standard optimization formulation of multi-class SVM (e.g. [11]):

max s.t.

c h(xi) ((yi) - (k )  c(1 - ik), i  {1, ...n}, k  {1, ..., K}, ck = yi ik  0 ,

)

)

ik  B , (k ) p p = 1

i,k k

As both theorem 4.1 (using T = 1) and the optimization formulation indicate, the regularized solutions to this problem converge to the lp margin maximizing multi-class solution.


5 Discussion What are the properties we would like to have in a classi£cation loss function? Recently there has been a lot of interest in Bayes-consistency of loss functions and algorithms ([1] and references therein), as the data size increases. It turns out that practically all "reasonable" loss functions are consistent in that sense, although convergence rates and other measures of "degree of consistency" may vary. Margin maximization, on the other hand, is a £nite sample optimality property of loss functions, which is potentially of decreasing interest as sample size grows, since the training data-set is less likely to be separable. Note, however, that in very high dimensional predictor spaces, such as those typically used by boosting or kernel SVM, separability of any £nite-size data-set is a mild assumption, which is violated only in pathological cases. We have shown that the margin maximizing property is shared by some popular loss functions used in logistic regression, support vector machines and boosting. Knowing that these algorithms "converge", as regularization vanishes, to the same model (provided they use the same regularization) is an interesting insight. So, for example, we can conclude that 1-norm support vector machines, exponential boosting and l1-regularized logistic regression all facilitate the same non-regularized solution, which is an l1-margin maximizing separating hyper-plane. From Mangasarian's theorem [7] we know that this hyper-plane maximizes the l distance from the closest points on either side. The most interesting statistical question which arises is: are these "optimal" separating models really good for prediction, or should we expect regularized models to always do better in practice? Statistical intuition supports the latter, as do some margin-maximizing experiments by Breiman [2] and Grove and Schuurmans [5]. However it has also been observed that in many cases margin-maximization leads to reasonable prediction models, and does not necessarily result in over-£tting. We have had similar experience with boosting and kernel SVM. Settling this issue is an intriguing research topic, and one that is critical in determining the practical importance of our results, as well as that of margin-based generalization error bounds. References

[1] [2] [3] [4] [5] [6] [7] [8] [9]

Bartlett, P., Jordan, M. & McAuliffe, J. (2003). Convexity, Classi£cation and Risk Bounds. Technical reports, dept. of Statistics, UC Berkeley. Breiman, L. (1999). Prediction games and arcing algorithms. Neural Computation 7:1493-1517. Freund, Y. & Scahpire, R.E. (1995). A decision theoretic generalization of on-line learning and an application to boosting. Proc. of 2nd Eurpoean Conf. on Computational Learning Theory. Friedman, J. H., Hastie, T. & Tibshirani, R. (2000). Additive logistic regression: a statistical view of boosting. Annals of Statistics 28, pp. 337-407. Grove, A.J. & Schuurmans, D. (1998). Boosting in the limit: Maximizing the margin of learned ensembles. Proc. of 15th National Conf. on AI. Hastie, T., Tibshirani, R. & Friedman, J. (2001). Elements of Stat. Learning. Springer-Verlag. Mangasarian, O.L. (1999). Arbitrary-norm separating plane. Operations Research Letters, Vol. 24 1-2:15-23 Rosset, R., Zhu, J & Hastie, T. (2003). Boosting as a regularized path to a maximum margin classi£er. Technical report, Dept. of Statistics, Stanford Univ. Scahpire, R.E., Freund, Y., Bartlett, P. & Lee, W.S. (1998). Boosting the margin: a new explanation for the effectiveness of voting methods. Annals of Statistics 26(5):1651-1686

[10] Vapnik, V. (1995). The Nature of Statistical Learning Theory. Springer. [11] Weston, J. & Watkins, C. (1998). Multi-class support vector machines. Technical report CSDTR-98-04, dept of CS, Royal Holloway, University of London.


