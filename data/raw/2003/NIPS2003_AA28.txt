Large Scale Online Learning.

Leon Bottou ´ NEC Labs America Princeton NJ 08540 leon@bottou.org Yann Le Cun NEC Labs America Princeton NJ 08540 yann@lecun.com

Abstract We consider situations where training data is abundant and computing resources are comparatively scarce. We argue that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm. Both theoretical and experimental evidences are presented.

1 Introduction The last decade brought us tremendous improvements in the performance and price of mass storage devices and network systems. Storing and shipping audio or video data is now inexpensive. Network traffic itself provides new and abundant sources of data in the form of server log files. The availability of such large data sources provides clear opportunities for the machine learning community. These technological improvements have outpaced the exponential evolution of the computing power of integrated circuits (Moore's law). This remark suggests that learning algorithms must process increasing amounts of data using comparatively smaller computing resources. This work assumes that datasets have grown to practically infinite sizes and discusses which learning algorithms asymptotically provide the best generalization performance using limited computing resources. · Online algorithms operate by repetitively drawing a fresh random example and adjusting the parameters on the basis of this single example only. Online algorithms can quickly process a large number of examples. On the other hand, they usually are not able to fully optimize the cost function defined on these examples. · Batch algorithms avoid this issue by completely optimizing the cost function defined on a set of training examples. On the other hand, such algorithms cannot process as many examples because they must iterate several times over the training set to achieve the optimum. As datasets grow to practically infinite sizes, we argue that online algorithms outperform learning algorithms that operate by repetitively sweeping over a training set.


2 Gradient Based Learning Many learning algorithms optimize an empirical cost function Cn() that can be expressed as the average of a large number of terms L(z,). Each term measures the cost associated with running a model with parameter vector  on independent examples zi (typically input/output pairs zi = (xi,yi).)

n

Cn() = 1 n L(zi, ) (1)

i=1

Two kinds of optimization procedures are often mentioned in connection with this problem: · Batch gradient: Parameter updates are performed on the basis of the gradient and Hessian information accumulated over a predefined training set:

(k)

= = (k - 1) - k 1

Cn (



n

(k - 1) - k n

(k - 1)) L (

zi, (k - 1))  (2)

i=1

where k is an appropriately chosen positive definite symmetric matrix. · Online gradient: Parameter updates are performed on the basis of a single sample zt picked randomly at each iteration:

1 (t) = (t - 1) - t t L ( zt, (t - 1))

 (3)

where t is again an appropriately chosen positive definite symmetric matrix. Very often the examples zt are chosen by cycling over a randomly permuted training set. Each cycle is called an epoch. This paper however considers situations where the supply of training samples is practically unlimited. Each iteration of the online algorithm utilizes a fresh sample, unlikely to have been presented to the system before. Simple batch algorithms converge linearly1 to the optimum n of the empirical cost. Care-



ful choices of k make the convergence super-linear or even quadratic2 in favorable cases (Dennis and Schnabel, 1983). Whereas online algorithms may converge to the general area of the optimum at least as fast as batch algorithms (Le Cun et al., 1998), the optimization proceeds rather slowly during the final convergence phase (Bottou and Murata, 2002). The noisy gradient estimate causes the parameter vector to fluctuate around the optimum in a bowl whose size decreases like 1/t at best. Online algorithms therefore seem hopelessly slow. However, the above discussion compares the speed of convergence toward the minimum of the empirical cost Cn, whereas one should be much more interested in the convergence toward the minimum  of the expected cost C, which measures the generalization performance: C() = L(z, ) p(z) dz (4) Density p(z) represents the unknown distribution from which the examples are drawn (Vapnik, 1974). This is the fundamental difference between optimization speed and learning speed.

1 2 Linear convergence speed: log 1/|(k) - n| grows linearly with k. Quadratic convergence speed: log log 1/|(k) - n| grows linearly with k.

 2

 2


3 Learning Speed Running an efficient batch algorithm on a training set of size n quickly yields the empirical optimum n. The sequence of empirical optima n usually converges to the solution 

 

when the training set size n increases. In contrast, online algorithms randomly draw one example zt at each iteration. When these examples are drawn from a set of n examples, the online algorithm minimizes the empirical error Cn. When these examples are drawn from the asymptotic distribution p(z), it minimizes the expected cost C. Because the supply of training samples is practically unlimited, each iteration of the online algorithm utilizes a fresh example. These fresh examples follow the asymptotic distribution. The parameter vectors (t) thus directly converge to the optimum  of the expected cost C. The convergence speed of the batch n and online (t) sequences were first compared by



Murata and Amari (1999). This section reports a similar result whose derivation uncovers a deeper relationship between these two sequences. This approach also provides a mathematically rigorous treatment (Bottou and Le Cun, 2003). Let us first define the Hessian matrix H and Fisher information matrix G:

2 T H = E  L (z, ) G = E L (z , )



L (z  , )

Manipulating a Taylor expansion of the gradient of Cn() in the vicinity of n-1 immediately provides the following recursive relation between n and n-1.  

1  

n = n-1 - n n

n 2

L (  zn, n-1) + O (5)





1 n2

with -1

n = 1 n  (zi, n-1)  L - H-1

t i=1

Relation (5) describes the n sequence as a recursive stochastic process that is essentially similar to the online learning algorithm (3). Each iteration of this "algorithm" consists in picking a fresh example zn and updating the parameters according to (5). This is not a practical algorithm because we have no analytical expression for the second order term. We can however apply the mathematics of online learning algorithms to this stochastic process. The similarity between (5) and (3) suggests that both the batch and online sequences converge at the same speed for adequate choices of the scaling matrix t. Under customary regularity conditions, the following asymptotic speed results holds when the scaling matrix t converges to the inverse H-1 of the Hessian matrix.

E |(t) - |2 + o 1 t = E |t - |2 + o  1 t = tr H-1 G H-1 t (6)



This convergence speed expression has been discovered many times. Tsypkin (1973) establishes (6) for linear systems. Murata and Amari (1999) address generic stochastic gradient algorithms with a constant scaling matrix. Our result (Bottou and Le Cun, 2003) holds when the scaling matrix t depends on the previously seen examples, and also holds when the stochastic update is perturbed by unspecified second order terms, as in equation (5). See the appendix for a proof sketch (Bottou and LeCun, 2003). Result (6) applies to both the online (t) and batch (t) sequences. Not only does it establish that both sequences have O (1/t) convergence, but also it provides the value of


the constant. This constant is neither affected by the second order terms of (5) nor by the convergence speed of the scaling matrix t toward H-1. In the Maximum Likelihood case, it is well known that both H and G are equal on the optimum. Equation (6) then indicates that the convergence speed saturates the Cramer-Rao bound. This fact was known in the case of the natural gradient algorithm (Amari, 1998). It remains true for a large class of online learning algorithms. Result (6) suggests that the scaling matrix t should be a full rank approximation of the Hessian H. Maintaining such an approximation becomes expensive when the dimension of the parameter vector increases. The computational cost of each iteration can be drastically reduced by maintaining only a coarse approximations of the Hessian (e.g. diagonal, blockdiagonal, multiplicative, etc.). A proper setup ensures that the convergence speed remains O(1/t) despite a less favorable constant factor. The similar nature of the convergence of the batch and online sequences can be summarized as follows. Consider two optimally designed batch and online learning algorithms. The best generalization error is asymptotically achieved by the learning algorithm that uses the most examples within the allowed time. 4 Computational Cost The discussion so far has established that a properly designed online learning algorithm performs as well as any batch learning algorithm for a same number of examples. We now establish that, given the same computing resources, an online learning algorithm can asymptotically process more examples than a batch learning algorithm. Each iteration of a batch learning algorithm running on N training examples requires a time K1N + K2. Constants K1 and K2 respectively represent the time required to process each example, and the time required to update the parameters. Result (6) provides the following

asymptotic equivalence:

(N - )2  

1 N

The batch algorithm must perform enough iterations to approximate N with at least the same accuracy ( 1/N). An efficient algorithm with quadratic convergence achieves this after a number of iterations asymptotically proportional to log log N. Running an online learning algorithm requires a constant time K3 per processed example. Let us call T the number of examples processed by the online learning algorithm using the same computing resources as the batch algorithm. We then have: K3T  (K1N + K2) log log N = T  N log log N The parameter (T) of the online algorithm also converges according to (6). Comparing the accuracies of both algorithms shows that the online algorithm asymptotically provides a better solution by a factor O (log log N).

((T ) - )2  1 N log log N 1 N  (N -)2 



This log log N factor corresponds to the number of iterations required by the batch algorithm. This number increases slowly with the desired accuracy of the solution. In practice, this factor is much less significant than the actual value of the constants K1, K2 and K3. Experience shows however that online algorithms are considerably easier to implement. Each iteration of the batch algorithm involves a large summation over all the available examples. Memory must be allocated to hold these examples. On the other hand, each iteration of the online algorithm only involves one random example which can then be discarded.


5 Experiments A simple validation experiment was carried out using synthetic data. The examples are input/output pairs (x,y) with x  R20 and y = ±1. The model is a single sigmoid unit trained using the least square criterion. where f(x) = 1.71tanh(0.66x) is the standard sigmoid discussed in LeCun et al. (1998). The sigmoid generates various curvature conditions in the parameter space, including negative curvature and plateaus. This simple model represents well the final convergence phase of the learning process. Yet it is also very similar to the widely used generalized linear models (GLIM) (Chambers and Hastie, 1992). The first component of the input x is always 1 in order to compensate the absence of a bias parameter in the model. The remaining 19 components are drawn from two Gaussian distributions, centered on (-1,-1,...,-1) for the first class and (+1,+1,...,+1) for the second class. The eigenvalues of the covariance matrix of each class range from 1 to 20. Two separate sets for training and testing were drawn with 1 000 000 examples each. One hundred permutations of the first set are generated. Each learning algorithm is trained using various number of examples taken sequentially from the beginning of the permuted sets. The resulting performance is then measured on the testing set and averaged over the one hundred permutations. Batch-Newton algorithm The reference batch algorithm uses the Newton-Raphson algorithm with Gauss-Newton approximation (Le Cun et al., 1998). Each iteration visits all the training and computes both gradient g and the Gauss-Newton approximation H of the Hessian matrix.

2

L(x, y, ) = (1.5y - f(x))

g = L ( xi, yi, k-1)

 2 H = (f (k-1xi)) xixT i

i i

The parameters are then updated using Newton's formula: k = k-1 - H-1g Iterations are repeated until the parameter vector moves by less than 0.01/N where N is the number of training examples. This algorithm yields quadratic convergence speed. Online-Kalman algorithm The online algorithm performs a single sequential sweep over the training examples. The parameter vector is updated after processing each example (xt,yt) as follows:

1 t = t-1 - t  L ( xt, yt, t-1)



The scalar  = max(20,t - 40) makes sure that the first few examples do not cause impractically large parameter updates. The scaling matrix t is equal to the inverse of a leaky average of the per-example Gauss-Newton approximation of the Hessian.

-1 2

t = 1 - 2  -1 +

t-1

2  (f (t-1xt)) xtxT

t

The implementation avoids the matrix inversions by directly computing t from t-1 using the matrix inversion lemma. (see (Bottou, 1998) for instance.)

-1

A-1 + uuT = 1  A - (Au)(Au)T / + uTAu


1e-1 1e-1

1e-2 1e-2

1e-3 1e-3

1e-4 1e-4

1000 10000 100000 100 1000 10000

Figure 1: Average (-)2 as a function of the number of examples. The gray line represents the theoretical prediction (6). Filled circles: batch. Hollow circles: online. The error bars indicate a 95% confidence interval. Figure 2: Average (-)2 as a function of the training time (milliseconds). Hollow circles: online. Filled circles: batch. The error bars indicate a 95% confidence interval.

The resulting algorithm slightly differs from the Adaptive Natural Gradient algorithm (Amari, Park, and Fukumizu, 1998). In particular, there is little need to adjust a learning rate parameter in the Gauss-Newton approach. The 1/t (or 1/) schedule is asymptotically optimal. Results The optimal parameter vector  was first computed on the testing set using the batchnewton approach. The matrices H and G were computed on the testing set as well in order to determine the constant in relation (6). Figure 1 plots the average squared distance between the optimal parameter vector  and the parameter vector  achieved on training sets of various sizes. The gray line represents the theoretical prediction. Both the batch points and the online points join the theoretical prediction when the training set size increases. Figure 2 shows the same data points as a function of the CPU time required to run the algorithm on a standard PC. The online algorithm gradually becomes more efficient when the training set size increases. This happens because the batch algorithm needs to perform additional iterations in order to maintain the same level of accuracy. In practice, the test set mean squared error (MSE) is usually more relevant than the accuracy of the parameter vector. Figure 3 displays a logarithmic plot of the difference between the MSE and the best achievable MSE, that is to say the MSE achieved by parameter vector . This difference can be approximated as (-)TH(-). Both algorithms yield virtually identical errors for the same training set size. This suggests that the small differences shown in figure 1 occur along the low curvature directions of the cost function. Figure 4 shows the MSE as a function of the CPU time. The online algorithm always provides higher accuracy in significantly less time. As expected from the theoretical argument, the online algorithm asymptotically outperforms the super-linear Newton-Raphson algorithm3. More importantly, the online algorithm achieves this result by performing a single sweep over the training data. This is a very significant advantage when the data does not fit in central memory and must be sequentially accessed from a disk based database.

3 Generalized linear models are usually trained using the IRLS method (Chambers and Hastie,

1992) which is closely related to the Newton-Raphson algorithm and requires similar computational resources.


Mse* +1e-1 Mse* +1e-2 Mse* +1e-3 Mse* +1e-4

0.366 0.362 0.358 0.354 0.350 0.346 0.342

1000 10000 100000 100 1000 10000

Figure 3: Average test MSE as a function of the number of examples (left). The vertical axis shows the logarithm of the difference between the error and the best error achievable on the testing set. Both curves are essentially superposed. Figure 4: Average test MSE as a function of the training time (milliseconds). Hollow circles: online. Filled circles: batch. The gray line indicates the best mean squared error achievable on the test set.

6 Conclusion Many popular algorithms do not scale well to large number of examples because they were designed with small data sets in mind. For instance, the training time for Support Vector Machines scales somewhere between N2 and N3, where N is the number of examples. Our baseline super-linear batch algorithm learns in N log log N time. We demonstrate that adequate online algorithms asymptotically achieve the same generalization performance in N time after a single sweep on the training set. The convergence of learning algorithms is usually described in terms of a search phase followed by a final convergence phase (Bottou and Murata, 2002). Solid empirical evidence (Le Cun et al., 1998) suggests that online algorithms outperform batch algorithms during the search phase. The present work provides both theoretical and experimental evidence that an adequate online algorithm outperforms any batch algorithm during the final convergence phase as well. Appendix4: Sketch of the convergence speed proof Lemma -- Let (ut) be a sequence of positive reals verifying the following recurrence:

ut =  1 - + o t 1 t ut-1 +  t2 + o 1 t2 (7)

The lemma states that tut -  -1 when  > 1 and  > 0. The proof is delicate because

the result holds regardless of the unspecified low order terms of the recurrence. However, it is easy to illustrate this convergence with simple numerical simulations. Convergence speed -- Consider the following recursive stochastic process:

1 (t) = (t - 1) - t t L ( zt, (t - 1)) + O



1 n2 (8)

Our discussion addresses the final convergence phase of this process. Therefore we assume that the parameters  remain confined in a bounded domain D where the cost function C() is convex and has a single non degenerate minimum   D. We can assume

4 This section has been added for the final version


 = 0 without loss of generality. We write Et (X) the conditional expectation of X given all that is known before time t, including the initial conditions 0 and the selected examples z1, . . . , zt-1. We initially assume also that t is a function of z1, . . . , zt-1 only. Using (8), we write Et (tt) as a function of t-1. Then we simplify5 and take the trace.

Et |t|2 2 = |t-1|2 - |t-1|2 + o t |t-1|2 t + tr H-1 G H-1 t2 + o 1 t2

Taking the unconditional expectation yields a recurence similar to (7). We then apply the lemma and conclude that tE(|t|2) - tr H-1 G H-1 . Remark 1 -- The notation o(Xt) is quite ambiguous when dealing with stochastic processes. There are many possible flavors of convergence, including uniform convergence, almost sure convergence, convergence in probability, etc. Furthermore, it is not true in general that E(o(Xt)) = o(E(Xt)). The complete proof precisely defines the meaning of these notations and carefully checks their properties. Remark 2 -- The proof sketch assumes that t is a function of z1,...,zt-1 only. In (5), t also depends on zt. The result still holds because the contribution of zt vanishes quickly when t grows large.

Remark 3 -- The same

1 2

1 t

behavior holds when t   and when  is greater than

H-1 in the semi definite sense. The constant however is worse by a factor roughly equal

to ||H||. Acknowledgments The authors acknowledge extensive discussions with Yoshua Bengio, Sami Bengio, Ronan Collobert, Noboru Murata, Kenji Fukumizu, Susanna Still, and Barak Pearlmutter. References Amari, S. (1998). Natural Gradient Works Efficiently in Learning. Neural Computation, 10(2):251­ 276. Bottou, L. (1998). Online Algorithms and Stochastic Approximations, 9-42. In Saad, D., editor, Online Learning and Neural Networks. Cambridge University Press, Cambridge, UK. Bottou, L. and Murata, N. (2002). Stochastic Approximations and Efficient Learning. In Arbib, M. A., editor, The Handbook of Brain Theory and Neural Networks, Second edition,. The MIT Press, Cambridge, MA. Bottou, L. and Le Cun, Y. (2003). Online Learning for Very Large Datasets. NEC Labs TR-2003L039. To appear: Applied Stochastic Models in Business and Industry. Wiley. Chambers, J. M. and Hastie, T. J. (1992). Statistical Models in S, Chapman & Hall, London. Dennis, J. and Schnabel, R. B. (1983). Numerical Methods For Unconstrained Optimization and Nonlinear Equations. Prentice-Hall, Inc., Englewood Cliffs, New Jersey. Amari, S. and Park, H. and Fukumizu, K. (1998). Adaptive Method of Realizing Natural Gradient Learning for Multilayer Perceptrons, Neural Computation, 12(6):1399­1409 Le Cun, Y., Bottou, L., Orr, G. B., and M¨uller, K.-R. (1998). Efficient Back-prop. In Neural Networks, Tricks of the Trade, Lecture Notes in Computer Science 1524. Springer Verlag. Murata, N. and Amari, S. (1999). Statistical analysis of learning dynamics. Signal Processing, 74(1):3­28. Vapnik, V. N. and Chervonenkis, A. (1974). Theory of Pattern Recognition (in russian). Nauka. Tsypkin, Ya. (1973). Foundations of the theory of learning systems. Academic Press.

5 Recall Et t L  (zt, ) = t C  () = tH + o (||) =  + o (||)


