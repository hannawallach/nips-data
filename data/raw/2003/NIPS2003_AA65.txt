No Unbiased Estimator of the Variance of K-Fold Cross-Validation

Yoshua Bengio and Yves Grandvalet Dept. IRO, Universite´ de Montreal ´ C.P. 6128, Montreal, Qc, H3C 3J7, Canada {bengioy,grandvay}@iro.umontreal.ca

Abstract Most machine learning researchers perform quantitative experiments to estimate generalization error and compare algorithm performances. In order to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the estimation of uncertainty around the K-fold cross-validation estimator. The main theorem shows that there exists no universal unbiased estimator of the variance of K-fold cross-validation. An analysis based on the eigendecomposition of the covariance matrix of errors helps to better understand the nature of the problem and shows that naive estimators may grossly underestimate variance, as con£rmed by numerical experiments.

1 Introduction The standard measure of accuracy for trained models is the prediction error (PE), i.e. the expected loss on future examples. Learning algorithms themselves are often compared on their average performance, which estimates expected value of prediction error (EPE) over training sets. If the amount of data is large enough, PE can be estimated by the mean error over a hold-out test set. The hold-out technique does not account for the variance with respect to the training set, and may thus be considered inappropriate for the purpose of algorithm comparison [4]. Moreover, it makes an inef£cient use of data which forbids its application to small sample sizes. In this situation, one resorts to computer intensive resampling methods such as cross-validation or bootstrap to estimate PE or EPE. We focus here on K-fold cross-validation. While it is known that cross-validation provides an unbiased estimate of EPE, it is also known that its variance may be very large [2]. This variance should be estimated to provide faithful con£dence intervals on PE or EPE, and to test the signi£cance of observed differences between algorithms. This paper provides theoretical arguments showing the dif£culty of this estimation. The dif£culties of the variance estimation have already been addressed [4, 7, 8]. Some distribution-free bounds on the deviations of cross-validation are available, but they are speci£c to locally de£ned classi£ers, such as nearest neighbors [3]. This paper builds upon the work of Nadeau and Bengio [8], which investigated in detail the theoretical and practical merits of several estimators of the variance of cross-validation. Our analysis departs from this work in the sampling procedure de£ning the cross-validation estimate. While [8] considers K independent training and test splits, we focus on the standard K-fold cross-


validation procedure, with no overlap between test sets: each example is used once and only once as a test example. 2 General Framework Formally, we have a training set D = {z1,...,zn}, with zi  Z, assumed independently sampled from an unknown distribution P. We also have a learning algorithm A : Z  F which maps a data set to a function. Here we consider symmetric algorithms, i.e. A is insensitive to the ordering of examples in the training set D. The discrepancy between the prediction and the observation z is measured by a loss functional L : F × Z  R. For example one may take in regression L(f,(x,y)) = (f(x) - y)2, and in classi£cation Let f = A(D) be the function returned by algorithm A on the training set D. In application-based evaluation, the goal of learning is usually stated as the minimization of the expected loss of f = A(D) on future test examples: PE(D) = E[L(f, z)] , (1) where the expectation is taken with respect to z  P. To evaluate and compare learning algorithms [4] we care about the expected performance of learning algorithm A over

L(f,(x,y)) = 1f (x)=y .

different training sets: EPE(n) = E[L(A(D), z)] , (2)

where the expectation is taken with respect to D×z independently sampled from P n × P. When P is unknown, PE and EPE have to be estimated, and it is crucial to assess the uncertainty attached to this estimation. Although this point is often overlooked, estimating the variance of the estimates PE and EPE requires caution, as illustrated here. 2.1 Hold-out estimates of performance The mean error over a hold-out test set estimates PE, and the variance of PE is given by the usual variance estimate for means of independent variables. However, this variance estimator is not suited to EPE: the test errors are correlated when the training set is considered as a random variable. Figure 1 illustrates how crucial it is to take these correlations into account. The average ratio (estimator of variance/empirical variance) is displayed for two variance estimators, in an ideal situation where 10 independent training and test sets are available. The average of 1/, the naive variance estimator ignoring correlations, shows that this estimate is highly down-biased, even for large sample sizes.

1

0.8

0.6

0.4 100 200 300 400 500 600

Figure 1: Average ratio (estimator of variance/empirical variance) on 100 000 experiments: 1/ (ignoring correlations, lower curve) and 2/ (taking into account correlations, upper curve) vs. sample size n. The error bars represent ±2 standard errors on the average value.


Experiment 1 Ideal hold-out estimate of EPE. We have K = 10 independent training sets D1,...,DK of n independent examples zi = (xi,yi), where xi = (xi ,...,xid) is a d-dimensional centered, unit covariance tered, unit variance Gaussian variables (the

1

Gaussian variable (d = 30), yi = 3/d d k=1

xik + i with i being independent, cen-

3/d factor provides R2 3/4). We also

have K independent test sets T1,...,TK of size n sampled from the same distribution. The learning algorithm consists in £tting a line by ordinary least squares, and the

estimate of EPE is the average quadratic loss on test examples EPE

1 K

K

1

k=1 n

Lki, where Lki = L(A(Dk),zi). ziTk

The £rst estimate of variance of EPE is 1 = 1 Kn(Kn-1) K k=1 i

= L = ¯

(Lki - L)2, which ¯

is unbiased provided there is no correlation between test errors. The second estimate is

2 = 1 K(K-1)n2 K k=1 i,j (Lki - L)(Lkj - L), which estimates correlations. ¯ ¯

Note that Figure 1 suggests that the naive estimator of variance 1 asymptotically converges to the true variance. This can be shown by taking advantage of the results in this paper, as the only source of variability of EPE is due to the £nite test size. 2.2 K-fold cross-validation estimates of performance In K-fold cross-validation [9], the data set D is £rst chunked into K disjoint subsets (or blocks) of the same size m = n/K (to simplify the analysis below we assume that n is a multiple of K). Let us write Tk for the k-th such block, and Dk the training set obtained by removing the elements in Tk from D. The estimator is

long as the learning algorithm converges (PE(D) a.s. limn   EPE(n)), i.e. provided that

K

CV = 1 K 1 m L(A(Dk),zi) . (3)

k=1 ziTk

Under stability assumptions on A, CV estimates PE(D) at least as accurately as the training error [6]. However, as CV is an average of unbiased estimates of PE(D1), ...,PE(DK), a more general statement is that CV estimates unbiasedly EPE(n-m). Note that the forthcoming analysis also applies to the version of cross-validation dedicated to comparing algorithms, using matched pairs

K

CV = 1 K 1 m L(A1(Dk),zi) - L(A2(Dk),zi) ,

k=1 ziTk

and to the delete-m jackknife estimate of PE(D) debiasing the training error (see e.g. [5]):

n K n

JK= 1 n L(A(D),zi)-(K-1) 1 K(n - m)

L(A(Dk),zi) -

k=1 ziDk

1 n L(A(D),zi) .

i=1 i=1

In what follows, CV, CV and JK will generically be denoted by µ: ^

n K

µ = ^ 1 n ei = 1 K 1 m ei ,

i=1 k=1 iTk

where, slightly abusing notation, i  Tk means zi  Tk and

i  Tk, ei =



 L(A(Dk),zi)

L(A1(Dk),zi) - L(A2(Dk),zi)

KL(A(D),zi) - =k



L(A(D ),zi) for µ = JK .

for µ = CV , ^ for µ = CV , ^ ^


Note that µ is the average of identically distributed (dependent) variables. Thus, it asymp^ totically converges to a normally distributed variable, which is completely characterized by its expectation E[^] and its variance Var[^]. µ µ 3 Structure of the Covariance Matrix

The variance of µ is  = ^

1 n2

Cov(ei,ej) . By using symmetry over permutations of i,j

the examples in D, we show that the covariance matrix has a simple block structure. Lemma 1 Using the notation introduced in section 2.2, 1) all ei are identically distributed; 2) all pairs (ei,ej) belonging to the same test block are jointly identically distributed; 3) all pairs (ei,ej) belonging to different test blocks are jointly identically distributed; Proof: derived immediately from the permutation-invariance of P(D) and the symmetry of A. See [1] for details and the proofs not shown here for lack of space. Corollary 1 The covariance matrix  of cross-validation errors e = (e1,...,en) has the simple block structure depicted in Figure 2: 1) all diagonal elements are identical i, Cov(ei,ei) = Var[ei] = 2; 2) all the off-diagonal entries of the K m × m diagonal blocks are identical (i,j)  Tk : j = i, T(j) = T(i), Cov(ei,ej) = ; 3) all the

2

remaining entries are identical i  Tk, j  T : = k, Cov(ei,ej) = .

n

m

Figure 2: Structure of the covariance matrix. Corollary 2 The variance of the cross-validation estimator is a linear combination of three moments:

 = 1 n2 Cov(ei,ej) =

n

1 2 mn- 1 + + n - m n (4)

i,j

Hence, the problem of estimating  does not involve estimating n(n + 1)/2 covariances, but it cannot be reduced to that of estimating a single variance parameter. Three components intervene, which may be interpreted as follows when µ is the K-fold cross-validation ^ estimate of EPE: 1. the variance 2 is the average (taken over training sets) variance of errors for "true" test examples (i.e. sampled independently from the training sets) when algorithm A is fed with training sets of size m(K - 1); 2. the within-block covariance  would also apply to these "true" test examples; it arises from the dependence of test errors stemming from the common training set. 3. the between-blocks covariance  is due to the dependence of training sets (which share n(K - 2)/K examples) and the fact that test block Tk appears in all the training sets D for = k.


4 No Unbiased Estimator of Var[^] Exists µ Consider a generic estimator ^ that depends on the sequence of cross-validation errors e = (e1,e2,...,en) . Assuming ^is analytic in e, consider its Taylor expansion: ^ = 0 + 1(i)ei + 2(i,j)eiej + 3(i,j,k)eiejek + ... (5)

i i,j i,j,k

We £rst show that for unbiased variance estimates (i.e. E[^] = Var[^]), all the i coef£µ cients must vanish except for the second order coef£cients 2,i,j. Lemma 2 There is no universal unbiased estimator of Var[^] that involves the ei in a µ non-quadratic way. Proof: Take the expected value of ^expressed as in (5), and equate it with Var[^] (4). µ Since estimators that include moments other than the second moments in their expectation are biased, we now focus on estimators which are quadratic forms of the errors, i.e. ^ = e We = Wijeiej . (6)

i,j

Lemma 3 The expectation of quadratic estimators ^de£ned as in (6) is a linear combination of only three terms

E[^] = a(2 + µ2) + b( + µ2) + c( + µ2) , where (a,b,c) are de£ned as follows:



 a =

 

 b =

 c =



n i=1 K Wii ,

(7)



k=1 iTk K k=1 =k

jTk:j=i

iTk

Wij ,

Wij . jT

A "trivial" representer of estimators with this expected value is ^ = as1 + bs2 + cs3 ,

(8)

where (s1,s2,s3) are the only quadratic statistics of e that are invariants to the within blocks and between blocks permutations described in Lemma 1:

 

 s1 = 

s2 = s3 =







1 n

n

e2 ,

i=1 i

1 n(m-1) 1 n(n-m)

K k=1 iTk K k=1 =k

jTk:j=i

iTk

eiej ,

eiej .

(9)

jT

Proof: in (6), group the terms that have the same expected values (from Corollary 1).    Theorem 1 There exists no universally unbiased estimator of Var[^]. µ Proof: thanks to Lemma 2 and 3, it is enough to show that E[^] = Var[^] has no solution µ for quadratic estimators:

E[^] = Var[^]  a(2 + µ2) + b( + µ2) + c( + µ2) = µ . n 1 2 mn- 1 + + n - m n

Finding (a,b,c) satisfying this equality for all admissible values of (µ,2,,) is impossible since it is equivalent to solving the following overdetermined system:

  

a b 1 n

,

 ca 

= = =

m-1 n n-m

n

, ,

+ b + c = 0

(10) Q.E.D.


5 Eigenanalysis of the covariance matrix One way to gain insight on the origin of the negative statement of Theorem 1 is via the eigenanalysis of , the covariance of e. This decomposition can be performed analytically thanks to the very particular block structure displayed in Figure 2. Lemma 4 Let vk be the binary vector indicating the membership of each example to test block k. The eigenvalues of  are as follows: · 1 = 2 - with multiplicity n-K and eigenspace orthogonal to {vk}K ;

k=1

· 2 = 2 + (m - 1) - m with multiplicity K - 1 and eigenspace de£ned in the orthogonal of 1 by the basis {vk}K ;

k=1

· 3 = 2 +(m-1) +(n-m) with eigenvector 1. Lemma 4 states that the vector e can be decomposed into three uncorrelated parts: n - K projections to the subspace orthogonal to {vk}K , K - 1 projections to the subspace

k=1

spanned by {vk}K in the orthogonal of 1, and one projection on 1. k=1

A single vector example with n independent elements can be seen as n independent examples. Similarly, the uncorrelated projections of e can be equivalently represented by respectively n - K, K - 1 and one uncorrelated one-dimensional examples. In particular, for the projection on 1, with a single example, the sample variance is null, resulting in the absence of unbiased variance estimator of 3. The projection of e on when we have only one realization of the vector e. For the same reason, even with simple parametric assumptions on e (such as e Gaussian), the maximum likelihood estimate of  is not de£ned. Only 1 and 2 can be estimated unbiasedly. Note that this problem cannot be addressed by performing multiple K-fold splits of the data set. Such a procedure would not provide independent realizations of e. 6 Possible values for  and  Theorem 1 states that no estimator is unbiased, and in its demonstration, it is shown that the bias of any quadratic estimator is a linear combination of µ2, 2,  and . Regarding estimation, it is thus interesting to see what constraints restrict their possible range. Lemma 5 For µ = CV and µ = CV, the following inequalities hold:

the eigenvector 1 is precisely µ 1 n ^. Hence there is no unbiased estimate of V ar[^] = µ 3 n

^

^ 0

  

-n

1 1 2

m

2 ( + (m - 1))

-m

0

m

-m

-n 

(2 + (m - 1))       2 2    2 .

The admissible (,) region is very large, and there is no constraint linking µ to 2. Hence, we cannot propose a variance estimate with universally small bias. 7 Experiments The bias of any quadratic estimator is a linear combination of µ2, 2,  and . The admissible values provided earlier suggest that  and  cannot be proved to be negligible compared to 2. This section illustrates that in practice, the contribution to the variance of µ due to  and  (see Equation (4)) can be of same order as the one due 2. This con£rms ^ that the estimators of  should indeed take into account the correlations of ei. Experiment 2 True variance of K-fold cross-validation. We repeat the experimental setup of Experiment 1, except that only one sample of size n is available. Since cross-validation is known to be sensitive to the instability of algorithms,


in addition to this standard setup, we also consider another one with outliers: The input xi = (xi ,...,xid) is still 30-dimensional, but it is now a mixture of two cen-

1

tered Gaussian: let ti be a binary variable, with P(ti = 1) = p = 0.95; ti = 1  xi  ti = 1  i  N(0,1/(p + 100(1 - p))), ti = 0  i  N(0,100/(p + 100(1 - p))). We now look at the variance of K-fold cross-validation (K = 10), and decompose in the three orthogonal components 2,  and . The results are shown in Figure 3.

N(0,I), ti = 0  xi  N(0,100I); yi = 3/(d(p + 100(1 - p))) d k=1 xik + i;

0.25 4

0.2

0.15

2  

3

2  

  2

1

0.1

0.05

0 0 60 80 100 120 160 220 280 360 460 600

n-m no outliers

60 80 100 120 160 220 280 360 460 600

n-m outliers

Figure 3: Contributions of (2,,) to total variance V ar[CV ] vs. n - m. Without outliers, the contribution of  is very important for small sample sizes. For large sample sizes, the overall variance is considerably reduced and is mainly caused by 2 because the learning algorithm returns very similar answers for all training sets. When there are outliers, the contribution of  is of same order as the one of 2 even when the ratio of examples to free parameters is large (here up to 20). Thus, in dif£cult situations, where A(D) varies according to the realization of D, neglecting the effect of  and  can be expected to introduce a bias of the order of the true variance. It is also interesting to see how these quantities are affected by the number of folds K. The decomposition of  in 2,  and  (4) does not imply that K should be set either to n or to 2 (according to the sign of -) in order to minimize the variance of µ. Modifying ^ K affects 2,  and  through the size and overlaps of the training sets D1,...,DK, as illustrated in Figure 4. For a £xed sample size, the variance of µ and the contribution of 2, ^  and  vary smoothly with K (of course, the mean of µ is also affected in the process). ^

0.25 2.5

0.2

0.15

2  

2

1.5

 

0.1 1

0.05 0.5

0 0 2 3 4 5 6 8 10 12 15 20 24 30 40 60120

K no outliers

2 3 4 5 6 8 10 12 15 20 24 30 40 60120

K outliers

Figure 4: Contributions of (2,,) to total variance V ar[CV ] vs. K for n = 120.

8 Discussion The analysis presented in this paper for K-fold cross-validation can be instantiated to several interesting cases. First, when having K independent training and test sets (K = 1


is the realistic case), the structure of hold-out errors resemble the one of cross-validation errors, with  = 0. Knowing that allows to build the unbiased estimate 2 given in 2.1: knowing that  = 0 removes the third equation of system (10) in the proof of Theorem 1. Two-fold cross-validation has been advocated to perform hypothesis testing [4]. It is a special case of K-fold cross-validation where the training blocks are mutually independent since they do not overlap. However, this independence does not modify the structure of e in the sense that  is not null. The between-block correlation stems from the fact that the training block D1 is the test block T2 and vice-versa. Finally, Leave-one-out cross validation is another particular case, with K = n. The structure of the covariance matrix is simpli£ed, without diagonal blocks. The estimation dif£culties however remain: even in this particular case, there is no unbiased estimate of variance. From the de£nition of b in Lemma 3, we have b = 0, and with m = 1 the linear system (10) still admits no solution. To summarize, it is known that K-fold cross-validation may suffer from high variability, which can be responsible for bad choices in model selection and erratic behavior in the estimated expected prediction error [2, 4, 8]. This paper demonstrates that estimating the variance of K-fold cross-validation is dif£cult. Not only there is no unbiased estimate of this variance, but we have no theoretical result showing that this bias should be negligible in the non-asymptotical regime. The eigenanalysis of the covariance matrix of errors traces the problem back to the dependencies between test-block errors, which induce the absence of redundant pieces of information regarding the average test error. i.e. the K-fold cross-validation estimate. It is clear that this absence of redundancy is bound to provide dif£culties in the estimation of variance. Our experiments show that the bias incurred by ignoring test errors dependencies can be of the order of the variance itself, even for large sample sizes. Thus, the assessment of the signi£cance of observed differences in cross-validation scores should be treated with much caution. The next step of this study consists in building and comparing variance estimators dedicated to the very speci£c structure of the test-block error dependencies. References [1] Y. Bengio and Y. Grandvalet. No unbiased estimator of the variance of K-fold cross-validation. Journal of Machine Learning Research, 2003. [2] L. Breiman. Heuristics of instability and stabilization in model selection. The Annals of Statistics, 24(6):2350­2383, 1996. [3] L. Devroye, L. Gy¨or£, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, 1996. [4] T. G. Dietterich. Approximate statistical tests for comparing supervised classi£cation learning algorithms. Neural Computation, 10(7):1895­1924, 1999. [5] B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap, volume 57 of Monographs on Statistics and Applied Probability. Chapman & Hall, 1993. [6] M. Kearns and D. Ron. Algorithmic stability and sanity-check bounds for leave-one-out crossvalidation. Neural Computation, 11(6):1427­1453, 1996. [7] R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the Fourteenth International Joint Conference on Arti£cial Intelligence, pages 1137­1143, 1995. [8] C. Nadeau and Y. Bengio. Inference for the generalization error. Machine Learning, 52(3):239­ 281, 2003. [9] M. Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the Royal Statistical Society, B, 36(1):111­147, 1974.


