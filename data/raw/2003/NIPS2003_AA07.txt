1-norm Support Vector Machines

Ji Zhu, Saharon Rosset, Trevor Hastie, Rob Tibshirani Department of Statistics Stanford University Stanford, CA 94305 {jzhu,saharon,hastie,tibs}@stat.stanford.edu

Abstract The standard 2-norm SVM is known for its good performance in twoclass classi즓ation. In this paper, we consider the 1-norm SVM. We argue that the 1-norm SVM may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. We also propose an ef즓ient algorithm that computes the whole solution path of the 1-norm SVM, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM.

1 Introduction In standard two-class classi즓ation problems, we are given a set of training data (x1, y1), . . . (xn, yn), where the input xi  Rp, and the output yi  {1, -1} is binary. We wish to 즢d a class즓ation rule from the training data, so that when given a new input x, we can assign a class y from {1, -1} to it. To handle this problem, we consider the 1-norm support vector machine (SVM):



jhj(xi) 

+

n q

- yi + min

0,

s.t.

 1  0

(1) (2)

i=1



j=1

= |1| + 렁 + |q|  s, 1

where D = {h1(x), . . . hq(x)} is a dictionary of basis functions, and s is a tuning parameter. The solution is denoted as ^0(s) and ^(s); the 즨ted model is f^(x) = ^0 + ^jhj(x). (3)

q

j=1

The classi즓ation rule is given by sign[f^(x)]. The 1-norm SVM has been successfully used in [1] and [9]. We argue in this paper that the 1-norm SVM may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. To get a good 즨ted model f^(x) that performs well on future data, we also need to select an appropriate tuning parameter s. In practice, people usually pre-specify a 즢ite set of values for s that covers a wide range, then either use a separate validation data set or use


cross-validation to select a value for s that gives the best performance among the given set. In this paper, we illustrate that the solution path ^(s) is piece-wise linear as a function of s (in the Rq space); we also propose an ef즓ient algorithm to compute the exact whole solution path {^(s), 0  s  }, hence help us understand how the solution changes with s and facilitate the adaptive selection of the tuning parameter s. Under some mild assumptions, we show that the computational cost to compute the whole solution path ^(s) is O(nq min(n, q)2) in the worst case and O(nq) in the best case. Before delving into the technical details, we illustrate the concept of piece-wise linearity of the solution path ^(s) with a simple example. We generate 10 training data in each of two classes. The 즦st class has two standard normal independent inputs x1, x2. The second class also has two standard normal independent inputs, but conditioned on 4.5  x21+x22  path ^(s) as a function of s is shown in Figure 1. Any segment between two adjacent vertical lines is linear. Hence the right derivative of ^(s) with respect to s is piece-wise constant (in Rq). The two solid paths are for x21 and x22, which are the two relevant features.

8. The dictionary of basis functions is D = {2x1,  

2x2, 2x1x2, x21, x22}. The solution

0.8

0.6

^  0.4

0.2

0.0

0.0 0.5 s 1.0 1.5

Figure 1: The solution path ^(s) as a function of s.

In section 2, we motivate why we are interested in the 1-norm SVM. In section 3, we describe the algorithm that computes the whole solution path ^(s). In section 4, we show some numerical results on both simulation data and real world data. 2 Regularized support vector machines The standard 2-norm SVM is equivalent to 즨 a model that

n q

- yi + min

0,j

 1  0  jhj(xi) +   

+

2 2 , (4)

i=1 j=1

where  is a tuning parameter. In practice, people usually choose hj(x)'s to be the basis functions of a reproducing kernel Hilbert space. Then a kernel trick allows the dimension of the transformed feature space to be very large, even in즢ite in some cases (i.e. q = ), without causing extra computational burden ([2] and [12]). In this paper, however, we will concentrate on the basis representation (3) rather than a kernel representation. Notice that (4) has the form loss + penalty, and  is the tuning parameter that controls the tradeoff between loss and penalty. The loss (1 - yf)+ is called the hinge loss, and


the penalty is called the ridge penalty. The idea of penalizing by the sum-of-squares of the parameters is also used in neural networks, where it is known as weight decay. The ridge penalty shrinks the 즨ted coef즓ients ^ towards zero. It is well known that this shrinkage has the effect of controlling the variances of ^, hence possibly improves the 즨ted model's prediction accuracy, especially when there are many highly correlated features [6]. So from a statistical function estimation point of view, the ridge penalty could possibly explain the success of the SVM ([6] and [12]). On the other hand, computational learning theory has associated the good performance of the SVM to its margin maximizing property [11], a property of the hinge loss. [8] makes some effort to build a connection between these two different views. In this paper, we replace the ridge penalty in (4) with the L1-norm of , i.e. the lasso penalty [10], and consider the 1-norm SVM problem:

n q

- yi + min

0,

 1  0  jhj(xi) +   

+

1 , (5)

i=1 j=1

which is an equivalent Lagrange version of the optimization problem (1)-(2). The lasso penalty was 즦st proposed in [10] for regression problems, where the response y is continuous rather than categorical. It has also been used in [1] and [9] for classi즓ation problems under the framework of SVMs. Similar to the ridge penalty, the lasso penalty also shrinks the 즨ted coef즓ients ^'s towards zero, hence (5) also bene즨s from the reduction in 즨ted coef즓ients' variances. Another property of the lasso penalty is that because of the L1 nature of the penalty, making  suf즓iently large, or equivalently s suf즓iently small, will cause some of the coef즓ients ^j's to be exactly zero. For example, when s = 1 in Figure 1, only three 즨ted coef즓ients are non-zero. Thus the lasso penalty does a kind of continuous feature selection, while this is not the case for the ridge penalty. In (4), none of the ^j's will be equal to zero. It is interesting to note that the ridge penalty corresponds to a Gaussian prior for the j's, while the lasso penalty corresponds to a double-exponential prior. The double-exponential density has heavier tails than the Gaussian density. This re쨎cts the greater tendency of the lasso to produce some large 즨ted coef즓ients and leave others at 0, especially in high dimensional problems. Recently, [3] consider a situation where we have a small number of training data, e.g. n = 100, and a large number of basis functions, e.g. q = 10, 000. [3] argue that in the sparse scenario, i.e. only a small number of true coef즓ients j's are nonzero, the lasso penalty works better than the ridge penalty; while in the non-sparse scenario, e.g. the true coef즓ients j's have a Gaussian distribution, neither the lasso penalty nor the ridge penalty will 즨 the coef즓ients well, since there is too little data from which to estimate these non-zero coef즓ients. This is the curse of dimensionality taking its toll. Based on these observations, [3] further propose the bet on sparsity principle for highdimensional problems, which encourages using lasso penalty.

3 Algorithm Section 2 gives the motivation why we are interested in the 1-norm SVM. To solve the 1-norm SVM for a 즬ed value of s, we can transform (1)-(2) into a linear programming problem and use standard software packages; but to get a good 즨ted model f^(x) that performs well on future data, we need to select an appropriate value for the tuning paramter s. In this section, we propose an ef즓ient algorithm that computes the whole solution path ^(s), hence facilitates adaptive selection of s.


3.1 Piece-wise linearity If we follow the solution path ^(s) of (1)-(2) as s increases, we will notice that since both not change when s increases unless a residual (1 - yif^i) changes from non-zero to zero, or a 즨ted coef즓ient ^j(s) changes from non-zero to zero, which correspond to the nonsmooth points of . This implies that the derivative of ^(s) with respect to s is piece-wise constant, because when the Karush-Kuhn-Tucker conditions do not change, the derivative of ^(s) will not change either. Hence it indicates that the whole solution path ^(s) is piece-wise linear. See [13] for details. Thus to compute the whole solution path ^(s), all we need to do is to 즢d the joints, i.e. the asterisk points in Figure 1, on this piece-wise linear path, then use straight lines to interpolate them, or equivalently, to start at ^(0) = 0, 즢d the right derivative of ^(s), let s increase and only change the derivative when ^(s) gets to a joint. 3.2 Initial solution (i.e. s = 0) The following notation is used. Let V = {j : ^j(s) = 0}, E = {i : 1 - yif^i = 0}, denotes the components of ^(s) with indices in V. Without loss of generality, we assume #{yi = 1}  #{yi = -1}; then ^0(0) = 1, ^j(0) = 0. To compute the path that ^(s) follows, we need to compute the derivative of ^(s) at 0. We consider a modi즕d problem: min (1 - yifi)+ + (1 - yifi) (6)

are piece-wise linear, the Karush-Kuhn-Tucker conditions will i (1 - yif^i)+ and  1

i

(1 - yif^i)+ and  1

L = {i : 1 - yif^i > 0} and u for the right derivative of ^V(s): u 1 = 1 and ^V(s)

0,j

yi=1 yi=-1

fi = 0 +

q

s.t.  1  s, jhj(xi). (7)

j=1

Notice that if yi = 1, the loss is still (1 - yifi)+; but if yi = -1, the loss becomes (1 - yifi). In this setup, the derivative of ^(s) with respect to s is the same no matter what value s is, and one can show that it coincides with the right derivative of ^(s) when s is suf즓iently small. Hence this setup helps us 즢d the initial derivative u of ^(s). Solving (6)-(7), which can be transformed into a simple linear programming problem, we get initial V, E and L. |V| should be equal to |E|. We also have:

^0(s) ^V(s) s starts at 0 and increases. 3.3 Main algorithm = 1 0 + s  u0 u . (8)

The main algorithm that computes the whole solution path ^(s) proceeds as following: 1. Increase s until one of the following two events happens:  A training point hits E, i.e. 1 - yifi = 0 becomes 1 - yifi = 0 for some i.  A basis function in V leaves V, i.e. ^j = 0 becomes ^j = 0 for some j. Let the current ^0, ^ and s be denoted by ^0 , ^old and sold.

old


2. For each j  V, we solve: /

u0 + V

ujhj(xi) + ujhj(xi)

V sign(^j old )uj + |uj|

= = 0 1 for i  E (9)

where u0, uj and uj are the unknowns. We then compute:

lossj s = yi u0 + ujhj(xi) + ujhj(xi) . (10)

L V

3. For each i  E, we solve: u0 +

V

V

ujhj(xi)

sign(^j old )uj

= = 0 1 for i  E\{i } (11)

where u0 and uj are the unknowns. We then compute:

lossi s = yi

loss s

u0 + ujhj(xi) . (12)

L V

from step 2 and step 3. There are q-|V|+ 4. Compare the computed values of

|E| = q + 1 such values. Choose the smallest negative

 If the smallest

loss s

loss s . Hence,

is non-negative, the algorithm terminates; else

 If the smallest negative loss s corresponds to a j in step 2, we update

V  V  {j},

 If the smallest negative loss s

u  u uj . (13)

corresponds to a i in step 3, we update u and L  L  {i } if necessary. (14)

E  E\{i },

In either of the last two cases, ^(s) changes as:

^0(sold + s) ^V(sold + s) and we go back to step 1. = ^0 ^V old old + s  u0 u , (15)

In the end, we get a path ^(s), which is piece-wise linear. 3.4 Remarks Due to the page limit, we omit the proof that this algorithm does indeed give the exact whole solution path ^(s) of (1)-(2) (see [13] for detailed proof). Instead, we explain a little what each step of the algorithm tries to do. Step 1 of the algorithm indicates that ^(s) gets to a joint on the solution path and the right derivative of ^(s) needs to be changed if either a residual (1-yif^i) changes from non-zero to zero, or the coef즓ient of a basis function ^j(s) changes from non-zero to zero, when s increases. Then there are two possible types of actions that the algorithm can take: (1) add a basis function into V, or (2) remove a point from E. Step 2 computes the possible right derivative of ^(s) if adding each basis function hj(x) into V. Step 3 computes the possible right derivative of ^(s) if removing each point i from E. The possible right derivative of ^(s) (determined by either (9) or (11)) is such that the training points in E are kept in E when s increases, until the next joint (step 1) occurs. loss/s indicates how fast the loss will decrease if ^(s) changes according to u. Step 4 takes the action corresponding to the smallest negative loss/s. When the loss can not be decreased, the algorithm terminates.


Table 1: Simulation results of 1-norm and 2-norm SVM Test Error (SE)

Simulation No noise input 2 noise inputs 4 noise inputs 6 noise inputs 8 noise inputs 1-norm 0.073 (0.010) 0.074 (0.014) 0.074 (0.009) 0.082 (0.009) 0.084 (0.011) 2-norm 0.08 (0.02) 0.10 (0.02) 0.13 (0.03) 0.15 (0.03) 0.18 (0.03) No Penalty 0.08 (0.01) 0.12 (0.03) 0.20 (0.05) 0.22 (0.06) 0.22 (0.06)

|D| # Joints

5 14 27 44 65 94 (13) 149 (20) 225 (30) 374 (52) 499 (67) 1 2 3 4 5

3.5 Computational cost We have proposed an algorithm that computes the whole solution path ^(s). A natural question is then what is the computational cost of this algorithm? Suppose |E| = m at a joint on the piece-wise linear solution path, then it takes O(qm2) to compute step 2 and step 3 of the algorithm through Sherman-Morrison updating formula. If we assume the training data are separable by the dictionary D, then all the training data are eventually going to have loss (1 - yif^i)+ equal to zero. Hence it is reasonable to assume the number of joints on the piece-wise linear solution path is O(n). Since the maximum value of m is min(n, q) and the minimum value of m is 1, we get the worst computational cost is O(nq min(n, q)2) and the best computational cost is O(nq). Notice that this is a rough calculation of the computational cost under some mild assumptions. Simulation results (section 4) actually indicate that the number of joints tends to be O(min(n, q)). 4 Numerical results In this section, we use both simulation and real data results to illustrate the 1-norm SVM. 4.1 Simulation results The data generation mechanism is the same as the one described in section 1, except that we generate 50 training data in each of two classes, and to make harder problems, we sequentially augment the inputs with additional two, four, six and eight standard normal noise inputs. Hence the second class almost completely surrounds the 즦st, like the skin surrounding the oragne, in a two-dimensional subspace. The Bayes error rate for this problem is 0.0435, irrespective of dimension. In the original input space, a hyperplane cannot separate the classes; we use an enlarged feature space corresponding to 2nd degree poly1, . . . p}. We generate 1000 test data to compare the 1-norm SVM and the standard 2-norm SVM. The average test errors over 50 simulations, with different numbers of noise inputs, are shown in Table 1. For both the 1-norm SVM and the 2-norm SVM, we choose the tuning parameters to minimize the test error, to be as fair as possible to each method. For comparison, we also include the results for the non-penalized SVM. From Table 1 we can see that the non-penalized SVM performs signi즓antly worse than the penalized ones; the 1-norm SVM and the 2-norm SVM perform similarly when there is no noise input (line 1), but the 2-norm SVM is adversely affected by noise inputs (line 2 - line 5). Since the 1-norm SVM has the ability to select relevant features and ignore redundant features, it does not suffer from the noise inputs as much as the 2-norm SVM. Table 1 also shows the number of basis functions q and the number of joints on the piece-wise linear solution path. Notice that q < n and there is a striking linear relationship between |D| and #Joints (Figure 2). Figure 2 also shows the 1-norm SVM result for one simulation.

nomial kernel, hence the dictionary of basis functions is D = {2xj, the 2xjxj , x2j , j, j =


0.20

1.0 500

0.15 400

0.5

Joints Error of

^ 

Test 300

0.10 Number

0.0 200

100

0.05 -0.5

0 2

s 4 6 0 2 s 4 6 10 20 30 40 50 60 Number of Bases

Figure 2: Left and middle panels: 1-norm SVM when there are 4 noise inputs. The left panel is the piece-wise linear solution path ^(s). The two upper paths correspond to x21 and x22, which are the relevant features. The middle panel is the test error along the solution path. The dash lines correspond to the minimum of the test error. The right panel illustrates the linear relationship between the number of basis functions and the number of joints on the solution path when q < n.

4.2 Real data results In this section, we apply the 1-norm SVM to classi즓ation of gene microarrays. Classi즓ation of patient samples is an important aspect of cancer diagnosis and treatment. The 2-norm SVM has been successfully applied to microarray cancer diagnosis problems ([5] and [7]). However, one weakness of the 2-norm SVM is that it only predicts a cancer class label but does not automatically select relevant genes for the classi즓ation. Often a primary goal in microarray cancer diagnosis is to identify the genes responsible for the classi즓ation, rather than class prediction. [4] and [5] have proposed gene selection methods, which we call univariate ranking (UR) and recursive feature elimination (RFE) (see [14]), that can be combined with the 2-norm SVM. However, these procedures are two-step procedures that depend on external gene selection methods. On the other hand, the 1-norm SVM has an inherent gene (feature) selection property due to the lasso penalty. Hence the 1-norm SVM achieves the goals of classi즓ation of patients and selection of genes simultaneously. We apply the 1-norm SVM to leukemia data [4]. This data set consists of 38 training data and 34 test data of two types of acute leukemia, acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL). Each datum is a vector of p = 7, 129 genes. We use the original input xj, i.e. the jth gene's expression level, as the basis function, i.e. q = p. The tuning parameter is chosen according to 10-fold cross-validation, then the 즢al model is 즨ted on all the training data and evaluated on the test data. The number of joints on the solution path is 104, which appears to be O(n) O(q). The results are summarized in Table 2. We can see that the 1-norm SVM performs similarly to the other methods in classi즓ation and it has the advantage of automatically selecting relevant genes. We should notice that the maximum number of genes that the 1-norm SVM can select is upper bounded by n, which is usually much less than q in microarray problems.

5 Conclusion We have considered the 1-norm SVM in this paper. We illustrate that the 1-norm SVM may have some advantage over the 2-norm SVM, especially when there are redundant features. The solution path ^(s) of the 1-norm SVM is a piece-wise linear function in the tuning


Table 2: Results on Microarray Classi즓ation

Method 2-norm SVM UR 2-norm SVM RFE 1-norm SVM CV Error 2/38 2/38 2/38 Test Error 3/34 1/34 2/34 # of Genes 22 31 17

parameter s. We have proposed an ef즓ient algorithm to compute the whole solution path ^(s) of the 1-norm SVM, and facilitate adaptive selection of the tuning parameter s. Acknowledgments Hastie was partially supported by NSF grant DMS-0204162, and NIH grant ROI-CA72028-01. Tibshirani was partially supported by NSF grant DMS-9971405, and NIH grant ROI-CA-72028. References [1] Bradley, P. & Mangasarian, O. (1998) Feature selection via concave minimization and support vector machines. In J. Shavlik (eds), ICML'98. Morgan Kaufmann. [2] Evgeniou, T., Pontil, M. & Poggio., T. (1999) Regularization networks and support vector machines. Advances in Large Margin Classi즕rs. MIT Press. [3] Friedman, J., Hastie, T, Rosset, S, Tibshirani, R. & Zhu, J. (2004) Discussion of "Consistency in boosting" by W. Jiang, G. Lugosi, N. Vayatis and T. Zhang. Annals of Statistics. To appear. [4] Golub,T., Slonim,D., Tamayo,P., Huard,C., Gaasenbeek,M., Mesirov,J., Coller,H., Loh,M., Downing,J. & Caligiuri,M. (1999) Molecular classi즓ation of cancer: class discovery and class prediction by gene expression monitoring. Science 286, 531-536. [5] Guyon,I., Weston,J., Barnhill,S. & Vapnik,V. (2002) Gene selection for cancer classi즓ation using support vector machines. Machine Learning 46, 389-422. [6] Hastie, T., Tibshirani, R. & Friedman, J. (2001) The Elements of Statistical Learning. SpringerVerlag, New York. [7] Mukherjee, S., Tamayo,P., Slonim,D., Verri,A., Golub,T., Mesirov,J. & Poggio, T. (1999) Support vector machine classi즓ation of microarray data. Technical Report AI Memo 1677, MIT. [8] Rosset, S., Zhu, J. & Hastie, T. (2003) Boosting as a regularized path to a maximum margin classi즕r. Technical Report, Department of Statistics, Stanford University, CA. [9] Song, M., Breneman, C., Bi, J., Sukumar, N., Bennett, K., Cramer, S. & Tugcu, N. (2002) Prediction of protein retention times in anion-exchange chromatography systems using support vector regression. Journal of Chemical Information and Computer Sciences, September. [10] Tibshirani, R. (1996) Regression shrinkage and selection via the lasso. J.R.S.S.B. 58, 267-288. [11] Vapnik, V. (1995) Tha Nature of Statistical Learning Theory. Springer-Verlag, New York. [12] Wahba, G. (1999) Support vector machine, reproducing kernel Hilbert spaces and the randomized GACV. Advances in Kernel Methods - Support Vector Learning, 69-88, MIT Press. [13] Zhu, J. (2003) Flexible statistical modeling. Ph.D. Thesis. Stanford University. [14] Zhu, J. & Hastie, T. (2003) Classi즓ation of gene microarrays by penalized logistic regression. Biostatistics. Accepted.


