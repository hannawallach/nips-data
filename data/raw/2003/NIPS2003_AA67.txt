Probability Estimates for Multi-class Classification by Pairwise Coupling

Ting-Fan Wu Chih-Jen Lin

Department of Computer Science National Taiwan University Taipei 106, Taiwan

Ruby C. Weng Department of Statistics National Chenechi University Taipei 116, Taiwan

Abstract Pairwise coupling is a popular multi-class classification method that combines together all pairwise comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than two existing popular methods: voting and [3]. 1 Introduction The multi-class classification problem refers to assigning each of the observations into one of k classes. As two-class problems are much easier to solve, many authors propose to use two-class classifiers for multi-class classification. In this paper we focus on techniques that provide a multi-class classification solution by combining all pairwise comparisons. A common way to combine pairwise comparisons is by voting [6, 2]. It constructs a rule for discriminating between every pair of classes and then selecting the class with the most winning two-class decisions. Though the voting procedure requires just pairwise decisions, it only predicts a class label. In many scenarios, however, probability estimates are desired. As numerous (pairwise) classifiers do provide class probabilities, several authors [12, 11, 3] have proposed probability estimates by combining the pairwise class probabilities. Given the observation x and the class label y, we assume that the estimated pairwise class probabilities rij of µij = p(y = i | y = i or j, x) are available. Here rij are obtained by some binary classifiers. Then, the goal is to estimate {pi}ki=1, where pi = p(y = i | x), i = 1, . . . , k. We propose to obtain an approximate solution to an identity, and then select the label with the highest estimated class probability. The existence of the solution is guaranteed by theory in finite Markov Chains. Motivated by the optimization formulation of this method, we propose a second approach. Interestingly, it can also be regarded as an improved version of the coupling approach given by [12]. Both of the proposed methods can be reduced to solving linear systems and are simple in practical implementation. Furthermore, from conceptual and experimental points of view, we show that the two proposed methods are more stable than voting and the method in [3]. We organize the paper as follows. In Section 2, we review two existing methods. Sections 3 and 4 detail the two proposed approaches. Section 5 presents the relationship among the four methods through their corresponding optimization formulas. In Section 6, we compare


these methods using simulated and real data. The classifiers considered are support vector machines. Section 7 concludes the paper. Due to space limit, we omit all detailed proofs. A complete version of this work is available at http://www.csie.ntu.edu.tw/ ~cjlin/papers/svmprob/svmprob.pdf. 2 Review of Two Methods Let rij be the estimates of µij = pi/(pi + pj). The voting rule [6, 2] is V = argmaxi[ I{rij>rji}]. (1)

j:j=i

A simple estimate of probabilities can be derived as pvi = 2 j:j=i I{rij>rji}/(k(k - 1)).

The authors of [3] suggest another method to estimate class probabilities, and they claim that the resulting classification rule can outperform V in some situations. Their approach is based on the minimization of the Kullback-Leibler (KL) distance between rij and µij: l(p) = nijrij log(rij/µij), (2)

i=j

pi = 1, pi > 0, i = 1, . . . , k, and nij is the number of instances in class i or where

k i=1

j. By letting l(p) = 0, a nonlinear system has to be solved. [3] proposes an iterative

procedure to find the minimum of (2). If rij > 0, i = j, the existence of a unique global

minimal solution to (2) has been proved in [5] and references therein. Let p point. Then the resulting classification rule is HT (x) = argmaxi[pi ]. It is shown in Theorem 1 of [3] that

2 rjs

pi > pj if and only if p~i > p~j, where p~j =

s:s=j

;



denote this

(3)

k(k - 1)

that is, the p~i are in the same order as the pi . Therefore, p are sufficient if one only requires ~ the classification rule. In fact, as pointed out by [3], p can be derived as an approximation ~ to the identity by replacing pi + pj with 2/k, and µij with rij.

pi = ( pk i

j:j=i

+ pj - 1 )( pi pi + pj ) = ( pk i + pj - 1 )µij (4)

j:j=i

3 Our First Approach Note that HT is essentially argmaxi[~i], and p is an approximate solution to (4). Instead p ~ of replacing pi + pj by 2/k, in this section we propose to solve the system:

k

pi = ( pk i

j:j=i

+ pj - 1 )rij, i, subject to pi = 1, pi  0, i. (5)

i=1

Let p denote the solution to (5). Then the resulting decision rule is ¯ 1 = argmaxi[¯i]. p As HT relies on pi + pj  k/2, in Section 6.1 we use two examples to illustrate possible problems with this rule.


To solve (5), we rewrite it as

k

Qp = p, Observe that

pi = 1, pi  0, i, where Qij =

i=1 k j=1

rij/(k - 1)

ris/(k - 1) s:s=i

if i = j, if i = j. (6)

Qij = 1 for i = 1, . . . , k and 0  Qij  1 for i, j = 1, . . . , k, so there

exists a finite Markov Chain whose transition matrix is Q. Moreover, if rij > 0 for all i = j, then Qij > 0, which implies this Markov Chain is irreducible and aperiodic. These conditions guarantee the existence of a unique stationary probability and all states being positive recurrent. Hence, we have the following theorem: Theorem 1 If rij > 0, i = j, then (6) has a unique solution p with 0 < pi < 1, i. With Theorem 1 and some further analyses, if we remove the constraint pi  0, i, the linear system with k + 1 equations still has the same unique solution. Furthermore, if any one of the k equalities Qp = p is removed, we have a system with k variables and k equalities, which, again, has the same single solution. Thus, (6) can be solved by Gaussian elimination. On the other hand, as the stationary solution of a Markov Chain can be derived by the limit of the n-step transition probability matrix Qn, we can solve p by repeatedly multiplying QT with any initial vector. Now we reexamine this method to gain more insight. The following arguments show that the solution to (5) is a global minimum of a meaningful optimization problem. To begin, we express (5) as rjipi - rijpj = 0, i = 1, . . . , k, using the property that rij + rji = 1, i = j. Then the solution to (5) is in fact the global minimum of the following problem: min pi = 1, pi  0, i. (7) Since the object function is always nonnegative, and it attains zero under (5) and (6). 4 Our Second Approach Note that both approaches in Sections 2 and 3 involve solving optimization problems using the relations like pi/(pi +pj)  rij or rjipi  rijpj. Motivated by (7), we suggest another optimization formulation as follows: min (rjipi - rijpj)2 subject to pi = 1, pi  0, i. (8)

j:j=i j:j=i

k k

( rjipi - rijpj)2 subject to

i=1 j:j=i j:j=i

p i=1

j:j=i j:j=i

k k

p

1 2

i=1 j:j=i i=1

In related work, [12] proposes to solve a linear system consisting of

k i=1

pi = 1 and

any k - 1 equations of the form rjipi = rijpj. However, pointed out in [11], the results of [12] strongly depends on the selection of k - 1 equations. In fact, as (8) considers all rijpj - rjipi, not just k - 1 of them, it can be viewed as an improved version of [12]. Let p denote the corresponding solution. We then define the classification rule as 2 = argmaxi[pi ]. Since (7) has a unique solution, which can be obtained by solving a simple linear system, it is desired to see whether the minimization problem (8) has these nice properties. In the rest of the section, we show that this is true. The following theorem shows that the nonnegative constraints in (8) are redundant.






Theorem 2 Problem (8) is equivalent to a simplification without conditions pi  0, i. Note that we can rewrite the objective function of (8) as

rsi 2 min =

p

1 2 T p Qp,

s:s=i

rjirij

where Qij = if i = j, if i = j. (9)

From here we can show that Q is positive semi-definite. Therefore, without constraints pi  0, i, (9) is a linear-constrained convex quadratic programming problem. Consequently, a point p is a global minimum if and only if it satisfies the KKT optimality condition: There is a scalar b such that

Q

T e

e 0 p b = 0 1 . (10)

Here e is the vector of all ones and b is the Lagrangian multiplier of the equality constraint (10). The existence of a unique solution is guaranteed by the invertibility of the matrix of (10). Moreover, if Q is positive definite(PD), this matrix is invertible. The following theorem shows that Q is PD under quite general conditions.

k i=1 pi = 1. Thus, the solution of (8) can be obtained by solving the simple linear system

Theorem 3 If for any i = 1, . . . , k, there are s = i and j = i such that then Q is positive definite. rsirsj ris = rjirjs rij ,

In addition to direct methods, next we propose a simple iterative method for solving (10):

Algorithm 1 1. Start with some initial pi  0, i and 2. Repeat (t = 1, . . . , k, 1, . . .)

pt  1 Qtt [-

k i=1 pi = 1.

Qtjpj + p Qp] T

j:j=t

normalize p

(11) (12)

until (10) is satisfied. Theorem 4 If rsj > 0, s = j, and {p } is the sequence generated by Algorithm 1, any convergent sub-sequence goes to a global minimum of (8). As Theorem 3 indicates that in general Q is positive definite, the sequence {p } from Algorithm 1 usually globally converges to the unique minimum of (8).

i i=1

i i=1

5 Relations Among Four Methods The four decision rules HT , 1, 2, and V can be written as argmaxi[pi], where p is

derived by the following four optimization formulations under the constants

k i=1

pi = 1


and pi  0, i:

k k

HT : min

p

[

i=1 j:j=i k k

[

i=1 j:j=i k k

(rijpj - rjipi)2,

(rij 1 k 1 - pi)]2, 2 (13)

1 : min

p

(rijpj - rjipi)]2, (14)

2 : min

p

(15)

i=1 j:j=i k k

V :

min

p

(I{rij>rji}pj - I{rji>rij}pi)2. (16)

i=1 j:j=i

Note that (13) can be easily verified, and that (14) and (15) have been explained in Sections 3 and 4. For (16), its solution is pi = , where c is the normalizing constant; and therefore, argmaxi[pi] is the same as (1). Clearly, (13) can be obtained from (14) by letting pj  1/k, j and rji  1/2, i, j. Such approximations ignore the differences between pi. Similarly, (16) is from (15) by taking the extreme values of rij: 0 or 1. As a result, (16) may enlarge the differences between pi. Next, compared with (15), (14) may tend to underestimate the differences between the pi's. The reason is that (14) allows the difference between rijpj and rjipi to get canceled first. Thus, conceptually, (13) and (16) are more extreme ­ the former tends to underestimate the differences between pi's, while the latter overestimate them. These arguments will be supported by simulated and real data in the next section. 6 Experiments 6.1 Simple Simulated Examples [3] designs a simple experiment in which all pi's are fairly close and their method HT outperforms the voting strategy V . We conduct this experiment first to assess the performance of our proposed methods. As in [3], we define class probabilities p1 = 1.5/k, pj = (1 - p1)/(k - 1), j = 2, . . . , k, and then set

j:j=i

c I{rji>rij}

rij = pi pi + pj

rji = 1 - rij

+ 0.1zij if i > j, if j > i, (17) (18)

where zij are standard normal variates. Since rij are required to be within (0,1), we truncate rij at below and 1 - above, with = 0.00001. In this example, class 1 has the highest probability and hence is the correct class. Figure 1 shows accuracy rates for each of the four methods when k = 3, 5, 8, 10, 12, 15, 20. The accuracy rates are averaged over 1,000 replicates. Note that in this experiment all classes are quite competitive, so, when using V , sometimes the highest vote occurs at two

 For I to be well defined, we consider rij = rji, which is generally true. In addition, if there is

an i for which Pj:j=i I{rji>rij} = 0, an optimal solution of (16) is pi = 1, and pj = 0, j = i.

The resulting decision is the same as that of (1).


Rates

Accuracy

1.05 1 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 2

Rates

Accuracy

3 4 5 6 7

1.05 1 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 2

Rates

Accuracy

3 4 5 6 7

1.05 1 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 2

3 4 5 6 7

log k 2 log k 2 log k 2

(a) balanced pi (b) unbalanced pi (c) highly unbalanced pi

Figure 1: Accuracy of predicting the true class by the methods HT (solid line, cross marked), V (dash line, square marked), 1 (dotted line, circle marked), and 2 (dashed line, asterisk marked) from simulated class probability pi, i = 1, 2 ··· k.

or more different classes. We handle this problem by randomly selecting one class from the ties. This partly explains why V performs poor. Another explanation is that the rij here are all close to 1/2, but (16) uses 1 or 0 instead; therefore, the solution may be severely biased. Besides V , the other three rules have done very well in this example. Since HT relies on the approximation pi + pj  k/2, this rule may suffer some losses if the class probabilities are not highly balanced. To examine this point, we consider the following two sets of class probabilities: (1) We let k1 = k/2 if k is even, and (k + 1)/2 if k is odd; then we define p1 = 0.95×1.5/k1, pi = (0.95-p1)/(k1-1) for i = 2, . . . , k1, and pi = 0.05/(k-k1) for i = k1 + 1, . . . , k. (2) If k = 3, we define p1 = 0.95 × 1.5/2, p2 = 0.95 - p1, and p3 = 0.05. If k > 3, we define p1 = 0.475, p2 = p3 = 0.475/2, and pi = 0.05/(k - 3) for i = 4, . . . , k. After setting pi, we define the pairwise comparisons rij as in (17)-(18). Both experiments are repeated for 1,000 times. The accuracy rates are shown in Figures 1(b) and 1(c). In both scenarios, pi are not balanced. As expected, HT is quite sensitive to the imbalance of pi. The situation is much worse in Figure 1(c) because the approximation pi + pj  k/2 is more seriously violated, especially when k is large. In summary, 1 and 2 are less sensitive to pi, and their overall performance are fairly stable. All features observed here agree with our analysis in Section 5. 6.2 Real Data In this section we present experimental results on several multi-class problems: segment, satimage, and letter from the Statlog collection [9], USPS [4], and MNIST [7]. All data sets are available at http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ t. Their numbers of classes are 7, 6, 26, 10, and 10, respectively. From thousands of instances in each data, we select 300 and 500 as our training and testing sets.

We consider support vector machines (SVM) with RBF kernel e- xi-xj 2 as the binary

classifier. The regularization parameter C and the kernel parameter  are selected by crossvalidation. To begin, for each training set, a five-fold cross-validation is conducted on the following points of (C, ): [2-5, 2-3, . . . , 215] × [2-5, 2-3, . . . , 215]. This is done by modifying LIBSVM [1], a library for SVM. At each (C, ), sequentially four folds are


Table 1: Testing errors (in percentage) by four methods: Each row reports the testing errors based on a pair of the training and testing sets. The mean and std (standard deviation) are from five 5-fold cross-validation procedures to select the best (C, ).

Dataset k

satimage 6

segment 7

USPS 10

MNIST 10

letter 26

HT mean 14.080 12.960 14.520 12.400 16.160 9.960 6.040 6.600 5.520 7.440 14.840 12.080 10.640 12.320 13.400 17.400 15.200 17.320 14.720 12.560 39.880 41.640 41.320 35.240 43.240

std 1.306 0.320 0.968 0.000 0.294 0.480 0.528 0.000 0.466 0.625 0.388 0.560 0.933 0.845 0.310 0.000 0.400 1.608 0.449 0.294 1.412 0.463 1.700 1.439 0.637

1 mean 14.600 13.400 14.760 12.200 16.400 9.480 6.280 6.680 5.200 8.160 13.520 11.440 10.000 11.960 12.640 16.560 14.600 14.280 14.160 12.600 37.160 39.400 38.920 32.920 40.360

std 0.938 0.400 1.637 0.000 0.379 0.240 0.299 0.349 0.420 0.637 0.560 0.625 0.657 1.031 0.080 0.080 0.000 0.560 0.196 0.000 1.106 0.769 0.854 1.121 1.472

2 mean 14.760 13.400 13.880 12.640 16.120 9.000 6.200 6.920 5.400 8.040 12.760 11.600 9.920 11.560 12.920 15.760 13.720 13.400 13.360 13.080 34.560 35.920 35.800 29.240 36.960

std 0.784 0.400 0.392 0.294 0.299 0.400 0.456 0.271 0.580 0.408 0.233 1.081 0.483 0.784 0.299 0.196 0.588 0.657 0.686 0.560 2.144 1.389 1.453 1.335 1.741

V mean 15.400 13.360 14.080 12.680 16.160 8.880 6.760 7.160 5.480 7.840 12.520 11.440 10.320 11.840 12.520 15.960 12.360 13.760 13.520 12.440 33.480 33.440 35.000 27.400 34.520

std 0.219 0.080 0.240 1.114 0.344 0.271 0.445 0.196 0.588 0.344 0.160 0.991 0.744 1.248 0.917 0.463 0.196 0.794 0.325 0.233 0.325 1.061 1.066 1.117 1.001

used as the training set while one fold as the validation set. The training of the four folds consists of k(k - 1)/2 binary SVMs. For the binary SVM of the ith and the jth classes, using decision values f^ of training data, we employ an improved implementation [8] of Platt's posterior probabilities [10] to estimate rij:

rij = P (i | i or j, x) = 1 1 + eA , (19)

f^+B

where A and B are estimated by minimizing the negative log-likelihood function. Then, for each validation instance , we apply the four methods to obtain classification decisions. The error of the five validation sets is thus the cross-validation error at (C, ). After the cross-validation is done, each rule obtains its best (C, ). Using these parameters, we train the whole training set to obtain the final model. Next, the same as (19), the decision values from the training data are employed to find rij. Then, testing data are tested using each of the four rules. Due to the randomness of separating training data into five folds for finding the best (C, ), we repeat the five-fold cross-validation five times and obtain the mean and standard deviation of the testing error. Moreover, as the selection of 300 and 500 training and testing instances from a larger dataset is also random, we generate five of such pairs. In Table 1, each row reports the testing error based on a pair of the training and testing sets. The results show that when the number of classes k is small, the four methods perform similarly; however, for problems with larger k, HT is less competitive. In particular, for problem letter which has 26 classes, 2 or V outperforms HT by at least 5%. It seems that for

 [10] suggests to use f^from the validation instead of the training. However, this requires a further

cross-validation on the four-fold data. For simplicity, we directly use f^from the training. with the smallest C.

 If more than one parameter sets return the smallest cross-validation error, we simply choose one


problems here, their characteristics are closer to the setting of Figure 1(c), rather than that of Figure 1(a). All these results agree with the previous findings in Sections 5 and 6.1. Note that in Table 1, some standard deviations are zero. That means the best (C, ) by different cross-validations are all the same. Overall, the variation on parameter selection due to the randomness of cross-validation is not large. 7 Discussions and Conclusions As the minimization of the KL distance is a well known criterion, some may wonder why the performance of HT is not quite satisfactory in some of the examples. One possible explanation is that here KL distance is derived under the assumptions that nijrij  Bin(nij, µij) and rij are independent; however, as pointed out in [3], neither of the assumptions holds in the classification problem. In conclusion, we have provided two methods which are shown to be more stable than both HT and V . In addition, the two proposed approaches require only solutions of linear systems instead of a nonlinear one in [3]. The authors thank S. Sathiya Keerthi for helpful comments. References [1] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.

[2] J. Friedman. Another approach to polychotomous classification. cal report, Department of Statistics, Stanford University, 1996. TechniAvailable at

http://www-stat.stanford.edu/reports/friedman/poly.ps.Z. [3] T. Hastie and R. Tibshirani. Classification by pairwise coupling. The Annals of Statistics, 26(1):451­471, 1998. [4] J. J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(5):550­554, May 1994. [5] D. R. Hunter. MM algorithms for generalized Bradley-Terry models. The Annals of Statistics, 2004. To appear. [6] S. Knerr, L. Personnaz, and G. Dreyfus. Single-layer learning revisited: a stepwise procedure for building and training a neural network. In J. Fogelman, editor, Neurocomputing: Algorithms, Architectures and Applications. Springer-Verlag, 1990. [7] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, November 1998. MNIST database available at http://yann.lecun.com/exdb/mnist/. [8] H.-T. Lin, C.-J. Lin, and R. C. Weng. A note on Platt's probabilistic outputs for support vector machines. Technical report, Department of Computer Science and Information Engineering, National Taiwan University, 2003. [9] D. Michie, D. J. Spiegelhalter, and C. C. Taylor. Machine Learning, Neural and Statistical Classification. Prentice Hall, Englewood Cliffs, N.J., 1994. Data available at http://www.ncc.up.pt/liacc/ML/statlog/datasets.html. [10] J. Platt. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. In A. Smola, P. Bartlett, B. Sch¨olkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, Cambridge, MA, 2000. MIT Press. [11] D. Price, S. Knerr, L. Personnaz, and G. Dreyfus. Pairwise nerual network classifiers with probabilistic outputs. In G. Tesauro, D. Touretzky, and T. Leen, editors, Neural Information Processing Systems, volume 7, pages 1109­1116. The MIT Press, 1995. [12] P. Refregier and F. Vallet. Probabilistic approach for multiclass classification with neural networks. In Proceedings of International Conference on Artificial Networks, pages 1003­1007, 1991.


