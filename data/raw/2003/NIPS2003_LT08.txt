PAC-Bayesian Generic Chaining

Jean-Yves Audibert Universite´ Paris 6 

Laboratoire de Probabilites et Modeles aleatoires 175 rue du Chevaleret 75013 Paris - France jyaudibe@ccr.jussieu.fr Olivier Bousquet Max Planck Institute for Biological Cybernetics Spemannstrasse 38 D-72076 Tubingen - Germany ¨ olivier.bousquet@tuebingen.mpg.de ´ ` ´

Abstract There exist many different generalization error bounds for classification. Each of these bounds contains an improvement over the others for certain situations. Our goal is to combine these different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester [1], which is interesting for averaging classifiers, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand [2]. This combination is quite natural since the generic chaining is based on the notion of majorizing measures, which can be considered as priors on the set of classifiers, and such priors also arise in the PAC-bayesian setting. 1 Introduction Since the first results of Vapnik and Chervonenkis on uniform laws of large numbers for classes of {0, 1}-valued functions, there has been a considerable amount of work aiming at obtaining generalizations and refinements of these bounds. This work has been carried out by different communities. On the one hand, people developing empirical processes theory like Dudley and Talagrand (among others) obtained very interesting results concerning the behaviour of the suprema of empirical processes. On the other hand, people exploring learning theory tried to obtain refinements for specific algorithms with an emphasis on data-dependent bounds. One crucial aspect of all the generalization error bounds is that they aim at controlling the behaviour of the function that is returned by the algorithm. This function is data-dependent and thus unknown before seeing the data. As a consequence, if one wants to make statements about its behaviour (e.g. the difference between its empirical error and true error), one has to be able to predict which function is likely to be chosen by the algorithm. But

 Secondary affiliation: CREST, ENSAE, Laboratoire de Finance et Assurance, Malakoff, France


since this cannot be done exactly, there is a need to provide guarantees that hold simultaneously for several candidate functions. This is known as the union bound. The way to perform this union bound optimally is now well mastered in the empirical processes community. In the learning theory setting, one is interested in bounds that are as algorithm and data dependent as possible. This particular focus has made concentration inequalities (see e.g. [3]) popular as they allow to obtain data-dependent results in an effortless way. Another aspect that is of interest for learning is the case where the classifiers are randomized or averaged. McAllester [1, 4] has proposed a new type of bound that takes the randomization into account in a clever way. Our goal is to combine several of these improvements, bringing together the power of the majorizing measures as an optimal union bound technique and the power of the PACBayesian bounds that handle randomized predictions efficiently, and obtain a generalization of both that is suited for learning applications. The paper is structured as follows. Next section introduces the notation and reviews the previous improved bounds that have been proposed. Then we give our main result and discuss its applications, showing in particular how to recover previously known results. Finally we give the proof of the presented results. 2 Previous results We first introduce the notation and then give an overview of existing generalization error bounds. We consider an input space X , an output space Y and a probability distribution distributed according to P andXfor×aYgiven integer n, let Z1, . . . , Zn and Z1, . . . , Zn be two P on the product space Z . Let Z (X, Y ) denote a pair of random variables empirical measures associated respectively to the first, the second and the union of both samples. To each function g : X  Y we associate the corresponding loss function f : Z  R defined by f (z) = L[g(x), y] where L is a loss function. In classification, the loss such functions. For such functions, we denote their expectation under P by Pf and their expectation with respect to the first, second and union of both training samples. We consider the pseudo-distances d2(f1, f2) = P(f1 - f2)2 and similarly dn, dn and d2 . needed to cover F in the pseudo-distance d. We define the covering number N(F, , d) as the minimum number of balls of radius We denote by  and  two probability measures on the space F, so that Pf will actually mean the expectation of Pf when f is sampled according to the probability measure . For two such measures, K(, ) will denote their Kullback-Leibler divergence (K(, ) = Also,  denotes some positive real number while C is some positive constant (whose value assume that the functions in F have range in [a, b]. may differ from line to line) and M1+(F) is the set of probability measures on F. We Generalization error bounds give an upper bound on the difference between the true and empirical error of functions in a given class, which holds with high probability with respect to the sampling of the training set. Single function. By Hoeffding's inequality one easily gets that for each fixed f  F, with

independent samples of n independent copies of Z. We denote by Pn, Pn and P2 n the

function is L = Ig (x)=y where I denotes the indicator function. F will denote a set of

empirical expectation by Pnf (i.e. Pnf = n- 1 n i=1 f(Zi)). En, En and E2 n denote the

n

 log d d when  is absolutely continuous with respect to  and K(, ) = + otherwise).

probability at least 1 - ,

P f - Pnf  C log 1/ n . (1)

Finite union bound. It is easy to convert the above statement into one which is valid


that with probability at least 1 - , simultaneously for a finite set of functions F. The simplest form of the union bound gives

f  F, Pf - Pnf  C log |F| + log 1/ n . (2)

Symmetrization. Z1, . . . , Zn and to consider the set of vectors formed by the values of each function in F on the double sample. When the functions have values in {0,1}, this is a finite set and When F is infinite, the trick is to introduce the second sample the above union bound applies. This idea was first used by Vapnik and Chervonenkis [5] to obtain that with probability at least 1 - ,

f  F, Pf - Pnf  C log E2 N (F, 1/n, d2 ) + log 1/

n n

n

. (3)

Weighted union bound and localization. The finite union bound can be directly extended function and gives that with probability at least 1 - , to the countable case by introducing a probability distribution  over F which weights each

f  F, Pf - Pnf  C log 1/(f ) + log 1/ n . (4)

It is interesting to notice that now the bound depends on the actual function f being conVariance. Since the deviations between Pf and Pnf for a given function f actually desidered and not just on the set F. This can thus be called a localized bound. pend on its variance (which is upper bounded by Pf2/n or Pf/n when the functions are in [0, 1]), one can refine (1) into

P f - Pnf  C P f2 log 1/ n + log 1/ n , (5)

and combine this improvement with the above union bounds. This was done by Vapnik and Chervonenkis [5] (for functions in {0, 1}). Averaging. Consider a probability distribution  defined on a countable F, take the expectation of (4) with respect to  and use Jensen's inequality. This gives with probability at

least 1 - ,

, (Pf - Pnf)  C K(, ) + H() + log 1/ n ,

where H() is the Shannon entropy. The l.h.s. is the difference between true and empirical error of a randomized classifier which uses  as weights for choosing the decision function (independently of the data). The PAC-Bayes bound [1] is a refined version of the above bound since it has the form (for possibly uncountable F)

, (Pf - Pnf)  C K(, ) + log n + log 1/ n . (6)

To some extent, one can consider that the PAC-Bayes bound is a refined union bound where the gain happens when  is not concentrated on a single function (or more precisely  has entropy larger than log n).

Rademacher averages. The quantity EnE supfF

1 n

if(Zi), where the i are inde-

F, is, up to a constant equal to En supfF Pf -Pnf which means that it best captures the pendent random signs (+1, -1 with probability 1/2), called the Rademacher average for complexity of F. One has with probability 1 - ,

f  F, Pf - Pnf  C 1 n EnE sup

f F

if(Zi) + log 1/ n . (7)


Chaining. Another direction in which the union bound can be refined is by considering finite covers of the set of function at different scales. This is called the chaining technique, pioneered by Dudley (see e.g. [6]) since one constructs a chain of functions that approximate a given function more and more closely. The results involve the Koltchinskii-Pollard entropy integral as, for example in [7], with probability 1 - , f  F, Pf - Pnf  C 1nEn . (8)



0

log N (F, , dn)d + log 1/ n

of F of diameter r- w.r.t. the distance dn such that Aj

Generic chaining. It has been noticed by Fernique and Talagrand that it is possible to capture the complexity in a better way than using minimal covers by considering majorizing measures (essentially optimal for Gaussian processes). Let r > 0 and (Aj)j be partitions techniques from [2] we obtain that with probability 1 - , f  F

1

refines Aj. Using (7) and

j +1

P f - Pnf  C

  1n



En inf M1+(F)

sup

f F

r- log 1/Aj(f) +

j

j

log 1/  n

. (9)

j=1

(8)

a constant. If one takes partitions induced by minimal covers of F at radii r- , one recovers  up to Concentration. Using concentration inequalities as in [3] for example, one can get rid of the expectation appearing in the r.h.s. of (3), (8), (7) or (9) and thus obtain a bound that can be computed from the data. Refining the bound (7) is possible as one can localize it (see e.g. [8]) by computing the Rademacher average only on a small ball around the function of interest. So this comes close to combining all improvements. However it has not been combined with the PACBayes improvement. Our goal is to try and combine all the above improvements. 3 Main results

partitions as in (9) we use approximating sets (which also induce partitions but are easier Let F be as defined in section 2 with a = 0, b = 1 and   M1+(F). Instead of using

to handle here). Consider a sequence Sj of embedded finite subsets of F: {f0} ···  Sj-  Sj  ···.

1

S0 

for f  Sj and pj-  pj = pj- . Let pj : F  Sj be maps (which can be thought of as projections) satisfying pj(f) = f exchanging Xi and Xi does not affect their value). For a probability distribution  on the Dirac measure on f. To shorten notations, we denote the average distance between

1 1

The quantities , Sj and pj are allowed to depend on X1 2n in an exchangeable way (i.e.

F, define its j-th projection as j = f Sj {f : pj(f ) = f }f , where f denotes

two successive "projections" by d2j Pn[f - pj(f)] - Pn[f - pj(f)]. d22n[pj(f), pj- (f)]. Finally, let n,j(f) 1

Theorem 1 If the following condition holds lim sup n,j(f ) = 0,

j+ f F

a.s. (10)

then for any 0 <  < 1/2, with probability at least 1 - , for any distribution , we have

+ +

Pnf - Pnf0  Pnf - Pnf0 + 5 j

d2j K(j, j) n + 1n j(d2j ),

=1 j=1


where j(x) = 4 x log 4j2- log(e2/x) . 1

Remark 1 Assumption (10) is not very restrictive. For instance, it is satisfied when F is finite, or when limj supfF |f-pj(f)| = 0, almost surely or also when the empirical

+

is uniformly continuous (which happens for classes with finite V C dimension in particular) and limj supfF d2 (f, pj(f)) = 0.

+ n

be Remark 2 Let G be a model (i.e. a set of prediction functions). Let zg~ a reference function (not necessarily in G). Consider the class of functions F =

process f  Pf - Pnf

L[g(x), y] :

g(x), y]. The previous theorem compares the risk on the second sample of any (randomized) estimator with the risk on the second sample of the reference g  G {g~} . Let f0 = L[~ function g~. Now let us give a version of the previous theorem in which the second sample does not appear. Theorem 2 If the following condition holds lim sup En n,j(f ) = 0, a.s. (11) then for any 0 <  < 1/2, with probability at least 1 - , for any distribution , we have

j+ f F

+

P f - P f0  Pnf - Pnf0 + 5 j 4 Discussion

E [d2j ]En[K(j , j )] n n

+

+ 1n j En[d2j ] .

=1 j=1

We now discuss in which sense the result presented above combines several previous improvements in a single bound. Notice that our bound is localized in the sense that it depends on the function of interest (or rather on the averaging distribution ) and does not involve a supremum over the class. Also, the union bound is performed in an optimal way since, if one plugs in a distribution  the squared distance by the diameter of the partition, one recovers a result similar to (9) concentrated on a single function, takes a supremum over F in the r.h.s., and upper bounds up to logarithmic factors but which is localized. Also, when two successive projections are identical, they do not enter in the bound (which comes from the fact that the variance weights the complexity terms). Moreover Theorem 1 also includes the PAC-Bayesian improvement for averaging classifiers since if one considers the set S1 = F one recovers a result similar to McAllester's (6) which in addition contains the variance improvement such as in [9]. Finally due to the power of the generic chaining, it is possible to upper bound our result by Rademacher averages, up to logarithmic factors (using the results of [10] and [11]). As a remark, the choice of the sequence of sets Sj can generally be done by taking successive covers of the hypothesis space with geometrically decreasing radii. However, the obtained bound is not completely empirical since it involves the expectation with respect to an extra sample. In the transduction setting, this is not an issue, it is even an advantage as one can use the unlabeled data in the computation of the bound. However, in the induction setting, this is a drawback. Future work will focus on using concentration inequalities to give a fully empirical bound.


5 Proofs Proof of Theorem 1: The proof is inspired by previous works on PAC-bayesian bounds [12, 13] and on the generic chaining [2]. We first prove the following lemma. M1+(F), with probability at least 1 - , for any probability distribution   M1+(F), we

Lemma 1 For any  > 0,  > 0, j  N and any exchangeable function  : X 2 have n 

 Pn[pj(f) - pj- (f)] - Pn[pj(f) - pj- (f)]

1 1 2 n n

 d22 [pj(f),pj- (f)] + n 1

Proof Let  > 0 and let  : X 2

quantity i pj(f)(Zn

+i

h

) - pj- (f )(Zn 1 +i

K(,)+log(-1)  .

 M1+(F))be an exchangeable function. Introduce the

+ pj- (f )(Zi) - pj(f )(Zi) and 1

Pn pj(f) - pj- (f) - Pn pj(f) - pj- (f) - 1 1 22 d2

n n pj(f), pj- (f) . (12) 1

By using the exchangeability of , for any   {-1; +1}n, we have

2

E2 e- 2n E2 e- 2n

n 2

n

E2 eh n

= = d2n[pj (f ),pj-1(f )]+ n d2n[pj (f ),pj-1(f )]+ n



 Pn

i=1

Pn i=1

i ii

.

Now take the expectation wrt , where  is a n-dimensional vector of Rademacher variables. We obtain

2

E2 e- 2n n E2 eh n =

n i=1

ePni=1

cosh

2 2n2 2i

 n i

 E2 e-2n n 2

d2n[pj (f ),pj-1(f )] d2n[pj (f ),pj-1(f )]

where at the last step we use that cosh s  e

2i  2 pj(f )(Zn +i ) - pj- (f )(Zn 1 +i

s2 2

. Since ) + 2 pj(f )(Zi) - pj- (f )(Zi)

2

1

2 ,

we obtain that for any  > 0, E2 eh  1. Therefore, for any  > 0, we have

n n log eh+log >0

E2 I bution   M1+(F), we have On the event log eh

+log 

= E2 I n eh+log >1  E2 eh n +log   , (13)

0 , by the Legendre's transform, for any probability distri-

h + log   log eh +log  + K(, )  K(, ), (14)

which proves the lemma. Now let us apply this result to the projected measures j and j. Since, by definition, , Sj probability at least 1 - , uniformly in , we have and pj are exchangeable, j is also exchangeable. Since pj(f) = f for any f  Sj, with

j Pn[f - pj- (f)] - Pn[pj(f) - pj- (f)] 1 1

where Kj

 2 jd22

n n [f, pj- (f )] + 1

Kj  ,

K(j, j) + log(- ). By definition of j, it implies that 1

 Pn[pj(f)-pj- (f)]-Pn[pj(f)-pj- (f)] 1 1  2 d22

n n [pj(f ), pj- (f )]+ 1

Kj  . (15)


To shorten notations, define d2j

1 1

ous equation depends on . Therefore, we need to get a version of this inequality which pj- (f)] - Pn[pj(f) - pj- (f)] . The parameter  minimizing the RHS of the previholds uniformly in .

First let us note that when d2j = 0, we have j = 0. When d2j > 0, let m

k = mek/ and let b be a function from R to (0, 1] such that 2

k1

log 2 2n and

d22 [pj(f), pj- (f)] and j n 1  Pn[pj(f) -

b(k)  1. From the

previous lemma and a union bound, we obtain that for any  > 0 and any integer j with probability at least 1 - , for any k  N and any distribution , we have

j 

2k d2j n K(j, j) + log [b(k)]- - k

log

1 1

+ .

Let us take the function b such that  

Then there exists a parameter  > 0 such that

[b()]-1  2

n

d2j =

is continuous and decreasing.

K(j ,j )+log [b()]-1-1  ( ) . For

any  < 1/2, we have ()2dj  .

2

log 2 2

1/2 

n, hence   m. So there exists an integer

k  N such that ke- ed2j kKThen we have

j

 =

2 n

(1 +

+

2 n

(j,j)+log [b()]-1-1  (

e )

)

(16)

d2j K(j, j) + log ([b()]- - ) . 1 1

To have an explicit bound, it remains to find an upperbound of [b()]- . When b is

decreasing, this comes down to upperbouding . Let us choose b() =

1

2

when

1 [log( em )]2

  m and b() = 1/4 otherwise. Since b(k) =

4

(k+4)2

, we have

Tedious computations give   7mK2 which combined with (16), yield

j

dj

d2j K(j, j) n j  5 + 3.75 d2j n log 2- log 1 e2 d2j

k1 b(k)  1.

.

By simply using an union bound with weights taken proportional to 1/j2, we have that the previous inequation holds uniformly in j  N provided that - is replaced with

1

2 2 j - since 1

6 jN

1/j2 = 2/6  1.64 . Notice that

J

 Pnf - Pnf0 + Pnf0 - Pnf = n,J (f ) + j (Pn - Pn)f - (Pn - Pn)pj- (f) 1

j=1

because pj- = pj-  pj. So, with probability at least 1 - , for any distribution , we

1 1

have  Pnf - Pnf0 + Pnf0 - Pnf

 supF n,J + 5

J j=1

d2j n log

d2j K(j ,j ) n 1

3.3j2- log

+3.75 Making J  +, we obtain theorem 1.

J j=1 e2 d2j .

Proof of Theorem 2: It suffices to modify slightly the proof of theorem 1. Introduce U sup h + log  - K(, ) , where h is still defined as in equation (12). Inequations (14) implies that E2neU  . By Jensen's inequality, we get EneEnU  , hence En EnU  0  . So with probability at least 1 - , we have sup En h + log - K(,) 

E U  0.

n


6 Conclusion We have obtained a generalization error bound for randomized classifiers which combines several previous improvements. It contains an optimal union bound, both in the sense of optimally taking into account the metric structure of the set of functions (via the majorizing measure approach) and in the sense of taking into account the averaging distribution. We believe that this is a very natural way of combining these two aspects as the result relies on the comparison of a majorizing measure which can be thought of as a prior probability distribution and a randomization distribution which can be considered as a posterior distribution. Future work will focus on giving a totally empirical bound (in the induction setting) and investigating possible constructions for the approximating sets Sj. References

[1] D. A. McAllester. Some PAC-Bayesian theorems. In Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 230­234. ACM Press, 1998. [2] M. Talagrand. Majorizing measures: The generic chaining. Annals of Probability, 24(3):1049­ 1103, 1996. [3] S. Boucheron, G. Lugosi, and S. Massart. A sharp concentration inequality with applications. Random Structures and Algorithms, 16:277­292, 2000. [4] D. A. McAllester. PAC-Bayesian model averaging. In Proceedings of the 12th Annual Conference on Computational Learning Theory. ACM Press, 1999. [5] V. Vapnik and A. Chervonenkis. Theory of Pattern Recognition [in Russian]. Nauka, Moscow, 1974. (German Translation: W. Wapnik & A. Tscherwonenkis, Theorie der Zeichenerkennung, Akademie­Verlag, Berlin, 1979). [6] R. M. Dudley. A course on empirical processes. Lecture Notes in Mathematics, 1097:2­142, 1984. [7] L. Devroye and G. Lugosi. Combinatorial Methods in Density Estimation. Springer Series in Statistics. Springer Verlag, New York, 2001. [8] P. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. Preprint, 2003. [9] D. A. McAllester. Simplified pac-bayesian margin bounds. In Proceedings of Computational Learning Theory (COLT), 2003. [10] M. Ledoux and M. Talagrand. Probability in Banach spaces. Springer-Verlag, Berlin, 1991. [11] M. Talagrand. The Glivenko-Cantelli problem. Annals of Probability, 6:837­870, 1987. [12] O. Catoni. Localized empirical complexity bounds and randomized estimators, 2003. Preprint. [13] J.-Y. Audibert. Data-dependent generalization error bounds for (noisy) classification: a PACbayesian approach. 2003. Work in progress.


