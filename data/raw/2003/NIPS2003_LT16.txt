Approximate Analytical Bootstrap Averages for Support Vector Classifiers

Dorthe Malzahn1 е ,2 Manfred Opper3

1

Informatics and Mathematical Modelling, Technical University of Denmark, R.-Petersens-Plads, Building 321, Lyngby DK-2800, Denmark

2

Institute of Mathematical Stochastics, University of Karlsruhe, Englerstr. 2, Karlsruhe D-76131, Germany

3

Neural Computing Research Group, School of Engineering and Applied Science, Aston University, Birmingham B4 7ET, United Kingdom malzahnd@isp.imm.dtu.dk opperm@aston.ac.uk

Abstract We compute approximate analytical bootstrap averages for support vector classification using a combination of the replica method of statistical physics and the TAP approach for approximate inference. We test our method on a few datasets and compare it with exact averages obtained by extensive Monte-Carlo sampling. 1 Introduction The bootstrap method [1, 2] is a widely applicable approach to assess the expected qualities of statistical estimators and predictors. Say, for example, in a supervised learning problem, we are interested in measuring the expected error of our favorite prediction method on test can use the bootstrap approach to create artificial bootstrap data sets D by resampling with replacement training data from the original set D0. Each data point is taken with equal probability, i.e., some of the examples will appear several times in the bootstrap sample and others not at all. A proxy for the true average test error can be obtained by retraining the model on each bootstrap training set D, calculating the test error only on those points which are not contained in D and finally averaging over all possible sets D. While in general bootstrap averages can be approximated to any desired accuracy by the Monte-Carlo method, by generating a large enough number of random samples, it is useful to have also analytical approximations which avoid the time consuming retraining of the model for each new sample. Existing analytical approximations (based on asymptotic techniques) such as the delta method and the saddle point method require usually explicit analytical formulas for the estimators of the parameters for a trained model (see e.g. [3]). These may not be easily obtained for more complex models in machine learning such as support vector machines (SVMs). Recently, we introduced a novel approach for the approximate calculation of bootstrap averages [4] which avoids explicit formulas for parameter estimates. Instead, we define statistical estimators and predictors implicitly

points 1 which are not contained in the training set D0. If we have no hold out data, we

1 The average is over the unknown distribution of training data sets.


as expectations with suitably defined pseudo-posterior Gibbs distributions over model parameters. Within this formulation, it becomes possible to perform averages over bootstrap samples analytically using the so-called "replica trick" of statistical physics [5]. The latter involves a specific analytic continuation of the original statistical model. After the average, we are left with a typically intractable inference problem for an effective Bayesian probabilistic model. As a final step, we use techniques for approximate inference to treat the probabilistic model. This combination of techniques allows us to obtain approximate bootstrap averages by solving a set of nonlinear equations rather than by explicit sampling. Our method has passed a first test successfully on the simple case of Gaussian process (GP) regression, where explicit predictions are still cheaply computed. Also, since the original model is a smooth probabilistic one, the success of approximate inference techniques may be not too surprising. In this paper, we will address a more challenging problem, that of the support vector machine. In this case, the connection to a probabilistic model (a type of GP) can be only established by introducing a further parameter which must eventually diverge to obtain the SVM predictor. In this limit, the probabilistic model is becoming highly nonregular and approaches a deterministic model. Hence it is not clear a priori if our framework would survive these delicate limiting manipulations and still be able to give good approximate answers. 2 Hard Margin Support Vector Classifiers The hard margin SVM is a classifier which predicts binary class labels y = sign[f^D0(x)]  {-1,1} for inputs x  IRd based on a set of training points D0 = (z1,z2,...,zN), where zi = (xi, yi) (for details see [6]). The usually nonlinear activation function f^D0(x) (which is a positive definite kernel and the set of i's is computed from D0 by solving a certain convex optimization problem. For bootstrap problems, we fix the pool of training data D0, and consider the statistics ^ computed on randomly chosen subsets D of D0. Unfortunately, we do not have an explicit of vectors fD = (f^D(x1), . . . , f^D(xN)) at all inputs xi  D0, when the predictor f^ is analytical expression for fD, but it is obtained implicitly as the vector f = (f1, . . . , fN) ^ which solves the constraint optimization problem Minimize fT K-1f with fiyi  1 for all i such that (xi, yi)  D (1) K is the kernel matrix with elements K(xi, xj). 3 Deriving Predictors from Gibbs Distributions In this section, we will show how to obtain the SVM predictor f^D formally as the expectation over a certain type of Gibbs distribution over possible f's in the form

we will call "internal field") is expressed as f^D0(x) = N i=1 yiiK(x, xi), where K(x, x )

^fD = f = df f P [f|D] (2)

with respect to a density P[f|D] = function Z = 1 Z

prior distribution х[f], a certain type of "likelihood" P(D|f) and a normalizing partition

df х[f] P (D|f) . (3)

х[f] P (D|f) which is constructed from a suitable

Our general notation suggests that this principle will apply to a variety of estimators and predictors of the MAP type.


To represent the SVM in this framework, we use a well established relation between SVM's and Gaussian process (GP) models (see e.g. [7, 8]). We choose the GP prior

х[f] = 1 (2)N - det(K) exp -fTK-1f 2 . (4)

N

The pseudo-likelihood is defined by

P (D|f) =

2

P (zj|fj) = (yjfj - 1) (5)

j: zj D j: zj D

where (u) = 1 for u > 0 and 0 otherwise. In the limit   , the measure P[f|D]  х[f]P (D|f) obviously concentrates at the vector f which solves Eq. (1). ^ 4 Analytical Bootstrap Averages Using the Replica Trick With the bootstrap method, we would like to compute average properties of the estimator averages are of the type of a generalization error  which are expectations of loss functions g(f^D(xi); xi, yi) over test points i, i.e., those examples which are in D0 but not contained in the bootstrap training set D. Hence, we define

^fD, Eq. (2), when datasets D are random subsamples of D0. An important class of such

N

.  = 1 N ED si,0 g(f^D(xi); xi, yi) ED [si,0]

(6)

i=1

from the original training set D0. Each sample D is represented by a vector of "occupation" where ED[иии ] denotes the expectation over random bootstrap samples D which are created numbers s = (s1, . . . , sN) where si is the number of times example zi appears in the set guarantees that only realizations of bootstrap training sets D contribute to Eq. (6) which do not contain the test point. For fixed bootstrap sample size S, the distribution of si's is multinomial. It is simpler (and does not make a big difference when S is sufficiently large) when we work with a Poisson distribution for the size of the set D with S as the mean number of data points in the sample. Then we get the simpler, factorizing joint distribution

D and N i=1 si = S. The Kronecker symbol, defined by si,0 = 1 for si = 0 and 0 else,

N

P (s) = ( N )sie- si! S S/N (7)

i=1

for the occupation numbers si. From Eq. (7) we get ED[si,0] = e-N . Since we can represent general loss functions g by their Taylor expansions in powers of f^D (or polynomial approximations in case of non-smooth losses) it is sufficient to consider only monomials g(f^D(x); x, y) = (f^D(x))r for arbitrary r in the following and regain the general case at the end by resumming the series. Using the definition of the estimator fD, ^ Eq. (2), the bootstrap expectation Eq. (6) can be rewritten as

r a a

(P (zj|fj ))sj N ED si,0 Z- N j=1

S

(S) = 1 N

r a=1

i=1

dfa х[fa] fi ED [si,0]

. (8)

which involves r copies3, i.e. replicas f1, . . . , fr of the parameter vector f. If the partition functions Z in the numerator of Eq. (8) were raised to positive powers rather than negative

2 3 It does not allow a full probabilistic interpretation [8]. The superscripts should NOT be confused with powers of the variables.


ones, one could perform the bootstrap average over the distribution Eq. (7) analytically. To enable such an analytical average over the vector s (which is the "quenched disorder" in the language of statistical physics) one introduces the following "trick" extensively used in statistical physics of amorphous systems [5]. We introduce the auxiliary quantity

N r N

n(S) = 1 e- N N

S

ED



Zn -r х[fa] fi a a

i=1 a=1

for arbitrary real n, which allows to write si,0

 dfa   (P (zj|fj ))sj  

(9)

j=1

(S) = lim n(S). n0 (10)

terms of n replicas f1, f2, . . . , fn of the original variable f for which an explicit average The advantage of this definition is that for integers n  r, n(S) can be represented in over si's is possible. At the end of all calculations an analytical continuation to arbitrary real n and the limit n  0 must be performed. For integer n  r, we use the definition of the partition function Eq. (3), exchange the expectation over datasets with the expectation over f's and use the explicit form of the distribution Eq. (7) to perform the average over

bootstrap sets. The resulting expressions can be rewritten as n(S) = fi

\n N

i N r

a

i=1 a=1

4

, (11)

\i

where иии

\i denotes an average with respect to the so called cavity distribution P\ for i

replicated variables fi = (fi , . . . , fi ) defined by

N

P\ (fi)  i 1 Li(fi) dfj P (f1, . . . , fN ) .

1 n

(12)

j=1,j=i

The joint distribution of replica variables P(f1, . . . , fN)  defined by the new likelihoods

n

Lj(fj) = exp S -N 1 - P (zj|fj )

n a=1 х[fa] N j=1 Lj(fj) is

a . (13)

a=1

5 TAP Approximation We have mapped the original bootstrap problem to an inference problem for an effective Bayesian probabilistic model (the hidden variables have the dimensionality N О n) for which we have to find a tractable approximation which allows analytical continuation of is often found to give more accurate results than, e.g., a simple mean field or a variational n  0 and   . We use the adaptive TAP approach of Opper and Winther [9] which Gaussian approximation. The ADATAP approach replaces the analytically intractable cavity distribution Eq. (12) by a Gaussian distribution. In our case this can be written as

P\ (fi)  e- 2

i 1 f T c(i)f +c(i)T f , (14)

where the parameters c and c are computed selfconsistently from the dataset D0 by solving a set of coupled nonlinear equations. Details are given in the appendix. The form Eq. (14) allows a simple way of dealing with the parameters n and . We utilize the exchangeability of variables fi , . . . , fi and assume replica symmetry and further

1 n P\i(fi), Eq. (12), has the normalizing partition function \n where \n  1 for n  0. i i

4


introduce an explicit scaling of all parameters with . This scaling was found to make all final expressions finite in the limit   . We set ab(i) = c(i) = 2c(i) (15)

c

aa(i) = 0c(i) = 2c(i) .

c 0

for a = b and

0

c (i) = c(i) for all a = 1, . . . , n . a

Eq. (15) keeps the number of adjustable parameters independent of n and allows to perform we start the final numerical parameter optimization. the "replica limit" n  0 and the "SVM-limit"    in all equations analytically before Computing the expectation Eq. (11) with Eq. (14) and (15) and resumming the power series over r yields the final theoretical expression for Eq. (6)

N

c(i) + u

We also assume that c(i) = -1(c(i) - c(i)) remains finite for   . The ansatz

(S) = 1 N dG(u) g

i=1

c(i-c( i) ;

) xi, yi (16)

2

du(2)- 2 e- u2 1 where dG(u) = and g is an arbitrary loss function. With

g(f^D(xi); xi, yi) = (-yif^D(xi)) we obtain the bootstrapped classification error

N

(S) = 1 N  -

i=1

yic(i) -c(i) (17)

where (x) = x - dG(u).

Besides the computation of generalization errors, we can use our method to quantify the uncertainty of the SVM prediction at test points. This can be obtained by computing the bootstrap distribution of the "internal fields" f^D(xi) at a test input xi. This is obtained from Eq. (16) by inserting g(f^D(xi); xi, yi) = (f^D(xi) - h) using the Dirac -function

i(h) =

c(i) c(i)

c(i) -2c(i) exp -( hc(i) - c(i))2

2(-c(i)) , (18)

i.e., mci =

and Vii = -(

c(i))2

c

c(i)

are the predicted mean and variance of the internal

field. (The predicted posterior variance of the internal field is (c(i))-1 and goes to zero as    indicating the transition to/ a deterministic model.) It is possible to extend the This replaces c(i), c(i), c(i) by result Eq. (18) to "real" test inputs x  D0, which is of greater importance to applications.

N -1

c(x) = K(x, x) -

N

c(x)

K(x, xi)(i)Ti(x) (19)

i=1

c(x) = Ti(x)(i)

N

i=1

(c(x))2 c(x)

N j=1

= (Ti(x))2(i)

i=1

K(x, xj)(I + diag()K)-1. The parameters (i), (i), (i) are ji with Ti(x) =

determined from D0 according to Eq. (22), (23).


0.5 2.0

error 0.4

0.3

0.14 0.12 0.10 0.08 0.06 0.04

Wisconsin, N=683 Simulation: p(-1|x) 0 0.2 0.4 0.6 0.8

1 1.0

1.5

0 200 400 600

1.0 classification

Pima, N=532 0.2 Density

0.8 0.6 0.4 Theory: 0.2 0.0

p(-1|x)

0.5 0.1

Bootstrapped

0.00 200

Sonar, N=208 Crabs, N=200 400

600 0.0-2

S: 0.376 T: 0.405 -1.5

-1 -0.5 0 0.5 1

Bootstrap sample size S Bootstrapped local field at a test input x

Figure 1: Left: Average bootstrapped generalization error for hard margin support vector classification on different data sets (simulation: symbols, theory: lines). Right: Boot/ distributions are Gaussian-like and in good agreement with the theory Eq. (18). We show strapped distribution of the internal field for Sonar data at a test input x  D0. Most an atypical case (simulation: histogram, theory line) which nevertheless predicts the relative weights for both class labels fairly well. The inset shows true versus estimated values of the probability p(-1|x) for predicting label y = -1 . 6 Results for Bootstrap of Hard Margin Support Vector Classifiers We determined the set of theoretical parameters by solving Eq. (21)-(23) for four benchmark data sets D0 [10] and different sample sizes S using a RBF kernel K(x, x ) = panel of Fig.1 compares our theoretical results for the bootstrapped learning curves obtained by Eq. (17) (lines) with results from Monte-Carlo simulations (symbols). The Gaussian approximation of the cavity distribution is based on the assumption that the model prediction at a training input is influenced by a sufficiently large number of neighboring inputs. We expect it to work well for sufficiently broad kernel functions. This was the case for the Crabs and Wisconsin data sets where our theory is very accurate. It predicts correctly the interesting non-monotonous learning curve for the Wisconsin data (inset Fig.1, left). In comparison, the Sonar and Pima data sets were learnt with narrow RBF kernels. Here, we see that the quality of the TAP approximation becomes less good. However, our results provide still a reasonable estimate for the bootstrapped generalization error at sample size S = N. While for practical applications of estimating the "true" generalization error using Efron's 0.632 bootstrap estimator the case S = N is of main importance, it is also interesting to discuss the limit of extreme oversampling S  . Since the hard margin SVM gains no additional information from the multiple presentation of the same data point, in this limit all bootstrap sets D supply exactly the same information as the data set D0 and the data average ED[. . . ] becomes trivial. Variances with respect to ED[. . . ] go to zero. With Eq. (21)-(23), we can write the average prediction mi at input xi  D0 as

exp(- 2 1 d k=1 vk(xk - xk)2)) with individually customized hyperparameters vk. The left

mi = N j=1 yjjK(xi, xj) with weights j = (j)c(j) (j)+c(j) (yjmj - yjmcj) and recover

generalization error Eq. (17) is found to converge to the approximate leave-one-out error for S   the Kuhn-Tucker conditions i  0 and i(yimi -1) = 0. The bootstrapped of Opper and Winther [8] lim (S) =  (20)

N SV

S

1 N  (-yimci ) = i [K-1 ]ii

SV

- 1

i=1 i


where the weights i are given by the SVM algorithm on D0 and KSV is the kernel matrix on the set of SV's. While the leave-one-out estimate is a non-smooth function of model parameters, Efron's 0.632 (N) bootstrap estimate [2] of the generalization error approximated within our theory results in a differentiable expression Eq. (17) which may be used for kernel hyperparameter estimation. Preliminary results are promising. The right panel of Fig. 1 shows results for the bootstrapped distribution of the internal field on test inputs x  D0. The data set D0 contained N = 188 Sonar data and the bootstrap / is at sample size S = N. We find that the true distribution is often very Gaussian-like and well described by the theory Eq. (18). Figure 1 (right) shows a rare case where a bi-modal distribution (histogram) is found. Nevertheless, the Gaussian (line) predicted by our theory estimates the probability p(-1|x) of a negative output quite accurately in comparison to the probability obtained from the simulation. Both SVM training and the computation of our approximate SVM bootstrap requires the running of iterative algorithms. We compared the time ttrain for training a single SVM on each of the four benchmark data sets D0 with the time ttheo needed to solve our theory for SVM bootstrap estimates on these data for S = N. For sufficiently broad kernels we For the latter (Pima example in Fig.1 (left)) we find ttheo > ttrain where our theory is still find ttrain  ttheo and our theory is reliable. The exception are extremely narrow kernels. faster to compute but less reliable than a good Monte-Carlo estimate of the bootstrap. 7 Outlook Our experiments on SVMs show that the approximate replica bootstrap approach appears to be highly robust to apply to models which only fit into our framework after some delicate limiting process. The SVM is also an important application because the prediction for each dataset requires the solution of a costly optimization problem. Experiments on benchmark data showed that our theory is appreciably faster to compute than a good Monte-Carlo estimate of the bootstrap and yields reliable results for kernels which are sufficiently broad. It will be interesting to apply our approach to other kernel methods such as kernel PCA. Since our method is based on a fairly general framework, we will also investigate if it can be applied to models where the bootstrapped parameters have a more complicated structure like, e.g., trees or hidden Markov models. Acknowledgments DM gratefully acknowledges financial support by the Copenhagen Image and Signal Processing Graduate School and by the Postgraduate Programme "Natural Disasters" at the University of Karlsruhe. Appendix: TAP Equations The ADATAP approach computes the set of parameters c(i), c(i) by constructing an

alternative set of tractable likelihoods Lj(f) = e-2 ^ 1 f T (j)f +(j)T f defining an auxiliary

Gaussian joint distribution PG(f1, . . . , fN)  n a=1

х(fa)

N j=1 ^ Lj(fj). We use replica

symmetry and a specific scaling of the parameters with : a(j) = (j), aa(j) = (j)). All unknown parameters are found by moment matching: We assume that the first 0(j) = 20(j) for all a, ab(j) = (j) = 2(j) for a = b and (j) = -1(0(j)-

two marginal moments mi = lim fi n0

fi fi a b

a

, Vii = lim n0 fi fi

a b

-(mi)2, ii =  lim

n0

fi fi -

a a

of the variables fi can be computed 1) by marginalizing PG and 2) by using the


relations between cavity distribution and marginal distributions P(fi)  Li(fi)P\ (fi) as well as PG(fi)  Li(fi)P\ (fi) for all i = 1, . . . , N. This yields i

^ i

S

1 - (1 - e- N )(ci )

S

1 - (1 - e- N )(ci )

S

1 - (1 - e- N )(ci )

ii mi Vii = = = cii mci Vii

c

(21)

Vii e- 2  1 (ci ) + 2 c S + yi(1 - e- N ) (ci )2

+ (1 - yimi)(yimi - yimci )

where mci = c(i) c(i) , Vii = -(

c(i))2

ii mi Vii = = =

c

c(i)

, cii = (G)ii 1 c(i) and ci =

1-yimci

Vii

c

. Further

(22)

(G )i -(G diag() G)ii

with the N О N matrix G = (K-1 + diag(1))-1 and

ii mi Vii = = =

(i) + c(i) (i) + c(i) (i) + c(i) (i) + c(i))2

(i) + c(i)

-(

(23)

We solve Eq. (21)-(23) by iteration using Eqs. (21) and (22) to evaluate the moments {{mi,Vii,ii} and Eq. (23) to update the sets of parameters {i=1i)1+ii(i)(,c(i)} (i), (i), (i)}, respectively. Reasonable start values are1 Ni) =, -i)(1=--(1 -, ( e- )(c)) with c = -0.5 and i are the eigenvalues of kernel matrix K. References [1] B. Efron. Ann. Statist., 7: 1-26, 1979. [2] B. Efron, R. J. Tibshirani. An Introduction to the Bootstrap. Monographs on Statistics and Applied Probability 57, Chapman & Hall, 1993. [3] J. Shao, D. Tu, The Jackknife and Bootstrap, Springer Series in Statistics, Springer, 1995. [4] D. Malzahn, M. Opper, A statistical mechanics approach to approximate analytical Bootstrap averages, NIPS 15, S. Becker, S. Thrun, K. Obermayer eds., MIT Press, 2003. [5] M. Mezard, G. Parisi, M. A. Virasoro, Spin Glass Theory and Beyond, Lecture Notes ┤ in Physics 9, World Scientific, 1987. [6] B. Scholkopf, C. J. C. Burges, A. J. Smola (eds.), Advances in Kernel Methods: Supе port Vector Learning, MIT, Cambridge, MA, 1999. [7] P. Sollich, Probabilistic interpretation and Bayesian methods for Support Vector Machines, In: ICANN99, pp.91-96, Springer 1999. [8] M. Opper, O. Winther, Neural Computation, 12: 2655-2684, 2000. [9] M. Opper, O. Winther, Phys. Rev. Lett. , 86: 3695, 2001. [10] From http://www1.ics.uci.edu/~mlearn/MLSummary.html and http://www.stats.ox.ac.uk/pub/PRNN/.

c ( , c and

(i) = yi where  is obtained as the root of 0 = 1 - N

S/N


