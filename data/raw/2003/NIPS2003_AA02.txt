Simplicial Mixtures of Markov Chains: Distributed Modelling of Dynamic User Profiles

Mark Girolami Department of Computing Science University of Glasgow Glasgow, UK girolami@dcs.gla.ac.uk Ata Kaban ┤ School of Computer Science University of Birmingham Birmingham, UK a.kaban@cs.bham.ac.uk

Abstract To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the definition of individual specific and global models. This paper proposes a linear-time distributed model for finite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be `explained' by a relatively small number of common structurally simple behavioral patterns which may interleave randomly in a user-specific proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efficient representation scheme, reflected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations.

1 Introduction The now commonplace ability to accurately and inexpensively log the activity of individuals in a digital environment makes available a variety of traces of user activity and with it the necessity to develop efficient representations, or profiles, of individuals. Most often, such recordings take the form of streams of discrete symbols ordered in time. The modelling of time dependent sequences of discrete symbols employing n'th order Markov chains has been extensively studied in a number of domains. The representation provided by such models is global in the sense that it is assumed that one global generating process underlies all observed sequences. To capture the possible heterogeneous nature of the observed sequences a model with a number of differing generating processes needs to be considered. Indeed the notion of a heterogeneous population, characterized for example by occupational mobility and consumer brand preferences, has been captured in the MoverStayer model [3]. This model is a discrete time stochastic process that is a two component mixture of first order Markov chains, one of which is degenerate and possesses an identity transition matrix characterizing the stayers in the population. The original notion of a twocomponent mixture of Markov chains has recently been extended to the general form of a mixture model of Markov chains in [2]. Whilst the main motivation was the visualization of the class structure inherent in the browsing patterns of visitors to a commercial website, each class of users being characterized by their global behavior, such mixture models will


not be appropriate for identifying the shared behavioral patterns which are the basis of multiple relationships between users and groups of users and which may yield a more realistic model of the population. The purpose of this paper is to develop a dynamic user model for individuals within a group that explicitly captures the assumption of the existence of a common set of behavioral patterns which can be estimated from all observed users along with their user-specific proportion of participation and these form the basis of individual profiles within a group. This is also a computationally attractive model, as simple structural characteristics may be assumed at the generative level, while allowing them to interleave randomly can account for more complex individual behavior. The resulting model is thus a distributed dynamic model which benefits from the recent technical developments in distributed parts based modelling of static vectorial data [7, 9, 5, 1, 8], with various applications including image decomposition, document modelling, information retrieval and collaborative filtering. Consistent generative semantics similar to the recently introduced latent Dirichlet allocation (LDA) [1] will be adopted and by analogy with [8] the resulting model will be referred to as a simplicial mixture. 2 Simplicial Mixtures of Markov Chains Assume that a sequence of L symbols sLsL , иии ,s0, denoted by s, can be drawn

-1

from a dictionary S by a process k, which has initial state probability P1(k) and has of times that the symbol s0 follows from the state defined by the m-tuple of symbols ity of the sequence of symbols under the k'th m-th order Markov process is P(s|k) = pact notation we represent the elements of the state transition matrix for the k'th Markov

|S|m +1 state transition probabilities denoted by T(sm, иии ,s1  s0|k). The number

sm,иии ,s1 within the n-th sequence is denoted as rn sm,иии,s1s0 and so the probabil-

P1(k) |S| sm=1 иии |S| s0=1 T(sm,иии ,s1  s0|k)rsm, иии,s1s0 . To introduce a more com-

process by Tm

rn

mиии0

иии0,k

and the counts rsm, иии,s1s0

within the n'th observed sequence as

. In addition, we employ Start and Stop states in each symbol sequence sn and

incorporate the initial state distribution of the Start state as the transition probabilities from this state within the state transition matrix Tk. We denote the set of all state transition matrices {T1, иии , Tk, иии , TK} as T. Suppose that we are given a set of symbolic posed and somewhat complementary to cluster models for trajectories which try to model inter-sequence heterogeneities, our intuition is that sequences over a common finite state space, provided they are sufficiently long and possibly non-stationary, could have several randomly interleaved generator processes, some of which might be common to several sequences. To account for this idea, we will adopt a similar modelling strategy to LDA. The complete generative semantics of LDA allows us to describe the process of sequence generation where mixing components  = [1, иии ,k, иии ,K] are K-dimensional Dirichlet random variables and so are drawn from the K - 1 dimensional simplex defined by the Dirichlet distribution D(|) with parameters . These are then combined with the individual state-transition probabilities Tk, which are model parameters to be estimated, ability for a sequence sn under such a mixture, which we shall now refer to as a simplicial mixture [8], denoted as P(sn|T, ) is equal to

trajectories {sn}n =1:N over a common finite state space, each having length Ln. As op-

and yield the symbol transition probabilities Tm иии0 = K k=1 Tm иии0,k k  . The overall prob-

|S| |S|

иии

K rn mиии0

P(sn|T,)D(|)d = dD(|) Tm иии0,k k  (1)

sm=1 s0=1 k=1

Each sequence will have its own expectation under the Dirichlet mixing coefficients and


so the ability of such a representation to model intra-sequence heterogeneity emerges naturally. The following subsections briefly present the details of the identification of this model, which also highlights the close relationship between two existing related models, specifically the probabilistic latent semantic analysis (PLSA) [5] and LDA [1] as being instances of the same theoretical model and differing only in the estimation procedure adopted [4]. 2.1 Parameter Estimation and Inference Exact inference within the LDA framework is not possible [1], however the likelihood can be lower-bounded by introducing a sequence specific parameterised variational posterior Qn() whose parameters will depend on n

log P(sn|T,)  EQn( ) log P(sn|T,)D

(|)

(2)

Qn()

Where EQn( denotes expectation with respect to Qn(). The bound can be defined using ) the Maximum a Posteriori (MAP) estimator, such that Qn() = ( - MAP ), in which

n

case (2) is equal to log P(sn|T, MAP ) + log D(MAP |) + H where H denotes the

n n

entropy of the delta function around MAP (which can be discarded in this setting as it

n

does not depend on the model parameters, although it amounts to minus infinity). Forming a Lagrangian from the above to enforce the constraint that MAP is a sample point from a Dirichlet variable then taking derivatives with respect to the MAP , a convergent series of

k

updates tkn is obtained where the superscript denotes the t'th iteration. As in [7], for each observed sequence in the sample a MAP value for the variable  is iteratively estimated by the following multiplicative updates

|S| |S|

иии kn = (k-1)+tkn ~ rn mиии0

Tm

K l=1

иии0,k

; +1 ~ kn

k tln

tkn =

sm=1 s0=1 Tm иии0,l Ln +

(k - 1) (3)

where Ln =

rm

smиииs0 n

иии0

is the length of the sequence sn. Once the MAP values MAP

n

for each sn are obtained a similar multiplicative iteration for the transition probabilities can be obtained

N

T~m

MAP

kn l=1 mиии0,l ln

Tt K MAP

T~m

|S| s0=1

иии0,k = Tm t иии0,k rn mиии0 ; Tm t+1

иии0,k

иии0,k

T~m иии0 ,k

= (4)

n=1

The final parameter is that of the prior Dirichlet distribution, maximum likelihood estimation yields the estimated distribution parameters  given the MAP [6, 1]. Note that both

n

(3) and (4) require an elementwise matrix multiplication and division so these iterations will scale linearly with the number of non-zero state-transition counts. It is interesting to note that the MAP estimator under a uniform Dirichlet distribution exactly recovers the aspect mixture model of [5] as a special case of the MAP estimated LDA model. 2.1.1 Variational Parameter Estimation and Inference While being optimal in analyzing an existing data set, MAP estimators are notoriously prone to overfitting, especially where there is a paucity of available data [10] and so the variational Bayes (VB) approach detailed in [1] can be adopted by considering Qn() = D(|n), where n is a sequence-specific variational free parameter vector. The above (2) can be further lower-bounded by noting that

|S| |S|

иии

K

log P(sn|T,)  rn

sm=1 s0=1 k=1

mиии0 Tm иии0,k

иии0,n (k)

(k)log k Qm Qm иии0,n (5)


where

k Qm иии0,n (k) = 1, Qm иии0,n

(k)  0 are additional variational parameters. Al-

ternatively, Qm

иии0,n

(.) can also be understood as a variational distribution on a discrete

hidden variable with K possible outcomes that selects which transition matrix is active at each time step of the generative process.

Replacing (5) in (2), expanding and evaluating ED ( |

 n)[log

k] = (k) - ( k k ),

where  denotes the digamma function, then solving for Qm иии0,n (k) and kn and finally

combining yields the following multiplicative iterative update for the sequence specific variational free parameter n

|S| |S|

иии kn = k + exp{(kn)} t+1 t rn mиии0

Tm иии0,k

(6)

sm=1 s0=1

K k =1 Tm exp{(k )}

n t иии0,k

Solving for the transition probabilities and combining with the fixed point solutions for

each Qm иии0,n

T~m

(k) yields the following

N

= Tm rn

exp{(kn)}

Tt

k =1 mиии0,k

exp{(k )} n

t t mиии0

; Tm иии0,k t+1

T~m иии0,k

T~m иии0 ,k s0

иии0,k иии0,k K =

n=1 t

(7) As before the parameters of the prior Dirichlet distribution  given the variational parameters n are estimated using standard methods [6, 1]. 2.2 Prediction with Simplicial Mixtures The predictive probability of observing symbol snext given a sequence of L symbols

sn = {sLn,иии ,s1} is given as P(snext|sn) = EP

K k=1

( |sn) 

{P(snext|sm иииs1,)} 

T(snext|sm иииs1,k)EQn( {k}. It should be noted that while m-th order Markov )

chains form the basis of the representation, the resulting simplicial mixture is not m-th order Markov with any global transition model. Rather it approximates the individual m-th order models while keeping the generative parameter set compact. The m-th order information of each individual's past behaviour is embodied in the individual-specific latent variable estimate. On the other hand in a mixture model one component is responsible for sequence generation so within a cluster the representation is still global m-th order. Employing the MAP approximation for the Dirichlet distribution then EQn( {k} = )

E (-MAP ) n

{k} = MAP where MAP is the k-th dimension of MAP. Employing the

kn kn n

variational Dirichlet approximation then EQn( {k} = ED ) (|n)

{k} = kn/

K l=1 ln

therefore given a new sequence snew, the symbol snext which is most likely to be predicted from the model as a suggested continuation of the sequence, is the maximum argument of P(snext|sn). 3 Distributed Modelling of Dynamic Profiles 3.1 Datasets 3.1.1 Telephone Usage Modelling The ability to model the usage of a telephone service is of importance at a number of levels, e.g. to obtain a predictive model of customer specific activity and service usage for the purposes of service provision planning, resource management of switching capacity, identification of fraudulent usage of services. A representative description can be based on the distribution of the destination numbers dialled and connected by the customer, in which


case a multinomial distribution over the dialling codes can be employed. One method of encoding the destination numbers dialled by a customer is to capture the geographic location of the destination, or the mobile service provider if not a land based call. This is useful in determining the potential demand placed on telecommunication switches which route traffic from various geographical regions on the service providers network. Two weeks of transactions from a UK telecommunications operator were logged during weekdays, amounting to 36,492,082 and 45,350,654 transactions in each week respectively. All transactions made by commercial customers in the Glasgow region of the UK were considered in this study. This amounts to 1,172,578 transactions from 12,202 high usage customers in the first week considered and 1,753,304 transactions being made in the following week. The mapping from dialling number to geographic region or mobile operator was encoded with 87 symbols amounting to a possible 7,569 symbol transitions. Each customers activity is defined by a sequence of symbols defining the sequence of calls made over each period considered and these are employed to encode activity in a customer specific generative representation. 3.1.2 Web Page Browsing The second data set used in this study is a selected subset of the msnbc.com user navigation collection employed in [2]. Sequences of users who visited at least 9 of the overall 17 page categories (frontpage, news, tech, local, opinion, on-air, misc,weather, msn-news, health, living, business, msn-sports, sports, summary, bbs, travel) have been retained, this selection criteria is motivated by the observation that there would be little scope in trying to model interleaved dynamic behavior in observables which are too short to reveal any intra-sequence heterogeneity. The resulting data set, referred to as WEB, totals 119,667 page requests corresponding to 1,480 web browsing sessions.

0.62

11

0.61

Error 10

Perplexity 0.6

Prediction 9

Fraction 0.59 Predictive 8

7 0.58

0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 0.4 0.6 0.8 1 1.2 1.4

Number of Factors ( log 10 )

Number of Factors ( log 10 1.6 ) 1.8

Figure 1: Left: percentage of incorrect predictions against the number of model factors; right: predictive perplexity of each model against model order for the PHONE dataset. Solid straight line: global first order MC, dash: MAP estimated simplicial mixture, solid line: VB estimated simplicial mixture, dash-dot: mixture model.

3.2 Results In each experiment the objective assessment of model performance is evaluated by the curacy of all models is measured under a 0-1 loss. Given a number of previously unobserved truncated sequences, the number of times the model correctly predicts the symbol which follows in the sequence is then counted. In all mixture models naive random initialization of the parameters was employed and parameter estimation was halted when the

predictive perplexity, exp{-1/N Ntest m=1 log P(snext|sm)}. In addition, the predictive ac-


5

) 4

)d|

0.4 0.35 0.3

n

0.25

k 0.2

bits(

3

Rate P(

2



Entropy

1

0.15 0.1 0.05 0

1 Simplicial Model 2 Mixture Model 5 10 (k) 15 20

Figure 2: Left: distribution of entropy rates for the transition matrices of a 20-factor mixture and simplicial mixture models (VB). Right: the expected value of the Dirichlet variable under the variational approximation for one customer indicating the levels of participation in factor specific behaviors.

in-sample likelihood did not improve by more than 0.001%, no annealing or early stopping was utilized, fifteen randomly initialized parameter estimation runs for each model were performed. The number of mixture components for the models ranged from 2 up to 200. On the PHONE data set the parameters of a global first-order Markov chain (bigram), mixtures of Markov chains [2], and simplicial mixtures of Markov chains (using both the MAP and VB estimation procedures) are estimated using the first week of customer transactions and the predictive capabilities of the models are assessed on the transactions from the following week. The results are summarized in Figure 1, from the predictive perplexity measures it is clear that the simplicial representation provides a statistically (tested at the 5% level using a Wilcoxon Rank Sum test) and practically significant reduction in perplexity over the global and mixture models. This is also reflected in the levels of prediction error under each model, however the mixture models tend to perform slightly worse than the global model. As expected the MAP estimated simplicial model performs slightly worse than that obtained using VB [1]. This also provides an additional insight as to why LDA models improve upon PLSA, as they are in fact both the same model using different approximations to the likelihood, refer to [10] for an illustrative discussion on the weaknesses of MAP estimators. As a comparison to different structural models hidden Markov models with a range of hidden states were also tested on this data set the best results obtained were for a ten state model which achieved a predictive perplexity score of (mean▒standard-deviation) 11.119 ▒ 0.624 and fraction prediction error of 0.674 ▒ 0.959, considerably poorer than that obtained by the models considered here. In addition to the predictive capability of a simplicial representation of a customers activity the cost of encoding such a representation can be assessed by measuring the entropy rate of each of the constituent transition matrices which act as a basis in the representation of the individual specific generative process. The left hand plot of Figure (2) shows the distribution of the entropy rates for the transition probabilities in twenty factor simplicial and mixture models, the results are obtained from fifty randomly initialized estimation procedures. The entropy rates for the simplicial mixture are significantly lower than that of a mixture model indicating that the basis of each representation describes a number of simpler processes. The final experiment demonstrated considers the WEB data set. The results of ten-fold cross-validated predictive perplexities again show statistically significant improvement obtained with the VB-estimated simplicial mixture (again tested using the ranksum Wilcoxon test at the 5% level). The results are summarized in Figure 3. Five of the estimated transition factors of a twenty-factor model are shown in Figure 4, demonstrating once more that the proposed model creates a low entropy and an easily interpretable dynamic factorial representation. The numbers on the axes on these charts correspond to the 17 page cat-


egories enumerated earlier and the average strength of each of these factors amongst the chart. We can see that a behavioral feature manifested is a keen interest to visit pages about `news' along with a quite dynamic transition model (left hand chart) which characterizes around 12% of the behavioral patterns of the entire user population under consideration while static state-repetition (second chart) or an almost exclusive interest in viewing the homepage (last chart) etc represent also relatively strong common characteristics of browsing behavior. The distribution of the entropy rates of the full set of these twenty basistransitions in comparison to those obtained from the mixture model is given on the right hand plot of Figure 3. Clearly, the coding efficiency of a simplicial mixture representation is significantly (statistically tested) superior. Note also these basis-transitions embody correlated transitions (transitions which appear in similar dynamical contexts and so have similar functionality), as can be seen from the multiplicative nature of the equations used for identifying the model. It is not surprising then that state repetitions or transitions which express focused interest in one of the topic categories appear together on distinct factors. We can also see a joint interest in msnnews and msnsport being present together on the 4-th chart of Figure 4 -- indeed, as the prefix of these page categories also indicates, these are related page categories.

full set of twenty factors computed as 1 N N n=1 ED (|n) {k} is also given above each

7.8

3

7.6

2.5

7.4

7.2 2

(bits)

7

Perplexity Rates 1.5

6.8

Predictive 6.6 Entropy 1

6.4

0.5

6.2

6 0.4 0.6 0.8 1

1.2 10

) 1.4 1.6

Nr of Factors (log Simplicial Model Mixture Model

Figure 3: Left: the predictive perplexity for the WEB data (straight line: global firstorder Markov chain, dash-dot: mixture of Markov chains, dotted line: simplicial mixture estimated by MAP, solid line: simplicial mixture estimated by VB). Right: the distribution of entropy rates.

4 Conclusions This paper has presented a linear time method to model finite-state sequences of discrete symbols which may arise from user or customer activity traces. The main feature of the proposed approach has been the assumption that heterogeneous user behavior may be `explained' by the interleaved action of some structurally simple common generator processes.

0.12 0.23 0.03 0.02 0.07

2 4 6 8 10 12 14 16 2 4 6 8 10 12 14 16 2 4 6 8 10 12 14 16 2 4 6 8 10 12 14 16 2 4 6 8 10 12 14 16

5 10 15 5 10 15 5 10 15 5 10 15 5 10 15

Figure 4: State transition matrices of selected factors from a 20-factor run on WEB.


An empirical study has been conducted on two real-world collections of user activity which has demonstrated this to be an efficient representation, revealed by both objective measures of prediction performance, low entropy rates, and interpretable representations of the user profiles provided. Acknowledgements Mark Girolami is part of the DETECTOR project funded by the Department of Trade and Industry (DTI) Management of Information (LINK) Programme and the Engineering & Physical Sciences Research Council (EPSRC) grant GR/R55184. References [1] D. M. Blei, A. Y. Ng & M. I. Jordan, Latent Dirchlet Allocation, Journal of Machine Learning Research, 3(5):993Г1022, 2003. [2] I. Cadez, D. Heckerman, C. Meek, P. Smyth & S. White, Model-based clustering and visualisation of navigation patterns on a web site, Journal of data Mining and Knowledge Discovery, in press. [3] H. Frydman, Maximum likelihood estimation in the mover-stayer model, Journal of the American Statistical Society, 79, 632-638, 1984. [4] M. Girolami and A. Kaban, On an equivalence between PLSI and LDA, Proc. 26-th ┤ Annual International ACM SIGIR Conference, 2003, pp. 433Г434. [5] T. Hofmann,Unsupervised learning by probabilistic latent semantic analysis, Machine Learning, 42, 177-196, 2001. [6] G. Ronning, Maximum likelihood estimation of Dirichlet distributions, Journal of Statistical Computation and Simulation, 32:4, 215-221, 1989. [7] D. Lee & H. Sebastian Seung, Algorithms for Non-negative Matrix Factorization, Advances in Neural Information Processing Systems 13, ed's Leen, Todd K, Dietterich, Thomas G. and Tresp, Volker, 556Г562, MIT Press, 2001. [8] T. Minka & J. Lafferty, Expectation-propogation for the generative aspect model, Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence, 2002. [9] D. A. Ross & R. S. Zemel, Multiple-cause vector quantiztion, Advances in Neural Information Processing Systems 15, 2003. [10] H. Lappalainen & J. W. Miskin. Ensemble Learning. In M. Girolami, editor, Advances in Independent Component Analysis, 75-92, Springer-Verlag, 2000.


