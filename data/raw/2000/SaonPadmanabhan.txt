Minimum Bayes Error Feature Selection for
Continuous Speech Recognition
George Saon and Mukund Padmanabhan
IBM T. J. Watson Research Center, Yorktown Heights, NY, 10598
E­mail: fsaon,mukundg@watson.ibm.com, Phone: (914)­945­2985
Abstract
We consider the problem of designing a linear transformation  2 IR pn ,
of rank p  n, which projects the features of a classifier x 2 IR n onto
y = x 2 IR p such as to achieve minimum Bayes error (or probabil­
ity of misclassification). Two avenues will be explored: the first is to
maximize the ­average divergence between the class densities and the
second is to minimize the union Bhattacharyya bound in the range of .
While both approaches yield similar performance in practice, they out­
perform standard LDA features and show a 10% relative improvement
in the word error rate over state­of­the­art cepstral features on a large
vocabulary telephony speech recognition task.
1 Introduction
Modern speech recognition systems use cepstral features characterizing the short­term
spectrum of the speech signal for classifying frames into phonetic classes. These features
are augmented with dynamic information from the adjacent frames to capture transient
spectral events in the signal. What is commonly referred to as MFCC+ +  features
consist in ''static'' mel­frequency cepstral coefficients (usually 13) plus their first and sec­
ond order derivatives computed over a sliding window of typically 9 consecutive frames
yielding 39­dimensional feature vectors every 10ms. One major drawback of this front­end
scheme is that the same computation is performed regardless of the application, channel
conditions, speaker variability, etc. In recent years, an alternative feature extraction pro­
cedure based on discriminant techniques has emerged: the consecutive cepstral frames
are spliced together forming a supervector which is then projected down to a manageable
dimension. One of the most popular objective functions for designing the feature space
projection is linear discriminant analysis.
LDA [2, 3] is a standard technique in statistical pattern classification for dimensionality
reduction with a minimal loss in discrimination. Its application to speech recognition has
shown consistent gains for small vocabulary tasks and mixed results for large vocabulary
applications [4, 6]. Recently, there has been an interest in extending LDA to heteroscedastic
discriminant analysis (HDA) by incorporating the individual class covariances in the ob­
jective function [6, 8]. Indeed, the equal class covariance assumption made by LDA does

not always hold true in practice making the LDA solution highly suboptimal for specific
cases [8].
However, since both LDA and HDA are heuristics, they do not guarantee an optimal pro­
jection in the sense of a minimum Bayes classification error. The aim of this paper is to
study feature space projections according to objective functions which are more intimately
linked to the probability of misclassification. More specifically, we will define the proba­
bility of misclassification in the original space, , and in the projected space,   , and give
conditions under which   = . Since after a projection y = x discrimination infor­
mation is usually lost, the Bayes error in the projected space will always increase, that is
    therefore minimizing   amounts to finding  for which the equality case holds. An
alternative approach is to define an upper bound on   and to directly minimize this bound.
The paper is organized as follows: in section 2 we recall the definition of the Bayes error
rate and its link to the divergence and the Bhattacharyya bound, section 3 deals with the
experiments and results and section 4 provides a final discussion.
2 Bayes error, divergence and Bhattacharyya bound
2.1 Bayes error
Consider the general problem of classifying an n­dimensional vector x into one of C dis­
tinct classes. Let each class i be characterized by its own prior  i and probability density
function p i , i = 1; : : : ; C. Suppose x is classified as belonging to class j through the Bayes
assignment j = argmax 1iC  i p i (x). The expected error rate for this classifier is called
Bayes error [3] or probability of misclassification and is defined as
 = 1
Z
IR n
max
1iC
 i p i (x)dx (1)
Suppose next that we wish to perform the linear transformation f : IR n ! IR p , y =
f(x) = x, with  a p  n matrix of rank p  n. Moreover, let us denote by p 
i the
transformed density for class i. The Bayes error in the range of  now becomes
  = 1
Z
IR p
max
1iC
 i p 
i (y)dy (2)
Since the transformation y = x produces a vector whose coefficients are linear combi­
nations of the input vector x, it can be shown [1] that, in general, information is lost and
   .
For a fixed p, the feature selection problem can be stated as finding ^  such that
^
 = argmin
2IR pn ; rank()=p
  (3)
We will take however an indirect approach to (3): by maximizing the average pairwise
divergence and relating it to   (subsection 2.2) and by minimizing the union Bhattacharyya
bound on   (subsection 2.3).

2.2 Interclass divergence
Since Kullback [5], the symmetric divergence between class i and j is given by
D(i; j) =
Z
IR n
p i (x) log
p i (x)
p j (x)
+ p j (x) log
p j (x)
p i (x)
dx (4)
D(i; j) represents a measure of the degree of difficulty of discriminating between the
classes (the larger the divergence, the greater the separability between the classes).
Similarly, one can define D  (i; j), the pairwise divergence in the range of . Kull­
back [5] showed that D  (i; j)  D(i; j). If the equality case holds,  is called a suf­
ficient statistic for discrimination. The average pairwise divergence is defined as D =
2
C(C 1)
P
1i<jC D(i; j) and respectively D  = 2
C(C 1)
P
1i<jC D  (i; j). It fol­
lows that D   D. The next theorem due to Decell [1] provides a link between Bayes
error and divergence for classes with uniform priors  1 = : : : = C (= 1
C ).
Theorem [Decell'72] If D  = D then   = .
The main idea of the proof is to show that if the divergences are the same then the Bayes
assignment is preserved because the likelihood ratios are preserved almost everywhere:
p i (x)
p j (x) = p 
i (x)
p 
j (x)
; i 6= j. The result follows by noting that for any measurable set A  IR p
Z
A
p 
i (y)dy =
Z
 1 (A)
p i (x)dx (5)
where  1 (A) = fx 2 IR n jx 2 Ag. The previous theorem provides a basis for selecting
 such as to maximize D  .
Let us make next the assumption that each class i is normally distributed with mean  i and
covariance  i , that is p i (x) = N (x;  i ;  i ) and p 
i (y) = N (y;  i ;  i  T ), i = 1; : : : ; C.
It is straightforward to show that in this case the divergence is given by
D(i; j) = 1
2
tracef 1
i [ j + ( i  j )( i  j ) T ] + 1
j [ i + ( i  j )( i  j ) T ]g n
(6)
Thus, the objective function to be maximized becomes
D  =
1
C(C 1)
tracef
C
X
i=1
( i  T ) 1 S i  T g p (7)
where S i =
X
j 6=i
 j + ( i  j )( i  j ) T , i = 1; : : : ; C.
Following matrix differentiation results from [9], the gradient of D  with respect to  has
the expression
@D 
@ =
1
C(C 1)
C
X
i=1
( i  T ) 1 [S i  T ( i  T ) 1  i S i ] (8)

Unfortunately, it turns out that @D 
@ = 0 has no analytical solutions for the stationary points.
Instead, one has to use numerical optimization routines for the maximization of D  .
2.3 Bhattacharyya bound
An alternative way of minimizing the Bayes error is to minimize an upper bound on this
quantity. We will first prove the following statement
 
X
1i<jC
p
 i  j
Z
IR n
q
p i (x)p j (x)dx (9)
Indeed, from (1), the Bayes error can be rewritten as
 =
Z
IR n
C
X
i=1
 i p i (x)dx
Z
IR n
max
1iC
 i p i (x)dx
=
Z
IR n
min
1iC
X
j 6=i
 j p j (x)dx
(10)
and for every x, there exists a permutation of the indices  x : f1; : : : ; Cg !
f1; : : : ; Cg such that the terms  1 p 1 (x); : : : ; C pC (x) are sorted in increasing order, i.e.
 x (1) p x (1) (x)  : : :   x (C) p x (C) (x). Moreover, for 1  k  C 1
 x (k) p x (k) (x) 
q
 x (k) p x (k) (x) x (k+1) p x (k+1) (x) (11)
from which follows that
min
1iC
X
j 6=i
 j p j (x) =
C 1
X
k=1
 x (k) p x (k) (x) 
C 1
X
k=1
q
 x (k) p x (k) (x) x (k+1) p x (k+1) (x)

X
1i<jC
q
 i p i (x) j p j (x)
(12)
which, when integrated over IR n , leads to (9).
As previously, if we assume that the p i 's are normal distributions with means  i and co­
variances  i , the bound given by the right­hand side of (9) has the closed form expression
X
1i<jC
p
 i  j e (i;j) (13)
where
(i; j) =
1
8
( i  j ) T
  i + j
2
 1
( i  j ) +
1
2
log
j  i + j
2 j
p
j i jj j j
(14)

is called the Bhattacharyya distance between the normal distributions p i and p j [3]. Simi­
larly, one can define   (i; j), the Bhattacharyya distance between the projected densities p 
i
and p 
j . Combining (9) and (13), one obtains the following inequality involving the Bayes
error rate in the projected space
  
X
1i<jC
p
 i  j e   (i;j) (= B  ) (15)
It is necessary at this point to introduce the following simplifying notations:
 B ij = 1
4 ( i  j )( i  j ) T and
 W ij = 1
2 ( i +  j ), 1  i < j  C.
From (14), it follows that
  (i; j) =
1
2
tracef(W ij  T ) 1 B ij  T g +
1
2
log
jW ij  T j
p
j i  T jj j  T j
(16)
and the gradient of B  with respect to  is
@B 
@ =
X
1i<jC
p
 i  j e   (i;j) @  (i; j)
@ (17)
with, again by making use of differentiation results from [9]
@  (i; j)
@ =
1
2
(W ij  T ) 1 [B ij  T (W ij  T ) 1 W ij B ij ] + (W ij  T ) 1 W ij
1
2
[( i  T ) 1  i + ( j  T ) 1  j ]
(18)
3 Experiments and results
The speech recognition experiments were conducted on a voicemail transcription task [7].
The baseline system has 2.3K context dependent HMM states and 134K diagonal gaus­
sian mixture components and was trained on approximately 70 hours of data. The test
set consists of 86 messages (approximately 7000 words). The baseline system uses 39­
dimensional frames (13 cepstral coefficients plus deltas and double deltas computed from
9 consecutive frames). For the divergence and Bhattacharyya projections, every 9 con­
secutive 24­dimensional cepstral vectors were spliced together forming 216­dimensional
feature vectors which were then clustered to estimate 1 full covariance gaussian density for
each state. Subsequently, a 39216 transformation  was computed using the objective
functions for the divergence (7) and the Bhattacharyya bound (15), which projected the
models and feature space down to 39 dimensions. As mentioned in [4], it is not clear what
the most appropriate class definition for the projections should be. The best results were
obtained by considering each individual HMM state as a separate class, with the priors of
the gaussians summing up to one across states. Both optimizations were initialized with

the LDA matrix and carried out using a conjugate gradient descent routine with user sup­
plied analytic gradient from the NAG 1 Fortran library. The routine performs an iterative
update of the inverse of the hessian of the objective function by accumulating curvature
information during the optimization.
Figure 1 shows the evolution of the objective functions for the divergence and the Bhat­
tacharyya bound.
0
50
100
150
200
250
300
0 5 10 15 20 25 30 35
Interclass
divergence
Iteration
"divg.dat"
5.3
5.4
5.5
5.6
5.7
5.8
5.9
6
0 20 40 60 80 100
Bhattacharyya
bound
Iteration
"bhatta.dat"
Figure 1: Evolution of the objective functions.
The parameters of the baseline system (with 134K gaussians) were then re­estimated in the
transformed spaces using the EM algorithm. Table 1 summarizes the improvements in the
word error rates for the different systems.
1 Numerical Algebra Group

System Word error rate
Baseline (MFCC+ +) 39.61%
LDA 37.39%
Interclass divergence 36.32%
Bhattacharyya bound 35.73%
Table 1: Word error rates for the different systems.
4 Summary
Two methods for performing discriminant feature space projections have been presented.
Unlike LDA, they both aim to minimize the probability of misclassification in the projected
space by either maximizing the interclass divergence and relating it to the Bayes error or
by directly minimizing an upper bound on the classification error. Both methods lead to
defining smooth objective functions which have as argument projection matrices and which
can be numerically optimized. Experimental results on large vocabulary continuous speech
recognition over the telephone show the superiority of the resulting features over their LDA
or cepstral counterparts.
References
[1] H. P. Decell and J. A. Quirein. An iterative approach to the feature selection problem.
Proc. Purdue Univ. Conf. on Machine Processing of Remotely Sensed Data, 3B1­
3B12, 1972.
[2] R. O. Duda and P. B. Hart. Pattern classification and scene analysis. Wiley, New York,
1973.
[3] K. Fukunaga. Introduction to statistical pattern recognition. Academic Press, New
York, 1990.
[4] R. Haeb­Umbach and H. Ney. Linear Discriminant Analysis for improved large vo­
cabulary continuous speech recognition. Proceedings of ICASSP'92, volume 1, pages
13--16, 1992.
[5] S. Kullback. Information theory and statistics. Wiley, New York, 1968.
[6] N. Kumar and A. G. Andreou. Heteroscedastic discriminant analysis and reduced
rank HMMs for improved speech recognition. Speech Communcation, 26:283--297,
1998.
[7] M. Padmanabhan, G. Saon, S. Basu, J. Huang and G. Zweig. Recent improvements
in voicemail transcription. Proceedings of EUROSPEECH'99, Budapest, Hungary,
1999.
[8] G. Saon, M. Padmanabhan, R. Gopinath and S. Chen. Maximum likelihood discrim­
inant feature spaces. Proceedings of ICASSP'2000, Istanbul, Turkey, 2000.
[9] S. R. Searle. Matrix algebra useful for statistics. Wiley Series in Probability and
Mathematical Statistics, New York, 1982.

