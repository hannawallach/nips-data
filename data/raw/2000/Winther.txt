Computing with Finite and Infinite Networks
Ole Winther \Lambda
Theoretical Physics, Lund University
S˜olvegatan 14 A, S­223 62 Lund, Sweden
winther@nimis.thep.lu.se
Abstract
Using statistical mechanics results, I calculate learning curves (average
generalization error) for Gaussian processes (GPs) and Bayesian neural
networks (NNs) used for regression. Applying the results to learning a
teacher defined by a two­layer network, I can directly compare GP and
Bayesian NN learning. I find that a GP in general requires O(d s )­training
examples to learn input features of order s (d is the input dimension),
whereas a NN can learn the task with order the number of adjustable
weights training examples. Since a GP can be considered as an infinite
NN, the results show that even in the Bayesian approach, it is important
to limit the complexity of the learning machine. The theoretical findings
are confirmed in simulations with analytical GP learning and a NN mean
field algorithm.
1 Introduction
Non­parametric kernel methods such as Gaussian Processes (GPs) and Support Vector Ma­
chines (SVMs) are closely related to neural networks (NNs). These may be considered as
single layer networks in a possible infinite dimensional feature space. Both the Bayesian
GP approach and SVMs regularize the learning problem so that only a finite number of the
features (dependent on the amount of data) is used.
Neal [1] has shown that Bayesian NNs converge to GPs in the limit of infinite number of
hidden units and furthermore argued that (1) there is no reason to believe that real­world
problem should require only a `small' number of hidden units and (2) there are in the
Bayesian approach no reasons (besides computational) to limit the size of the network.
Williams [2] has derived kernels allowing for efficient computation with both infinite feed­
forward and radial basis networks.
In this paper, I show that learning with a finite rather than infinite networks can make a
profound difference by studying the case where the task to be learned is defined by a large
but finite two­layer NN. A theoretical analysis of the Bayesian approach to learning this
task shows that the Bayesian student makes a learning transition from a linear model to
specialized non­linear one when the number of examples is of the order of the number of
adjustable weights in the network. This effect--which is also seen in the simulations--is a
consequence of the finite complexity of the network. In an infinite network, i.e. a GP on the
\Lambda http://www.thep.lu.se/tf2/staff/winther/

other hand such a transition will not occur. It will eventually learn the task but it requires
O(d s )­training examples to learn features of order s, where d is the input dimension.
Here, I focus entirely on regression. However, the basic conclusions regarding learning
with kernel methods and NNs turn out to be valid more generally, e.g. for classification
unpublished results and [3].
I consider the usual Bayesian setup of supervised learning: A training set DN =
f(x i ; y i )ji = 1 : : : ; Ng (x 2 R d and y 2 R) is known and the output for the new in­
put x is predicted by the function f(x) which is sampled from the prior distribution of
model outputs. I will consider both a Gaussian process prior and the prior implied by
a large (but finite) two­layer network. The output noise is taken to be Gaussian, so the
Likelihood becomes p(yjf(x)) = e \Gamma(y\Gammaf (x)) 2 =2 =
p
2ßoe 2 . The error measure is minus the
log­Likelihood and Bayes regressor (which minimizes the expected error) is the posterior
mean prediction
hf(x)i =
E f f(x)
Q
i p(y i jf(x i ))
E f
Q
i p(y i jf(x i ))
; (1)
where I have introduced E f , f = f(x 1 ); : : : ; f(xN ); f(x), to denote an average with re­
spect to the model output prior.
Gaussian processes. In this case, the model output prior is by definition Gaussian
p(f ) =
1
p
(2ß) N det C exp
`
\Gamma 1
2
f T C \Gamma1 f
'
; (2)
where C is the covariance matrix. The covariance matrix is computed from the kernel
(covariance function) C(x; x 0 ). Below I give an explicit example corresponding to an
infinite two­layer network.
Bayesian neural networks The output of the two­layer NN is given by f(x; w;W) =
1
p
K
P K
k W k \Phi(w k \Delta x), where an especially convenient choice of transfer function in what
follows is \Phi(z) =
R 1
\Gammaz dte \Gammat 2 =2 =
p
2ß. I consider a Bayesian framework (with fixed
known hyperparameters) with a weight prior that factorizes over hidden units p(w; W) =
Q
k [p(W k )p(w k )] and Gaussian input­to­hidden weights w k ¸ N (0; \Sigma).
From Bayesian NNs to GPs. The prior over outputs for the Bayesian neural network is
p(f ) =
R dwdWp(w;W)
Q
i ffi (f(x i ) \Gamma f(x i ; w;W)). In the infinite hidden unit limit,
K ! 1, when p(W k ) has zero mean and finite, say unit variance, it follows from the
central limit theorem (CLT) that the prior distribution converges to a Gaussian process
f ¸ N (0; C) with kernel [1, 2]
C(x; x 0 ) =
Z
dw p(w) \Phi(w \Delta x) \Phi(w \Delta x 0 )
=
2
ß
arcsin
x T \Sigmax 0
p
(1 + x T \Sigmax)(1 + x 0T \Sigmax 0 )
!
: (3)
The rest of the paper deals with theoretical statistical mechanics analysis and simulations
for GPs and Bayesian NNs learning tasks defined by either a NN or a GP. For the simula­
tions, I use analytical GP learning (scaling like O(N 3 )) [4] and a TAP mean field algorithm
for Bayesian NN.

2 Statistical mechanics of learning
The aim of the average case statistical mechanics analysis is to derive learning curves, i.e.
the expected generalization error as a function of the number of training examples. The
generalization error of the Bayes regressor hf(x)i eq. (1) is
ffl g = hh(y \Gamma hf(x)i) 2 ii ; (4)
where double brackets hh: : :ii =
R Q
i [dx i dy i p(x i ; y i )] : : : denote an average over both
training examples and the test example (x; y). Rather than using eq. (4) directly, ffl g will--as
usually done--be derived from the average of the free energy \Gammahhln Zii, where the partition
function is given by
Z = E f
1
p
2ßoe 2
N exp
/
\Gamma 1
2oe 2
X
i
(y i \Gamma f(x i )) 2
!
: (5)
I will not give many details of the actual calculations here since it is beyond the scope of
the paper, but only outline some of the basic assumptions.
2.1 Gaussian processes
The calculation for Gaussian processes is given in another NIPS contribution [5]. The basic
assumption made is that y \Gammaf (x) becomes Gaussian with zero mean 1 under an average over
the training example y \Gamma f(x) ¸ N (0; hh(y \Gamma f(x)) 2 ii). This assumption can be justified
by the CLT when f(x) is a sum of many random parts contributing on the same scale.
Corrections to the Gaussian assumption may also be calculated [5]. The free energy may
be written in term of a set of order parameters which is found by saddlepoint integration.
Assuming that the teacher is noisy y = f \Lambda (x) + j, hhj 2 ii = oe 2
\Lambda , the generalization error is
given by the following equation which depends upon an orderparameter š
ffl g =
oe 2
\Lambda + hhf 2
\Lambda (x)ii \Gamma @ š (š 2 “
E f hhf(x)f \Lambda (x)ii 2 )
1 + – 2 @ š “
E f hhf 2 (x)ii=N
(6)
š =
N
oe 2 + “
E f hhf 2 (x)ii
; (7)
where the new normalized measure “
E f : : : / E f exp
\Gamma
\Gammaš hhf 2 (x)ii=2
\Delta : : : has been intro­
duced.
Kernels in feature space. By performing a Karhunen­Loeve expansion, f(x) can be
written as a linear perceptron with weights ! ae in a possible infinite feature space
f(x) =
X
ae
! ae
p
– ae OE ae (x) ; (8)
where the features OE ae (x) are orthonormal eigenvectors of the covariance function with
eigenvalues – ae :
R dx p(x)C(x 0 ; x)OE ae (x) = – ae OE ae (x 0 ) and
R dx p(x)OE ae 0 (x)OE ae (x) = ffi aeae 0 .
The teacher f \Lambda (x) may also be expanded in terms of the the features:
f \Lambda (x) =
X
ae
a ae
p
– ae OE ae (x) ; a ae
p
– ae =
Z
dxp(x)f \Lambda (x)OE ae (x) :
Using the orthonormality the averages may be found: hhf 2 (x)ii =
P
ae – ae ! 2
ae ,
hhf(x)f \Lambda (x)ii =
P
ae – ae ! ae a ae and hhf 2
\Lambda (x)ii =
P
ae – ae a 2
ae . For a Gaussian process prior,
1 Generalization to non­zero mean is straightforward.

the prior over the weight is a spherical Gaussian ! ¸ N (0; I). Averaging over !, the sad­
dlepoint equations can be written in terms of the number of examples N , the noise levels
oe 2 and oe 2
\Lambda , the eigenvectors of the covariance function – ae and the teacher projections a ae :
ffl g =
N
š
/
oe 2
\Lambda +
X
ae
– ae a 2
ae
(1 + š– ae ) 2
!/
oe 2 +
X
ae
– ae
(1 + š– ae ) 2
! \Gamma1
(9)
š = N
/
oe 2 +
X
ae
– ae
1 + š– ae
! \Gamma1
(10)
These eqs. are valid for a fixed teacher. However, eq. (9) may also be averaged over the
distribution of teachers. In the Bayes optimal scenario, the teacher is sampled from the
same prior as the student and oe 2 = oe 2
\Lambda . Thus a ae ¸ N (0; I) implying a 2
ae = 1, where the
average over the teacher is denoted by an overline. In this case the equations reduce to the
Bayes optimal result first derived by Sollich [6]: ffl g = ffl Bayes
g = N=š.
Learning finite nets. Next, I consider the case where the teacher is the two­layer network
f \Lambda (x) = f(w; W) and the GP student uses the infinite net kernel eq. (3). The average
over the teacher corresponds to an average over the weight prior and since f \Lambda (x)f \Lambda (x 0 ) =
C(x; x 0 ), I get
a 2
ae – ae =
Z
dxdx 0 p(x)p(x 0 )C(x; x 0 )OE ae (x)OE ae (x 0 ) = – ae ; (11)
where the eigenvalue equation and the orthonormality have been used. The theory therefore
predicts that a GP student (with the infinite network kernel) will have the same learning
curve irrespectively of the number of hidden units of the NN teacher. This result is a direct
consequence of the Gaussian assumption made for the average over examples. However,
what is more surprising is that it is found to be a very good approximation in simulations
down to K = 1, i.e. a simple perceptron with a sigmoid non­linearity.
Inner product kernels. I specialize to inner product kernels C(x; x 0 ) = c(x \Delta x 0 =d)
and consider large input dimensionality d and input components which are iid with
zero mean and unit variance. The eigenvectors are products of the input components
OE ae (x) =
Q
m2ae xm and are indexed by subsets of input indices, e.g. ae = f1; 2; 42g [3].
The eigenvalues are – ae = c jaej (0)
d jaej
with degeneracy n jaej =
i
d
jaej
j
ß d jaej =jaej!, where jaej is
the cardinality (in the example above jaej = 3). Plugging these results into eqs. (9) and (10),
it follows that to learn features that are order s in the inputs, O(d s ) examples are needed.
The same behavior has been predicted for learning in SVMs [3].
The infinite net eq. (3) reduces to an inner product covariance function for \Sigma = T I=d (T
controls the degree on non­linearity of the rule) and large d, x \Delta x ß d:
C(x; x 0 ) = c(x \Delta x 0 =d) =
2
ß
arcsin
` Tx \Delta x 0
d(1 + T )
'
: (12)
Figure 1 shows learning curves for GPs for the infinite network kernel. The mismatch
between theory and simulations is expected to be due to O(1=d)­corrections to the eigen­
values – ae . The figure clearly shows that learning of the different order features takes place
on different scales. The stars on the ffl g ­axis show the theoretical prediction of asymptotic
error for N = O(d); O(d 3 ); : : : (the teacher is an odd function).
2.2 Bayesian neural networks
The limit of large but finite NNs allows for efficient computation since the prior over
functions can be approximated by a Gaussian. The hidden­to­output weights are for sim­

0 500 1000 1500 2000
0
0.05
0.1
0.15
0 20 40 60 80 100
0
0.2
0.4
0.6
ffl g
N N
ffl g
Small N = O(d) Large N = O(d 3 )
Figure 1: Learning curve for Gaussian processes with the infinite network kernel (d = 10,
T = 10 and oe 2 = 0:01) for two scales of training examples. The full line is the the
theoretical prediction for the Bayes optimal GP scenario. The two other curves (almost on
top of each other as predicted by theory) are simulations for the Bayes optimal scenario
(dotted line) and for GP learning a neural network with K = 30 hidden units (dash­dotted
line).
plicity set to one and we introduce the `fields' h k (x) = w k \Delta x and write the output as
f(x; w) = f(h(x)) = 1
p
K
P K
k \Phi(h k (x)), h(x) = h 1 (x); : : : ; hK (x). In the following, I
discuss the TAP mean field algorithm used to find an approximation to the Bayes regressor
and briefly the theoretical statistical mechanics analysis for the NN task.
Mean field algorithm. The derivation sketched here is a straightforward generalization
of previous results for neural networks [7]. The basic cavity assumption [7, 8] is that for
large d; K and for a suitable input distribution, the predictive distribution p(f(x)jDN ) is
Gaussian:
p(f(x)jDN ) ß N (hf(x)i; hf 2 (x)i \Gamma hf(x)i 2 ) :
The predictive distribution for the fields h(x) is also assumed to be Gaussian
p(h(x)jDN ) ß N (hh(x)i; V) ;
where V = hh(x)h(x) T i \Gamma hh(x)ihh(x) T i. Using these assumptions, I get an approxi­
mate Bayes regressor
hf(x)i ß 1
p
K
X
k
\Phi
` hh k (x)i
p
1 + V kk
'
: (13)
To make predictions, we therefore need the two first moments of the weights since
hh k (x)i = hw k i \Delta x and V kl =
P
mn xmxn (hwmk w nl i \Gamma hwmk ihw nl i). We can simplify
this in the large d limit by taking the inputs to by iid with zero mean and unit variance:
V kl ß hw k \Delta w l i \Gamma hw k i \Delta hw l i. This approximation can be avoided at a substantial com­
putational cost [8]. Furthermore, hw k \Delta w l i turns out equal to the prior covariance ffi kl T=d
[7]. The following exact relation is obtained for the mean weights
hw k i =
X
i
ff ki x i ; ff ki =
@
@hh k (x i )i
ln p(y i jDN n(x i ; y i )) (14)
where
p(y i jDN n(x i ; y i )) =
Z
dh(x i ) p(y i jh(x i )) p(h(x i )jDN n(x i ; y i )) :

0 2 4 6 8 10
0
0.01
0.02
0.03
0.04
0.05
N
dK
ffl g
Figure 2: . Learning curves for Bayesian NNs and GPs. The dashed line is simulations
for the TAP mean field algorithm (d = 30, K = 5, T = 1 and oe 2 = 0:01) learning a
corresponding NN task, i.e. an approximation to the Bayes optimal scenario. The dash­
dotted line is the simulations for GPs learning the NN task. Virtually on top of that curve
is the curve for Bayes optimal GP scenario (dotted line). The full lines are the theoretical
prediction. Up to N = N c = 2:51dK , the learning curves for Bayesian NNs and GPs coin­
cide. At N c , the statistical mechanics theory predicts a first order transition to a specialized
solution for the NN Bayes optimal scenario (lower full line).
p(y i jh(x i )) is the Likelihood and p(h(x i )jDN n(x i ; y i )) is a predictive distribution for
h(x i ) for a training set where the ith example has been left out. In accordance with above,
I assume p(h(x i )jDN n(x i ; y i )) ß N (hh(x i )i ni ; V). Finally, generalizing the relation
found in Refs. [7, 8], I can relate the reduced mean to the full posterior mean:
hh k (x i )i ni = hh k (x i )i \Gamma
X
l
V kl ff li
to express everything in terms of hw k i and ff ki , k = 1; : : : ; K and i = 1; : : : ; N .
The mean field eqs. are solved by iteration in ff ki and hwmk i following the recipe given in
Ref. [8]. The algorithm is tested using a teacher sampled from the NN prior, i.e. the Bayes
optimal scenario. Two types of solutions are found: a linear symmetric and a non­linear
specialized. In the symmetric solution, hw k i = hw l i and hw k i \Delta hw k i = O(T=dK). This
means that the machine is linear (when T ø K). For N = O(dK), a transition to a
specialized solution occurs, where each hw k i, k = 1; : : : ; K, aligns to a distinct weight
vector of the teacher and hw k i \Delta hw k i = O(T=d). The Bayesian student thus learns the
linear features for N = O(d). However, unlike the GP, it learns all of the remaining non­
linear features for N = O(dK). The resulting empirical learning curve averaged over 25
independent runs is shown in figure 2. It turned out that setting hh k (x i )i ni = hh k (x i )i
was a necessary heuristic in order to find the specialized solution. The transition to the
specialized solution­although very abrupt for the individual run--is smeared out because it
occurs at different N for each run.
The theoretical learning curve is also shown in figure 2. It has been derived by gener­
alizing the results of Ref. [9] for the Gibbs algorithm to the Bayes optimal scenario. The
picture that emerges is in accordance with the empirical findings. The transition to the
specialized solution is predicted to be first order, i.e. with a discontinuous jump in the rele­
vant order parameters at the number of examples N c (oe 2 ; T ), where the specialized solution
becomes the physical solution (i.e. the lowest free energy solution).
The mean field algorithm cannot completely reproduce the theoretical predictions because
the solution gets trapped in the meta­stable symmetric solution. This is often observed

for first order transitions and should also be observable in the Monte Carlo approach to
Bayesian NNs [1].
3 Discussion
Learning a finite two­layer regression NN using (1) the Bayes optimal algorithm and (2)
the Bayes optimal algorithm for an infinite network (implemented by a GP) is compared.
It is found that the Bayes optimal algorithm can have a very superior performance.
This can be explained as an entropic effect: The infinite network will--although the cor­
rect finite network solution is included a priori-- have a vanishing probability of finding
this solution. The finite network on the other hand is much more constraint wrt the func­
tions it implements. It can thus--even in the Bayesian setting--give a great pay off to limit
complexity.
For d­dimensional inner product kernel with iid input distribution, it is found that it in
general requires O(d s ) training examples to learn features of O(s). Unpublished results
and [3] show that these conclusions remain true also for SVM and GP classification.
For SVM hand­written digit recognition, fourth order kernels give good results in prac­
tise. Since N = O(10 4 ) \Gamma O(10 5 ), it can be concluded that the `effective' dimension,
d effective = O(10) against typically d = 400, i.e. some inputs must be very correlated
and/or carry very little information. It could therefore be interesting to develop methods
to measure the effective dimension and to extract the important lower dimensional features
rather than performing the classification directly from the images.
Acknowledgments
I am thankful to Manfred Opper for valuable discussions and for sharing his results with
me and to Klaus­Robert M˜uller for discussions at NIPS. This research is supported by the
Swedish Foundation for Strategic Research.
References
[1] R. Neal, Bayesian Learning for Neural Networks, Lecture Notes in Statistics, Springer (1996).
[2] C. K. I. Williams, Computing with Infinite Networks, in Neural Information ProcessingSystems
9, Eds. M. C. Mozer, M. I. Jordan and T. Petsche, 295­301, MIT Press (1997).
[3] R. Dietrich, M. Opper and H. Sompolinsky, Statistical Mechanics of Support Vector Machines,
Phys. Rev. Lett. 82, 2975­2978 (1999).
[4] C. K. I. Williams and C. E. Rasmussen, Gaussian Processes for Regression , In Advances in
Neural Information Processing Systems 8 (NIPS'95). Eds. D. S. Touretzky, M. C. Mozer and
M. E. Hasselmo, 514­520, MIT Press (1996).
[5] D. Malzahn and M. Opper, In this volume.
[6] P. Sollich, Learning Curves for Gaussian Processes, In Advances in Neural Information Pro­
cessing Systems 11 (NIPS'98), Eds. M. S. Kearns, S. A. Solla, and D. A. Cohn, 344­350, MIT
Press (1999).
[7] M. Opper and O. Winther, Mean Field Approach to Bayes Learning in Feed­Forward Neural
Networks, Phys. Rev. Lett. 76, 1964­1967 (1996).
[8] M. Opper and O. Winther, Gaussian Processes for Classification: Mean Field Algorithms, Neu­
ral Computation 12, 2655­2684 (2000).
[9] M. Ahr, M. Biehl and R. Urbanczik, Statistical physics and practical training of soft­committee
machines Eur. Phys. J. B 10, 583 (1999).

