Sparse Greedy
Gaussian Process Regression
Alex J. Smola 
RSISE and Department of Engineering
Australian National University
Canberra, ACT, 0200
Alex.Smola@anu.edu.au
Peter Bartlett
RSISE
Australian National University
Canberra, ACT, 0200
Peter.Bartlett@anu.edu.au
Abstract
We present a simple sparse greedy technique to approximate the
maximum a posteriori estimate of Gaussian Processes with much
improved scaling behaviour in the sample size m. In particular,
computational requirements are O(n 2 m), storage is O(nm), the
cost for prediction is O(n) and the cost to compute condence
bounds is O(nm), where n  m. We show how to compute a
stopping criterion, give bounds on the approximation error, and
show applications to large scale problems.
1 Introduction
Gaussian processes have become popular because they allow exact Bayesian analysis
with simple matrix manipulations, yet provide good performance. They share with
Support Vector machines and Regularization Networks the concept of regularization
via Reproducing Kernel Hilbert spaces [3], that is, they allow the direct specication
of the smoothness properties of the class of functions under consideration. However,
Gaussian processes are not always the method of choice for large datasets, since they
involve evaluations of the covariance function at m points (where m denotes the
sample size) in order to carry out inference at a single additional point. This may
be rather costly to implement | practitioners prefer to use only a small number of
basis functions (i.e. covariance function evaluations).
Furthermore, the Maximum a Posteriori (MAP) estimate requires computation,
storage, and inversion of the full m  m covariance matrix K ij = k(x i ; x j ) where
x 1 ; : : : ; xm are training patterns. While there exist techniques [2, 8] to reduce the
computational cost of nding an estimate to O(km 2 ) rather than O(m 3 ) when
the covariance matrix contains a signicant number of small eigenvalues, all these
methods still require computation and storage of the full covariance matrix. None
of these methods addresses the problem of speeding up the prediction stage (except
for the rare case when the integral operator corresponding to the kernel can be
diagonalized analytically [8]).
We devise a sparse greedy method, similar to those proposed in the context of
wavelets [4], solutions of linear systems [5] or matrix approximation [7] that nds
 Supported by the DFG (Sm 62-1) and the Australian Research Council.

an approximation of the MAP estimate by expanding it in terms of a small subset
of kernel functions k(x i ; ). Briey, the technique works as follows: given a set of
(already chosen) kernel functions, we seek the additional function that increases
the posterior probability most. We add it to the set of basis functions and repeat
until the maximum is approximated su∆ciently well. A similar approach for a tight
upper bound on the posterior probability gives a stopping criterion.
2 Gaussian Process Regression
Consider a nite set X = fx 1 ; : : : xm g of inputs. In Gaussian Process Regression,
we assume that for any such set there is a covariance matrix K with elements
K ij = k(x i ; x j ). We assume that for each input x there is a corresponding output
y(x), and that these outputs are generated by
y(x) = t(x) +  (1)
where t(x) and  are both normal random variables, with   N(0;  2 ) and
t = (t(x 1 ); : : : ; t(x m )) >  N(0; K). We can use Bayes theorem to determine the
distribution of the output y(x) at a (new) input x. Conditioned on the data (X; y),
the output y(x) is normally distributed. It follows that the mean of this distribution
is the maximum a posteriori probability (MAP) estimate of y. We are interested in
estimating this mean, and also the variance.
It is possible to give an equivalent parametric representation of y that is more con-
venient for our purposes. We may assume that the vector y = (y(x 1 ); : : : ; y(x m )) >
of outputs is generated by
y = K+ ; (2)
where   N(0; K 1 ) and   N(0;  2 1). Consequently the posterior probability
p(jy; X) over the latent variables  is proportional to
exp 1
2 2 ky Kk 2 
exp 1
2  > K

(3)
and the conditional expectation of y(x) for a (new) location x is E[y(x)jy; X] =
k >  opt , where k > denotes the vector (k(x 1 ; x); : : : ; k(xm ; x)) and  opt is the value
of  that maximizes (3). Thus, it su∆ces to compute  opt before any predictions
are required. The problem of choosing the MAP estimate of  is equivalent to the
problem of minimizing the negative log-posterior,
minimize
2R m

y > K+ 1
2  >  2 K +K > K 
 
(4)
(ignoring constant terms and rescaling by  2 ). It is easy to show that the mean of
the conditional distribution of y(x) is k > (K + 2 1) 1 y, and its variance is k(x; x)+
 2 k > (K +  2 1) 1 k (see, for example, [2]).
3 Approximate Minimization of Quadratic Forms
For Gaussian process regression, searching for an approximate solution to (4) relies
on the assumption that a set of variables whose posterior probability is close to that
of the mode of the distribution will be a good approximation for the MAP estimate.
The following theorem suggests a simple approach to estimating the accuracy of an
approximate solution to (4). It uses an idea from [2] in a modied, slightly more
general form.
Theorem 1 (Approximation Bounds for Quadratic Forms) Denote by K 2
R
mm a positive semidenite matrix, y;  2 R
m and dene the two quadratic forms
Q() := y > K+
1
2  > ( 2 K +K > K); (5)

Q  () := y >  + 1
2  > ( 2 1 +K): (6)
Suppose Q and Q  have minima Q min and Q 
min . Then for all ;   2 R
m we have
Q()  Q min  1
2 kyk 2  2 Q  (  ); (7)
Q  (  )  Q 
min   2
 1
2 kyk 2 Q()

; (8)
with equalities throughout when Q() = Q min and Q  (  ) = Q 
min .
Hence, by minimizing Q  in addition to Q we can bound Q's closeness to the
optimum and vice versa.
Proof The minimum of Q() is obtained for  opt = (K +  2 1) 1 y (which also
minimizes Q  ), hence
Q min = 1
2 y > K(K +  2 1) 1 y and Q 
min = 1
2 y > (K +  2 1) 1 y: (9)
This allows us to combine Q min and Q 
min to Q min +  2 Q 
min = 1
2 kyk 2 . Since by
denition Q()  Q min for all  (and likewise Q  (  )  Q 
min for all   ) we may
solve Q min +  2 Q 
min for either Q or Q  to obtain lower bounds for each of the two
quantities. This proves (7) and (8).
Equation (7) is useful for computing an approximation to the MAP solution,
whereas (8) can be used to obtain error bars on the estimate. To see this, note that
in calculating the variance, the expensive quantity to compute is k > (K+ 2 1) 1 k.
However, this can be found by solving
minimize
2R m

k >  + 1
2  >  2 1 +K



; (10)
and the expression inside the parentheses is Q  () with y = k (see (6)). Hence, an
approximate minimizer of (10) gives an upper bound on the error bars, and lower
bounds can be obtained from (8).
In practice we will use the quantiy gap(;   ) := 2(Q()+ 2 Q  (  )+ 1
2
kyk 2 )
Q()+ 2 Q  (  )+ 1
2 kyk 2
, i.e. the
relative size of the dierence between upper and lower bound as stopping criterion.
4 A Sparse Greedy Algorithm
The central idea is that in order to obtain a faster algorithm, one has to reduce
the number of free variables. Denote by P 2 R mn with m  n and m;n 2 N an
extension matrix (i.e. P > is a projection) with P > P = 1. We will make the ansatz
P := P where  2 R
n (11)
and nd solutions  such that Q(P ) (or Q  (P )) is minimized. The solution is
 opt = P >  2 K +K > K

P
 1
P > K > y: (12)
Clearly if P is of rank m, this will also be the solution of (4) (the minimum negative
log posterior for all  2 R m ). In all other cases, however, it is an approximation.
Computational Cost of Greedy Decompositions
For a given P 2 R
mn let us analyze the computational cost involved in the esti-
mation procedures. To compute (12) we need to evaluate P > Ky which is O(nm),
(KP ) > (KP ) which is O(n 2 m) and invert an nn matrix, which is O(n 3 ). Hence the
total cost is O(n 2 m). Predictions then cost only k >  which is O(n). Using P also
to minimize Q  (P  ) costs no more than O(n 3 ), which is needed to upper-bound
the log posterior.

For error bars, we have to approximately minimize (10) which can done for  = P
at O(n 3 ) cost. If we compute (PKP > ) 1 beforehand, this can be done by at O(n 2 )
and likewise for upper bounds. We have to minimize k > KP + 1
2  > P > ( 2 K +
K > K)P which costs O(n 2 m) (once the inverse matrices have been computed,
one may, however, use them to compute error bars at dierent locations, too, thus
costing only O(n 2 )). The lower bounds on the error bars may not be so crucial,
since a bad estimate will only lead to overly conservative condence intervals and
not have any other negative eect. Finally note that all we ever have to compute
and store is KP , i.e. the m  n submatrix of K rather than K itself. Table 1
summarizes the scaling behaviour of several optimization algorithms.
Exact Conjugate Optimal Sparse Sparse Greedy
Solution Gradient [2] Decomposition Approximation
Memory O(m 2 ) O(m 2 ) O(nm) O(nm)
Initialization O(m 3 ) O(nm 2 ) O(n 2 m) O(n 2 m)
Pred. Mean O(m) O(m) O(n) O(n)
Error Bars O(m 2 ) O(nm 2 ) O(n 2 m) or O(n 2 ) O(n 2 m) or O(n 2 )
Table 1: Computational Cost of Optimization Methods. Note that n  m and also
note that the n used in Conjugate Gradient, Sparse Decomposition, and Sparse
Greedy Approximation methods will dier, with nCG  nSD  nSGA since the
search spaces are more restricted.  = 60 gives near-optimal results.
Sparse Greedy Approximation
Several choices for P are possible, including choosing the principal components
of K [8], using conjugate gradient descent to minimize Q [2], symmetric Cholesky
factorization [1], or using a sparse greedy approximation of K [7]. Yet these methods
have the disadvantage that they either do not take the specic form of y into account
[8, 7] or lead to expansions that cost O(m) for prediction and require computation
and storage of the full matrix [8, 2].
If we require a sparse expansion of y(x) in terms of k(x i ; x) (i.e. many  i in y = k > 
will be 0) we must consider matrices P that are a collection of unit vectors e i (here
(e i ) j = ∆ ij ). We use a greedy approach to nd a good approximation. First, for
n = 1, we choose P = e i such that Q(P) is minimal. In this case we could
permit ourselves to consider all possible indices i 2 f1; : : : mg and nd the best one
by trying out all of them. Next assume that we have found a good solution P
where P contains n columns. In order to improve this solution, we may expand
P into the matrix P new := [P old ; e i ] 2 R m(n+1) and seek the best e i such that
P new minimizes min  Q(P new ). (Performing a full search over all possible n + 1
out of m indices would be too costly.) This greedy approach to nding a sparse
approximate solution is described in Algorithm 1. The algorithm also maintains an
approximate minimum of Q  , and exploits the bounds of Theorem 1 to determine
when the approximation is su∆ciently accurate. (Note that we leave unspecied
how the subsets M  I; M   I  are chosen. Assume for now that we choose
M = I; M  = I  , the full set of indices that have not yet been selected.) This
method is very similar to Matching Pursuit [4] or iterative reduced set Support
Vector algorithms [6], with the dierence that the target to be approximated (the
full solution ) is only given implicitly via Q().
Approximation Quality
Natarajan [5] studies the following Sparse Linear Approximation problem: Given
A 2 R
mn , b 2 R
m ,  > 0, nd x 2 R
n with minimal number of nonzero entries
such that kAx bk 2  .

If we dene A := ( 2 K + K > K) 1
2 and b := A 1 Ky, then we may write Q() =
1
2 kb Ak 2 + c where c is a constant independent of . Thus the problem of
sparse approximate minimization of Q() is a special case of Natarajan's problem
(where the matrix A is square, symmetric, and positive denite). In addition, the
algorithm considered by Natarajan in [5] involves sequentially choosing columns of
A to maximally decrease kAx bk. This is clearly equivalent to the sparse greedy
algorithm described above. Hence, it is straightforward to obtain the following
result from Theorem 2 in [5].
Theorem 2 (Approximation Rate) Algorithm 1 achieves Q()  Q( opt ) + 
when  has
n  18n  (=4)
 2
1
ln

kA 1 Kyk


non-zero components, where n  (=4) is the minimal number of nonzero components
in vectors  for which Q()  Q( opt ) + =4, A = ( 2 K + K > K) 1=2 , and  1 is
the minimum of the magnitudes of the singular values of A, the matrix obtained by
normalizing the columns of A.
Randomized Algorithms for Subset Selection
Unfortunately, the approximation algorithm considered above is still too expensive
for large m since each search operation involves 
 m) indices. Yet, if we are satised
with nding a relatively good index rather than the best, we may resort to selecting
a random subset of size   m. In Algorithm 1, this corresponds to choosing
M  I; M   I  as random subsets of size . In fact, a constant value of  will
typically su∆ce. To see why, we recall a simple lemma from [7]: the cumulative
distribution function of the maximum of m i.i.d. random variables  1 ; : : : ;  m is
F () m , where F () is the cdf of  i . Thus, in order to nd a column to add to P
that is with probability 0:95 among the best 0:05 of all such columns, a random
subsample of size dlog 0:05= log 0:95e = 59 will su∆ce.
Algorithm 1 Sparse Greedy Quadratic Minimization.
Require: Training data X = fx 1 ; : : : ; xm g, Targets y, Noise  2 , Precision 
Initialize index sets I; I  = f1; : : : ; mg; S; S  = ;.
repeat
Choose M  I, M   I  .
Find arg min i2M Q ([P; e i ] opt ), arg min i  2M  Q  [P  ; e i  ] 
opt

.
Move i from I to S, i  from I  to S  .
Set P := [P; e i ], P  := [P  ; e i  ].
until Q(P opt ) + 2 Q  (P 
opt ) + 1
2 kyk 2  
2 (jQ(P opt )j + j 2 Q  (P 
opt ) + 1
2 kyk 2 j
Output: Set of indices S,  opt , (P > KP ) 1 , and (P > (K > K +  2 K)P ) 1 .
Numerical Considerations
The crucial part is to obtain the values of Q(P opt ) cheaply (with P = [P old ; e i ]),
provided we solved the problem for P old . From (12) one can see that all that needs
to be done is a rank-1 update on the inverse. In the following we will show that this
can be obtained in O(mn) operations, provided the inverse of the smaller subsystem
is known. Expressing the relevant terms using P old and k i we obtain
P > K > y = [P old ; e i ] > K > y = (P >
old K > y; k >
i y)
P > K > K +  2 K

P =
"
P >
old K > K +  2 K

P old P >
old K > +  2 1

k i
k >
i (K +  2 1)P old k >
i k i +  2 K ii
#

Thus computation of the terms costs only O(nm), given the values for P old . Fur-
thermore, it is easy to verify that we can write the inverse of a symmetric positive
semidenite matrix as

A B
B > C
 1
=

A 1 + (A 1 B) > (A 1 B) (A 1 B)
((A 1 B)) > 

(13)
where  := (C +B > A 1 B) 1 . Hence, inversion of P > K > K +  2 K

P costs only
O(n 2 ). Thus, to nd P of size m  n takes O(n 2 m) time. For the error bars,
(P > KP ) 1 will generally be a good starting value for the minimization of (10),
so the typical cost for (10) will be O(mn) for some  < n, rather than O(mn 2 ).
Finally, for added numerical stability one may want to use an incremental Cholesky
factorization in (13) instead of the inverse of a matrix.
5 Experiments and Discussion
We used the Abalone dataset from the UCI Repository to investigate the properties
of the algorithm. The dataset is of size 4177, split into 4000 training and 177 testing
split to analyze the numerical performance, and a (3000; 1177) split to assess the
generalization error (the latter was needed in order to be able to invert (and keep in
memory) the full matrix K + 2 1 for a comparison). The data was rescaled to zero
mean and unit variance coordinate-wise. Finally, the gender encoding in Abalone
(male/female/infant) was mapped into f(1; 0; 0); (0; 1; 0); (0; 0; 1)g.
In all our experiments we used Gaussian kernels k(x; x 0 ) = exp( kx x 0 k 2
2! 2
) as co-
variance kernels. Figure 1 analyzes the speed of convergence for dierent .
0 20 40 60 80 100 120 140 160 180 200
10 -2
10 -1
Number of Iterations
Gap
Figure 1: Speed of Convergence.
We plot the size of the gap be-
tween upper and lower bound of the
log posterior (gap(;   )) for the
rst 4000 samples from the Abalone
dataset ( 2 = 0:1 and 2! 2 = 10).
From top to bottom: subsets of size
1, 2, 5, 10, 20, 50, 100, 200. The
results were averaged over 10 runs.
The relative variance of the gap size
was less than 10%.
One can see that that subsets of size
50 and above ensure rapid conver-
gence.
For the optimal parameters (2 2 = 0:1 and 2! 2 = 10, chosen after [7]) the average
test error of the sparse greedy approximation trained until gap(;   ) < 0:025 on a
(3000; 1177) split (the results were averaged over ten independent choices of training
sets.) was 1:785  0:32, slightly worse than for the GP estimate (1:782  0:33).
The log posterior was 1:572  10 5 (1  0:005), the optimal value 1:571  10 5 (1 
0:005). Hence for all practical purposes full inversion of the covariance matrix and
the sparse greedy approximation have statistically indistinguishable generalization
performance.
In a third experiment (Table 2) we analyzed the number of basis functions needed
to minimize the log posterior to gap(;   ) < 0:025, depending on dierent choices
of the kernel width . In all cases, less than 10% of the kernel functions su∆ce to

nd a good minimizer of the log posterior, for the error bars, even less than 2% are
su∆cient. This is a dramatic improvement over previous techniques.
Kernel width 2! 2 1 2 5 10 20 50
Kernels for log-posterior 373 287 255 257 251 270
Kernels for error bars 7961 4943 2627 1716 129 85
Table 2: Number of basis functions needed to minimize the log posterior on the
Abalone dataset (4000 training samples), depending on the width of the kernel !.
Also, number of basis functions required to approximate k > (K +  2 1) 1 k which is
needed to compute the error bars. We averaged over the remaining 177 test samples.
To ensure that our results were not dataset specic and that the algorithm scales well
we tested it on a larger synthetic dataset of size 10000 in 20 dimensions distributed
according to N(0; 1). The data was generated by adding normal noise with variance
 2 = 0:1 to a function consisting of 200 randomly chosen Gaussians of width 2! 2 =
40 and normally distributed coe∆cients and centers.
We purposely chose an inadequate Gaussian process prior (but correct noise level)
of Gaussians with width 2! 2 = 10 in order to avoid trivial sparse expansions. After
500 iterations (i.e. after using 5% of all basis functions) the size of the gap(;   )
was less than 0:023 (note that this problem is too large to be solved exactly).
We believe that sparse greedy approximation methods are a key technique to scale
up Gaussian Process regression to sample sizes of 10.000 and beyond. The tech-
niques presented in the paper, however, are by no means limited to regression.
Work on the solutions of dense quadratic programs and classication problems is in
progress. The authors thank Bob Williamson and Bernhard Scholkopf.
References
[1] S. Fine and K. Scheinberg. E∆cient SVM training using low-rank kernel representation.
Technical report, IBM Watson Research Center, New York, 2000.
[2] M. Gibbs and D.J.C. Mackay. E∆cient implementation of gaussian processes. Technical
report, Cavendish Laboratory, Cambridge, UK, 1997.
[3] F. Girosi. An equivalence between sparse approximation and support vector machines.
Neural Computation, 10(6):1455{1480, 1998.
[4] S. Mallat and Z. Zhang. Matching Pursuit in a time-frequency dictionary. IEEE
Transactions on Signal Processing, 41:3397{3415, 1993.
[5] B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal of
Computing, 25(2):227{234, 1995.
[6] B. Scholkopf, S. Mika, C. Burges, P. Knirsch, K.-R. Muller, G. Ratsch, and A. Smola.
Input space vs. feature space in kernel-based methods. IEEE Transactions on Neural
Networks, 10(5):1000 { 1017, 1999.
[7] A.J. Smola and B. Scholkopf. Sparse greedy matrix approximation for machine learn-
ing. In P. Langley, editor, Proceedings of the 17th International Conference on Machine
Learning, pages 911 { 918, San Francisco, 2000. Morgan Kaufman.
[8] C.K.I. Williams and M. Seeger. The eect of the input density distribution on kernel-
based classiers. In P. Langley, editor, Proceedings of the Seventeenth International
Conference on Machine Learning, pages 1159 { 1166, San Francisco, California, 2000.
Morgan Kaufmann.

