Constrained Independent Component
Analysis
Wei Lu and Jagath C. Rajapakse
School of Computer Engineering
Nanyang Technological University, Singapore 639798
email:asjagath@ntu.edu.sg
Abstract
The paper presents a novel technique of constrained independent
component analysis (CICA) to introduce constraints into the clas-
sical ICA and solve the constrained optimization problem by using
Lagrange multiplier methods. This paper shows that CICA can
be used to order the resulted independent components in a specic
manner and normalize the demixing matrix in the signal separation
procedure. It can systematically eliminate the ICA's indeterminacy
on permutation and dilation. The experiments demonstrate the use
of CICA in ordering of independent components while providing
normalized demixing processes.
Keywords: Independent component analysis, constrained indepen-
dent component analysis, constrained optimization, Lagrange mul-
tiplier methods
1 Introduction
Independent component analysis (ICA) is a technique to transform a multivari-
ate random signal into a signal with components that are mutually independent
in complete statistical sense [1]. There has been a growing interest in research for
eÆcient realization of ICA neural networks (ICNNs). These neural algorithms pro-
vide adaptive solutions to satisfy independent conditions after the convergence of
learning [2, 3, 4].
However, ICA only denes the directions of independent components. The magni-
tudes of independent components and the norms of demixing matrix may still be
varied. Also the order of the resulted components is arbitrary. In general, ICA has
such an inherent indeterminacy on dilation and permutation. Such indetermina-
tion cannot be reduced further without additional assumptions and constraints [5].
Therefore, constrained independent component analysis (CICA) is proposed as a
way to provide a unique ICA solution with certain characteristics on the output by
introducing constraints:
 To avoid the arbitrary ordering on output components: statistical measures
give indices to sort them in order, and evenly highlight the salient signals.

 To produce unity transform operators: normalization of the demixing chan-
nels reduces dilation eect on resulted components. It may recover the exact
original sources.
With such conditions applied, the ICA problem becomes a constrained optimization
problem. In the present paper, Lagrange multiplier methods are adopted to provide
an adaptive solution to this problem. It can be well implemented as an iterative
updating system of neural networks, referred to ICNNs. Next section briey gives an
introduction to the problem, analysis and solution of Lagrange multiplier methods.
Then the basic concept of ICA will be stated. And Lagrange multiplier methods are
utilized to develop a systematic approach to CICA. Simulations are performed to
demonstrate the usefulness of the analytical results and indicate the improvements
due to the constraints.
2 Lagrange Multiplier Methods
Lagrange multiplier methods introduce Lagrange multipliers to resolve a constrained
optimization iteratively. A penalty parameter is also introduced to t the condition
so that the local convexity assumption holds at the solution. Lagrange multiplier
methods can handle problems with both equality and inequality constraints.
The constrained nonlinear optimization problems that Lagrange multiplier methods
deal take the following general form:
minimize f(X); subject to g(X)  0; h(X) = 0 (1)
where X is a matrix or a vector of the problem arguments, f(X) is an objective
function, g(X) = [g 1 (X)    gm (X)] T denes a set of m inequality constraints and
h(X) = [h 1 (X)    hn (X)] T denes a set of n equality constraints. Because La-
grangian methods cannot directly deal with inequality constraints g i (X)  0, it
is possible to transform inequality constraints into equality constraints by intro-
ducing a vector of slack variables z = [z 1    z m ] T to result in equality constraints
p i (X) = g i (X) + z 2
i = 0, i = 1    m.
Based on the transformation, the corresponding simplied augmented Lagrangian
function for problem (1) is dened as:
L(X; ; ) = f(X)+ 1
2
m
X
i=1
f[maxf0;  g i (X)g] 2  2
i g + T h(X)+ 1
2  jjh(X)jj 2 (2)
where  = [ 1    m ] T and  = [ 1    n ] T are two sets of Lagrange multipliers, 
is the scalar penalty parameter,  g i (X) equals to  i +g i (X), jj  jj denotes Euclidean
norm, and 1
2 jj  jj 2 is the penalty term to ensure that the optimization problem
is held at the condition of local convexity assumption: r 2
XX L > 0. We use the
augmented Lagrangian function in this paper because it gives wider applicability
and provides better stability [6].
For discrete problems, the changes in the augmented Lagrangian function can be
dened as XL(X; ; ) to achieve the saddle point in the discrete variable space.
The iterative equations to solve the problem in eq.(2) are given as follows:
X(k + 1) = X(k) XL(X(k);(k); (k))
(k + 1) = (k) +  p(X(k)) = maxf0; 
g(X(k))g (3)
(k + 1) = (k) +  h(X(k))
where k denotes the iterative index and  g(X(k)) = (k) +  g(X(k)).

3 Unconstrained ICA
Let the time varying input signal be x = (x 1 ; x 2 ; : : : ; xN ) T and the interested signal
consisting of independent components (ICs) be c = (c 1 ; c 2 ; : : : ; c M ) T , and gener-
ally M  N . The signal x is considered to be a linear mixture of independent
components c: x = Ac; where A is an NM mixing matrix with full column rank.
The goal of general ICA is to obtain a linear M N demixing matrix W to recover
the independent components c with a minimal knowledge of A and c, normally
M = N . Then, the recovered components u are given by u = Wx:
In the present paper, the contrast function used is the mutual information (M) of
the output signal which is dened in the sense of variable's entropy to measure the
independence:
M(W) =
XM
i=1
H(u i ) H(u) (4)
where H(u i ) is the marginal entropy of component u i and H(u) is the output
joint entropy. M has non-negative value and equals to zero when components are
completely independent.
While minimizing M, the learning equation for demixing matrix W to perform ICA
is given by [1]:
W / W T +(u)x T (5)
where W T is the transpose of the inverse matrix W 1 and (u) is a nonlinear
function depending on the activation functions of neurons or p.d.f. of sources [1].
With above assumptions, the exact components c are indeterminant because of
possible dilation and permutation. The independent components and the columns
of A and the rows of W can only be estimated up to a multiplicative constant. The
denitions of normal ICA imply no ordering of independent components [5].
4 Constrained ICA
In practice, the ordering of independent components is quite important to separate
non-stationary signals or interested signals with signicant statistical characters.
Eliminating indeterminacy in the permutation and dilation is useful to produce a
unique ICA solution with systematically ordered signals and normalized demixing
matrix. This section presents an approach to CICA by enhancing classical ICA
procedure using Lagrange multiplier methods to obtain unique ICs.
4.1 Ordering of Independent Components
The independent components are ordered in a descent manner according to a certain
statistical measure dened as index I(u). The constrained optimization problem
to CICA is then dened as follows:
minimize Mutual Information M(W)
subject to g(W)  0; g(W) = [g 1 (W)    gM 1 (W)] T (6)
where g(W) is a set of (M 1) inequality constraints, g i (W) = I(u i+1 ) I(u i )
denes the descent order and I(u i ) is the index of some statistical measures of
output components u i , e.g. variance, normalized kurtosis.
Using Lagrange multiplier methods, the augmented Lagrangian function is dened

based on eq.(2) as:
L(W;) = M(W)+ 1
2
M 1
X
i=1
f[maxf0;  g i (W)g] 2  2
i g (7)
With discrete solutions applied, the changes of individual element w ij can be for-
mulated by minimizing eq.(7):
w ij / w ij
L(W(k); (k)) = min
w ij
M(W(k)) + [maxf0;  g i 1 (W(k))g
maxf0; 
g i (W(k))g] I 0 (u i (k)) x j (8)
where I 0 () is the rst derivative of index measure.
The iterative equation for nding individual multipliers  i is
 i (k + 1) = maxf0;  i (k) +  [I(u i+1 (k)) I(u i (k))]g (9)
With the learning equation of normal ICNN given in (5) and the multiplier  i 's
iterative equation (9), the iterative procedure to determine the demixing matrix W
is given as follows:
W / WL(W;) = W T +	(u)x T (10)
where 	(u) =
2
6 6 6 6 4
 1 (u 1 )  1 I 0 (u 1 )
 2 (u 2 ) + ( 1  2 )I 0 (u 2 )
. . .
M 1 (u M 1 ) + (M 2 M 1 )I 0 (uM 1 )
M (uM ) + M 1 I 0 (uM )
3
7 7 7 7 5
We apply measures of variance and kurtosis as examples to emerge the ordering
among the signals. Then the functions I and corresponding rst-derivative I 0 be-
come as below.
variance : I var (u i ) = Efu T
i u i g I 0
var (u i ) = 2Efu i g (11)
kurtosis : I kur (u i ) = Efu 4
i g
Efu 2
i g 2 3 I 0
kur (u i ) = 4u 3
i
Efu 2
i g 2
4Efu 4
i gu i
Efu 2
i g 3 (12)
The signal with the most variance shows the majority of information that input
signals consist of. The ordering based on variance sorts the components in informa-
tion magnitude that needs to reconstruct the original signals. However, it should
be used accompanying with other preprocessing or constraints, such as PCA or nor-
malization, because the normal ICA's indeterminacy on dilation of demixing matrix
may cause the variance of output components being amplied or reduced.
Normalized kurtosis is a kind of 4th-order statistical measure. The kurtosis of a
stationary signal to be extracted is constant under the situation of indeterminacy on
signals' amplitudes. Kurtosis shows the high order statistical character. Any signal
can be categorized into super-Gaussian, Gaussian and sub-Gaussianly distributed
ones by using kurtosis. The components are ordered in the distribution of sparseness
(i.e. super-Gaussian) to denseness (i.e. sub-Gaussian). Kurtosis has been widely
used to produce one-unit ICA [7]. In contrast to their sequential extraction, our
approach can extract and order the components in parallel.
4.2 Normalization of Demixing Matrix
The denition of ICA implies an indeterminacy in the norm of the mixing and
demixing matrix, which is in contrast to, e.g. PCA. Rather than the unknown

mixing matrix A was to be estimated, the rows of the demixing matrix W can be
normalized by applying a constraint term in the ICA energy function to establish
a normalized demixing channel. The constrained ICA problem is then dened as
follows:
minimize Mutual Information M(W)
subject to h(W) = [h 1 (W)    hM (W)] T = 0 (13)
where h(W) denes a set of M equality constraints, h i (w i ) = w T
i w i 1 (i =
1;    ; M ), which dene the row norms of the demixing matrix W equal to 1.
Using Lagrange multiplier methods, the augmented Lagrangian function is dened
based on eq.(2) as:
L(W;) = M(W)+  T diag[WW T I] + 1
2  jjdiag[WW T I]jj 2 (14)
where diag[] denotes the operation to select the diagonal elements in the square
matrix as a vector.
By applying discrete Lagrange multiplier method, the iterative equation minimizing
the augmented function for individual multiplier  i is
 i (k + 1) =  i (k) +  (w T
i w i 1) (15)
and the iterative equation of demixing matrix W is given as follows:
W / WL(W;) = W T +(u)x T
+
 (W) (16)
where
 i (w i ) = 2 i w T
i
Let assume c is the normalized source with unit variance such that Efcc T g = I, and
the input signal x is processed by a prewhitening matrix P such that p = Px obeys
Efpp T g = I. Then with the normalized demixing matrix W, the network output
u contains exact independent components with unit magnitude, i.e. u i contains one
c j for some non-duplicative assignment j ! i.
5 Experiments and Results
The CICA algorithms were simulated in MATLAB version 5. The learning proce-
dure ran 500 iterations with certain learning rate. All signals were preprocessed by
a whitening process to have zero mean and uniform variance. The accuracy of the
recovered components compared to the source components was measured by the
signal to noise ratio (SNR) in dB, where signal power was measured by the variance
of the source component, and noise was the mean square error between the sources
and recovered ones. The performance of the network separating the signals into ICs
was measured by an individual performance index (IPI) of the permutation error  i
for ith output:
 i = (
Xn
j=1
jp ij j
max k jp ik j ) 1 (17)
where p ij were elements of the permutation matrix P = WA. IPI was close to zero
when the corresponding output was closely independent to other components.
5.1 Ordering ICs in Signal Separation
Three independent random signals distributed in Gaussian, sub- and super-Gaussian
manner were simulated. Their statistical congurations were similar to those used

in [1]. These source signals c were mixed with a random matrix to derive inputs
to the network. The networks were trained to obtain the 3  3 demixing matrix
using the algorithm of kurtosis-constraint CICA eq.(10) and (12) to separate three
independent components in complete ICA manner.
The source components, mixed input signals and the resulted output waveforms
are shown in gure 1 (a), (b) and (c), respectively. The network separated and
-1
-0.5
0.5
c 1
-1
-0.5
0.5
c 2
50 100 150 200 250 300 350 400 450 500
-1
-0.5
0.5
c 3
Samples in Time Series
-2
-1
0
1
2
x 1
-1
-0.5
0
0.5
1
x 2
50 100 150 200 250 300 350 400 450 500
-1
0
1
2
x 3
Samples in Time Series
-10
-5
10
y 1
-4
-2
y 2
50 100 150 200 250 300 350 400 450 500
-4
-2
y 3
Samples in Time Series
(a) (b) (c)
Figure 1: Result of extraction of one super-Gaussian, one Gaussian and one sub-
Gaussian signals in the kurtosis descent order. Normalized kurtosis measurements
are  4 (y 1 ) = 32:82,  4 (y 2 ) = 0:02 and  4 (y 3 ) = 1:27. (a) Source components,
(b) input mixtures and (c) resulted components.
sorted the output components in a decreasing manner of kurtosis values, where the
component y 1 had kurtosis 32.82 (> 0, super-Gaussian), y 2 is 0.02 ( 0, Gaussian)
and y 3 is -1.27 (< 0, sub-Gaussian). The nal performance index value of 0:28
and output components' average SNR value of 15dB show all three independent
components well separated too.
5.2 Demixing Matrix Normalization
Three deterministic signals and one Gaussian noise were simulated in this experi-
ment. All signals were independently generated with unit variance and mixed with
a random mixing matrix. All input mixtures were preprocessed by a whitening pro-
cess to have zero mean and unit variance. The signals were separated using both
unconstrained ICA and constrained ICA as given by eq.(5) and (16) respectively.
Table 1 compares their resulted demixing matrix, row norms, variances of separated
components and SNR values. The dilation eect can be seen from the dierence
y Demixing Matrix W Norms Variance SNR
uncons:
ICA
y 1
y 2
y 3
y 4
0:90 0:08 0:12 0:82
0:06 1:11 0:07 0:07
0:07 0:07 1:47 0:09
1:04 0:08 0:04 1:16
1:23
1:11
1:47
1:56
1:50
1:24
2:17
2:43
4:55
10:88
21:58
16:60
cons:
ICA
y 1
y 2
y 3
y 4
0:65 0:43 0:02 0:61
0:37 0:91 0:05 0:20
0:01 0:04 1:00 0:04
0:65 0:07 0:02 0:76
0:99
1:01
1:00
1:00
0:98
1:02
1:00
1:00
4:95
13:94
25:04
22:56
Table 1: Comparison of the demixing matrix elements, row norms, output variances
and resulted components' SNR values in ICA, and CICA with normalization.

among components' variances caused by the non-normalized demixing matrix in
unconstrained ICA. The CICA algorithm with normalization constraint normalized
rows of the demixing matrix and separated the components with variances remained
at unit. Therefore, the source signals are exactly recovered without any dilation.
The increment of separated components' SNR values using CICA also can be seen
in the table. Their source components, input mixture, separated components using
normalization are given in gure 2. It shows that the resulted signals from CICA
are exactly match with the source signals in the sense of waveforms and amplitudes.
-5
c 1
-5
c2
-1
c 3
50 100 150 200 250 300 350 400 450 500
-2
c 4
Samples in Time Series
-5
0
5
x 1
-5
0
5
x 2
-5
0
5
x 3
50 100 150 200 250 300 350 400 450 500
-2
0
2
x 4
Samples in Time Series
-5
y1
-5
y 2
-2
y 3
50 100 150 200 250 300 350 400 450 500
-2
y 4
Samples in Time Series
(a) (b) (c)
Figure 2: (a) Four source deterministic components with unit variances, (b) mixture
inputs and (c) resulted components through normalized demixing channel W.
6 Conclusion
We present an approach of constrained ICA using Lagrange multiplier methods to
eliminate the indeterminacy of permutation and dilation which are present in clas-
sical ICA. Our results provide a technique for systematically enhancing the ICA's
usability and performance using the constraints not restricted to the conditions
treated in this paper. More useful constraints can be considered in similar manners
to further improve the outputs of ICA in other practical applications. Simulation
results demonstrate the accuracy and the usefulness of the proposed algorithms.
References
[1] Jagath C. Rajapakse and Wei Lu. Unied approach to independent component
networks. In Second International ICSC Symposium on NEURAL COMPUTA-
TION (NC'2000), 2000.
[2] A. Bell and T. Sejnowski. An information-maximization approach to blind sep-
aration and blind deconvolution. Neurocomputing, 7:1129{1159, 1995.
[3] S. Amari, A. Chchocki, and H. Yang. A new learning algorithm for blind signal
separation. In Advances in Neural Information Processing Systems 8, 1996.
[4] T-W. Lee, M. Girolami, and T. Sejnowski. Independent component analysis us-
ing an extended informax algorithm for mixed sub-gaussian and super-gaussian
sources. Neural Computation, 11(2):409{433, 1999.
[5] P. Comon. Independent component analysis: A new concept? Signal Processing,
36:287{314, 1994.
[6] Dimitri P. Bertsekas. Constrained optimization and Lagrange multiplier meth-
ods. New York : Academic Press, 1982.
[7] A. Hyvarinen and Erkki Oja. Simple neuron models for independent component
analysis. Neural Systems, 7(6):671{687, December 1996.

