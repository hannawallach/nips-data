Learning curves for Gaussian processes
regression: A framework for good
approximations
Dorthe Malzahn Manfred Opper
Neural Computing Research Group
School of Engineering and Applied Science
Aston University, Birmingham B4 7ET, United Kingdom.
[malzahnd,opperm]@aston.ac.uk
Abstract
Based on a statistical mechanics approach, we develop a method
for approximately computing average case learning curves for Gaus-
sian process regression models. The approximation works well in
the large sample size limit and for arbitrary dimensionality of the
input space. We explain how the approximation can be systemati-
cally improved and argue that similar techniques can be applied to
general likelihood models.
1 Introduction
Gaussian process (GP) models have gained considerable interest in the Neural Com-
putation Community (see e.g.[1, 2, 3, 4] ) in recent years. Being non-parametric
models by construction their theoretical understanding seems to be less well devel-
oped compared to simpler parametric models like neural networks. We are especially
interested in developing theoretical approaches which will at least give good approx-
imations to generalization errors when the number of training data is su∆ciently
large.
In this paper we present a step in this direction which is based on a statistical me-
chanics approach. In contrast to most previous applications of statistical mechanics
to learning theory we are not limited to the so called "thermodynamic" limit which
would require a high dimensional input space.
Our work is very much motivated by recent papers of Peter Sollich (see e.g. [5]) who
presented a nice approximate treatment of the Bayesian generalization error of GP
regression which actually gives good results even in the case of a one dimensional
input space. His method is based on an exact recursion for the generalization
error of the regression problem together with approximations that decouple certain
correlations of random variables. Unfortunately, the method seems to be limited
because the exact recursion is an artifact of the Gaussianity of the regression model
and is not available for other cases such as classication models. Second, it is
not clear how to assess the quality of the approximations made and how one may
systematically improve on them. Finally, the calculation is (so far) restricted to

a full Bayesian scenario, where a prior average over the unknown data generating
function simplies the analysis.
Our approach has the advantage that it is more general and may also be applied to
other likelihoods. It allows us to compute other quantities besides the generalization
error. Finally, it is possible to compute the corrections to our approximations.
2 Regression with Gaussian processes
To explain the Gaussian process scenario for regression problems [2], we assume
that we observe corrupted values y(x) 2 R of an unknown function f(x) at input
points x 2 R d . If the corruption is due to independent Gaussian noise with variance
 2 , the likelihood for a set of m example data D = (y(x 1 ); : : : ; y(xm ))) is given by
P (Djf) =
exp
 P m
i=1
(y i f(x i )) 2
2 2

(2 2 ) m
2
: (1)
where y i
:
= y(x i ). The goal of a learner is to give an estimate of the function f(x).
The available prior information is that f is a realization of a Gaussian process
(random eld) with zero mean and covariance C(x; x 0 ) = E[f(x)f(x 0 )]; where E
denotes the expectation over the Gaussian process. We assume that the prediction
at a test point x is given by the posterior expectation of f(x), i.e. by
^
f(x) = Eff(x)jDg = Ef(x)P (Djf)
Z
(2)
where the partition function Z normalises the posterior. Calling the true data
generating function f  (in order to distinguish it from the functions over which
we integrate in the expectations) we are interested in the learning curve, i.e. the
generalization (mean square) error averaged over independent draws of example
data, i.e. " g = [h(f  (x) ^
f(x)) 2 i] D as a function of m, the sample size. The
brackets [: : :] D denote averages over example data sets where we assume that the
inputs x i are drawn independently at random from a density p(x). h: : :i denotes
an average over test inputs drawn from the same density. Later, the same brackets
will also be used for averages over several dierent test points and for joint averages
over test inputs and test outputs.
3 The Partition Function
As typical of statistical mechanics approaches, we base our analysis on the averaged
"free energy" [ ln Z]D where the partition function Z (see Eq. (2)) is
Z = EP (Djf): (3)
[ln Z]D serves as a generating function for suitable posterior averages. The concrete
application to " g will be given in the next section. The computation of [ln Z]D is
based on the replica trick ln Z = lim n!0
Z n 1
n , where we compute [Z n ] D for integer
n and perform the continuation at the end.
Introducing a set of auxiliary integration variables z ka in order to decouple the
squares, we get
[Z n ] D =
Z m
Y
k=1
n
Y
a=1
dz ka
2 e
 2
2
P
k;a z 2
ka En
2
4 exp
0
@ i
X
k;a
z ka (f a (x k ) y k )
1
A
3
5
D
(4)

where En denotes the expectation over the n times replicated GP measure. In
general, it seems impossible to perform the average over the data. Using a cumu-
lant expansion, an innite series of terms would be created. However one may be
tempted to try the following heuristic approximation: If (for xed function f ), the
distribution of f(x k ) y k was a zero mean Gaussian, we would simply end up with
only the second cumulant and
[Z n ] D 
Z Y
k;a
dz ka
2 exp
0
@  2
2
X
k;a
z 2
ka
1
A  (5)
 En exp
0
@ 1
2
X
a;b
X
k
z ka z kb h(f a (x) y)(f b (x) y)i
1
A :
Although such a reasoning may be justied in cases where the dimensionality of in-
puts x is large, the assumption of approximate Gaussianity is typically (in the sense
of the prior measure over functions f) completely wrong for small dimensions. Nev-
ertheless, we will argue in the next section that the expression Eq. (5) (justied by
a dierent reason) is a good approximation for large sample sizes and nonzero noise
level. We will postpone the argument and proceed to evaluate Eq. (5) following a
fairly standard recipe: The high dimensional integrals over z ka are turned into low
dimensional integrals by the introduction of "order-parameters"  ab =
P m
k=1 z ka z kb
so that
[Z n ] D 
Z Y
ab
d ab exp
 
1
2  2
X
a
 aa +G(fg)
!
 (6)
 En exp
0
@ 1
2
X
a;b
 ab h(f a (x) y)(f b (x) y)i
1
A
where e G(fg) =
R Q
k;a
dzka
2
Q
ab ∆ (
P m
k=1 z ka z kb  ab ) : We expect that in the
limit of large sample size m, the integrals are well approximated by the saddle-point
method. To perform the limit n ! 0, we make the assumption that the saddle-point
of the matrix  is replica symmetric, i.e.  ab =  for a 6= b and  aa =  0 . After
some calculations we arrive at
[ln Z]D =  2  0
2 + m
2 ln( 0 ) + m
2( 0 )

2 hE 0 f 2 (x)i (7)
+ ln E exp
  0 
2 h(f(x) y) 2 i
 m
2 (ln(2m) 1)
into which we have to insert the values  and  0 that make the right hand side an
extremum. We have dened a new auxiliary (translated) Gaussian measure over
functions by
E 0 fffgg = E exp
 0 
2 hf 2 (x)i

ffg
E exp
 0 
2 hf 2 (x)i
 (8)
where  is a functional of f . For a given input distribution it is possible to compute
the required expectations in terms of sums over eigenvalues and eigenfunctions of
the covariance kernel C(x; x 0 ). We will give the details as well as the explicit order
parameter equations in a full version of the paper.

4 Generalization error
To relate the generalization error with the order parameters, note that in the replica
framework (assuming the approximation Eq. (5)) we have
" g +  2 = lim
n!0
Z Y
ab
d ab exp
"
1
2  2
X
a
 aa +G(fg)
#

 @
@ 12
En exp
0
@ 1
2
X
a;b
 ab h(f a (x) y)(f b (x) y)i
1
A
which by a partial integration and a subsequent saddle point integration yields
" g = m
( 0 ) 2  2 : (9)
It is also possible to compute other error measures in terms of the order parameters
like the expected error on the (noisy) training data dened by
" t = 1
m
X
i
[(y i ^
f(x i )) 2 ] D =  4 
m
: (10)
The "true" training error which compares the prediction with the data generating
function f  is somewhat more complicated and will be given elsewhere.
5 Why (and when) the approximation works
Our intuition behind the approximation Eq. (5) is that for su∆ciently large sample
size, the partition function is dominated by regions in function space which are close
to the data generating function f  such that terms like h(f a (x) y)(f b (x) y)i are
typically small and higher order polynomials in f a (x) y generated by a cumulant
expansion are less important. This intuition can be checked self consistently by es-
timating the omitted terms perturbatively. We use the following modied partition
function
[Z n ()] D =
Z Y
k;a
dz ka
2 e
 2
2
P
k;a z 2
ka En

exp

i
X
k;a
z ka (f a (x k ) y)
1  2
2
X
a;b
X
k
z ka z kb h(f a (x) y)(f b (x) y)i

D
(11)
which for  = 1 becomes the "true" partition function, whereas Eq. (5) is ob-
tained for  = 0. Expanding in powers of  (the terms with odd powers vanish)
is equivalent to generating the cumulant expansion and subsequently expanding
the non-quadratic terms down. Within the saddle-point approximation, the rst
nonzero correction to our approximation of [ln Z] is given by
 4
 ( 0 ) 2
2m

 2 h ^
C(x; x)i + h ^
C(x; x)F 2 (x)i h ^
C(x; x 0 )F (x)F (x 0 )i
+h ^
C(x; x 0 ) ^
C(x; x 00 ) ^
C(x 0 ; x 00 )i h ^
C(x; x) ^
C 2 (x; x 0 )i

+ 1
4
  2
m
+  2
0
m
 
h ^
C 2 (x; x)i h ^
C 2 (x; x 0 )i
 
: (12)

^
C(x; x 0 ) = E 0 ff(x)f(x 0 )g denotes the covariance with respect to the auxiliary
measure and F (x) :
= f  (x) h ^
C(x; x 00 )f  (x 00 )i. The signicance of the individ-
ual terms as m ! 1 can be estimated from the following scaling. We nd
that ( 0 ) = O(m) is a positive quantity, whereas  = O(m) is negative.
^
C(x; x 0 ) = O(1=m). Using these relations, we can show that Eq. (12) remains
nite as m !1, whereas the leading approximation Eq. (7) diverges with m.
We have not (yet) computed the resulting correction to " g . However, we have
studied the somewhat simpler error measure " 0 :
= 1
m
P
i [Ef(f  (x i ) f(x i )) 2 jDg] D
which can be obtained from a derivative of [ln Z]D with respect to  2 . It equals the
error of a Gibbs algorithm (sampling from the posterior) on the training data. We
can show that the correction to " 0 is typically by a factor of O(1=m) smaller than
the leading term. However, our approximation becomes worse with decreasing noise
variance  2 .  = 0 is a singular case for which (at least for some GPs with slowly
decreasing eigenvalues) it can be shown that our approximation for " g decays to
zero at the wrong rate. For small values of ,  ! 0, we expect that higher order
terms in the perturbation expansion will become relevant.
6 Results
We compare our analytical results for the error measures " g and " t with simula-
tions of GP regression. For simplicity, we have chosen periodic processes of the
form f(x) =
p
2
P
n (a n cos(2nx) + b n sin(2nx)) for x 2 [0; 1] where the coe∆-
cients an ; b n are independent Gaussians with Efa 2
n g = Efb 2
n g = n . This choice
is convenient for analytical calculations by the orthogonality of the trigonometric
functions when we sample the x i from a uniform density in [0; 1]. The n and
the translation invariant covariance kernel are related by c(x y) :
= C(x; y) =
2
P
n n cos(2n(x y)) and n =
R 1
0 c(x) cos(2nx) dx: We specialise on the (pe-
riodic) RBF kernel c(x) =
P 1
k=1 exp
 (x k) 2 =2l 2
 with l = 0:1. For an il-
lustration we generated learning curves for two target functions f  as displayed in
Fig. 1. One function is a sine-wave f  (x) = p
2 1 sin(2x) while the other is a ran-
dom realisation from the prior distribution. The symbols in the left panel of Fig. 1
represent example sets of fty data points. The data points have been obtained by
corruption of the target function with Gaussian noise of variance  2 = 0:01. The
right panel of Fig. 1 shows the data averaged generalization and training errors " g ,
" t as a function of the number m of example data. Solid curves display simulation
results while the results of our theory Eqs. (9), (10) are given by dashed lines. The
training error " t converges to the noise level  2 . As one can see from the pictures
our theory is very accurate when the number m of example data is su∆ciently large.
While the generalization error " g diers initially, the asymptotic decay is the same.
7 The Bayes error
We can also apply our method to the Bayesian generalization error (previously ap-
proximated by Peter Sollich [5]). The Bayes error is obtained by averaging the
generalization error over "true" functions f  drawn at random from the prior dis-
tribution. Within our approach this can be achieved by an average of Eq. (7) over
f  . The resulting order parameter equations and their relation to the Bayes error
turn out be identical to Sollich's result. Hence, we have managed to re-derive his
approximation within a broader framework from which also possible corrections can
be obtained.

0 0.2 0.4 0.6 0.8 1
x
-2
-1
0
1
2
f * (x)
0 0.2 0.4 0.6 0.8 1
x
-2
-1
0
1
2
f *
(x)
Data generating function
0 50 100 150 200
Number m of example data
10 -5
10 -4
10 -3
10 -2
10 -1
10 0
0 50 100 150 200
Number m of example data
10 -4
10 -3
10 -2
10 -1
10 0
Learning curves
e t
e g
e g
e t
Figure 1: The left panels show two data generating functions f  (x) and example
sets of 50 data points. The right panels display the corresponding averaged learning
curves. Solid curves display simulation results for generalization and training errors
" g , " t . The results of our theory Eqs. (9), (10) are given by dashed lines.
8 Future work
At present, we extend our method in the following directions:
 The statistical mechanics framework presented in this paper is based on
a partition function Z which can be used to generate a variety of other
data averages for posterior expectations. An obvious interesting quantity
is given by the sample uctuations of the generalization error
h
h(f  (x) ^
f(x)) 2 i 2
i
D

[h(f  (x) ^
f(x)) 2 i] D
 2
(13)
which gives condence intervals on " g .
 Obviously, our method is not restricted to a regression model (in this case
however, all resulting integrals are elementary) but can also be directly
generalized to other likelihoods such as the classication case [4, 6]. A
further application to Support Vector Machines should be possible.
 The saddle-point approximation neglects uctuations of the order parame-
ters. This may be well justied when m is su∆ciently large. It is possible
to improve on this by including the quadratic expansion around the saddle-
point.

 Finally, one may criticise our method as being of minor relevance to prac-
tical applications, because our calculations require the knowledge of the
unknown function f  and the density of the inputs x. However, Eqs. (9)
and (10) show that important error measures are solely expressed by the
order parameters  and  0 . Hence, estimating some error measures and
the posterior variance at the data points empirically would allow us to pre-
dict values for the order parameters. Those in turn could be used to make
predictions for the unknown generalization error.
Acknowledgement
This work has been supported by EPSRC grant GR/M81601.
References
[1] D. J. C. Mackay, Gaussian Processes, A Replacement for Neu-
ral Networks, NIPS tutorial 1997, May be obtained from
http://wol.ra.phy.cam.ac.uk/pub/mackay/.
[2] C. K. I. Williams and C. E. Rasmussen, Gaussian Processes for Regression, in
Neural Information Processing Systems 8, D. S. Touretzky, M. C. Mozer and
M. E. Hasselmo eds., 514-520, MIT Press (1996).
[3] C. K. I. Williams, Computing with Innite Networks, in Neural Information
Processing Systems 9, M. C. Mozer, M. I. Jordan and T. Petsche, eds., 295-301.
MIT Press (1997).
[4] D. Barber and C. K. I. Williams, Gaussian Processes for Bayesian Classication
via Hybrid Monte Carlo, in Neural Information Processing Systems 9, M . C.
Mozer, M. I. Jordan and T. Petsche, eds., 340-346. MIT Press (1997).
[5] P. Sollich, Learning curves for Gaussian processes, in Neural Information Pro-
cessing Systems 11, M. S. Kearns, S. A. Solla and D. A. Cohn, eds. 344 - 350,
MIT Press (1999).
[6] L. Csato, E. Fokoue, M. Opper, B. Schottky, and O. Winther. E∆cient ap-
proaches to Gaussian process classication. In Advances in Neural Information
Processing Systems, volume 12, 2000.

