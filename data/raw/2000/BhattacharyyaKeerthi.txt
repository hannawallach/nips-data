A variational mean-eld theory for
sigmoidal belief networks
C. Bhattacharyya
Computer Science and Automation
Indian Institute of Science
Bangalore, India, 560012
cbchiru@csa.iisc.ernet.in
S. Sathiya Keerthi
Mechanical and Production Engineering
National University of Singapore
mpessk@guppy.mpe.nus.edu.sg
Abstract
A variational derivation of Plefka's mean-eld theory is presented.
This theory is then applied to sigmoidal belief networks with the
aid of further approximations. Empirical evaluation on small scale
networks show that the proposed approximations are quite com-
petitive.
1 Introduction
Application of mean-eld theory to solve the problem of inference in Belief Net-
works(BNs) is well known [1]. In this paper we will discuss a variational mean-eld
theory and its application to BNs, sigmoidal BNs in particular.
We present a variational derivation of the mean-eld theory, proposed by Plefka[2].
The theory will be developed for a stochastic system, consisting of N binary random
variables, S i 2 f0; 1g, described by the energy function E( ~
S), and the following
Boltzmann Gibbs distribution at a temperature T :
P ( ~
S) = e E( ~ S)
T
Z
; Z =
X
~ S
e E( ~
S)
T :
The application of this mean-eld method to Boltzmann Machines(BMs) is already
done [3]. A large class of BNs are described by the following energy function:
E( ~
S) =
N
X
i=1
fS i ln f(M i ) + (1 S i ) ln(1 f(M i )g M i =
i 1
X
j=1
w ij S j + h i
The application of the mean-eld theory for such energy functions is not straight-
forward and further approximations are needed. We propose a new approximation
scheme and discuss its utility for sigmoid networks, which is obtained by substitut-
ing
f(x) = 1
1 + e x

in the above energy function. The paper is organized as follows. In section 2 we
present a variational derivation of Plefka's mean-eld theory. In section 3 the theory
is extended to sigmoidal belief networks. In section 4 empirical evaluation is done.
Concluding remarks are given in section 5.
2 A Variational mean-eld theory
Plefka,[2] proposed a mean-eld theory in the context of spin glasses. This theory
can, in principle, yield arbitrarily close approximation to log Z: In this section we
present an alternate derivation from a variational viewpoint, see also [4],[5].
Let  be a real parameter that takes values from 0 to 1. Let us dene a  dependent
partition and distribution function,
Z  =
X
~
S
e E( ~ S)=T ; p  = e E( ~ S)=T
Z 
(1)
Note that Z 1 = Z and p 1 = p. Introducing an external real vector, ~
 let us rewrite
(1) as
Z  =
X
~
S
e  E
T +
P
i
 i S i
e
Z
e
P
i  i S i e
Z (2)
where e
Z is the partition function associated with the distribution function ~
p  given
by
e
Z =
X
~ S
e  E
T +
P
i
 i S i ; ~
p  = e  E
T +
P
i
 i S i
e
Z
(3)
Using Jensen's Inequality, he x i  e hxi , we get
Z  = e
Z
X
~
S
e p  e
P
i
 i S i  e
Ze
P
i
 i u i (4)
where
u i = hS i i ep 
(5)
Taking logarithms on both sides of (4) we obtain
log Z   log e
Z
X
i
 i u i (6)
The right hand side is dened as a function of ~u and  via the following assumption.
Invertibility assumption: For each xed ~u and , (5) can be solved for ~ .
If the invertibility assumption holds then we can use ~u as the independent vector
(with ~  dependent on ~u) and rewrite (6) as
ln Z   G(~u; ) (7)
where G is as dened in
G(~u; ) = ln e
Z +
X
i
 i u i :

This then gives a variational feel : treat ~u as an external variable vector and choose
it to minimize G for a xed . The stationarity conditions of the above minimization
problem yield
 i = @G
@u i
= 0:
At the minimum point we have the equality G = log Z  :
It is di∆cult to invert (5) for  6= 0, thus making it impossible to write an algebraic
expression for G for any nonzero . At  = 0 the inversion is straightforward and
one obtains
G(~u; 0) =
N
X
i=1
(u i ln u i + (1 u i ) ln(1 u i )) ; ~
p 0 =
Y
i
u i (1 u i ):
A Taylor series approach is then undertaken around  = 0 to build an approximation
to G. Dene
e
GM = G(~u; 0) +
X
k
 k
k!
@ k G
@ k
    =0
(8)
Then e
GM can be considered as an approximation of G: The stationarity conditions
are enforced by setting
 i = @G
@u i

@ e
GM
@u i
= 0:
In this paper we will restrict ourselves to M = 2: To do this we need to evaluate
the following derivatives
@G
@
    =0
= hEi ~ p0 (9)
@ 2 G
@ 2
    =0
= 1
T 2
 
h(E hEi ~ p0 ) 2 i ~
p0
X
i
cov 2 (E; S i )
var(S i )
!
(10)
where
cov(E; S i ) = h(E hEi ~ p0 )(S i u i )i ~
p0 ; var(S i ) = h(S i u i ) 2 i ~
p0 :
For M = 1 we have the standard mean-eld approach. The expression for M = 2
can be identied with the TAP correction. The term (10) yields the TAP term for
BM energy function.
3 Mean-eld approximations for BNs
The method, as developed in the previous section, is not directly useful for BNs
because of the intractability of the partial derivatives at  = 0. To overcome this
problem, we suggest an approximation based on Taylor series expansion. Though
in this paper we will be restricting ourselves to sigmoid activation function, this
method is applicable to other activation functions also. This method enables cal-
culation of all the necessary terms required for extending Plefka's method for BNs.
Since, for BN operation T is xed to 1, T will be dropped from all equations in the
rest of the paper.

Let us dene a new energy function
b
E(; ~
S; ~u; ~
w) =
N
X
i=1
fS i ln f( c
M i ()) + (1 S i ) ln(1 f( c
M i ())g (11)
where 0    1,
c
M i () =
i 1
X
j=1
w ij (S j u j ) +M i ; M i =
i 1
X
j=1
w ij u j + h i
where
u k =
X
~
S
S k p  8k ; p  = e  b
E+
P
i
 i S i
P
~s e  b E+
P
i
 i S i
(12)
Since  is the important parameter, b
E(; ~
S; ~u; ~
w) will be referred to as b
E() so as
to avoid notational clumsiness. We use a Taylor series approximation of b
E() with
respect to . Let us dene
b
EC () = b
E(0) +
C
X
k=1
 k
k!
@ k b
E
@ k
     =0
: (13)
If b
EC approximates b
E, then we can write
E = b
E(1)  b
EC (1): (14)
Let us now dene the following function
A(; ; ~u) = ln
X
~s
e  b
E+
P
i
 i S i +
X
i
 i u i (15)
The  i are assumed to be functions of ~u; ; , which are obtained by inverting
equations (12) By replacing b
E by b
EC in (15) we obtain AC
AC (; ; ~u) = ln
X
~s
e  b
EC+
P
i
 i S i +
X
i
 i u i (16)
where the denition of ~u is obtained by replacing b
E by b
EC : In view of (14) one can
consider AC as an approximation to A: This observation suggests an approximation
to G.
G(; ~u) = A(; 1; ~u)  AC (; 1; ~u) (17)
The required terms needed in the Taylor expansion of G in  can be approximated
by
G(0; ~u) = A(0; 1; ~u) = AC (0; 1; ~u)
@ k G
@ k
    =0
= @ k A
@ k
    =0;=1

@ k AC
@ k
    =0;=1
The biggest advantage in working with AC rather than G is that the partial deriva-
tives of AC with respect to  at  = 0 and  = 1 can be expressed as functions of
~u: We dene
b
GMC (~u; ) = AC (0; 1; ~u) +
M
X
k=1
 k
k!
@ k AC
@ k
    =0;=1
(18)

Figure 1: Three layer BN (2  4  6) with top down propagation of beliefs. The
activation function was chosen to be sigmoid.
In light of the above discussion one can consider e
GM  b
GMC ; hence the mean-eld
equations can be stated as
 i = @G
@u i

@ ~
GM
@u i

@ b
GMC
@u i
= 0 (19)
In this paper we will restrict ourselves to M = 2: The relevant objective functions
for a general C is given by
b
G 1C =
X
i
u i log u i + (1 u i ) log(1 u i ) + hEC i ~ p0 (20)
b
G 2C = b
G 1C
1
2
 
 (EC hEC i ~
p0 ) 2
 X
i
cov 2 (EC ; S i )
var(S i )
!
(21)
All these objective functions can be expressed as a function of ~u:
4 Experimental results
To test the approximation schemes developed in the previous schemes, numerical
experiments were conducted. Saul et al.[1] pioneered the application of mean-eld
theory to BNs. We will refer to their method as the SJJ approach. We compare
our schemes with the SJJ approach.
Small Networks were chosen so that ln Z can be computed by exact enumeration
for evaluation purposes. For all the experiments the network topology was xed
to the one shown in gure 1. This choice of the network enables us to compare
the results with those of [1]. To compare the performance of our methods with
their method we repeated the experiment conducted by them for sigmoid BNs. Ten
thousand networks were generated by randomly choosing weight values in [ 1; 1].
The bottom layer units, or the visible units of each network were instantiated to
zero. The likelihood, ln Z, was computed by exact enumeration of all the states in
the higher two layers. The approximate value of ln Z was computed by b
GMC ;
~u was computed by solving the xed point equations obtained from (19). The
goodness of approximation scheme was tested by the following measure
E =
b
GMC
ln Z 1 (22)
For a proper comparison we also implemented the SJJ method. The goodness
of approximation for the SJJ scheme is evaluated by substituting b
GMC , in (22)
by L sapprox , for specic formula see [1]. The results are presented in the form
of histograms in Figure 2. We also repeated the experiment with weights and

hEi hEi
small weights [ 1; 1] large weights [ 5; 5]
b
G 11 -0.0404 -0.0440
b
G 12 0.0155 0.0231
b
G 22 0.0029 -0.0456
SJJ 0.0157 0.0962
Table 1: Mean of E for randomly generated sigmoid networks, in dierent weight
ranges.
biases taking values between -5 and 5, the results are again presented in the form of
histograms in Figure 3. The ndings are summarized in the form of means tabulated
in Table 1.
For small weights b
G 12 and the SJJ approach show close results, which was expected.
But the improvement achieved by the b
G 22 scheme is remarkable; it gave a mean
value of 0:0029 which compares substantially well against the mean value of 0:01139
reported in [6]. The improvement in [6] was achieved by using mixture distribution
which requires introduction of extra variational variables; more than 100 extra vari-
ational variables are needed for a 5 component mixture. This results in substantial
increase in the computation costs. On the other hand the extra computational
cost for b
G 22 over b
G 12 is marginal. This makes the b
G 22 scheme computationally
attractive over the mixture distribution.
-0.12 -0.1 -0.08 -0.06 -0.04 -0.02 0.02 0.04 0.06 0.08
500
1000
1500
2000
2500
3000
3500
4000
0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08
0
500
1000
1500
2000
2500
3000
3500
4000
4500
-2 2 4 6 8 10 12 14 16
x 10 -3
0
500
1000
1500
2000
2500
3000
3500
4000
4500
Figure 2: Histograms for b
G 1C and SJJ scheme for weights taking values in [ 1; 1],
for sigmoid networks. The plot on the left show histograms for E for the schemes
b
G 11 and b
G 12 They did not have any overlaps; b
G 11 , gives a mean of -0.040 while b
G 12
gives a mean of 0.0155. The middle plot shows the histogram for the SJJ scheme,
mean is given by 0.0157.The plot at the extreme right is for the scheme b
G 22 , having
a mean of 0.0029
Of the three schemes b
G 12 is the most robust and also yields reasonably accurate
results. It is outperformed only by b
G 22 in the case of sigmoid networks with low
weights. Empirical evidence thus suggests that the choice of a scheme is not straight-
forward and depends on the activation function and also parameter values.

-0.8 -0.6 -0.4 -0.2 0.2 0.4 0.6 0.8 1.2
1000
2000
3000
4000
5000
6000
7000
-0.5 0.5 1.5 2.5
1000
2000
3000
4000
5000
6000
7000
0.5 1.5 2.5
1000
2000
3000
4000
5000
6000
7000
8000
-0.5 0.5 1.5 2.5
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Figure 3: Histograms for the b
G 1C and SJJ schemes for weights taking values in
[ 5; 5] for sigmoid networks. The leftmost histogram shows E for b
G 11 scheme having
a mean of 0:0440, second from left is for b
G 12 scheme having a mean of 0:0231, and
second from right is for SJJ scheme, having a mean of 0:0962. The scheme b
G 22 is
at the extreme right with mean 0:0456:
5 Discussion
Application of Plefka's theory to BNs is not straightforward. It requires compu-
tation of some averages which are not tractable. We presented a scheme in which
the BN energy function is approximated by a Taylor series, which gives a tractable
approximation to the terms required for Plefka's method. Various approximation
schemes depending on the degree of the Taylor series expansion are derived. Unlike
the approach in [1], the schemes discussed here are simpler as they do not introduce
extra variational variables. Empirical evaluation on small scale networks shows that
the quality of approximations is quite good. For a more detailed discussion of these
points see [7].
References
[1] Saul, L. K. and Jaakkola, T. and Jordan, M. I.(1996), Mean eld theory for sigmoid
belief networks, Journal of Articial Intelligence Research,4
[2] Plefka, T. (1982), Convergence condition of the TAP equation for the Innite-ranged
Ising glass model,J. Phys. A: Math. Gen.,15
[3] Kappen, H. J and Rodriguez, F. B(1998), Boltzmann machine learning using mean
eld theory and linear response correction, Advances in Neural Information Process-
ing Systems 10, (eds.) M. I. Jordan and M. J. Kearns and S. A. Solla, MIT press
[4] Georges, A. and Yedidia, J. S.(1991), How to expand around mean-eld theory using
high temperature expansions,J. Phys. A: Math. Gen., 24
[5] Bhattacharyya, C. and Keerthi, S. S.(2000), Information geometry and Plefka's mean-
eld theory, J. Phys. A: Math. Gen.,33
[6] Bishop, M. C. and Lawrence, N. and Jaakkola, T. and Jordan, M. I.(1997), Approxi-
mating Posterior Distributions in Belief Networks using Mixtures, Advances in Neural
Information Processing Systems 10, (eds.) Jordan, M. I. and Kearns, M. J. and Solla,
S., MIT press
[7] Bhattacharyya, C. and Keerthi, S. S. (1999), Mean eld theory for a special class of
belief networks, accepted in Journal of Articial Intelligence Research

