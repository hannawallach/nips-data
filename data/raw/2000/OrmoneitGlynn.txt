Kernel­Based Reinforcement Learning in
Average­Cost Problems: An Application
to Optimal Portfolio Choice
Dirk Ormoneit
Department of Computer Science
Stanford University
Stanford, CA 94305­9010
ormoneit@cs.stanford.edu
Peter Glynn
EESOR
Stanford University
Stanford, CA 94305­4023
Abstract
Many approaches to reinforcement learning combine neural net­
works or other parametric function approximators with a form of
temporal­difference learning to estimate the value function of a
Markov Decision Process. A significant disadvantage of those pro­
cedures is that the resulting learning algorithms are frequently un­
stable. In this work, we present a new, kernel­based approach to
reinforcement learning which overcomes this difficulty and provably
converges to a unique solution. By contrast to existing algorithms,
our method can also be shown to be consistent in the sense that
its costs converge to the optimal costs asymptotically. Our focus
is on learning in an average­cost framework and on a practical ap­
plication to the optimal portfolio choice problem.
1 Introduction
Temporal­difference (TD) learning has been applied successfully to many real­world
applications that can be formulated as discrete state Markov Decision Processes
(MDPs) with unknown transition probabilities. If the state variables are continuous
or high­dimensional, the TD learning rule is typically combined with some sort of
function approximator -- e.g. a linear combination of feature vectors or a neural
network -- which may well lead to numerical instabilities (see, for example, [BM95,
TR96]). Specifically, the algorithm may fail to converge under several circumstances
which, in the authors' opinion, is one of the main obstacles to a more wide­spread
use of reinforcement learning (RL) in industrial applications. As a remedy, we
adopt a non­parametric perspective on reinforcement learning in this work and we
suggest a new algorithm that always converges to a unique solution in a finite
number of steps. In detail, we assign value function estimates to the states in a
sample trajectory and we update these estimates in an iterative procedure. The

updates are based on local averaging using a so­called ``weighting kernel''. Besides
numerical stability, a second crucial advantage of this algorithm is that additional
training data always improve the quality of the approximation and eventually lead
to optimal performance -- that is, our algorithm is consistent in a statistical sense.
To the authors' best knowledge, this is the first reinforcement learning algorithm
for which consistency has been demonstrated in a continuous space framework.
Specifically, the recently advocated ``direct'' policy search or perturbation methods
can by construction at most be optimal in a local sense [SMSM00, VRK00].
Relevant earlier work on local averaging in the context of reinforcement learning
includes [Rus97] and [Gor99]. While these papers pursue related ideas, their ap­
proaches differ fundamentally from ours in the assumption that the transition prob­
abilities of the MDP are known and can be used for learning. By contrast, kernel­
based reinforcement learning only relies on sample trajectories of the MDP and it
is therefore much more widely applicable in practice. While our method addresses
both discounted­ and average­cost problems, we focus on average­costs here and
refer the reader interested in discounted­costs to [OS00]. For brevity, we also defer
technical details and proofs to an accompanying paper [OG00]. Note that average­
cost reinforcement learning has been discussed by several authors (e.g. [TR99]).
The remainder of this work is organized as follows. In Section 2 be provide basic
definitions and we describe the kernel­based reinforcement learning algorithm. Sec­
tion 3 focuses on the practical implementation of the algorithm and on theoretical
issues. Sections 4 and 5 present our experimental results and conclusions.
2 Kernel­Based Reinforcement Learning
Consider a MDP defined by a sequence of states X t taking values in IR d , a sequence
of actions a t taking values in A = f1; 2; : : : ; Mg, and a family of transition kernels
fP a (x; B)ja 2 Ag characterizing the conditional probability of the event X t 2 B
given X t\Gamma1 = x and a t\Gamma1 = a. The cost function c(x; a) represents an immediate
penalty for applying action a in state x. Strategies, policies, or controls are under­
stood as mappings of the form ¯ : IR d ! A, and we let P x;¯ denote the probability
distribution governing the Markov chain starting from X 0 = x associated with the
policy ¯. Several regularity conditions are listed in detail in [OG00].
Our goal is to identify policies that are optimal in that they minimize the long­run
average­cost j ¯ j lim T!1 E x;¯
h 1
T
P T \Gamma1
t=0 c(X t ; ¯(X t ))
i
. An optimal policy, ¯ \Lambda , can
be characterized as a solution to the Average­Cost Optimality Equation (ACOE):
j \Lambda + h \Lambda (x) = min
a
fc(x; a) + (\Gamma a h \Lambda )(x)g; (1)
¯ \Lambda (x) = arg min a
fc(x; a) + (\Gamma a h \Lambda )(x)g ; (2)
where j \Lambda is the minimum average­cost and h \Lambda (x) has an interpretation as the differ­
ential value of starting in x as opposed to drawing a random starting position from
the stationary distribution under ¯ \Lambda . \Gamma a denotes the conditional expectation oper­
ator (\Gamma a h)(x) j E x;a [h(X 1 )], which is assumed to be unknown so that (1) cannot
be solved explicitly. Instead, we simulate the MDP using a fixed proposal strat­
egy ¯
¯ in reinforcement learning to generate a sample trajectory as training data.
Formally, let S j fz 0 ; : : : ; z m g denote such an m­step sample trajectory and let

A j fa 0 ; : : : ; am\Gamma1 ja s = ¯
¯(z s )g and C j fc(z s ; a s )j0 Ÿ s ! mg be the sequences
of actions and costs associated with S. Then our objective can be reformulated as
the approximation of ¯ \Lambda based on information in S, A, and C. In detail, we will
construct an approximate expectation operator, “
\Gamma m;a , based on the training data,
S, and use this approximation in place of the true operator \Gamma a in this work. For­
mally substituting “ \Gamma m;a for \Gamma a in (1) and (2) gives the Approximate Average­Cost
Optimality Equation (AACOE):
“ jm + “
hm (x) = min
a
fc(x; a) + ( “
\Gamma m;a “ hm )(x)g; (3)
“
¯m (x) = arg min a
n
c(x; a) + ( “
\Gamma m;a “ hm )(x)
o
: (4)
Note that, if the solutions “
jm and “
hm to (3) are well­defined, they can be interpreted
as statistical estimates of j \Lambda and h \Lambda in equation (1). However, “
jm and “ hm need not
exist unless “
\Gamma m;a is defined appropriately. We therefore employ local averaging in
this work to construct “
\Gamma m;a in a way that guarantees the existence of a unique
fixed point of (3). For the derivation of the local averaging operator, note that
the task of approximating (\Gamma a h)(x) = E x;a [h(X 1 )] can be interpreted alternatively
as a regression of the ``target'' variable h(X 1 ) onto the ``input'' X 0 = x. So­called
kernel­smoothers address regression tasks of this sort by locally averaging the target
values in a small neighborhood of x. This gives the following approximation:
( “
\Gamma m;a h)(x) j
m\Gamma1 X
s=0
k m;a (z s ; x)h(z s+1 ); (5)
k m;a (z s ; x) j 11(a s = a) exp
\Gamma
\Gammajjz s \Gamma xjj 2 =(2b 2 )
\Delta
P m\Gamma1
s 0 =0 11(a s 0 = a) exp (\Gammajjz s 0 \Gamma xjj 2 =(2b 2 ))
: (6)
In detail, we employ the weighting function or weighting kernel k m;a (z s ; x) in (6) to
determine the weights that are used for averaging in equation (5). Here k m;a (z s ; x) is
a multivariate Gaussian, normalized so as to satisfy the constraints k m;¯ (z s ; x) ? 0
if a s = a, k m;a (z s ; x) = 0 if a s 6= a, and
P m\Gamma1
s=0 k m;¯ (z s ; x) = 1. Intuitively, (5)
assesses the future differential cost of applying action a in state x by looking at all
times in the training data where a has been applied previously in a state similar
to x, and by averaging the current differential value estimates at the outcomes of
these previous transitions. Because the weights k m;¯ (z s ; x) are related inversely
to the distance jjz s \Gamma xjj, transitions originating in the neighborhood of x are most
influential in this averaging procedure. A more statistical interpretation of (5) would
suggest that ideally we could simply generate a large number of independent samples
from the conditional distribution P x;a and estimate E x;a [h(X 1 )] using Monte­Carlo
approximation. Practically speaking, this approach is clearly infeasible because in
order to assess the value of the simulated successor states we would need to sample
recursively, thereby incurring exponentially increasing computational complexity. A
more realistic alternative is to estimate “
\Gamma m;a h(x) as a local average of the rewards
that were generated in previous transitions originating in the neighborhood of x,
where the membership of an observation z s in the neighborhood of x is quantified
using k m;a (z s ; x). Here the regularization parameter b determines the width of the
Gaussian kernel and thereby also the size of the neighborhood used for averaging.
Depending on the application, it may be advisable to choose b either fixed or as a
location­dependent function of the training data.

3 ``Self­Approximating Property''
As we illustrated above, kernel­based reinforcement learning formally amounts to
substituting the approximate expectation operator “
\Gamma m;a for \Gamma a and then applying
dynamic programming to derive solutions to the approximate optimality equation
(3). In this section, we outline a practical implementation of this approach and
we present some of our theoretical results. In particular, we consider the relative
value iteration algorithm for average­cost MDPs that is described, for example, in
[Ber95]. This procedure iterates a variant of equation (1) to generate a sequence of
value function estimates, h k
m , that eventually converge to a solution of (1) (or (3),
respectively). An important practical problem in continuous state MDPs is that the
intermediate functions h k
m need to be represented explicitly on a computer. This re­
quires some form of function approximation which may be numerically undesirable
and computationally burdensome in practice. In the case of kernel­based reinforce­
ment learning, the so­called ``self­approximating'' property allows for a much more
efficient implementation in vector format (see also [Rus97]). Specifically, because
our definition of “
\Gamma m;a h in (5) only depends on the values of h at the states in S,
the AACOE (3) can be solved in two steps:
“ jm + “ hm (z s ) = min
a
fc(z s ; a) + ( “ \Gamma m;a “ hm )(z s )g; (7)
“ hm (x) j min a
fc(x; a) + ( “
\Gamma m;a “
hm )(x)g \Gamma “
jm : (8)
In other words, we first determine the values of “ hm at the points in S using (7)
and then compute the values at new locations x in a second step using (8). Note
that (7) is a finite equation system by contrast to (3). By introducing the vectors
and matrices ¯h ¯ (i) j h m;¯ (z i ), c¯ (i) j c ¯ (z i ), \Phi ¯ (i; j) j k m;¯ (z j ; z i ) for i =
1; : : : ; m and j = 1; : : : ; m, the relative value iteration algorithm can thus be written
conveniently as (for details, see [Ber95, OG00]):
¯h k
new := min
a
(c a + \Phi a ¯h k ); ¯h k+1 := ¯h k
new \Gamma ¯h k
new (1): (9)
Hence we end up with an algorithm that is analogous to value iteration except that
we use the weighting matrix \Phi a in place of the usual transition probabilities and ¯h k
and c a are vectors of points in the training set S as opposed to vectors of states.
Intuitively, (9) assigns value estimates to the states in the sample trajectory and
updates these estimates in an iterative fashion. Here the update of each state is
based on a local average over the costs and values of the samples in its neighborhood.
Since \Phi a (i; j) ? 0 and
P m
j=1 \Phi a (i; j) = 1 we can further exploit the analogy between
(9) and the usual value iteration in an ``artificial'' MDP with transition probabilities
\Phi a to prove the following theorem:
Theorem 1 The relative value iteration (9) converges to a unique fixed point.
For details, the reader is referred to [OS00, OG00]. Note that Theorem 1 illustrates
a rather unique property of kernel­based reinforcement learning by comparison to
alternative approaches. In addition, we can show that -- under suitable regularity
conditions -- kernel­based reinforcement learning is consistent in the following sense:
Theorem 2 The approximate optimal cost “
jm converges to the true optimal cost
j \Lambda in the sense that
E x0 ;¯¯ j“j m \Gamma j \Lambda j m!1 \Gamma! 0:

Also, the true cost of the approximate strategy “
¯m converges to the optimal cost:
E x0 ; ¯ ¯ jj “ ¯m \Gamma j \Lambda j m!1
\Gamma! 0:
Hence “ ¯m performs as well as ¯ \Lambda asymptotically and we can also predict the op­
timal cost j \Lambda using “ jm . From a practical standpoint, Theorem 2 asserts that the
performance of approximate dynamic programming can be improved by increasing
the amount of training data. Note, however, that the computational complexity
of approximate dynamic programming depends on the sample size m. In detail,
the complexity of a single application of (9) is O(m 2 ) in a naive implementation
and O(m log m) in a more elaborate nearest neighbor approach. This complexity
issue prevents the use of very large data sets using the ``exact'' algorithm described
above. As in the case of parametric reinforcement learning, we can of course restrict
ourselves to a fixed amount of computational resources simply by discarding obser­
vations from the training data or by summarizing clusters of data using ``sufficient
statistics''. Note that the convergence property in Theorem 1 remains unaffected
by such an approximation.
4 Optimal Portfolio Choice
In this section, we describe the practical application of kernel­based reinforcement
learning to an investment problem where an agent in a financial market decides
whether to buy or sell stocks depending on the market situation. In the finance
and economics literature, this task is known as ``optimal portfolio choice'' and has
created an enormous literature over the past decades. Formally, let S t symbolize
the value of the stock at time t and let the investor choose her portfolio a t from the
set A j f0; 0:1; 0:2; : : : ; 1g, corresponding to the relative amount of wealth invested
in stocks as opposed to an alternative riskless asset. At time t + 1, the stock price
changes from S t to S t+1 , and the portfolio of the investor participates in the price
movement depending on her investment choice. Formally, if her wealth at time t is
W t , it becomes W t+1 =
i
1 + a t
S t+1 \GammaS t
S t
j
W t at time t + 1. To render this simulation
as realistic as possible, our investor is assumed to be risk­averse in that her fear of
losses dominates her appreciation of gains of equal magnitude. A standard way to
express these preferences formally is to aim at maximizing the expectation of a con­
cave ``utility function'', U(z), of the final wealth W T . Using the choice U(z) = log(z),
the investor's utility can be written as U(W T ) =
P T \Gamma1
t=0 log
i
1 + a t
S t+1 \GammaS t
S t
j
: Hence
utilities are additive over time, and the objective of maximizing E[U(W T )] can be
stated in an average­cost framework where c(x; a) = E x;a
h
log
i
1 + a S t+1 \GammaS t
S t
ji
.
We present results using simulated and real stock prices. With regard to the simu­
lated data, we adopt the common assumption in finance literature that stock prices
are driven by an Ito process with stochastic, mean­reverting volatility:
dS t = ¯S t dt + p
v t S t dB t ;
dv t = OE(¯v \Gamma v t )dt + ae p
v t d “
B t :
Here v t is the time­varying volatility, and B t and “
B t are independent Brownian mo­
tions. The parameters of the model are ¯ = 1:03, ¯
v = 0:3, OE = 10:0, and ae = 5:0. We

simulated daily data for the period of 13 years using the usual Euler approximation
of these equations. The resulting stock prices, volatilities, and returns are shown in
Figure 1. Next, we grouped the simulated time series into 10 sets of training and
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
70
80
90
100
110
120
130
140
150
160
S t
t
Stock Price
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
v t
Volatility
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
-0.1
-0.08
-0.06
-0.04
-0.02
0.02
0.04
0.06
0.08
0.1
t
return t
Return
Figure 1: The simulated time­series of stock prices (left), volatility (middle), and
daily returns (right; r t j log(S t =S t\Gamma1 )) over a period of one year.
test data such that the last 10 years are used as 10 test sets and the three years
preceding each test year are used as training data. Table 1 reports the training and
test performances on each of these experiments using kernel­based reinforcement
learning and a benchmark buy & hold strategy. Performance is measured using
Year Reinforcement Learning Buy & Hold
Training Test Training Test
4 0.129753 0.096555 0.058819 0.052533
5 0.125742 0.107905 0.043107 0.081395
6 0.100265 ­0.074588 0.053755 ­0.064981
7 0.059405 0.201186 0.018023 0.172968
8 0.082622 0.227161 0.041410 0.197319
9 0.077856 0.098172 0.074632 0.092312
10 0.136525 0.199804 0.137416 0.194993
11 0.145992 0.121507 0.147065 0.118656
12 0.126052 ­0.018110 0.125978 ­0.017869
13 0.127900 ­0.022748 0.077196 ­0.029886
Table 1: Investment performance on the simulated data (initial wealth W0 = 100).
the Sharpe­ratio which is a standard measure of risk­adjusted investment perfor­
mance. In detail, the Sharpe­ratio is defined as SR = log(W T =W 0 )=~oe where ~ oe is
the standard deviation of log(W t =W t\Gamma1 ) over time. Note that large values indicate
good risk­adjusted performance in years of positive growth, whereas negative val­
ues cannot readily be interpreted. We used the root of the volatility (standardized
to zero mean and unit variance) as input information and determined a suitable
choice for the bandwidth parameter (b = 1) experimentally. Our results in Table 1
demonstrate that reinforcement learning dominates buy & hold in eight out of ten
years on the training set and in all seven years with positive growth on the test set.
Table 2 shows the results of an experiment where we replaced the artificial time
series with eight years of daily German stock index data (DAX index, 1993­2000).
We used the years 1996­2000 as test data and the three years preceding each test
year for training. As the model input, we computed an approximation of the (root­
) volatility using a geometric average of historical returns. Note that the training
performance of reinforcement learning always dominates the buy & hold strategy,
and the test results are also superior to the benchmark except in the year 2000.

Year Reinforcement Learning Buy & Hold
Training Test Training Test
1996 0.083925 0.173373 0.038818 0.120107
1997 0.119875 0.121583 0.119875 0.096369
1998 0.123927 0.079584 0.096183 0.035204
1999 0.141242 0.094807 0.035137 0.090541
2000 0.085236 ­0.007878 0.081271 0.148203
Table 2: Investment performance on the DAX data.
5 Conclusions
We presented a new, kernel­based reinforcement learning method that overcomes
several important shortcomings of temporal­difference learning in continuous­state
domains. In particular, we demonstrated that the new approach always converges
to a unique approximation of the optimal policy and that the quality of this approx­
imation improves with the amount of training data. Also, we described a financial
application where our method consistently outperformed a benchmark model in an
artificial and a real market scenario. While the optimal portfolio choice problem is
relatively simple, it provides an impressive proof of concept by demonstrating the
practical feasibility of our method. Efficient implementations of local averaging for
large­scale problems have been discussed in the data mining community. Our work
makes these methods applicable to reinforcement learning, which should be valuable
to meet the real­time and dimensionality constraints of real­world problems.
Acknowledgements. The work of Dirk Ormoneit was partly supported by the Deutsche
Forschungsgemeinschaft. ' Saunak Sen helped with valuable discussions and suggestions.
References
[Ber95] D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 1 and
2. Athena Scientific, 1995.
[BM95] J. A. Boyan and A. W. Moore. Generalization in reinforcement learning: Safely
approximating the value function. In NIPS 7,1995.
[Gor99] G. Gordon. Approximate Solutions to Markov Decision Processes. PhD thesis,
Computer Science Department, Carnegie Mellon University, 1999.
[OG00] D. Ormoneit and P. Glynn. Kernel­based reinforcement learning in average­
cost problems. Working paper, Stanford University. In preparation.
[OS00] D. Ormoneit and '
S. Sen. Kernel­based reinforcement learning. Machine Learn­
ing, 2001. To appear.
[Rus97] J. Rust. Using randomization to break the curse of dimensionality. Economet­
rica, 65(3):487--516, 1997.
[SMSM00] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient meth­
ods for reinforcement learning with function approximation. In NIPS 12, 2000.
[TR96] J. N. Tsitsiklis and B. Van Roy. Feature­based methods for large­scale dynamic
programming. Machine Learning, 22:59--94, 1996.
[TR99] J. N. Tsitsiklis and B. Van Roy. Average cost temporal­difference learning.
Automatica, 35(11):1799--1808, 1999.
[VRK00] J. N. Tsitsiklis V. R. Konda. Actor­critic algorithms. In NIPS 12, 2000.

