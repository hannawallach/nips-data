Ensemble Learning and Linear Response Theory
for ICA
Pedro A.d.F.R. H¯jen≠S¯rensen 1 , Ole Winther 2 , Lars Kai Hansen 1
1 Department of Mathematical Modelling, Technical University of Denmark B321
DK≠2800 Lyngby, Denmark, phs,lkhansen@imm.dtu.dk
2 Theoretical Physics, Lund University, Sòolvegatan 14 A
S≠223 62 Lund, Sweden, winther@nimis.thep.lu.se
Abstract
We propose a general Bayesian framework for performing independent
component analysis (ICA) which relies on ensemble learning and lin≠
ear response theory known from statistical physics. We apply it to both
discrete and continuous sources. For the continuous source the underde≠
termined (overcomplete) case is studied. The naive mean≠field approach
fails in this case whereas linear response theory--which gives an improved
estimate of covariances--is very efficient. The examples given are for
sources without temporal correlations. However, this derivation can eas≠
ily be extended to treat temporal correlations. Finally, the framework
offers a simple way of generating new ICA algorithms without needing
to define the prior distribution of the sources explicitly.
1 Introduction
Reconstruction of statistically independent source signals from linear mixtures is an active
research field. For historical background and early references see e.g. [1]. The source
separation problem has a Bayesian formulation, see e.g., [2, 3] for which there has been
some recent progress based on ensemble learning [4].
In the Bayesian framework, the covariances of the sources are needed in order to estimate
the mixing matrix and the noise level. Unfortunately, ensemble learning using factorized
trial distributions only treats self≠interactions correctly and trivially predicts: hS i S i i
hS i ihS j i = 0 for i 6= j. This naive mean≠field (NMF) approximation first introduced in
the neural computing context by Ref. [5] for Boltzmann machine learning may completely
fail in some cases [6]. Recently, Kappen and Rodríêguez [6] introduced an efficient learning
algorithm for Boltzmann Machines based on linear response (LR) theory. LR theory gives
a recipe for computing an improved approximation to the covariances directly from the
solution to the NMF equations [7].
Ensemble learning has been applied in many contexts within neural computation, e.g. for
sigmoid belief networks [8], where advanced mean field methods such as LR theory or
TAP [9] may also be applicable. In this paper, we show how LR theory can be applied
to independent component analysis (ICA). The performance of this approach is compared
to the NMF approach. We observe that NMF may fail for high noise levels and binary

sources and for the underdetermined continuous case. In these cases the NMF approach
ignores one of the sources and consequently overestimates the noise. The LR approach on
the other hand succeeds in all cases studied.
The derivation of the mean≠field equations are kept completely general and are thus valid
for a general source prior (without temporal correlations). The final eqs. show that the
mean≠field framework may be used to propose ICA algorithms for which the source prior
is only defined implicitly.
2 Probabilistic ICA
Following Ref. [10], we consider a collection of N temporal measurements, X = fX dt g,
where X dt denotes the measurement at the dth sensor at time t. Similarly, let S = fSmtg
denote a collection of M mutually independent sources where Sm is the mth source which
in general may have temporal correlations. The measured signals X are assumed to be an
instantaneous linear mixing of the sources corrupted with additive Gaussian noise , that
is,
X = AS+ ; (1)
where A is the mixing matrix. Furthermore, to simplify this exposition the noise is assumed
to be iid Gaussian with variance  2 . The likelihood of the parameters is then given by,
P (XjA;  2 ) =
Z
dSP (XjA;  2 ; S) P (S) ; (2)
where P (S) is the prior on the sources which might include temporal correlations. We
will, however, throughout this paper assume that the sources are temporally uncorrelated.
We choose to estimate the mixing matrix A and noise level  2 by Maximum Likelihood
(ML≠II). The saddlepoint of P (XjA;  2 ) is attained at,
@ log P (XjA;  2 )
@A = 0 : A = XhSi T hSS T i 1 (3)
@ log P (XjA;  2 )
@ 2 = 0 :  2 =
1
DN hTr(X AS) T (X AS)i ; (4)
where hi denotes an average over the posterior and D is the number of sensors.
3 Mean field theory
First, we derive mean field equations using ensemble learning. Secondly, using linear
response theory, we obtain improved estimates of the off≠diagonal terms of hSS T i which
are needed for estimating A and  2 . The following derivation is performed for an arbitrary
source prior.
3.1 Ensemble learning
We adopt a standard ensemble learning approach and approximate
P (SjX; A; 2 ) =
P (XjA;  2 ; S)P (S)
P (XjA;  2 )
(5)
in a family of product distributions Q(S) =
Q
mt Q(Smt ). It has been shown in Ref. [11]
that for a Gaussian P (XjA;  2 ; S), the optimal choice of Q(Smt ) is given by a Gaussian
times the prior:
Q(Smt ) =
P (Smt )e 1
2
mtS 2
mt +mtSmt
R dSP (S)e 1
2
mtS 2 +mtS
: (6)

In the following, it is convenient to use standard physics notation to keep everything as
general as possible. We therefore parameterize the Gaussian as,
P (XjA;  2 ; S) = P (XjJ; h; S) = Ce 1
2
Tr(S T JS)+Tr(h T S) ; (7)
where J = A T A= 2 is the M M interaction matrix and h = A T X= 2 has the same
dimensions as the source matrix S. Note that h acts as an external field from which we can
obtain all moments of the sources. This is a property that we will make use of in the next
section when we derive the linear response corrections. The Kullback≠Leibler divergence
between the optimal product distribution Q(S) and the true source posterior is given by
KL =
Z
dSQ(S) ln
Q(S)
P (SjX; A; 2 )
= ln P (XjA;  2 ) ln ^
P (XjA;  2 ) (8)
ln ^
P (XjA;  2 ) =
X
mt
log
Z
dSP (S)e 1
2
mtS 2 +mtS +
1
2
X
mt
(J mm mt )hS 2
mt i
+
1
2
TrhS T i(J diag(J)hSi +Tr(h ) T hSi + ln C ; (9)
where ^
P (XjA;  2 ) is the naive mean field approximation to the Likelihood and diag(J) is
the diagonal matrix of J. The saddlepoints define the mean field equations;
@KL
@hSi = 0 :  = h + (J diag(J))hSi (10)
@KL
@hS 2
mt i
= 0 : mt = Jmm : (11)
The remaining two equations depend explicitly on the source prior, P (S);
@KL
@mt = 0 : hSmt i =
@
@mt log
Z
dSmtP (Smt )e 1
2
mtS 2
mt +mtSmt
 f(mt ; mt ) (12)
@KL
@mt = 0 : hS 2
mt i = 2
@
@mt log
Z
dSmtP (Smt )e 1
2
mtS 2
mt +mtSmt : (13)
In section 4, we calculate f(mt ; mt ) for some of the prior distributions found in the ICA
literature.
3.2 Linear response theory
As mentioned already, h acts as an external field. This makes it possible to calculate the
means and covariances as derivatives of log P (XjJ; h), i.e.
hSmt i =
@ log P (XjJ; h)
@hmt (14)
 tt 0
mm 0  hSmt Sm 0 t 0 i hSmt ihS m 0 t 0 i =
@ 2 log P (XjJ; h)
@hm 0 t 0 @hmt =
@hSmt i
@hm 0 t 0
: (15)
To derive an equation for  tt 0
mm 0
, we use eqs. (10), (11) and (12) to get
 tt 0
mm 0 =
@f(mt ; mt )
@mt
@mt
@hm 0 t 0
=
@f(mt ; mt )
@mt
0
@ X
m 00 ;m 00 6=m
Jmm 00  tt
m 00 m 0
+ ∆ mm 0
1
A ∆ tt 0 : (16)

-2 0 2
-2
-1
0
1
2
X 1
X
2
-2 0 2
-2
-1
0
1
2
X 1
X
2
-2 0 2
-2
-1
0
1
2
X 1
X
2
Figure 1: Binary source recovery for low noise level (M = 2, D = 2). Shows from left
to right: +/≠ the column vectors of; the true A (with the observations superimposed); the
estimated A (NMF); estimated A (LR).
-0.5 0 0.5
-0.5
0
0.5
X 1
X
2
-0.5 0 0.5
-0.5
0
0.5
X 1
X
2
0 20 40
0
0.1
0.2
0.3
0.4
iteration
variance
Figure 2: Binary source recovery for low noise level (M = 2, D = 2). Shows the dynamics
of the fix≠point iterations. From left to right; +/≠ the column vectors of A (NMF); +/≠ the
column vectors of A (LR); variance  2 (solid:NMF, dashed:LR, thick dash≠dotted: the true
empirical noise variance).
We now see that the ≠matrix factorizes in time  tt 0
mm 0 = ∆ tt 0  t
mm 0
. This is a direct conse≠
quence of the fact that the model has no temporal correlations. The above equation is linear
and may straightforwardly be solved to yield
 t
mm 0 =

( t J) 1

mm 0
; (17)
where we have defined the diagonal matrix
 t = diag
 
1
@f(1t ;1t )
@1t
+ J 11 ; : : : ; 1
@f(Mt ;Mt )
@Mt
+ JMM
!
:
At this point is appropriate to explain why linear response theory is more precise than us≠
ing the factorized distribution which predicts  t
mm 0 = 0 for non≠diagonal terms. Here,
we give an argument that can be found in Parisi's book on statistical field theory [7]:
Let us assume that the approximate and exact distribution is close in some sense, i.e.
Q(S) P (SjX; A; 2 ) = " then hSmt Sm 0 t i ex = hSmt Sm 0 t i ap + O("). Mean field the≠
ory gives a lower bound on the log≠Likelihood since KL, eq. (8) is non≠negaitive. Conse≠
quently, the linear term vanishes in the expansion of the log≠Likelihood: log P (XjA;  2 ) =
log ^
P (XjA;  2 ) + O(" 2 ). It is therefore more precise to obtain moments of the variables
through derivatives of the approximate log≠Likelihood, i.e. by linear response.
A final remark to complete the picture: if diag(J) in equation eq. (10) is exchanged with
 t = diag( 1t ; : : : ; Mt ) and likewise in the definition of  t above we get TAP equations
[9]. The TAP equation for mt is  t
mm = @f(mt ;mt )
@mt =

( t J) 1

mm .

-2 0 2
-2
-1
0
1
2
X 1
X
2
-2 0 2
-2
-1
0
1
2
X 1
X
2
-2 0 2
-2
-1
0
1
2
X 1
X
2
Figure 3: Binary source recovery for high noise level (M = 2, D = 2). Shows from left
to right: +/≠ the column vectors of; the true A (with the observations superimposed); the
estimated A (NMF); estimated A (LR).
-0.5 0 0.5
-0.5
0
0.5
X 1
X
2
-0.5 0 0.5
-0.5
0
0.5
X 1
X
2
0 200 400 600
0.2
0.3
0.4
0.5
0.6
0.7
iteration
variance
Figure 4: Binary source recovery for high noise level (M = 2, D = 2). Same plot as in
figure 2.
4 Examples
In this section we compare the LR approach and the NMF approach on the noisy ICA
model. The two approaches are demonstrated using binary and continous sources.
4.1 Binary source
Independent component analysis of binary sources (e.g. studied in [12]) is considered for
data transmission using binary modulation schemes such as MSK or biphase (Manchester)
codes. Here, we consider a binary source Smt 2 f1; 1g with prior distribution P (Smt ) =
1
2 [∆(S mt 1) + ∆(Smt + 1)]. In this case we get the well known mean field equations
hSmt i = tanh(mt ). Figures 1 and 2 show the results of the NMF approach as well as LR
approach in a low≠noise variance setting using two sources (M = 2) and two sensors (D =
2). Figures 3 and 4 show the same but in a high≠noise setting. The dynamical plots show
the trajectory of the fix≠point iteration where 'x' marks the starting point and 'o' the final
point. Ideally, the noise≠less measurements would consist of the four combinations (with
signs) of the columns in the mixing matrix. However, due to the noise, the measurement
will be scattered around these ``prototype'' observations.
In the low≠noise level setting both approaches find good approximations to the true mixing
matrix and sources. However, the convergence rate of the LR approach is found to be faster.
For high≠noise variance the NMF approach fails to recover the true statistics. It is seen that
one of the directions in the mixing matrix vanishes which in turn results in overestimating
the noise variance.

-5 0 5
-10
-5
0
5
10
X 1
X
2
-2 0 2
-2
-1
0
1
2
-2 0 2
-2
-1
0
1
2
X 1
X
2
-2 0 2
-2
-1
0
1
2
X 1
X
2
Figure 5: Overcomplete continuous source recovery with M = 3 and D = 2. Shows
from left to right: the observations, +/≠ the column vectors of; the true A; the estimated A
(NMF); estimated A (LR).
-2 0 2
-2
-1
0
1
2
X 1
X
2
-2 0 2
-2
-1
0
1
2
X 1
X
2
0 1000 2000
0.5
1
1.5
2
2.5
iteration
variance
Figure 6: Overcomplete continuous source recovery with M = 3 and D = 2. Same plot as
in figure 2. Note that the initial iteration step for A is very large.
4.2 Continuous Source
To give a tractable example which illustrates the improvement by LR, we consider the
Gaussian prior P (Smt ) / exp( S 2
mt =2) (not suitable for source separation). This leads
to f(mt ; mt ) = mt =( mt ). Since we have a factorized distribution, ensemble
learning predicts hSmt Sm 0 t 0 i hSmt ihS m 0 t 0 i = ∆ mm 0 ∆ tt 0 ( mt ) 1 = ∆ mm 0 ∆ tt 0 (
Jmm ) 1 , where the second equality follows from eq. (11). Linear response eq. (17) gives
hSmt Sm 0 t 0 i hSmt ihS m 0 t 0 i = ∆ tt 0

(I J) 1

mm 0
which is identical with the exact
result obtained by direct integration.
For the popular choice of prior P (Smt ) = 1
 cosh Smt [1], it is not possible to derive
f(mt ; mt ) analytically. However, f(mt ; mt ) can be calculated analytically for the
very similar Laplace distribution. Both these examples have positive kurtosis.
Mean field equations for negative kurtosis can be obtained using the prior P (Smt ) /=
exp( (Smt ) 2 =2) + exp( (Smt + ) 2 =2) [1] leading to
hSmt i =
1
1 mt

mt +  tanh
 mt
1 mt

:
Figure 5 and 6 show simulations using this source prior with  = 1 in an overcomplete
setting with D = 2 and M = 3. Note that  = 1 yields a unimodal source distribution
and hence qualitatively different from the bimodal prior considered in the binary case. In
the overcomplete setting the NMF approach fails to recover the true sources. See [13] for
further discussion of the overcomplete case.

5 Conclusion
We have presented a general ICA mean field framework based upon ensemble learning
and linear response theory. The naive mean≠field approach (pure ensemble learning) fails
in some cases and we speculate that it is incapable of handling the overcomplete case
(more sources than sensors). Linear response theory, on the other hand, succeeds in all the
examples studied.
There are two directions in which we plan to extend this work: (1) to sources with temporal
correlations and (2) to source models defined not by a parametric source prior, but directly
in terms of the function f , which defines the mean field equations. Starting directly from
the f≠function makes it possible to test a whole range of implicitly defined source priors.
A detailed analysis of a large selection of constrained and unconstrained source priors as
well as comparisons of LR and the TAP approach can be found in [14].
Acknowledgments
PHS wishes to thank Mike Jordan for stimulating discussions on the mean field and vari≠
ational methods. This research is supported by the Swedish Foundation for Strategic Re≠
search as well as the Danish Research Councils through the Computational Neural Network
Center (CONNECT) and the THOR Center for Neuroinformatics.
References
[1] T.≠W. Lee: Independent Component Analysis, Kluwer Academic Publishers, Boston (1998).
[2] A. Belouchrani and J.≠F. Cardoso: Maximum Likelihood Source Separation by the Expectation≠
Maximization Technique: Deterministic and Stochastic Implementation In Proc. NOLTA, 49--53
(1995).
[3] D. MacKay: Maximum Likelihood and Covariant Algorithms for Independent Components
Analysis. ``Draft 3.7'' (1996).
[4] H. Lappalainen and J.W. Miskin: Ensemble Learning, Advances in Independent Component
Analysis, Ed. M. Girolami, In press (2000).
[5] C. Peterson and J. Anderson: A Mean Field Theory Learning Algorithm for Neural Networks,
Complex Systems 1, 995--1019 (1987).
[6] H. J. Kappen and F. B. Rodríêguez: Efficient Learning in Boltzmann Machines Using Linear
Response Theory, Neural Computation 10, 1137--1156 (1998).
[7] G. Parisi: Statistical Field Theory, Addison Wesley, Reading Massachusetts (1988).
[8] L. K. Saul, T. Jaakkola and M. I. Jordan: Mean Field Theory of Sigmoid Belief Networks,
Journal of Artificial Intelligence Research 4, 61--76 (1996).
[9] M. Opper and O. Winther: Tractable Approximations for Probabilistic Models: The Adaptive
TAP Mean Field Approach, Submitted to Phys. Rev. Lett. (2000).
[10] L. K. Hansen: Blind Separation of Noisy Image Mixtures, Advances in Independent Component
Analysis, Ed. M. Girolami, In press (2000).
[11] L. Csatío, E. Fokouíe, M. Opper, B. Schottky and O. Winther: Efficient Approaches to Gaussian
Process Classification, in Advances in Neural Information Processing Systems 12 (NIPS'99),
Eds. S. A. Solla, T. K. Leen, and K.≠R. Mòuller, MIT Press (2000).
[12] A.≠J. van der Veen: Analytical Method for Blind Binary Signal Separation IEEE Trans. on
Signal Processing 45(4) 1078--1082 (1997).
[13] M. S. Lewicki and T. J. Sejnowski: Learning Overcomplete Representations, Neural Computa≠
tion 12, 337--365 (2000).
[14] P. A. d. F. R. H¯jen≠S¯rensen, O. Winther and L. K. Hansen: Mean Field Approaches to Inde≠
pendent Component Analysis, In preparation.

