Spike-Timing-Dependent Learning for
Oscillatory Networks
Silvia Scarpetta
Dept. of Physics \E.R. Caianiello"
Salerno University 84081 (SA) Italy
and INFM, Sezione di Salerno Italy
scarpetta@na.infn.it
Zhaoping Li
Gatsby Comp. Neurosci. Unit
University College, London, WC1N 3AR
United Kingdom
zhaoping@gatsby.ucl.ac.uk
John Hertz
Nordita
2100 Copenhagen , Denmark
hertz@nordita.dk
Abstract
We apply to oscillatory networks a class of learning rules in which
synaptic weights change proportional to pre- and post-synaptic ac-
tivity, with a kernel A() measuring the eect for a postsynaptic
spike a time  after the presynaptic one. The resulting synaptic ma-
trices have an outer-product form in which the oscillating patterns
are represented as complex vectors. In a simple model, the even
part of A() enhances the resonant response to learned stimulus by
reducing the eective damping, while the odd part determines the
frequency of oscillation. We relate our model to the olfactory cortex
and hippocampus and their presumed roles in forming associative
memories and input representations.
1 Introduction
Recent studies of synapses between pyramidal neocortical and hippocampal neu-
rons [1, 2, 3, 4] have revealed that changes in synaptic eÆcacy can depend on the
relative timing of pre- and postsynaptic spikes. Typically, a presynaptic spike fol-
lowed by a postsynaptic one leads to an increase in eÆcacy (LTP), while the reverse
temporal order leads to a decrease (LTD). The dependence of the change in synap-
tic eÆcacy on the dierence  between the two spike times may be characterized
by a kernel which we denote A() [4]. For hippocampal pyramidal neurons, the
half-width of this kernel is around 20 ms.
Many important neural structures, notably hippocampus and olfactory cortex, ex-
hibit oscillatory activity in the 20-50 Hz range. Here the temporal variation of the
neuronal ring can clearly aect the synaptic dynamics, and vice versa. In this
paper we study a simple model for learning oscillatory patterns, based on the struc-
ture of the kernel A() and other known physiology of these areas. We will assume

that these synaptic changes in long range lateral connections are driven by oscilla-
tory, patterned input to a network that initially has only local synaptic connections.
The result is an imprinting of the oscillatory patterns in the synapses, such that
subsequent input of a similar pattern will evoke a strong resonant response. It can
be viewed as a generalization to oscillatory networks with spike-timing-dependent
learning of the standard scenario whereby stationary patterns are stored in Hopeld
networks using the conventional Hebb rule.
2 Model
The computational neurons of the model represent local populations of biological
neurons that share common input. They follow the equations of motion [5]
_
u i = u i  0
i g v (v i ) +
X
j
J 0
ij g u (u j ) + I i ; (1)
_
v i = v i +  0
i g u (u i ) +
X
j 6=i
W 0
ij g u (u j ): (2)
Here u i and v i are membrane potentials for excitatory and inhibitory (formal)
neuron i,  1 is their membrane time constant, and the sigmoidal functions
g u ( ) and g v ( ) model the dependence of their outputs (interpreted as instanta-
neous ring rates) on their membrane potentials. The couplings  0
i and  0
i are
inhibitory-to-excitatory (resp. excitatory-to-inhibitory) connection strengths within
local excitatory-inhibitory pairs, and for simplicity we take the external drive I i (t)
to act only on the excitatory units. We include nonlocal excitatory couplings J 0
ij
between excitatory units and W 0
ij from excitatory units to inhibitory ones. In this
minimal model, we ignore long-range inhibitory couplings, appealing to the fact
that real anatomical inhibitory connections are predominantly short-ranged. (In
what follows, we will sometimes use bold and sans serif notation (e.g., u, J) for
vectors and matrices, respectively.) The structure of the couplings is shown in Fig.
1A.
The model is nonlinear, but here we will limit our treatment to an analysis of small
oscillations around a stable xed point f u;  vg determined by the DC part of the
input. Performing the linearization and eliminating the inhibitory units [6, 5], we
obtain

u + [2 J] _
u + [ 2 + ( +W) J]u = (@ t + )ÆI: (3)
Here u is now measured from the xed point  u, ÆI is the time-varying part of the
input, and the elements of J and W are related to those of J 0 and W 0 by W ij =
g 0
u (u j )W 0
ij and J ij = g 0
u (u j )J 0
ij . For simplicity, we have assumed that the eective
local couplings  i = g 0
v (v i ) 0
i and  i = g 0
u (u i ) 0
i are independent of i:  i = ,
 i = . With oscillatory inputs ÆI = e
i!t + c:c:, the oscillatory pattern elements
 i = j i je i i are complex, reecting possible phase dierences across the units.
We likewise separate the response u = u + + u (after the initial transients) into
positive- and negative-frequency components u  (with u = u + and u  / e i!t ).
Since _
u  = i!u  , Eqn. (3) can be written

2 
i
! ( 2 +  ! 2 )

u  = M  u  +

1 
i
!

ÆI  ; (4)
a form that shows how the matrix
M  (!)  J 
i
! (W J): (5)
describes the eective coupling between local oscillators. 2 is the intrinsic damping
and
p
 2 +  the frequency of the individual oscillators.

A
+
­
+
­
uj
vj
bi
g i
o
Ii
ij
W o
ij
J o
u
v i
o
B.1 B.2
Figure 1: A. The model: In addition to the local excitatory-inhibitory connections
(vertical solid lines), there are nonlocal long-range connections (dashed lines) be-
tween excitatory units (J ij ) and from excitatory to inhibitory units (W ij ). External
inputs are fed to the excitatory units. B: Activation function used in simulations
for excitatory units (B.1) and inhibitory units (B.2). Crosses mark the equilibrium
point (u;  v) of the system.
2.1 Learning phase
We employ a generalized Hebb rule of the form
ÆC ij (t) = 
Z T
0
dt
Z 1
1
d y i (t + )A()x j (t) (6)
for synaptic weight C ij , where x j and y i are the pre- and postsynaptic activities,
measured relative to stationary levels at which no changes in synaptic strength
occur. We consider a general kernel A( ), although experimentally A() > 0 (< 0)
for  > 0 (< 0). We will apply the rule to both J and W in our linearized network,
where the ring rates g u (u i ) and g v (v i ) vary linearly with u i and v i , so we will
use Eqn. (6) with x j = u j and y i = u i or v i (measured from the xed point  v i ),
respectively.
We assume oscillatory input ÆI = 
0 e i!0 t + c:c: during learning. In the brain
structures we are modeling, cholinergic modulation makes the long-range connec-
tions ineective during learning [7]. Thus we set J = W = 0 in Eqn. (3) and
nd
u +
i = (! 0 + i) 0
i e i!0 t
2! 0 + i( 2 +  ! 2
0 )  U 0  0
i e i!0 t (7)
and, from (@ t + )v i = u i ,
v +
i = 
i! 0 +  U 0  0
i e i!0 t : (8)
Using these in the learning rule (6) leads to
J ij = 2J 0 Re [ ~
A(! 0 ) 0
i  0
j ]; W ij = 2( W = J )J 0 Re
 ~
A(!0 ) 0
i  0
j
 i!0

; (9)
where ~
A(!) =
R 1
1 d A()e i! is the Fourier transform of A( ), J 0 =
2 J jU 0 j 2 =! 0 , and  J(W) are the respective learning rates. When the rates are tuned
such that  J = W =( 2 +! 2
0 ) and when ! = ! 0 , we have M +
ij = J 0 ~
A(! 0 ) 0
i  0
j , a

generalization of the outer-product learning rule to the complex patterns 
 from
the Hopeld-Hebb form for real-valued patterns. For learning multiple patterns 
 ,
 = 1; 2; :::, the learned weights are simply sums of contributions from individual
patterns like Eqns. (9) with  0
i replaced by  
i .
2.2 Recall phase
We return to the single-pattern problem and study the simple case when  J =
W =( 2 + ! 2
0 ). Consider rst an input pattern ÆI = e i!t + c:c: that matches
the stored pattern exactly ( = 
0 ), but possibly oscillating at a dierent frequency.
We then nd, using Eqns. (9) in Eqn. (3), the (positive-frequency) response
u + = (! + i) 0 e i!t
2! J0
2 (! + ! 0 ) ~
A 0 (! 0 ) + i[ 2 +  J0
2 (! + ! 0 ) ~
A 00 (! 0 ) ! 2 ]
: (10)
where ~
A 0 (! 0 )  Re ~
A(! 0 ) and ~
A 00 (! 0 )  Im ~
A(! 0 ). For strong response at ! = ! 0 ,
we require
! 0 =
q
 2 +  J 0 ! 0 ~
A 00 (! 0 ); J 0 ~
A 0 (! 0 )  2: (11)
This means (1) the resonance frequency ! 0 is determined by ~
A 00 , (2) the eective
damping 2 J 0 ~
A 0 should be small, and (3) deviation of ! from ! 0 reduces the
responses.
It is instructive to consider the case where the width of the time window for synaptic
change is small compared with the oscillation period. Then we can expand ~
A(! 0 )
in ! 0 :
~
A 0 (! 0 ) 
R dA()  a 0 ; ~
A 00 (! 0 )  ! 0
R dA()  ! 0 a 1 : (12)
In particular, A() = Æ() gives a 0 = 1 and a 1 = 0 and the conventional Hebbian
learning [5]. Experimentally, a 1 > 0 , implying a resonant frequency greater than the
intrinsic local frequency,
p
 2 +  obtained in the absence of long-range coupling.
If the drive  does not match the stored pattern (in phase and amplitude), the
response will consist of two terms. The rst has the form of Eqn. (10) but reduced
in amplitude by an overlap factor 
0  . (For convenience we use normalized
pattern vectors.) The second term is proportional to the part of  orthogonal to
the stored pattern. The J and W matrices do not act in this subspace, so the
frequency dependence of this term is just that of uncoupled oscillators, i.e., Eqn.
(10) with J 0 set equal to zero. This response is always highly damped and therefore
small.
It is straightforward to extend this analysis to multiple imprinted patterns. The
response consists of a sum of terms, one for each stored pattern. The term for each
stored pattern is just like that just described in the single-stored-pattern case: it
has one part for the input component parallel to the stored pattern and another
part for the component orthogonal to the stored pattern.
We note that, in this linear analysis, an input which overlaps several stored pat-
terns will (if the imprinting and input frequencies match) evoke a resonant response
which is a linear combination of the stored patterns. Thus, a network tuned to
operate in a nearly linear regime is able to interpolate in forming its representation
of the input. For categorical associative memory, on the other hand, a network has
to work in the extreme nonlinear limit, responding with only the strongest stored
pattern in an input mixture. As our network operates near the threshold for sponta-
neous oscillations, we expect that it should exhibit properties intermediate between

A B C
20 30 40 50 60
0
100
200
w(Hz)
Amplitude
0 0.2 0.4 0.6 0.8 1
0
100
200
Overlap
Amplitude
0 45 90
0
45
90
Input angle (degrees)
Output
angle
(degrees)
Figure 2: Circles show non-linear simulation results, stars show the linear simulation
results, while the dotted line show the analytical prediction for the linearized model.
A. Importance of frequency match: amplitude of response of output units as a
function of the frequency of the current input. The frequency of the imprinted
pattern is 41 Hz. B.Importance of amplitude and phase mismatch: amplitude of
response as a function of overlap between current input and imprinted pattern (i.e.
j 0  j), for dierent presented input patterns . C: Input - output relationship
when two orthogonal patterns 
1 and 
2 , have been imprinted at the same frequency
! = 41Hz. The angle of input pattern with respect to 
1 is shown as a function of
the angle of the output pattern with respect to 
1 , for many dierent input patterns.
these limits. We nd that this is indeed the case in the simulations reported in
the next section. From our analysis it turns out that the network behaves like a
Hopeld-memory (separate basins, without interpolation capability) for patterns
with dierent imprinting frequencies, but at the same time it is able to interpolate
among patterns which share a common frequency.
3 Simulations
Checking the validity of our linear approximation in the analysis, we performed
numerical simulations of both the non-linear equations (1,2) and the linearized ones
(3). We simulated the recall phase of a network consisting of 10 excitatory and 10
inhibitory cells. The connections J ij and W ij were calculated from Eqns. (9), where
we used the approximations (12) for the kernel shape A( ). Parameters were set
in such a way that the selective resonance was in the 40-Hz range. In non-linear
simulations we used dierent piecewise linear activation functions for g u ( ) and g v ( ),
as shown in Fig.1B. We chose the parameters of the functions g u ( ) and g v ( ) so that
the network equilibrium points 
u i ; 
v i were close to, but below, the high-gain region,
i.e. at the points marked with crosses in Fig. 1B.
The results conrm that when the input pattern matches the imprinted one in
frequency, amplitude and phase, the network responds with strong resonant oscil-
lations. However, it does not resonate if the frequencies do not match, as shown
in the frequency tuning curve in Fig. 2A. The behavior when the two frequencies
are close to each other diers in the linear and nonlinear cases. However, in both
cases a sharp selectivity in frequency is observed. The dependence on the overlap
between the input and the stored pattern is shown in Fig. 2B. The non-linear case,
indicated by circles, should be compared with the linear case, where the amplitude
is always linear in the overlap. In the nonlinear case, the linearity holds roughly
only for overlaps lower than about 0.4; for larger overlaps the amplication is as
high as for the perfect match case. This means that input patterns with an overlap
with the imprinted one greater than 0.4 lie within the basis of attraction of the

! = ! 1 ! = ! 2 ! = ! 2 ! = ! 2
 = 
1
 = 
2
 = 1
=a
1 +b 2
0 200 400
-50
0
50
u 2
0 200 400
-50
0
50
u 3
0 200 400
-50
0
50
u 6
0 200 400
-50
0
50
u 2
0 200 400
-50
0
50
u 3
0 200 400
-50
0
50
u 6
0 200 400
-50
0
50
u 2
0 200 400
-50
0
50
u 3
0 200 400
-50
0
50
u 6
0 200 400
-50
0
50
u 2
0 200 400
-50
0
50
u 3
0 200 400
-50
0
50
u 6
Figure 3: Frequency selectivity: Response evoked on 3 of the 10 neurons. Oscillatory
patterns 
1 e i!1 t + c:c: and 
2 e i!2 t c:c: have been imprinted, with 
1 ? 
2 and
! 1 = 41 Hz, ! 2 = 63 Hz. During the learning phases the parameter a 1 of kernel was
tuned appropriately, i.e. a 1 = 0:1 when imprinting 
1 and a 1 = 1:1 when imprinting

2 .
imprinted pattern.
The response elicited when two orthogonal patterns have been imprinted with the
same frequency is shown in Fig. 2C. Let 
1 e i!o t + c:c: and 
2 e i!o t + c:c: denote
the imprinted patterns, and e
i!o t + c:c: be the input to the trained network. In
both linear and non-linear simulations the network responds vigorously(with high-
amplitude oscillations) to the drive if  is in the subspace spanned by the imprinted
patterns, and fails to respond appreciably if  is orthogonal to that plane. When
the input pattern  is in the plane spanned by the stored patterns, the resonant
response u also lies in this plane. However, while in the linear case the output is
proportional to the input, in agreement with the analytical analysis, in the non-
linear case there are preferred directions, in the stored pattern plane. The gure
shows that, in case simulated here, there are three stable attractors: 
1 , 
2 , and
the symmetric linear combination ( 1 + 
2 )=
p
2).
Finally we performed linear simulations storing two orthogonal patterns 
1 e i!1 t +
c:c: and 
2 e i!2 t + c:c: with two dierent imprinting frequencies. Fig. 3 shows a
good performance of the network in separating the basins of attraction in this case.
The response to a linear combination of the two patterns, (a 1 + b 2 )e i!2 t + c:c:
is proportional to the part of the input whose imprinting frequency matches the
current driving frequency. Linear combinations of the two imprinted patterns are
not attractors if the two patterns do not share the same imprinting frequency.
4 Summary and Discussion
We have presented a model of learning for memory or input representations in neural
networks with input-driven oscillatory activity. The model structure is an abstrac-

tion of the hippocampus or the olfactory cortex. We propose a simple generalized
Hebbian rule, using temporal-activity-dependent LTP and LTD, to encode both
magnitudes and phases of oscillatory patterns into the synapses in the network. Af-
ter learning, the model responds resonantly to inputs which have been learned (or,
for networks which operate essentially linearly, to linear combinations of learned in-
puts), but negligibly to other input patterns. Encoding both amplitude and phase
enhances computational capacity, for which the price is having to learn both the
excitatory-to-excitatory and the excitatory-to-inhibitory connections. Our model
puts contraints on the form of the learning kernal A() that should be experime-
nally observed, e.g., for small oscillation frequencies, it requires that the overall LTP
dominates the overall LTD, but this requirement should be modied if the stored
oscillations are of high frequencies. Plasticity in the excitatory-to-inhibitory connec-
tions (for which experimental evidence and investigation is still scarce) is required
by our model for storing phase locked but unsynchronous oscillation patterns.
As for the Hopeld model, we distinguish two functional phases: (1) the learning
phase, in which the system is clamped dynamically to the external inputs and (2)
the recall phase, in which the system dynamics is determined by both the external
inputs and the internal interactions.
A special property of our model in the linear regime is the following interpolation
capability: under a given oscillation frequency, once the system has learned a set of
representation states, all other states in the subspace spanned by the learned states
can also evoke vigorous responses. Hippocampal place cells could employ such a
representation. Each cell has a localised \place eld", and the superposition of
activity of several cells wth nearby place elds can represent continuously-varying
position. The locality of the place elds also means that this representation is
conservative (and thus robust), in the sense that interpolation does not extend
beyond the spatial range of the experienced locations or to locations in between
two learned but distant and disjoint spatial regions.
Of course, this interpolation property is not always desirable. For instance, in cat-
egorical memory, one does not want inputs which are linear combinations of stored
patterns to elicit responses which are also similar linear combinations. Suitable
nonlinearity can (as we saw in the last section), enable the system to perform cate-
gorization: one way involved storing dierent patterns (or, by implication, dierent
classes of patterns) at dierent frequencies. For instance, in a multimodal area,
\place elds" might be stored at one oscillation frequency, and (say) odor mem-
ories at another. It seems likely to us that the brain may employ dierent kinds
and degrees of nonlinearity in dierent areas or at dierent times to enhance the
versatility of its computations.
References
[1] H Markram, J Lubke, M Frotscher, and B Sakmann, Science 275 213 (1997).
[2] J C Magee and D Johnston, Science 275 209 (1997).
[3] D Debanne, B H Gahwiler, and S M Thompson, J Physiol 507 237 (1998).
[4] G Q Bi and M M Poo, J Neurosci 18 10464 (1998).
[5] Z Li and J Hertz, Network: Computation in Neural Systems 11 83-102 (2000).
[6] Z Li and J J Hopeld, Biol Cybern 61 379-92 (1989).
[7] M E Hasselmo, Neural Comp 5 32-44 (1993).

