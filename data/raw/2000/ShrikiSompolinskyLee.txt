An Information Maximization Approach to
Overcomplete and Recurrent Representations
Oren Shriki and Haim Sompolinsky
Racah Institute of Physics and
Center for Neural Computation
Hebrew University
Jerusalem, 91904, Israel
Daniel D. Lee
Bell Laboratories
Lucent Technologies
Murray Hill, NJ 07974
Abstract
The principle of maximizing mutual information is applied to learning
overcomplete and recurrent representations. The underlying model con­
sists of a network of input units driving a larger number of output units
with recurrent interactions. In the limit of zero noise, the network is de­
terministic and the mutual information can be related to the entropy of
the output units. Maximizing this entropy with respect to both the feed­
forward connections as well as the recurrent interactions results in simple
learning rules for both sets of parameters. The conventional independent
components (ICA) learning algorithm can be recovered as a special case
where there is an equal number of output units and no recurrent con­
nections. The application of these new learning rules is illustrated on a
simple two­dimensional input example.
1 Introduction
Many unsupervised learning algorithms such as principal component analysis, vector quan­
tization, self­organizing feature maps, and others use the principle of minimizing recon­
struction error to learn appropriate features from multivariate data [1, 2]. Independent
components analysis (ICA) can similarly be understood as maximizing the likelihood of
the data under a non­Gaussian generative model, and thus is related to minimizing a re­
construction cost [3, 4, 5]. On the other hand, the same ICA algorithm can also be derived
without regard to a particular generative model by maximizing the mutual information be­
tween the data and a nonlinearly transformed version of the data [6]. This principle of
information maximization has also been previously applied to explain optimal properties
for single units, linear networks, and symplectic transformations [7, 8, 9].
In these proceedings, we show how the principle of maximizing mutual information can
be generalized to overcomplete as well as recurrent representations. In the limit of zero
noise, we derive gradient descent learning rules for both the feedforward and recurrent
weights. Finally, we show the application of these learning rules to some simple illustrative
examples.

M output variables
N input variables
x 1 x N
s 1 s 2 s 3 s M
11
W MN
W
12
K 3M
K
Feedforward
Weights
Recurrent
Weights
Figure 1: Network diagram of an overcomplete, recurrent representation. x are input data
which influence the output signals s through feedforward connections W . The signals s
also interact with each other through the recurrent interactions K.
2 Information Maximization
The ``Infomax'' formulation of ICA considers the problem of maximizing the mutual in­
formation between N­dimensional data observations fxg which are input to a network
resulting in N­dimensional output signals fsg [6]. Here, we consider the general problem
where the signals s are M­dimensional with M  N . Thus, the representation is overcom­
plete because there are more signal components than data components. We also consider
the situation where a signal component s i can influence another component s j through a
recurrent interaction K ji . As a network, this is diagrammed in Fig. 1 with the feedfor­
ward connections described by the M N matrix W and the recurrent connections by the
M M matrix K. The network response s is a deterministic function of the input x:
s i = g
0
@ N
X
j=1
W ij x j +
M
X
k=1
K ik s k
1
A (1)
where g is some nonlinear squashing function. In this case, the mutual information between
the inputs x and outputs s is functionally only dependent on the entropy of the outputs:
I(s; x) = H(s) H(sjx)  H(s): (2)
The distribution of s is a N­dimensional manifold embedded in a M­dimensional vector
space and nominally has a negatively divergent entropy. However, as shown in Appendix
1, the probability density of s can be related to the input distribution via the relation:
P (s) /
P (x)
p
det( T )
(3)
where the susceptibility (or Jacobian) matrix  is defined as:
 ij =
@s i
@x j
: (4)
This result can be understood in terms of the singular value decomposition (SVD) of the
matrix . The transformation performed by  can be decomposed into a series of three
transformations: an orthogonal transformation that rotates the axes, a diagonal transfor­
mation that scales each axis, followed by another orthogonal transformation. A volume
element in the input space is mapped onto a volume element in the output space, and its
volume change is described by the diagonal scaling operation. This scale change is given

by the product of the square roots of the eigenvalues of  T . Thus, the relationship be­
tween the probability distribution in the input and output spaces includes the proportionality
factor,
p
det( T ), as formally derived in Appendix 1.
We now get the following expression for the entropy of the outputs:
H(s) 
Z
dxP (x) log
 
P (x)
p
det( T )
!
=
1
2

log det( T )

+H(x); (5)
where the brackets indicate averaging over the input distribution.
3 Learning rules
From Eq. (5), we see that minimizing the following cost function:
E =
1
2
Tr hlog( T )i; (6)
is equivalent to maximizing the mutual information. We first note that the susceptibility 
satisfies the following recursion relation:
 ij = g 0
i 
 
W ij +
X
k
K ik  kj
!
= (GW +GK) ij ; (7)
where G ij = Æ ij g 0
i and g 0
i  g 0
 P
j W ij x j +
P
k K ik s k

.
Solving for  in Eq. (7) yields the result:
 = (G 1 K) 1 W = W; (8)
where  1  G 1 K.  ij can be interpreted as the sensitivity in the recurrent network
of the ith unit's output to changes in the total input of the jth unit.
We next derive the learning rules for the network parameters using gradient descent, as
shown in detail in Appendix 2. The resulting expression for the learning rule for the feed­
forward weights is:
W =  @E
@W =

 T + T x T

(9)
where  is the learning rate, the matrix is defined as
= ( T ) 1  T  (10)
and the vector  is given by
 i = () ii
g 00
i
(g 0
i ) 3
: (11)
Multiplying the gradient in Eq. (9) by the matrix (WW T ) yields an expression analogous
to the ``natural'' gradient learning rule [10]:
W = W I
+
  T x T

: (12)
Similarly, the learning rule for the recurrent interactions is
K =  @E
@K =

 () T + T s T

: (13)
In the case when there are equal numbers of input and output units, M = N , and there
are no recurrent interactions, K = 0, most of the previous expressions simplify. The
susceptibility matrix  is diagonal,  = G, and = W 1 . Substituting back into Eq. (9)
for the learning rule for W results in the update rule:
W = 

(W T ) 1
+
 zx T
 ; (14)
where z i = g 00
i =g 0
i . Thus, the well­known Infomax ICA learning rule is recovered as a
special case of Eq. (9) [6].

(a) (b) (c)
Figure 2: Results of fitting 3 filters to a 2­dimensional hexagon distribution with 10000
sample points.
4 Examples
We now apply the preceding learning algorithms to a simple two­dimensional (N = 2)
input example. Each input point is generated by a linear combination of three (two­
dimensional) unit vectors with angles of 0 Æ , 120 Æ and 240 Æ . The coefficients are taken
from a uniform distribution on the unit interval. The resulting distribution has the shape
of a unit hexagon, which is slightly more dense close to the origin than at the boundaries.
Samples of the input distribution are shown in Fig. 2. The second order cross correlations
vanish, so that all the structure in the data is described only by higher order correlations.
We fix the sigmoidal nonlinearity to be g(x) = tanh(x).
4.1 Feedforward weights
A set of M = 3 overcomplete filters for W are learned by applying the update rule in
Eq. (9) to random normalized initial conditions while keeping the recurrent interactions
fixed at K = 0. The length of the rows of W were constrained to be identical so that the
filters are projections along certain directions in the two­dimensional space. The algorithm
converged after about 20 iterations. Examples of the resulting learned filters are shown
by plotting the rows of W as vectors in Fig. 2. As shown in the figure, there are several
different local minimum solutions. If the lengths of the rows of W are left unconstrained,
slight deviations from these solutions occur, but relative orientation differences of 60 Æ or
120 Æ between the various filters are preserved.
4.2 Recurrent interactions
To investigate the effect of recurrent interactions on the representation, we fixed the feed­
forward weights in W to point in the directions shown in Fig. 2(a), and learned the optimal
recurrent interactions K using Eq. (13). Depending upon the length of the rows of W
which scaled the input patterns, different optimal values are seen for the recurrent connec­
tions. This is shown in Fig. 3 by plotting the value of the cost function against the strength
of the uniform recurrent interaction. For small scaled inputs, the optimal recurrent strength
is negative which effectively amplifies the output signals since the 3 signals are negatively
correlated. With large scaled inputs, the optimal recurrent strength is positive which tend to
decrease the outputs. Thus, in this example, optimizing the recurrent connections performs
gain control on the inputs.

­1.5 ­1 ­0.5 0 0.5 1 1.5
­1
­0.5
0
0.5
1
1.5
2
2.5
3
k
Cost
|W|=1 |W|=5
Figure 3: Effect of adding recurrent interactions to the representation. The cost function
is plotted as a function of the recurrent interaction strength, for two different input scaling
parameters.
5 Discussion
The learned feedforward weights are similar to the results of another ICA model that can
learn overcomplete representations [11]. Our algorithm, however, does not need to perform
approximate inference on a generative model. Instead, it directly maximizes the mutual in­
formation between the outputs and inputs of a nonlinear network. Our method also has
the advantage of being able to learn recurrent connections that can enhance the representa­
tional power of the network. We also note that this approach can be easily generalized to
undercomplete representations by simply changing the order of the matrix product in the
cost function. However, more work still needs to be done in order to understand technical
issues regarding speed of convergence and local minima in larger applications. Possible
extensions of this work would be to optimize the nonlinearity that is used, or to adaptively
change the number of output units to best match the input distribution.
We acknowledge the financial support of Bell Laboratories, Lucent Technologies, and the
US­Israel Binational Science Foundation.
6 Appendix 1: Relationship between input and output distributions
In general, the relation between the input and output distributions is given by
P (s) =
Z
dxP (x)P (sjx): (15)
Since we use a deterministic mapping, the conditional distribution of the response given
the input is given by P (sjx) = Æ(s g(Wx + Ks)). By adding independent Gaussian
noise to the responses of the output units and considering the limit where the variance of
the noise goes to zero, we can write this term as
P (sjx) = lim
!0
1
(2 2 ) N=2 e 1
2 2 ks g(Wx+Ks)k 2
: (16)
The output space can be partitioned into those points which belong to the image of the
input space, and those which are not. For points outside the image of the input space,
P (s) = 0. Consider a point s inside the image. This means that there exists x 0 such that
s = g(Wx 0 + Ks). For small , we can expand g(Wx + Ks) s ' Æx, where  is

defined in Eq. (4), and Æx = x x 0 . We then get
P (sjx) = lim
!0
1
(2 2 ) N=2 e
1
2 Æx T

 T 
 2

Æx
=
1
p
det ( T )
2
6 6 4 lim
!0
e
1
2 Æx T

 T 
 2

Æx
(2) N=2
r
det

 2 ( T ) 1

3
7 7 5 : (17)
The expression in the square brackets is a delta function in x around x 0 . Using Eq. (15) we
finally get
P (s) =
P (x)
p
det( T ) s) (18)
where the characteristic
function
 s) is 1 if s belongs to the image of the input space
and is zero otherwise. Note that for the case when  is a square matrix (M = N ), this
expression reduces to the relation P (s) = P (x)=j det()j.
7 Appendix 2: Derivation of the learning rules
To derive the appropriate learning rules, we need to calculate the derivatives of E with
respect to some set of parameters . In general, these derivatives are obtained from the
expression:
@E
@ =
1
2
Tr

( T ) 1 @( T )
@

= Tr

( T ) 1  T @
@

: (19)
7.1 Feedforward weights
In order to derive the learning rule for the weights W , we first calculate
@ ab
@Wlm =
X
c

 ac
@W cb
@W lm
+
@ ac
@Wlm W cb

=  al Æ bm +
X
c
@ ac
@W lm
W cb : (20)
From the definition of , we see that:
@ ac
@Wlm =
X
ij
 ai
@G 1
ij
@Wlm  jc (21)
and
@G 1
ij
@Wlm =
Æ ij
(g 0
i ) 2
@g 0
i
@Wlm = Æ ij
g 00
i
(g 0
i ) 3
@s i
@W lm
; (22)
where g 00
i  g 00
 P
j W ij x j +
P
k K ik s k

.
The derivatives of s also satisfy a recursion relation similar to Eq. (7):
@s i
@Wlm = g 0
i 
0
@ Æ il xm +
X
j
K ij
@s j
@W lm
1
A ; (23)
which has the solution:
@s i
@Wlm =  il xm : (24)
Putting all these results together in Eq. (19) and taking the trace, we get the gradient descent
rule in Eq. (9).

7.2 Recurrent interactions
To derive the learning rules for the recurrent weights K, we first calculate the derivatives
of  ab with respect to K lm :
@ ab
@Klm =
X
c
@ ac
@Klm W cb =
X
c;i;j
 ai
@ 1
ij
@Klm  jc W cb : (25)
From the definition of , we obtain:
@ 1
ij
@Klm =
Æ ij
(g 0
i ) 2
@g 0
i
@K lm
Æ il Æ jm : (26)
The derivatives of g 0 are obtained from the following relations:
@g 0
i
@Klm =
g 00
i
g 0
i
@s i
@Klm (27)
and
@s i
@Klm =  il s m : (28)
which results from a recursion relation similar to Eq. (23). Finally, after combining these
results and calculating the trace, we get the gradient descent learning rule in Eq. (13).
References
[1] Jolliffe, IT (1986). Principal Component Analysis. New York: Springer­Verlag.
[2] Haykin, S (1999). Neural networks: a comprehensive foundation. 2nd ed., Prentice­Hall, Upper
Saddle River, NJ.
[3] Jutten, C & Herault, J (1991). Blind separation of sources, part I: An adaptive algorithm based
on neuromimetic architecture. Signal Processing 24, 1--10.
[4] Hinton, G & Ghahramani, Z (1997). Generative models for discovering sparse distributed rep­
resentations. Philosophical Transactions Royal Society B 352, 1177--1190.
[5] Pearlmutter, B & Parra, L (1996). A context­sensitive generalization of ICA. In ICONIP'96,
151--157.
[6] Bell, AJ & Sejnowski, TJ (1995). An information maximization approach to blind separation
and blind deconvolution. Neural Comput. 7, 1129--1159.
[7] Barlow, HB (1989). Unsupervised learning. Neural Comput. 1, 295--311.
[8] Linsker, R (1992). Local synaptic learning rules suffice to maximize mutual information in a
linear network. Neural Comput. 4, 691--702.
[9] Parra, L, Deco, G, & Miesbach, S (1996). Statistical independence and novelty detection with
information preserving nonlinear maps. Neural Comput. 8, 260--269.
[10] Amari, S, Cichocki, A &Yang, H (1996). A new learning algorithm for blind signal separation.
Advances in Neural Information Processing Systems 8, 757--763.
[11] Lewicki, MS & Sejnowski, TJ (2000). Learning overcomplete representations. Neural Compu­
tation 12 337--365.

