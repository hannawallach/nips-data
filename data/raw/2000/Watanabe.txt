Algebraic Information Geometry for
Learning Machines with Singularities
Sumio Watanabe
Precision and Intelligence Laboratory
Tokyo Institute of Technology
4259 Nagatsuta, Midori­ku, Yokohama, 226­8503 Japan
swatanab@pi.titech.ac.jp
Abstract
Algebraic geometry is essential to learning theory. In hierarchical
learning machines such as layered neural networks and gaussian
mixtures, the asymptotic normality does not hold, since Fisher in­
formation matrices are singular. In this paper, the rigorous asymp­
totic form of the stochastic complexity is clarified based on resolu­
tion of singularities and two di#erent problems are studied. (1) If
the prior is positive, then the stochastic complexity is far smaller
than BIC, resulting in the smaller generalization error than regular
statistical models, even when the true distribution is not contained
in the parametric model. (2) If Je#reys' prior, which is coordi­
nate free and equal to zero at singularities, is employed then the
stochastic complexity has the same form as BIC. It is useful for
model selection, but not for generalization.
1 Introduction
The Fisher information matrix determines a metric of the set of all parameters of
a learning machine [2]. If it is positive definite, then a learning machine can be un­
derstood as a Riemannian manifold. However, almost all learning machines such as
layered neural networks, gaussian mixtures, and Boltzmann machines have singular
Fisher metrics. For example, in a three­layer perceptron, the Fisher information
matrix I(w) for a parameter w is singular (det I(w) = 0) if and only if w represents
a small model which can be realized with the fewer hidden units than the learning
model. Therefore, when the learning machine is in an almost redundant state, any
method in statistics and physics that uses a quadratic approximation of the loss
function can not be applied. In fact, the maximum likelihood estimator is not sub­
ject to the asymptotic normal distribution [4]. The Bayesian posterior probability
converges to a distribution which is quite di#erent from the normal one [8]. To
construct a mathematical foundation for such learning machines, we clarified the
essential relation between algebraic geometry and Bayesian statistics [9,10]. In this

paper, we show that the asymptotic form of the Bayesian stochastic complexity is
rigorously obtained by resolution of singularities. The Bayesian method gives pow­
erful tools for both generalization and model selection, however, the appropriate
prior for each purpose is quite di#erent.
2 Stochastic Complexity
Let p(x|w) be a learning machine, where x is a pair of an input and an output,
and w # R d is a parameter. We prepare a prior distribution #(w) on R d . Training
samples X n = (X 1 , X 2 , ..., Xn ) are independently taken from the true distribution
q(x), which is not contained in p(x|w) in general. The stochastic complexity F (X n )
and its average F (n) are defined by
F (X n ) = - log # n
# i=1
p(X i |w) #(w)dw
and F (n) = EX n{F (X n )}, respectively, where EX n{·} denotes the expectation
value overall training sets. The stochastic complexity plays a central role in Bayesian
statistics. Firstly, F (n+1)-F (n)-S, where S = - # q(x) log q(x)dx, is equal to the
average Kullback distance from q(x) to the Bayes predictive distribution p(x|X n ),
which is called the generalization error denoted by G(n). Secondly, exp(-F (X n ))
is in proportion to the posterior probability of the model, hence, the best model is
selected by minimization of F (X n ) [7]. And lastly, if the prior distribution has a hy­
perparameter #, that is to say, #(w) = #(w|#), then it is optimized by minimization
of F (X n ) [1].
We define a function F 0 (n) using the Kullback distance H(w),
F 0 (n) = - log # exp(-nH(w))#(w)dw, H(w) = # q(x) log q(x)
p(x|w)
dx.
Then by Jensen's inequality, F (n) - Sn # F 0 (n). Moreover, we assume that
L(x, w) # log q(x) - log p(x|w) is an analytic function from w to the Hilbert space
of all square integrable functions with the measure q(x)dx, and that the support of
the prior W = supp # is compact. Then H(w) is an analytic function on W , and
there exists a constant c 1 > 0 such that, for an arbitrary n,
F 0 ( n
2 ) - c 1 # F (n) - Sn # F 0 (n). (1)
3 General Learning Machines
In this section, we study a case when the true distribution is contained in the
parametric model, that is to say, there exists a parameter w 0 # W such that q(x) =
p(x|w 0 ). Let us introduce a zeta function J(z) (z # C) of H(w) and a state density
function v(t) by
J(z) = # H(w) z #(w)dw, v(t) = # #(t -H(w))#(w)dw.
Then, J(z) and F 0 (n) are represented by the Mellin and the Laplace transform of
v(t), respectively.
J(z) = # h
0
t z v(t)dt, F 0 (n) = - log # h
0
exp(-nt)v(t)dt,

where h = maxw#W H(w). Therefore F 0 (n), v(t), and J(z) are mathematically
connected. It is obvious that J(z) is a holomorphic function in Re(z) > 0. Moreover,
by using the existence of Sato­Bernstein's b­function [6], it can be analytically
continued to a meromorphic function on the entire complex plane, whose poles are
real, negative, and rational numbers. Let -# 1 > -# 2 > -# 3 > · · · be the poles of
J(z) and m k be the order of -# k . Then, by using the inverse Mellin tansform, it
follows that v(t) has an asymptotic expansion with coe#cients {c km},
v(t) # =
#
# k=1
mk
#
m=1
c km t #k-1 (- log t) m-1 (t # +0).
Therefore, also F 0 (n) has an asymptotic expansion, by putting # = # 1 and m = m 1 ,
F 0 (n) = # log n - (m - 1) log log n + O(1),
which ensures the asymptotic expansion of F (n) by eq.(1),
F (n) = Sn + # log n - (m - 1) log log n +O(1).
The Kullback distance H(w) depends on the analytic set W 0 = {w # W ; H(w) = 0},
resulting that both # and m depend on W 0 . Note that, if the Bayes generalization
error G(n) = F (n + 1) - F (n) - S has an asymptotic expansion, it should be
#/n - (m - 1)/(n log n). The following lemma is proven using the definition of
F 0 (n) and its asymptotic expansion.
Lemma 1 (1) Let (# i , m i ) (i = 1, 2) be constants corresponding to (H i (w), # i (w))
(i = 1, 2). If H 1 (w) # H 2 (w) and # 1 (w) # # 2 (w), then `# 1 < # 2 ' or `# 1 = # 2 and
m 1 # m 2 '.
(2) Let (# i , m i ) (i = 1, 2) be constants corresponding to (H i (w i ), # i (w i )) (i = 1, 2).
Let w = (w 1 , w 2 ), H(w) = H 1 (w 1 ) +H 2 (w 2 ), and #(w) = # 1 (w 1 )# 2 (w 2 ). Then the
constants of (H(w), #(w)) are # = # 1 + # 2 and m = m 1 + m 2 - 1.
The concrete values of # and m can be algorithmically obtained by the following
theorem. Let W i be the open kernel of W (the maximal open set contained in W ).
Theorem 1 (Resolution of Singularities, Hironaka [5]) Let H(w) # 0 be a real
analytic function on W i . Then there exist both a real d­dimensional manifold U and
a real analytic function g : U # W i such that, in a neighborhood of an arbitrary
u # U ,
H(g(u)) = a(u)u 2s1
1 u 2s2
2 · · · u 2sd
d
(2)
where a(u) > 0 is an analytic function and {s i } are non­negative integers. More­
over, for arbitrary compact set K # W , g -1 (K) # U is a compact set. Such a
function g(u) can be found by finite blowing­ups.
Remark. By applying eq.(2) to the definition of J(z), one can see the integral
in J(z) is decomposed into a direct product of the integral of each variable [3].
Applications to learning theory are shown in [9,10]. In general it is not so easy to
find g(u) that gives the complete resolution of singularities, however, in this paper,
we show that even a partial resolution mapping gives an upper bound of #.
Definition. We introduce two di#erent priors.
(1) The prior distribution #(w) is called positive if #(w) > 0 for an arbitrary

w # W i , (W = supp#(w)).
(2) The prior distribution #(w) is called Je#reys' one if
#(w) = 1
Z # det I(w), I ij (w) = # #L
#w i
#L
#w j
p(x|w)dx,
where Z is a normalizing constant and I(w) is the Fisher information matrix. In neu­
ral networks and gaussian mixtures, Je#reys' prior is not positive, since det I(w) = 0
on the parameters which represent the smaller models.
Theorem 2 Assume that there exists a parameter w 0 # W i such that q(x) =
p(x|w 0 ). Then followings hold.
(1) If the prior is positive, then 0 < # # d/2 and 1 # m # d. If p(x|w) satisfies the
condition of the asymptotic normality, then # = d/2 and m = 1.
(2) If Je#reys' prior is applied, then `# > d/2' or `# = d/2 and m = 1'.
(Outline of the Proof) (1) In order to examine the poles of J(z), we can divide the
parameter space into the sum of neighborhoods. Since H(w) is an analytic function,
in arbitrary neighborhood of w 0 that satisfies H(w 0 ) = 0, we can find a positive
definite quadratic form which is smaller than H(w). The positive definite quadratic
form satisfies # = d/2 and m = 1. By using Lemma 1 (1), we obtain the first half.
(2) Because Je#reys' prior is coordinate free, we can study the problem on the
parameter space U instead of W i in eq.(2). Hence, there exists an analytic function
t(x, u) such that, in each local coordinate,
L(x, u) = L(x, g(u)) = t(x, u)u s1
1 · · · u sd
d .
For simplicity, we assume that s i > 0 (i = 1, 2, ..., d). Then
#L
#w i
= ( #t
#w i
w i + s i t)u s1
1 · · · u s i -1
i · · · u sd
d .
By using blowing­ups u i = v 1 v 2 · · · v i (i = 1, 2, ..., d) and a notation # p = s p +s p+1 +
· · · + s d , it is easy to show
det I(v) #
d
# p=1
v 2d#p+p-d-2
p , du = (
d
# p=1
|v p | d-p )dv. (3)
By using H(g(u)) z = # p v 2#pz
p and Lemma.1 (1), in order to prove the latter half
of the theorem, it is su#cient to prove that
“
J(z) #
d
# p=1
# |v p |<h #
u 2#pz
p · |v p | d#p-1+(d-p)/2 dv p
has a pole z = -d/2 with the order m = 1. Direct calculation of integrals in “
J(z)
completes the theorem. (Q.E.D.)
4 Three­Layer Perceptron
In this section, we study some cases when the learner is a three­layer perceptron
and the true distribution is contained and not contained. We define the three layer

perceptron p(x, y|w) with M input units, K hidden units, and N output units,
where x is an input, y is an output, and w is a parameter.
p(x, y|w) = r(x)
(2## 2 ) N/2
exp(- 1
2# 2 #y - fK (x, w)# 2 )
fK (x, w) =
K
# k=1
a k #(b k · x + c k )
where w = {(a k , b k , c k ); a k # R N , b k # R M , c k # R 1
}, r(x) is the probability density
on the input, and # 2 is the variance of the output (either r(x) or # is not estimated).
Theorem 3 If the true distribution is represented by the three­layer perceptron with
K 0 # K hidden units, and if positive prior is employed, then
# #
1
2 {K 0 (M + N + 1) + (K -K 0 ) min(M + 1, N)}. (4)
(Outline of Proof) Firstly, we consider a case when g(x) = 0. Then,
H(w) = 1
2# 2 # # K
# k=1
a k tanh(b k · z) + c k # 2
r(x)dx. (5)
Let a k = (a k1 , ..., a kN ) and b k = (b k1 , ..., b kM ). Let us consider a blowing­up,
a 11 = #, a kj = #a # kj (k #= 1, j #= 1), b kl = b # kl , c k = c # k .
Then da db dc = # KN-1 d# da # db # dc # and there exists an analytic function
H 1 (a # , b # , c # ) such that H(a, b, c) = # 2 H 1 (a # , b # , c # ). Therefore J(z) has a pole at
z = -KN/2. Also by using another blowing­up,
a kj = a ## kj , c 1 = #, b kl = #b ## kl , c k = #c ## k (k #= 1),
then, da db dc = # (M+1)K-1 d# da ## db ## dc ## and there exists an analytic
function H 2 (a ## , b ## , c ## ) such that H(a, b, c) = # 2 H 2 (a ## , b ## , c ## ), which shows that
J(z) has a pole at z = -K(M + 1)/2. By combining both results, we obtain
# # (K/2) min(M + 1, N ). Secondly, we prove the general case, 0 < K 0 # K.
Then,
H(w) #
1
# 2
# {
K0
# k=1
a k tanh(b k · x + c k ) - g(x)} 2 r(x)dx
+ 1
# 2
# {
K
#
k=K0+1
a k tanh(b k · x + c k )} 2 r(x)dx. (6)
By combining Lemma.1 (2) and the above result, we obtain the Theorem. (Q.E.D.).
If the true regression function g(x) is not contained in the learning model, we assume
that, for each 0 # k # K, there exists a parameter w (k)
0 # W that minimizes the
square error
E k (w) = # #g(x) - f k (x, w)# 2 r(x)dx.

We use notations E(k) # E k (w (k)
0 ) and #(k) = (1/2){k(M + N + 1) + (K -
k) min(M + 1, N ).
Theorem 4 If the true regression function is not contained in the learning model
and positive prior is applied, then
F (n) # min
0#k#K # n
# 2 E(k) + #(k) log n # +O(1).
(Outline of Proof) This theorem can be shown by the same procedure as eq.(6) in
the preceding theorem. (Q.E.D.)
If G(n) has an asymptotic expansion G(n) = # Q
q=1 a q f q (n), where f q (n) is a de­
creasing function of n that satisfies f q+1 (n) = o(f q (n)) and fQ (n) = 1/n, then
G(n) # min
0#k#K # E(k)
# 2
+ #(k)
n # ,
which shows that the generalization error of the layered network is smaller than the
regular statistical models even when the true distribution is not contained in the
learning model. It should be emphasized that the optimal k that minimizes G(n)
is smaller than the learning model when n is not so large, and it becomes larger as
n increases. This fact shows that the positive prior is useful for generalization but
not appropriate for model selection. Under the condition that the true distribution
is contained in the parametric model, Je#reys' prior may enable us to find the true
model with higher probability.
Theorem 5 If the true regression function is contained in the three­layer perceptron
and Je#rey's prior is applied, then # = d/2 and m = 1, even if the Fisher metric is
degenerate at the true parameter.
(Outline of Proof) For simplicity, we prove the theorem for the case g(x) = 0. The
general cases can be proven by the same method. By direct calculation of the Fisher
information matrix, there exists an analytic function D(b, c) # 0 such that
det I(w) =
K
# k=1
(
N
# p=1
a kp ) 2(M+1) D(b, c)
By using a blowing­up
a 11 = #, a kj = #a # kj (k #= 1, j #= 1), b kl = b # kl , c k = c # k ,
we obtain H(w) = # 2 H 1 (a # , b # , c # ) same as eq.(5), det I(w) # # 2(M+1)K , and
da db dc = # NK-1 d# da # db dc. The integral
“
J(z) = # |#|<# #
# 2z # (M+1)K+NK-1 d#
has a pole at z = -(M + N + 1)K/2. By combining this result with Theorem 3,
we obtain Theorem.5. (Q.E.D.).

5 Discussion
In many applications of neural networks, rather complex machines are employed
compared with the number of training samples. In such cases, the set of optimal
parameters is not one point but an analytic set with singularities, and the set
of almost optimal parameters {w; H(w) < #} is not an `ellipsoid'. Hence neither
the Kullback distance can be approximated by any quadratic form nor the saddle
point approximation can be used in integration on the parameter space. The zeta
function of the Kullback distance clarifies the behavior of the stochastic complexity
and resolution of singularities enables us to calculate the learning e#ciency.
6 Conclusion
The relation between algebraic geometry and learning theory is clarified, and two
di#erent facts are proven.
(1) If the true distribution is not contained in a hierarchical learning model, then
by using a positive prior, the generalization error is made smaller than the regular
statistical models.
(2) If the true distribution is contained in the learning model and if Je#reys' prior
is used, then the average Bayesian factor has the same form as BIC.
Acknowledgments
This research was partially supported by the Ministry of Education, Science, Sports
and Culture in Japan, Grant­in­Aid for Scientific Research 12680370.
References
[1] Akaike, H. (1980) Likelihood and Bayes procedure. Bayesian Statistics, (Bernald J.M.
eds.) University Press, Valencia, Spain, 143­166.
[2] Amari, S. (1985) Di#erential­geometrical methods in Statistics. Lecture Notes in Statis­
tics, Springer.
[3] Atiyah, M. F. (1970) Resolution of singularities and division of distributions. Comm.
Pure and Appl. Math. , 13, pp.145­150.
[4] Dacunha­Castelle, D., & Gassiat, E. (1997). Testing in locally conic models, and
application to mixture models. Probability and Statistics, 1, 285­317.
[5] Hironaka, H. (1964) Resolution of Singularities of an algebraic variety over a field of
characteristic zero. Annals of Math., 79,109­326.
[6] Kashiwara, M. (1976) B­functions and holonomic systems. Inventions Math., 38,33­53.
[7] Schwarz, G. (1978) Estimating the dimension of a model. Ann. of Stat., 6 (2), 461­464.
[8] Watanabe, S. (1998) On the generalization error by a layered statistical model with
Bayesian estimation. IEICE Transactions, J81­A (10), 1442­1452. English version:
(2000)Electronics and Communications in Japan, Part 3, 83(6) ,95­104.
[9] Watanabe, S. (2000) Algebraic analysis for non­regular learning machines. Advances
in Neural Information Processing Systems, 12, 356­362.
[10] Watanabe, S. (2001) Algebraic analysis for non­identifiable learning machines. Neural
Computation, to appear.

