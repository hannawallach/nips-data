A Linear Programming Approach to
Novelty Detection
Colin Campbell
Dept. of Engineering Mathematics,
Bristol University, Bristol
Bristol, BS8 1TR,
United Kingdon
C.Campbell@bris.ac.uk
Kristin P. Bennett
Dept. of Mathematical Sciences
Rensselaer Polytechnic Institute
Troy, New York 12180­3590
United States
bennek@rpi.edu
Abstract
Novelty detection involves modeling the normal behaviour of a sys­
tem hence enabling detection of any divergence from normality. It
has potential applications in many areas such as detection of ma­
chine damage or highlighting abnormal features in medical data.
One approach is to build a hypothesis estimating the support of
the normal data i.e. constructing a function which is positive in the
region where the data is located and negative elsewhere. Recently
kernel methods have been proposed for estimating the support of
a distribution and they have performed well in practice ­ training
involves solution of a quadratic programming problem. In this pa­
per we propose a simpler kernel method for estimating the support
based on linear programming. The method is easy to implement
and can learn large datasets rapidly. We demonstrate the method
on medical and fault detection datasets.
1 Introduction.
An important classification task is the ability to distinguish between new instances
similar to members of the training set and all other instances that can occur. For
example, we may want to learn the normal running behaviour of a machine and
highlight any significant divergence from normality which may indicate onset of
damage or faults. This issue is a generic problem in many fields. For example,
an abnormal event or feature in medical diagnostic data typically leads to further
investigation.
Novel events can be highlighted by constructing a real­valued density estimation
function. However, here we will consider the simpler task of modelling the support
of a data distribution i.e. creating a binary­valued function which is positive in those
regions of input space where the data predominantly lies and negative elsewhere.
Recently kernel methods have been applied to this problem [4]. In this approach
data is implicitly mapped to a high­dimensional space called feature space [13].
Suppose the data points in input space are x i (with i = 1; : : : ; m) and the mapping

is x i ! OE(x i ) then in the span of fOE(x i )g, we can expand a vector w =
P
j ff j OE(x j ).
Hence we can define separating hyperplanes in feature space by w \Delta OE(x i ) + b = 0.
We will refer to w \Delta OE(x i ) + b as the margin which will be positive on one side of
the separating hyperplane and negative on the other. Thus we can also define a
decision function:
sign (w \Delta OE(z) + b) = sign
0
@ X
j
ff j OE(x j ) \Delta OE(z) + b
1
A (1)
where z is a new data point. The data appears in the form of an inner product
in feature space so we can implicitly define feature space by our choice of kernel
function:
K(x i ; x j ) = OE(x i ) \Delta OE(x j ) (2)
A number of choices for the kernel are possible, for example, RBF kernels:
K(x i ; x j ) = e \Gammajjx i \Gammax j jj 2 =2oe 2
(3)
With the given kernel the decision function is therefore given by:
sign
0
@ X
j
ff j K(x j ; z) + b
1
A (4)
One approach to novelty detection is to find a hypersphere in feature space with a
minimal radius R and centre a which contains most of the data: novel test points
lie outside the boundary of this hypersphere [3, 12]. This approach to novelty
detection was proposed by Tax and Duin [10] and successfully used on real life
applications [11]. The effect of outliers is reduced by using slack variables ¸ i to
allow for datapoints outside the sphere and the task is to minimise the volume of
the sphere and number of datapoints outside i.e.
min
\Theta R 2 + –
P
i ¸ i
\Lambda
s:t: (x i \Gamma a) \Delta (x i \Gamma a) Ÿ R 2 + ¸ i ; ¸ i – 0 (5)
Since the data appears in the form of inner products kernel substitution can be
applied and the learning task can be reduced to a quadratic programming problem.
An alternative approach has been developed by Sch¨olkopf et al. [7]. Suppose we
restricted our attention to RBF kernels (3) then the data lies on the surface of a
hypersphere in feature space since OE(x) \Delta OE(x) = K(x; x) = 1. The objective is
therefore to separate off the surface region constaining data from the region con­
taining no data. This is achieved by constructing a hyperplane which is maximally
distant from the origin with all datapoints lying on the opposite side from the ori­
gin and such that the margin is positive. The learning task in dual form involves
minimisation of:
min W (ff) = 1
2
P m
i;k=1 ff i ff j K(x i ; x j )
s:t: 0 Ÿ ff i Ÿ C;
P m
i=1 ff i = 1: (6)

However, the origin plays a special role in this model. As the authors point out
[9] this is a disadvantage since the origin effectively acts as a prior for where the
class of abnormal instances is assumed to lie. In this paper we avoid this problem:
rather than repelling the hyperplane away from an arbitrary point outside the data
distribution we instead try and attract the hyperplane towards the centre of the
data distribution.
In this paper we will outline a new algorithm for novelty detection which can be
easily implemented using linear programming (LP) techniques. As we illustrate in
section 3 it performs well in practice on datasets involving the detection of abnor­
malities in medical data and fault detection in condition monitoring.
2 The Algorithm
For the hard margin case (see Figure 1) the objective is to find a surface in input
space which wraps around the data clusters: anything outside this surface is viewed
as abnormal. This surface is defined as the level set, f(z) = 0, of some nonlinear
function. In feature space, f(z) =
P
i ff i K(z; x i ) + b, this corresponds to a hy­
perplane which is pulled onto the mapped datapoints with the restriction that the
margin always remains positive or zero. We make the fit of this nonlinear function
or hyperplane as tight as possible by minimizing the mean value of the output of
the function, i.e.,
P
i f(x i ). This is achieved by minimising:
W (ff; b) =
m
X
i=1
0
@ m
X
j=1
ff j K(x i ; x j ) + b
1
A (7)
subject to:
m
X
j=1
ff j K(x i ; x j ) + b – 0 (8)
m
X
i=1
ff i = 1; ff i – 0 (9)
The bias b is just treated as an additional parameter in the minimisation process
though unrestricted in sign. The added constraints (9) on ff bound the class of
models to be considered ­ we don't want to consider simple linear rescalings of the
model. These constraints amount to a choice of scale for the weight vector normal
to the hyperplane in feature space and hence do not impose a restriction on the
model. Also, these constraints ensure that the problem is well­posed and that an
optimal solution with ff 6= 0 exists. Other constraints on the class of functions are
possible, e.g. kffk 1 = 1 with no restriction on the sign of ff i .
Many real­life datasets contain noise and outliers. To handle these we can introduce
a soft margin in analogy to the usual approach used with support vector machines.
In this case we minimise:
W (ff; b) =
m
X
i=1
0
@ m
X
j=1
ff j K(x i ; x j ) + b
1
A + –
m
X
i=1
¸ i (10)

subject to:
m
X
j=1
ff j K(x i ; x j ) + b – \Gamma¸ i ; ¸ i – 0 (11)
and constraints (9). The parameter – controls the extent of margin errors (larger
– means fewer outliers are ignored: – !1 corresponds to the hard margin limit).
The above problem can be easily solved for problems with thousands of points us­
ing standard simplex or interior point algorithms for linear programming. With the
addition of column generation techniques, these same approaches can be adopted
for very large problems in which the kernel matrix exceeds the capacity of main
memory. Column generation algorithms incrementally add and drop columns each
corresponding to a single kernel function until optimality is reached. Such ap­
proaches have been successfully applied to other support vector problems [6, 2].
Basic simplex algorithms were sufficient for the problems considered in this paper,
so we defer a listing of the code for column generation to a later paper together
with experiments on large datasets [1].
3 Experiments
Artificial datasets. Before considering experiments on real­life data we will first
illustrate the performance of the algorithm on some artificial datasets. In Figure
1 the algorithm places a boundary around two data clusters in input space: a
hard margin was used with RBF kernels and oe = 0:2. In Figure 2 four outliers
lying outside a single cluster are ignored when the system is trained using a soft
margin. In Figure 3 we show the effect of using a modified RBF kernel K(x i ; x j ) =
e \Gammajx i \Gammax j j=2oe 2
. This kernel and the one in (3) use a measure x \Gamma y, thus K(x; x) is
constant and the points lie on the surface of a hypersphere in feature space. As a
consequence a hyperplane slicing through this hypersphere gives a closed boundary
separating normal and abnormal in input space: however, we found other choices
of kernels may not produce closed boundaries in input space.
­0.3
­0.2
­0.1
0
0.1
0.2
0.3
­0.35 ­0.3 ­0.25 ­0.2 ­0.15 ­0.1 ­0.05 0 0.05 0.1 0.15 0.2
Figure 1: The solution in input space for the hyperplane minimising W (ff; b) in
equation (7). A hard margin was used with RBF kernels trained using oe = 0:2
Medical Diagnosis. For detection of abnormalities in medical data we investi­
gated performance on the Biomed dataset [5] from the Statlib data archive [14].

­0.8
­0.6
­0.4
­0.2
0
0.2
0.4
0.6
0.8
­0.8 ­0.6 ­0.4 ­0.2 0 0.2 0.4 0.6 0.8
Figure 2: In this example 4 outliers are ignored by using a soft margin (with – =
10:0). RBF kernels were used with oe = 0:2
­0.3
­0.25
­0.2
­0.15
­0.1
­0.05
0
0.05
0.1
0.15
0.2
­0.3 ­0.25 ­0.2 ­0.15 ­0.1 ­0.05 0 0.05 0.1 0.15 0.2
Figure 3: The solution in input space for a modified RBF kernel K(x i ; x j ) =
e \Gammajx i \Gammax j j=2oe 2
with oe = 0:5
This dataset consisted of 194 observations each with 4 attributes corresponding to
measurements made on blood samples (15 observations with missing values were
removed). We trained the system on 100 randomly chosen normal observations
from healthy patients. The system was then tested on 27 normal observations and
67 observations which exhibited abnormalities due to the presense of a rare genetic
disease.
In Figure 4 we plot the results for training the novelty detector using a hard margin
and with RBF kernels. This plot gives the error rate (as a percentage) on the y­
axis, versus oe on the x­axis with the solid curve giving the performance on normal
observations in the test data and the dashed curve giving performance on abnormal
observations. Clearly, when oe is very small the system puts a Gaussian of narrow
width around each data point and hence all test data is labelled as abnormal. As oe
increases the model improves and at oe = 1:1 all but 2 of the normal test observations
are correctly labelled and 57 of the 67 abnormal observations are correctly labelled.
As oe increases to oe = 10:0 the solution has 1 normal test observation incorrectly
labelled and 29 abnormal observations correctly labelled.
The kernel parameter oe is therefore crucial is determining the balance between

0
20
40
60
80
100
0 1 2 3 4 5 6 7 8 9 10
Figure 4: The error rate (as a percentage) on the y­axis, versus oe on the x­axis.
The solid curve giving the performance on normal observations in the test data and
the dashed curve giving performance on abnormal observations.
normality and abnormality. Future research on model selection may indicate a
good choice for the kernel parameter. However, if the dataset is large enough and
some abnormal events are known then a validation study can be used to determine
the kernel parameter ­ as we illustrate with the application below. Interestingly, if
we use an ensemble of models instead, with oe chosen across a range, then the relative
proportion indicating abnormality gives an approximate measure of the confidence
in the novelty of an observation: 29 observations are abnormal for all oe in Figure 4
and hence must be abnormal with high confidence.
Condition Monitoring. Fault detection is an important generic problem in the
condition monitoring of machinery: failure to detect faults can lead to machine
damage, while an oversensitive fault detection system can lead to expensive and
unnecessary downtime. An an example we will consider detection of 4 classes of
fault in ball­bearing cages, which are often safety critical components in machines,
vehicles and other systems such as aircraft wing flaps.
In this study we used a dataset from the Structural Integrity and Damage Assess­
ment Network [15]. Each instance consisted of 2048 samples of acceleration taken
with a Bruel and Kjaer vibration analyser. After pre­processing with a discrete Fast
Fourier Transform each such instance had 32 attributes characterising the measured
signals.
The dataset consisted of 5 categories: normal data corresponding to measurements
made from new ball­bearings and 4 types of abnormalities which we will call type
1 (outer race completely broken), type 2 (broken cage with one loose element), type
3 (damaged cage with four loose elements) and type 4 (a badly worn ball­bearing
with no evident damage). To train the system we used 913 normal instances on new
ball­bearings. Using RBF kernels the best value of oe (oe = 320:0) was found using
a validation study consisting of 913 new normal instances, 747 instances of type 1
faults and 996 instances of type 2 faults. On new test data 98.7% of normal instances
were correctly labelled (913 instances), 100% of type 1 instances were correctly
labelled (747 instances) and 53.3% of type 2 instances were correctly labelled (996
instances). Of course, with ample normal and abnormal data this problem could
also be approached using a binary classifier instead. Thus to evaluate performance
on totally unseen abnormalities we tested the novelty detector on type 3 errors and
type 4 errors (with 996 instances of each). The novelty detector labelled 28.3% of

type 3 and 25.5% of type 4 instances as abnormal ­ which was statistically significant
against a background of 1.3% errors on normal data.
4 Conclusion
In this paper we have presented a new kernelised novelty detection algorithm which
uses linear programming techniques rather than quadratic programming. The al­
gorithm is simple, easy to implement with standard LP software packages and it
performs well in practice. The algorithm is also very fast in execution: for the 913
training examples used in the experiments on condition monitoring the model was
constructed in about 4 seconds using a Silicon Graphics Origin 200.
References
[1] K. Bennett and C. Campbell. A Column Generation Algorithm for Novelty
Detection. Preprint in preparation.
[2] K. Bennett, A. Demiriz and J. Shawe­Taylor, A Column Generation Algorithm
for Boosting. In Proceed. of Intl. Conf. on Machine Learning. Stanford, CA,
2000.
[3] C. Burges. A tutorial on support vector machines for pattern recognition. Data
Mining and Knowledge Discovery, 2, p. 121­167, 1998.
[4] C. Campbell. An Introduction to Kernel Methods. In: Radial Basis Function
Networks: Design and Applications. R.J. Howlett and L.C. Jain (eds). Physica
Verlag, Berlin, to appear.
[5] L. Cox, M. Johnson and K. Kafadar. Exposition of Statistical Graphics Tech­
nology. ASA Proceedings of the Statistical Computation Section, p. 55­56, 1982.
[6] O. L. Mangasarian and D. Musicant. Massive Support Vector Regression. Data
Mining Institute Technical Report 99­02, University of Wisconsin­Madison,
1999.
[7] B. Sch¨olkopf, J.C. Platt, J. Shawe­Taylor, A.J. Smola, R.C. Williamson. Es­
timating the support of a high­dimensional distribution. Microsoft Research
Corporation Technical Report MSR­TR­99­87, 1999, 2000
[8] B. Sch¨olkopf, R. Williamson, A. Smola, and J. Shawe­Taylor. SV estimation
of a distribution's support. In Neural Information Processing Systems, 2000, to
appear.
[9] B. Sch¨olkopf, J. Platt and A. Smola. Kernel Method for Percentile Feature
Extraction. Microsoft Technical Report MSR­TR­2000­22.
[10] D. Tax and R. Duin. Data domain description by Support Vectors. In Pro­
ceedings of ESANN99, ed. M Verleysen, D. Facto Press, Brussels, p. 251­256,
1999.
[11] D. Tax, A. Ypma, and R. Duin. Support vector data description applied to
machine vibration analysis. In: M. Boasson, J. Kaandorp, J.Tonino, M. Vossel­
man (eds.), Proc. 5th Annual Conference of the Advanced School for Computing
and Imaging (Heijen, NL, June 15­17), 1999, 398­405.
[12] V. Vapnik. The Nature of Statistical Learning Theory. Springer, N.Y., 1995.
[13] V. Vapnik. Statistical Learning Theory. Wiley, 1998.
[14] cf. http://lib.stat.cmu.edu/datasets
[15] http://www.sidanet.org

