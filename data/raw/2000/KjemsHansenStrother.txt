Generalizable Singular Value
Decomposition for Ill-posed Datasets
Ulrik Kjems Lars K. Hansen
Department of Mathematical Modelling
Technical University of Denmark
DK-2800 Kgs. Lyngby, Denmark
uk,lkhansen@imm.dtu.dk
Stephen C. Strother
PET Imaging Service
VA medical center
Minneapolis
steve@pet.med.va.gov
Abstract
We demonstrate that statistical analysis of ill-posed data sets is
subject to a bias, which can be observed when projecting indepen-
dent test set examples onto a basis dened by the training exam-
ples. Because the training examples in an ill-posed data set do not
fully span the signal space the observed training set variances in
each basis vector will be too high compared to the average vari-
ance of the test set projections onto the same basis vectors. On
basis of this understanding we introduce the Generalizable Singu-
lar Value Decomposition (GenSVD) as a means to reduce this bias
by re-estimation of the singular values obtained in a conventional
Singular Value Decomposition, allowing for a generalization perfor-
mance increase of a subsequent statistical model. We demonstrate
that the algorithm succesfully corrects bias in a data set from a
functional PET activation study of the human brain.
1 Ill-posed Data Sets
An ill-posed data set has more dimensions in each example than there are examples.
Such data sets occur in many elds of research typically in connection with image
measurements. The associated statistical problem is that of extracting structure
from the observed high-dimensional vectors in the presence of noise. The statistical
analysis can be done either supervised (i.e. modelling with target values: classi-
cation, regresssion) or unsupervised (modelling with no target values: clustering,
PCA, ICA). In both types of analysis the ill-posedness may lead to immediate prob-
lems if one tries to apply conventional statistical methods of analysis, for example
the empirical covariance matrix is prohibitively large and will be rank-decient.
A common approach is to use Singular Value Decomposition (SVD) or the analogue
Principal Component Analysis (PCA) to reduce the dimensionality of the data. Let
the N observed i-dimensional samples x j , j = 1:::N , collected in the data matrix
X = [x 1 ::: xN ] of size I N , I > N . The SVD-theorem states that such a matrix
can be decomposed as
X = UV T ; (1)

where U is a matrix of the same size as X with orthogonal basis vectors spanning
the space of X, so that U T U = I NN . The square matrix  contains the singular
values in the diagonal,  = diag( 1 ; :::; N ), which are ordered and positive  1 
 2  : : :  N  0, and V is N N and orthogonal V T V = I N . If there is a mean
value signicantly dierent from zero it may at times be advantageous to perform
the above analysis on mean-subtracted data, i.e. X 
X = UV T where columns
of 
X all contain the mean vector 
x =
P
j x j =N .
Each observation x j can be expressed in coordinates in the basis dened by the
vectors of U with no loss of information[Lautrup et al., 1995]. A change of basis is
obtained by q j = U T x j as the orthogonal basis rotation
Q = [q 1 ::: q N ] = U T X = U T UV T = V T : (2)
Since Q is only NN and N  I , Q is a compact representation of the data. Having
now N examples of N dimension we have reduced the problem to a marginally ill-
posed one. To further reduce the dimensionality, it is common to retain only a
subset of the coordinates, e.g. the top P coordinates (P < N) and the supervised
or unsupervised model can be formed in this smaller but now well-posed space.
So far we have considered the procedure for modelling from a training set. Our
hope is that the statistical description generalizes well to new examples proving
that is is a good description of the generating process. The model should, in other
words, be able to perform well on a new example, x  , and in the above framework
this would mean the predictions based on q  = U T x  should generalize well. We
will show in the following, that in general, the distribution of the test set projection
q  is quite dierent from the statistics of the projections of the training examples
q j . It has been noted in previous work [Hansen and Larsen, 1996, Roweis, 1998,
Hansen et al., 1999] that PCA/SVD of ill-posed data does not by itself represent a
probabilistic model where we can assign a likelihood to a new test data point, and
procedures have been proposed which make this possible. In [Bishop, 1999] PCA has
been considered in a Bayesian framework, but does not address the signicant bias
of the variance in training set projections in ill-posed data sets. In [Jackson, 1991]
an asymptotic expression is given for the bias of eigen-values in a sample covariance
matrix, but this expression is valid only in the well-posed case and is not applicable
for ill-posed data.
1.1 Example
Let the signal source be I-dimensional multivariate Gaussian distribution N (0; )
with a covariance matrix where the rst K eigen-values equal  2 and the last I K
are zero, so that the covariance matrix has the decomposition
 =  2 Y DY T ; D = diag(1; :::; 1; 0; :::; 0); Y T Y = I (3)
Our N samples of the distribution are collected in the matrix X =

x ij
 with the
SVD
X = UV T
;  = diag( 1 ; :::; N ) (4)
and the representation of the N examples in the N basis vector coordinates dened
by U is Q =

q ij

= U T X = V T . The total variance per training example is
1
N
X
i;j
x 2
ij = 1
N
Tr(X T X) = 1
N
Tr(V U T UV T ) = 1
N
Tr(V  2 V T )
= 1
N
Tr(V V T  2 ) = 1
N
Tr( 2 ) = 1
N
X
i
 2
i (5)

Note that this variance is the same in the U-basis coordinates:
1
N
X
i;j
q 2
ij = 1
N
Tr(Q T Q) = 1
N
Tr(V  2 V T ) = 1
N
X
i
 2
i (6)
We can derive the expected value of this variance:

 1
N
X
i;j
x 2
j;i

=

X
i
x 2
1;i

=
 x 1
T x 1
 = Tr =  2 K (7)
Now, consider a test example x   N (0; ) with the projection q  = U T x  which
will have the average total variance
X
i
q 
i
2 =
D
Tr
 (U T x  ) T
(U T x  )
 E
= Tr
h
 x  x T
 UU T
i
= Tr[UU T ] = Tr[DUU T ] =  2 min(N; K) (8)
In summary, this means that the orthogonal basis U computed from the training set
spans all the variance in the training set but fails to do so on the test examples when
N < K, i.e. for ill-posed data. The training set variance is K=N 2 on average per
coordinate, compared to  2 for the test examples. So which of the two variances is
\correct" ? From a modelling point of view, the variance from the test example tells
us the true story, so the training set variance should be regarded as biased. This
suggests that the training set singular values should be corrected for this bias, in the
above example by re-estimating the training set projections using ^
Q =
p
N=KQ.
In the more general case we do not know K, and the true covariance may have an ar-
bitrary eigen-spectrum. The GenSVD algorithm below is a more general algorithm
for correcting for the training set bias.
2 The GenSVD Algorithm
The data matrix consists of N statistically independent samples X =

x 1 ::: xN

so X is size I  N , and each column of X is assumed multivariate Gaussian,
x j  N (0; ) and is ill-posed with rank  > N .
With the SVD X = U 0  0 V T
0
, we now make the approximation that U 0 contains
an actual subset of the true eigen-vectors of 
 =

U 0 U ?

 2 0
0  2
?

U 0 U ?

(9)
where we have collected the remaining (unspanned by X) eigen-vectors and values in
U ? and  2
? , satisfying U ? T U ? = I and U T
0
U ? = 0. The unknown 'true' eigen-values
corresponding to the observed eigen-vectors are collected in  = diag ( 1 ; ::: N ),
which are the values we try to estimate in the following.
It should be noted that a direct estimation of  using ^
 = 1
N XX T yields ^
 =
1
N U 0  0 V T
0 V 0  0 U T
0 = 1
N U 0  2
0 U T
0 , i.e., the nonzero eigen-vectors and values of ^

is U 0 and  0 .
The distribution of test samples x  inside the space spanned by U 0 is
U T
0
x  N (0; U T
0
U 0 ) = N (0;  2 ) (10)

The problem is that U 0 and the examples x j are not independent, so U T
0
x j is
biased, e.g. the SVD estimate 1
N  2
0
of  2 assigns all variance to lie within U 0 .
The GenSVD algorithm bypasses this problem by, for each example, computing
a basis on all other examples, estimating the variances in  2 in a leave-one-out
manner. Consider
z j = U T
0 B j
B j
T x j (11)
where we introduce the notation X j
for the matrix of all examples except the
j'th, and this matrix is decomposed as X j
= B j
 j
V j
T . The operation B j
B j
T x j
projects the example onto the basis dened by the remaining examples, and back
again, so it 'strips' o the part of signal space which is special for x j which could
be signal which does not generalize across examples.
Since B j
and x j are independent B j
T x j has the same distribution as the projec-
tion of a test example x  , B j
T x  . Thus, B j
B j
T x j and B j
B j
T x  have the same
distribution as well. Now, since span B j
=span X j
and span U 0 =span

X j
x j

we
have that span B j
spanU 0 so we see that z j and U T
0 B j
B j
T x  are identically dis-
tributed. This means that z j has the covariance U T
0 B j
B j
T B j
B j
T U 0 and using
Eq. (9) and that U ?
T B j
= 0 (since U ?
T U 0 = 0) we get
z j  N 0; U T
0
B j
B j
T U 0 U T
0
B j
B j
T U 0

(12)
We note that this distribution is degenerate because the covariance is of rank N 1.
For a sample z j from the above distribution we have that
U T
0
B j
B j
T U 0 z j = U T
0
B j
B j
T U 0 U T
0
B j
B j
T x j = U T
0
B j
B j
T x j = z j (13)
As a second approximation, assume that the observed z j are independent so that
we can write the likelihood of 
log L( 2 ) '
X
j
log
"
(2) N=2
  (U T
0
B j
)(B j
T U 0 ) 2 (U T
0
B j
)(B j
T U 0 )
   1=2
#
+ 1
2
X
j
z j T (U T
0 B j
)(B j
T U 0 ) 2 (U T
0 B j
)(B j
T U 0 )z j
' c + N
2
X
j
log  2
j + 1
2
X
j
z j T  2 z j (14)
where we have used Eq. (13) and that the determinant 1 is approximated by
   2
  .
This above expression is maximized when
^  2
i = 1
N
X
j
z 2
ij : (15)
The GenSVD of X is then X = U 0
^
V T , ^
 = diag( ^
 1 ; :::; ^ N ).
In practice, using Eq. (11) directly to compute an SVD of the matrix X j
for each
example is computationally demanding. It is possible to compute z j in a more
e∆cient two-level procedure with the following algorithm:
Compute U 0  0 V T
0
= svd(X) and Q 0
=
 q j
 =  0 V T
0
1 Since z j
is degenerate, we dene the likelihood over the space where z j
occur, i.e. the
determinant in Eq. 14 should be read as 'the product of non-zero eigenvalues'.

foreach j = 1:::N
Compute B j
 j
V j
T = svd(Q j
)
z j = B j
B j
T q j
^
 2
i = 1
N
P
j z 2
ij
If the data has a mean value that we wish to remove prior to the SVD it is important
that this is done within the GenSVD algorithm. Consider a centered matrix X c =
X 
X where 
X contains the mean 
x replicated in all N columns. The signal space
in X c is now corrupted because each centered example will contain a component
of all examples, which means the 'stripping' of signal components not spanned by
other examples no longer works: B j
T x j is no longer distributed like B j
T x  . This
suggests the alternative algorithm for data with removal of mean component:
Compute U 0  0 V T
0
= svd(X) and Q 0
=
 q j
 =  0 V T
0
foreach j = 1:::N
 q j = 1
N 1
P
j 0 6=j q j 0
Compute B j
 j
V j
T = svd(Q j

Q j
)
z j = B j
B j
T (q j  q j )
^
 2
i = 1
N 1
P
j z 2
ij
Finally, note that it is possible to leave out more than one example at a time if the
data is independent only in block, i.e. let Q k
would be Q 0
with the k'th block left
out.
Example With PET Scans
We compared the performance of GenSVD to conventional SVD on a functional
 15 O
 water PET activation study of the human brain. The study consisted of
18 subjects, who were scanned four times while tracing a star-shaped maze with
a joy-stick with visual feedback, in total 72 scans of dimension  25000 spatial
voxels. After the second scan, the visual feedback was mirrored, and the subject
accomodated to and learned the new control environment during the last two scans.
Scans were normalized by 1) dividing each scan by the average voxel value measured
inside a brain mask and 2) for each scan subtracting the average scan for that sub-
ject thereby removing subject eects and 3) intra and inter-subject normalization
and transformation using rigid body reorientation and a∆ne linear transformations
respectively. Voxels inside aforementioned brain mask were arranged in the data
matrix with one scan per column.
Figure 1 shows the results of an SVD decomposition compared to GenSVD. Each
marker represents one scan and the glyphs indicate scan number out of the four
(circle-square-star-triangle). The ellipses indicate the mean and covariances of the
projections in each scan number. The 32 scans from eight subjects were used as a
training set and 40 scans from the remaining 10 subjects for testing. The training
set projections are lled markers, test-set projections onto the basis dened by the
training set are open markers (i.e. we plot the rst two columns of U 0  0 for SVD
and of U 0
^
 for GenSVD). We see that there is a clear dierence in variance in the
train- and test-examples, which is corrected quite well by GenSVD. The lower plot
in Figure 1 shows the singular values for the PET data set. We see that GenSVD
estimates are much closer to the actual test projection standard deviations than the
SVD singular values.

3 Conclusion
We have demonstrated that projection of ill-posed data sets onto a basis dened
by the same examples introduces a signicant bias on the observed variance when
comparing to projections of test examples onto the same basis. The GenSVD algo-
rithm has been presented as a tool for correcting for this bias using a leave-one-out
re-estimation scheme, and a computationally e∆cient implementation has been pro-
posed.
We have demonstrated that the method works well on an ill-posed real-world data
set, were the distribution of the GenSVD-corrected training test set projections
matched the distribution of the observed test set projections far better than the
uncorrected training examples. This allows a generalization performance increase
of a subsequent statistical model, in the case of both supervised and unsupervised
models.
Acknowledgments
This work was supported partly by the Human Brain Project grant P20 MH57180,
the Danish Research councils for the Natural and Technical Sciences through the
Danish Computational Neural Network Center (CONNECT) and the Technology
Center Through Highly Oriented Research (THOR).
References
[Bishop, 1999] Bishop, C. (1999). Bayesian pca. In Kearns, M. S., Solla, S. A., and Cohn,
D. A., editors, Advances in Neural Information Processing Systems, volume 11. The
MIT Press.
[Hansen et al., 1999] Hansen, L., Larsen, J., Nielsen, F., Strother, S., Rostrup, E., Savoy,
R., Lange, N., Sidtis, J., Svarer, C., and Paulson, O. (1999). Generalizable patterns in
neuroimaging: How many principal components? NeuroImage, 9:534{544.
[Hansen and Larsen, 1996] Hansen, L. K. and Larsen, J. (1996). Unsupervised learning
and generalization. In Proceedings of IEEE International Conference on Neural Net-
works, pages 25{30.
[Jackson, 1991] Jackson, J. E. (1991). A User's Guide to Principal Components. Wiley
Series on Probability and Statistics, John Wiley and Sons.
[Lautrup et al., 1995] Lautrup, B., Hansen, L. K., Law, I., Mrch, N., Svarer, C., and
Strother, S. (1995). Massive weight sharing: A cure for extremely ill-posed problems.
In Hermann, H. J., Wolf, D. E., and Poppel, E. P., editors, Proceedings of Workshop
on Supercomputing in Brain Research: From Tomography to Neural Networks: From
tomography to neural networks, HLRZ, KFA Julich, Germany, pages 137{148. World
Scientic.
[Roweis, 1998] Roweis, S. (1998). Em algorithms for pca and spca. In Jordan, M. I.,
Kearns, M. J., and Solla, S. A., editors, Advances in Neural Information Processing
Systems, volume 10. The MIT Press.

-4.00 -3.00 -2.00 -1.00 0.00 1.00 2.00 3.00 4.00
-3.00
-2.00
-1.00
0.00
1.00
2.00
3.00
First SVD component
Second
SVD
component
Conventional SVD
-2.00 -1.50 -1.00 -0.50 0.00 0.50 1.00 1.50 2.00
-1.50
-1.00
-0.50
0.00
0.50
1.00
1.50
First GenSVD component
Second
GenSVD
component
Generalizable SVD
Solid:
Open:
Train
Test
Trace scan 1
Trace scan 2
Mirror scan 1
Mirror scan 2
1 5 10 15 20
0.00
0.50
1.00
1.50
2.00
Component
Standard
deviation
SVD training set projection stdev
GenSVD training set proj. stdev
Test set projection stdev
Figure 1: Projections of PET data in SVD and GenSVD. Each subject's four scans
are indicated by: circle, square, star, triangle. Training set scans are marked with
lled glyphs and test set with open glyphs. Solid and dotted Ellipses indicate
test/train covariance per scan number. The third plot shows the standard deviations
for the training and test set for SVD and GenSVD projections.

