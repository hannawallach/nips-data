Weak Learners and Improved Rates of
Convergence in Boosting
Shie Mannor and Ron Meir
Department of Electrical Engineering
Technion, Haifa 32000, Israel
fshie,rmeirg@ftechunix,eeg.technion.ac.il
Abstract
The problem of constructing weak classiers for boosting algo-
rithms is studied. We present an algorithm that produces a linear
classier that is guaranteed to achieve an error better than random
guessing for any distribution on the data. While this weak learner
is not useful for learning in general, we show that under reasonable
conditions on the distribution it yields an eective weak learner for
one-dimensional problems. Preliminary simulations suggest that
similar behavior can be expected in higher dimensions, a result
which is corroborated by some recent theoretical bounds. Addi-
tionally, we provide improved convergence rate bounds for the gen-
eralization error in situations where the empirical error can be made
small, which is exactly the situation that occurs if weak learners
with guaranteed performance that is better than random guessing
can be established.
1 Introduction
The recently introduced boosting approach to classication (e.g., [10]) has been
shown to be a highly eective procedure for constructing complex classiers. Boost-
ing type algorithms have recently been shown [9] to be strongly related to other in-
cremental greedy algorithms (e.g., [6]). Although a great deal of numerical evidence
suggests that boosting works very well across a wide spectrum of tasks, it is not a
panacea for solving classication problems. In fact, many versions of boosting algo-
rithms currently exist (e.g., [4],[9]), each possessing advantages and disadvantages
in terms of classication accuracy, interpretability and ease of implementation.
The eld of boosting provides two major theoretical results. First, it is shown that
in certain situations the training error of the classier formed converges to zero
(see (2)). Moreover, under certain conditions, a positive margin can be guaranteed.
Second, bounds are provided for the generalization error of the classier (see (1)).
The main contribution of this paper is twofold. First, we present a simple and
e∆cient algorithm which is shown, for every distribution on the data, to yield a
linear classier with guaranteed error which is smaller than 1=2  where  is
strictly positive. This establishes that a weak linear classier exists. From the
theory of boosting [10] it is known that such a condition su∆ces to guarantee that
the training error converges to zero as the number of boosting iterations increases.

In fact, the empirical error with a nite margin is shown to converge to zero if 
is su∆ciently large. However, the existence of a weak learner with error 1=2 
is not always useful in terms of generalization error, since it applies even to the
extreme case where the binary labels are drawn independently at random with
equal probability at each point, in which case we cannot expect any generalization.
It is then clear that in order to construct useful weak learners, some assumptions
need to be made about the data. In this work we show that under certain natural
conditions, a useful weak learner can be constructed for one-dimensional problems,
in which case the linear hyper-plane degenerates to a point. We speculate that
similar results hold for higher dimensional problems, and present some supporting
numerical evidence for this. In fact, some very recent results [7] show that this
expectation is indeed borne out. The second contribution of our work consists of
establishing faster convergence rates for the generalized error bounds introduced
recently by Mason et al. [8]. These improved bounds show that faster convergence
can be achieved if we allow for convergence to a slightly larger value than in previous
bounds. Given the guaranteed convergence of the empirical loss to zero (in the
limited situations in which we have proved such a bound), such a result may yield a
better trade-o between the terms appearing in the bound, oering a better model
selection criterion (see Chapter 15 in [1]).
2 Construction of a Linear Weak Learner
We recall the basic generalization bound for convex combinations of classiers. Let
H be a class of binary classiers of VC-dimension d v , and denote by co(H) the
convex hull of H . Given a sample S = f(x 1 ; y 1 ); : : : ; (x m ; ym )g 2 (X f1;+1g) m
of m examples drawn independently at random from a probability distribution D
over X  f1;+1g, Schapire et al. [10] show that with probability at least 1 ∆,
for every f 2 co(H) and every  > 0,
PD [Y f(X)  0]  PS [Y f(X)  ] +O
 
1
p
m

d v log 2 (m=d v )
 2
+ log(1=∆)
1=2 !
;
(1)
where the margin-error PS [Y f(X)  ] denotes the fraction of training points for
which y i f(x i )  . Clearly, if the rst term can be made small for a large value of
the margin , a tight bound can be established. Schapire et al. [10] also show that
if each weak classier can achieve an error smaller than 1=2 , then
PS [Y f(X)  ]  (1 2) 1  (1 + 2) 1+
 T=2
; (2)
where T is the number of boosting iterations. Note that if  > , the bound
decreases to zero exponentially fast. It is thus clear that a large value of  is needed
in order to guarantee a small value for the margin-error. However, if  (and thus
) behaves like m  for some  > 0, the rate of convergence in the second term in
(1) will deteriorate, leading to worse bounds than those available by using standard
VC results [11]. What is needed is a characterization of conditions under which
the achievable  does not decrease rapidly with m. In this section we present such
conditions for one-dimensional problems, and mention recent work [7] that proves
a similar result in higher dimensions.
We begin by demonstrating that for any distribution on m points, a linear classier
can achieve an error smaller than 1=2 , where 
=
3 =m). In view of our com-
ments above, such a fast convergence of  to zero may be useless for generalization
bounds. We then use our construction to show that, under certain regularity con-
ditions, a value of , and thus of , which is independent of m can be established
for one-dimensional problems.

Let fx 1 ; : : : ; xm g be points in R d , and denote by fy 1 ; : : : ; ymg their binary labels,
i.e., y i 2 f1;+1g. A linear decision rule takes the form ^
y(x) = sgn(a  x + b),
where  is the standard inner product in R d . Let P 2  m be a probability measure
on the m points. The weighted misclassication error for a classier ^
y is P e (a; b) =
P m
i=1 P i I(y i 6= ^
y i ). For technical reasons, we prefer to use the expression 1 2P e =
P m
i=1 P i y i ^
y i . Obviously if 1 2P e ?  we have that P e 7 1
2

2
.
Lemma 1 For any sample of m distinct points, S = f(x i ; y i )g m
i=1 2 (R d 
f1;+1g) m , and a probability measure P 2  m on S, there is some a 2 R d and
b 2 R such that the weighted misclassication error of the linear classier ^
y =
sgn(a  x + b) is bounded away from 1=2, in particular
P m
i=1 P i I(y i 6= ^ y i )  1
2
1
4m .
Proof The basic idea of the proof is to project a nite number of points onto a line
h so that no two points coincide. Since there is at least one point x whose weight
is not smaller than 1=m, we consider the four possible linear classiers dened by
h with boundaries near x (at both sides of it and with opposite sign), and show
that one of these yields the desired result. We proceed to the detailed proof. Fix
a probability vector P = (P 1 ; : : : ; Pm ) 2  m . We may assume w.l.o.g that all the
x i are dierent, or we can merge two elements and get m 1 points. First, observe
that if j
P m
i=1 P i y i j  1
2m , then the problem is trivially solved. To see this, denote
by S the sub-samples of S labelled by 1 respectively. Assume, for example,
that
P
i2S+ P i 
P
i2S P i + 1
2m . Then the choice a = 0; b = 1, namely ^
y i = 1
for all i, implies that
P
i P i y i ^
y i  1
2m . Similarly, the choice a = 0; b = 1 solves
the problem if
P
i2S P i 
P
i2S+ P i + 1
2m . Thus, we can assume, without loss of
generality, that j
P m
i=1 P i y i j < 1
2m . Next, note that there exists a direction u such
that i 6= j implies that u  x i 6= u  x j . This can be seen by the following argument.
Construct all one-dimensional lines containing two data points or more; clearly the
number of such lines is at most m(m 1)=2. It is then obvious that any line, which
is not perpendicular to any of these lines obeys the required condition. Let x i be
a data-point for which P i  1=m, and set  to be a positive number such that
0 <  < minfju  x i u  x j j : i; j 2 1; : : : ; mg. Such an  always exists since the
points are assumed to be distinct. Note the following trivial algebraic fact:
jA +Bj < ∆ 1 & A > ∆ 2 ) A B > 2∆ 2 ∆ 1 : (3)
For each j = 1; 2; : : : ; m let the classication be given by ^
y j = sgn(u  x j + b), where
the bias b is given by b = ux i +y i . Then clearly ^
y i = y i and ^
y j = sgn(ux j ux i ),
and therefore P
j P j y j ^ y j = P i + P
j 6=i P j y j sgn(u  x j u  x i ). Let A = P i and
B =
P
j 6=i P j y j sgn(u  x j u  x i ). If jA + Bj  1=2m we are done. Otherwise, if
jA +Bj < 1=2m, consider the classier y 0
j = sgn( u  x j + b 0 ), with b 0 = u  x i + y i
(note that y 0
i = y i and y 0
j = ^
y j ; j 6= i). Using (3) with ∆ 1 = 1=2m and ∆ 2 = 1=m
the claim follows.
We comment that the upper bound in Lemma 1 may be improved to 1=2 1=(4(m
1)), m  2, using a more rened argument.
Remark 1 Lemma 1 implies that an error of 1=2 , where 
=
3 =m), can
be guaranteed for any set of arbitrarily weighted points. It is well known that the
problem of nding a linear classier with minimal classication error is NP-hard
(in d) [5]. Moreover, even the problem of approximating the optimal solution is
NP-hard [2]. Since the algorithm described in Lemma 1 is clearly polynomial (in
m and d), there seems to be a transition as a function of  between the class NP
and P (assuming, as usual, that they are dierent). This issue warrants further
investigation.

While the result given in Lemma 1 is interesting, its generality precludes its use-
fulness for bounding generalization error. This can be seen by observing that the
theorem guarantees the given margin even in the case where the labels y i are drawn
uniformly at random from f1g, in which case no generalization can be expected.
In order to obtain a more useful result, we need to restrict the complexity of the
data distribution. We do this by imposing constraints on the types of decision re-
gions characterizing the data. In order to generate complex, yet tractable, decision
regions we consider a multi-linear mapping from R d to f1; 1g k , generated by the
k hyperplanes P i = fx : w i x + w i0 ; x 2 R d g; i = 1; : : : ; k; as in the rst hidden
layer of a neural network. Such a mapping generates a partition of the input space
R d into M connected components,

R d n [ k
i=1 P i
	
, each characterized by a unique
binary vector of length k. Assume that the weight vectors (w i ; w i0 ) 2 R d+1 are in
general position. The number of connected components is given by (e.g., Lemma
3.3. in [1]) C(k; d + 1) = 2
P d
i=0
k 1
i

: This number can be bounded from below
by 2 k 1
d

, which in turn is bounded below by 2((k 1)=d) d . An upper bound is
given by 2(e(k 1)=d) d , m  d. In other words, C(k; d + 1) =  (k=d) d

. In order
to generate a binary classication problem, we observe that there exists a binary
function from f1; 1g k 7! f1; 1g, characterized by these M decision regions. This
can be seen as follows. Choose an arbitrary connected component, and label it by
+1 (say). Proceed by labelling all its neighbors by 1, where neighbors share a
common boundary (a (d 1)-dimensional hyperplane in d dimensions). Proceeding
by induction, we generate a binary classication problem composed of exactly M
decision regions. Thus, we have constructed a binary classication problem, char-
acterized by at least 2 k 1
d

 2((k 1)=d)) d decision regions. Clearly as k becomes
arbitrarily large, very elaborate regions are formed.
We now apply these ideas, together with Lemma 1, to a one dimensional problem.
Note that in this case the partition is composed of intervals.
Theorem 1 Let F be a class of functions from R to f1g which partitions the real
line into at most k intervals, k  2. Let  be an arbitrary probability measure on
R. Then for any f 2 F there exist a;   2 R for which,
 fx : f(x)sgn(ax   ) = 1g  1
2 + 1
4k (4)
Proof Let a function f be given, and denote its connected components by I 1 ; : : : ; I k ,
that is I 1 = [1; l 1 ), I 2 = [l 1 ; l 2 ), I 3 = [l 2 ; l 3 ), and so on until I k = [l k 1 ; 1], with
1 = l 0 < l 1 < l 2 <    < l k 1 . Associate with every interval a point in R,
x 1 = l 1 1; x 2 = (l 1 + l 2 )=2; : : : ; x k 1 = (l k 2 + l k 1 )=2; x k = l k 1 + 1;
a weight  i = (I i ); i = 1; : : : ; k, and a label f(x i ) 2 f1g. We now apply Lemma 1
to conclude that there exist a 2 f1g and  2 R such that P k
i=1  i f(x i )sgn(ax i
) > 1=(4k). The value of  lies between l i and l i+1 for some i 2 f0; 1; : : : ; k
1g (recall that l 0 = 1). We identify   of (4) as l i+1 . This is the case since
by choosing this   , f(x) in any segment I i is equal to f(x i ) so we have that
 fx : f(x)sgn(ax   ) = 1g = 1
2 +
P k
i=1  i f(x i )sgn(ax i   )  1
2 + 1
4k :
Note that the result in Theorem 1 is in fact more general than we need, as it applies
to arbitrary distributions, rather than distributions over a nite set of points. An
open problem at this point is whether a similar result applies to d-dimensional prob-
lems. We conjecture that in d dimensions  behaves like k l(d) for some function
l, where k is a measure for the number of homogeneous convex regions dened by
the data (a homogeneous region is one in which all points possess identical labels).

While we do not have a general proof at this stage, we have recently shown [7]
that the conjecture holds under certain natural conditions on the data. This result
implies that, at least under appropriate conditions, boosting-like algorithm are ex-
pected to have excellent generalization performance. To provide some motivation,
we present results of some numerical simulations for two-dimensional problems. For
this simulation we used random lines to generate a partition of the unit square in
R 2 . We then drew 1000 points at random from the unit square and assigned them
labels according to the partition. Finally, in order to have a non-trivial problem we
made sure that the cumulative weights of each class are equal. We then calculated
the optimal linear classier by exhaustive search. In Figure 1(b) we show a sam-
ple decision region with 93 regions. Figure 1(a) shows the dependence of  on the
number of regions k. As it turns out there is a signicant logarithmic dependence
between  and k, which leads us to conjecture that   Ck l +E for some C; l and
E. In the presented case it turns out that l = 3 turns out to t our model well.
It is important to note, however, that the procedure described above only supports
our claim in an average-case, rather than worst-case, setting as is needed.
0 50 100 150 200
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Number of Regions
gamma
(a) gamma as a function of regions (b) A complex partition
X
Y
0 0.2 0.4 0.6 0.8 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 1: (a)  as a function of the number of regions. (b) A typical complex
partition of the unit square used in the simulations.
3 Improved Convergence Rates
In Section 2 we proved that under certain conditions a weak learner exists with a
su∆ciently large margin, and thus the rst term in (1) indeed converges to zero.
We now analyze the second term in (1) and show that it may be made to converge
considerably faster, if the rst term is made somewhat larger. First, we briey
recall the framework introduced recently by Mason et al. [8]. These authors begin
by introducing the notion of a B-admissible family of functions. For completeness
we repeat their denition.
Denition 1 (Denition 2 in [8]) A family fCN : N 2 Ng of margin cost functions
is B-admissible for B  0 if for all N 2 N there is an interval Y  R of length no
more than B and a function 	N : [ 1; 1] 7! Y that satises
sgn( )  EZQN; [	N (Z)]  CN ()
for all  2 [ 1; 1], where EZQN; () denotes the expectation when Z is chosen
randomly as Z = (1=N)
P N
i=1 Z i , and P(Z i = 1) = (1 + )=2.
Denote the convex hull of a class H by co(H). The main theoretical result in [8] is
the following lemma.

Lemma 2 ([8], Theorem 3) For any B admissible family fCN : N 2 Ng of
margin functions, for any binary hypothesis class of VC dimension d v and any
distribution D on X  f1;+1g, with probability at least 1 ∆ over a random
sample S of m examples drawn at random according to D, every N and ev-
ery f 2 co(H) satises PD [yf(x)  0]  ES [CN (yf(x)] +  N , where  N =
O
 
(B 2 Nd v log m+ log(N=∆))=m
 1=2

:
Remark 2 The most appealing feature of Lemma 2, as of other results for convex
combinations, is the fact that the bound does not depend on the number of hy-
potheses from H dening f 2 co(H), which may in fact be innite. Using standard
VC results (e.g. [11]) would lead to useless bounds, since the VC dimension of these
classes is often huge (possibly innite).
Lemma 2 considers binary hypotheses. Since recent works has demonstrated the
eectiveness of using real valued hypotheses, we consider the case where the weak
classiers may be condence-rated, i.e., taking values in [ 1; 1] rather than f1g.
We rst extend Lemma 2 to condence-rated classiers. Note that the variables Z i
in Denition 1 are no longer binary in this case.
Lemma 3 Let the conditions of Lemma 2 hold, except that H is a class of real
valued functions from X to [ 1; +1] of pseudo-dimension d p . Assume further that
	N in Denition 1 obeys a Lipschitz condition of the form j	N (x) 	N (x 0 )j 
Ljx x 0 j for every x; x 0 2 X. Then with probability at least 1 ∆, PD [yf(x)  0] 
ES [CN (yf(x)] +  N , where  N = O
 
(LB 2 Nd p log m+ log(N=∆))=m
 1=2

:
Proof The proof is very similar to the proof of Theorem 2, and will be omitted for
the sake of brevity.
It is well known that in the standard setting where CN is replaced by the empirical
classication error, improved rates, replacing O(
p
log m=m) by O(log m=m), are
possible in two situations: (i) if the minimal value of CN is zero (the restricted
model of [1]), and (ii) if the empirical error is replaced by (1+)CN for some  > 0.
The latter case is especially important in a model selection setup, where nested
classes of hypothesis functions are considered, since in this case one expects that,
with high probability, CN becomes smaller as the classes become more complex. In
this situation, case (ii) provides better overall bounds, often leading to the optimal
minimax rates for non parametric problems (see a discussion of these issues in Sec.
15.4 of [1]).
We now establish a faster convergence rate to a slightly larger value than
ES [CN (Y f(X))]. In situations where the latter quantity approaches zero, the
overall convergence rate may be improved, as discussed above. We consider cost
functions CN (), which obey the condition
CN ()  (1 + N )I( < 0) + N ( N > 0; N > 0): (5)
for some positive N and N (see [8] for details on legitimate cost functions).
Theorem 2 Let D be a distribution over X f1;+1g, and let S be a sample of
m points chosen independently at random according to D. Let d p be the pseudo-
dimension of the class H, and assume that CN () obeys condition (5). Then for
su∆ciently large m, with probability at least 1 ∆, every function f 2 co(H) satises
the following bound for every 0 <  < 1=N
PD [Y f(X)  0] 

1 + 
1 N

ES [CN (Y f(X))] +O
 
d p N log m
dp + log 1
∆
m=(1 + 2)
!
:

Proof The proof combines two ideas. First, we use the method of [8] to transform
the problem from co(H) to a discrete approximation of it. Then, we use recent
results for relative uniform deviations of averages from their means [3]. Due to lack
of space, we defer the complete proof to the full version of the paper.
4 Discussion
In this paper we have presented two main results pertaining to the theory of boost-
ing. First, we have shown that, under reasonable conditions, an eective weak
classier exists for one dimensional problems. We conjectured, and supported our
claim by numerical simulations, that such a result holds for multi-dimensional prob-
lems as well. The non-trivial extension of the proof to multiple dimensions can be
found in [7]. Second, using recent advances in the theory of uniform convergence and
boosting we have presented bounds on the generalization error, which may, under
certain conditions, be signicantly better than standard bounds, being particularly
useful in the context of model selection.
Acknowledgment We thank Shai Ben-David and Yoav Freund for helpful discus-
sions.
References
[1] M. Anthony and P.L. Bartlett. Neural Network Learning: Theoretical Foun-
dations. Cambridge University Press, 1999.
[2] P. Bartlett and S. Ben-David. On the hardness of learning with neural net-
works. In Proceedings of the fourth European Conference on computational
Learning Theory, 99.
[3] P. Bartlett and G. Lugosi. An inequality for uniform deviations of sample
averages from their means. Statistics and Probability Letters, 44:55{62, 1999.
[4] T. Hastie J. Friedman and R. Tibshirani. Additive logistic regression: a sta-
tistical view of boosting. The Annals of Statistics, To appear, 2000.
[5] D.S. Johnson and F.P. Preparata. The densest hemisphere problem. Theoret-
ical Computer Science, 6:93{107, 1978.
[6] S. Mallat and Z. Zhang. Matching pursuit with time-frequencey dictionaries.
IEEE Trans. Signal Processing, 41(12):3397{3415, December 1993.
[7] S. Mannor and R. Meir. On the existence of weak learners and applications to
boosting. Submitted to Machine Learning
[8] L. Mason, P. Bartlett and J. Baxter. Improved generalization through explicit
optimization of margins. Machine Learning, 2000. To appear.
[9] L. Mason, P. Bartlett, J. Baxter and M. Frean. Functional gradient techniques
for combining hypotheses. In B. Scholkopf A. Smola, P. Bartlett and D. Schu-
urmans, editors, Advances in Large Margin Classiers. MIT Press, 2000.
[10] R.E. Schapire, Y. Freund, P. Bartlett and W.S. Lee. Boosting the margin:
a new explanation for the eectiveness of voting methods. The Annals of
Statistics, 26(5):1651{1686, 1998.
[11] V. N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer
Verlag, New York, 1982.

