From Margin To Sparsity
Thore Graepel, Ralf Herbrich
Computer Science Department
Technical University of Berlin
Berlin, Germany
{guru,ralfh}@cs.tu-berlin.de
Robert C. Williamson
Department of Engineering
Australian National University
Canberra, Australia
Bob.Williamson@anu.edu.au
Abstract
We present an improvement of Noviko's perceptron convergence
theorem. Reinterpreting this mistake bound as a margin dependent
sparsity guarantee allows us to give a PACstyle generalisation er-
ror bound for the classier learned by the perceptron learning algo-
rithm. The bound value crucially depends on the margin a support
vector machine would achieve on the same data set using the same
kernel. Ironically, the bound yields better guarantees than are cur-
rently available for the support vector solution itself.
1 Introduction
In the last few years there has been a large controversy about the signicance
of the attained margin, i.e. the smallest real valued output of a classiers before
thresholding, as an indicator of generalisation performance. Results in the VC, PAC
and luckiness frameworks seem to indicate that a large margin is a prerequisite
for small generalisation error bounds (see [14, 12]). These results caused many
researchers to focus on large margin methods such as the well known support vector
machine (SVM). On the other hand, the notion of sparsity is deemed important for
generalisation as can be seen from the popularity of Occam's razor like arguments
as well as compression considerations (see [8]).
In this paper we reconcile the two notions by reinterpreting an improved version of
Noviko's well known perceptron convergence theorem as a sparsity guarantee in
dual space: the existence of large margin classiers implies the existence of sparse
consistent classiers in dual space. Even better, this solution is easily found by
the perceptron algorithm. By combining the perceptron mistake bound with a
compression bound that originated from the work of Littlestone and Warmuth [8]
we are able to provide a PAC like generalisation error bound for the classier found
by the perceptron algorithm whose size is determined by the magnitude of the
maximally achievable margin on the dataset.
The paper is structured as follows: after introducing the perceptron in dual variables
in Section 2 we improve on Noviko's perceptron convergence bound in Section 3.
Our main result is presented in the subsequent section and its consequences on the
theoretical foundation of SVMs are discussed in Section 5.

2 (Dual) Kernel Perceptrons
We consider learning given m objects X = fx 1 ; : : : ; xm g 2 X m and a set Y =
fy 1 ; : : : ym g 2 Y m drawn iid from a xed distribution PXY = P Z over the space
X  f 1; +1g = Z of inputoutput pairs. Our hypotheses are linear classiers
x 7! sign (hw;  (x)i) in some xed feature space K  ` n
2 where we assume that a
mapping  : X ! K is chosen a priori 1 . Given the features  i : X ! R the classical
(primal) perceptron algorithm aims at nding a weight vector w 2 K consistent
with the training data. Recently, Vapnik [14] and others  in their work on SVMs
 have rediscovered that it may be advantageous to learn in the dual representation
(see [1]), i.e. expanding the weight vector in terms of the training data
w =
m
X
i=1
 i  (x i ) =
m
X
i=1
 i x i ; (1)
and learn the m expansion coecients  2 R m rather than the components of
w 2 K. This is particularly useful if the dimensionality n = dim (K) of the feature
space K is much greater (or possibly innite) than the number m of training points.
This dual representation can be used for a rather wide class of learning algorithms
(see [15])  in particular if all we need for learning is the real valued output hw; x i i K
of the classier at the m training points x 1 ; : : : ; xm . Thus it suces to choose a
symmetric function k : X  X ! R called kernel and to ensure that there exists a
mapping  k : X ! K such that
8x; x 0 2 X : k (x; x 0 ) = h k (x) ;  k (x 0 )i K : (2)
A sucient condition is given by Mercer's theorem.
Theorem 1 (Mercer Kernel [9, 7]). Any symmetric function k 2 L1 (X  X )
that is positive semidenite, i.e.
8f 2 L 2 (X ) :
Z
X
Z
X
k (x; x 0 ) f (x) f (x 0 ) dx dx 0  0 ;
is called a Mercer kernel and has the following property: if   i 2 L 2 (X ) solve the
eigenvalue problem
R
X k (x; x 0 )   i (x 0 ) dx 0 =  i   i (x) with
R
X   2
i (x) dx = 1 and
8i 6= j :
R
X   i (x)   j (x) dx = 0 then k can be expanded in a uniformly convergent
series, i.e.
k (x; x 0 ) =
1
X
i=1
 i   i (x)   i (x 0 ) :
In order to see that a Mercer kernel fulls equation (2) consider the mapping
 k (x) =
 p
 1   1 (x) ;
p
 2   2 (x) ; : : :

(3)
whose existence is ensured by the third property. Finally, the perceptron learning
algorithm we are going to consider is described in the following denition.
Denition 1 (Perceptron Learning). The perceptron learning procedure with
the xed learning rate  2 R + is as follows:
1. Start in step zero, i.e. t = 0, with the vector  t = 0.
2. If there exists an index i 2 f1; : : : ; mg such that y i hw t ; x i i K  0 then
( t+1 ) i = ( t ) i + y i , w t+1 = w t + y i x i : (4)
and t   t + 1.
1 Somtimes, we abbreviate  (x) by x always assuming  is xed.

3. Stop, if there is no i 2 f1; : : : ; mg such that y i hw t ; x i i K  0.
Other variants of this algorithm have been presented elsewhere (see [2, 3]).
3 An Improvement of Noviko's Theorem
In the early 60's Noviko [10] was able to give an upper bound on the number
of mistakes made by the classical perceptron learning procedure. Two years later,
this bound was generalised to feature spaces using Mercer kernels by Aizerman et
al. [1]. The quantity determining the upper bound is the maximally achievable
unnormalised margin max2R m Z () normalised by the total extent R (X) of the
data in feature space, i.e. R (X) = max x i 2X kx i k K .
Denition 2 (Unnormalised Margin). Given a training set Z = (X; Y ) and a
vector  2 R m the unnormalised margin Z () is given by
Z () = min
(x i ;y i )2Z
y i hw ; x i i K
kw k K
:
Theorem 2 (Novikos Perceptron Convergence Theorem [10, 1]). Let Z =
(X; Y ) be a training set of size m. Suppose that there exists a vector   2 R m such
that Z (  ) > 0. Then the number of mistakes made by the perceptron algorithm
in Denition 1 on Z is at most
 R (X)
Z (  )
 2
:
Surprisingly, this bound is highly inuenced by the data point x i 2 X with the
largest norm kx i k albeit rescaling of a data point would not change its classica-
tion. Let us consider rescaling of the training set X before applying the perceptron
algorithm. Then for the normalised training set we would have R (X norm ) = 1 and
Z () would change into the normalised margin Z () rst advocated in [6].
Denition 3 (Normalised Margin). Given a training set Z = (X; Y ) and a
vector  2 R m the normalised margin Z () is given by
Z () = min
(x i ;y i )2Z
y i hw ; x i i K
kw k K kx i k K
:
By denition, for all x i 2 X we have R (X)  kx i k K . Hence for any  2 R m and
all (x i ; y i ) 2 Z such that y i hw ; x i i K > 0
R (X)
y i hw ;x i i K
kwk K
 kx i k K
y i hw ;x i i K
kwk K
=
1
y i hw ;x i i K
kwk K kx i k K
;
which immediately implies for all Z = (X; Y ) 2 Z m such that Z () > 0
R (X)
Z ()
 1
Z ()
: (5)
Thus when normalising the data in feature space, i.e.
k norm (x; x 0 ) =
k (x; x 0 )
p
k (x; x)  k (x 0 ; x 0 )
;
the upper bound on the number of steps until convergence of the classical perceptron
learning procedure of Rosenblatt [11] is provably decreasing and is given by the
squared r.h.s of (5).

Considering the form of the update rule (4) we observe that this result not only
bounds the number of mistakes made during learning but also the number kk 0
of nonzero coecients in the  vector. To be precise, for  = 1 it bounds the ` 1
norm kk 1 of the coecient vector  which, in turn, bounds the zero norm kk 0
from above for all vectors with integer components. Theorem 2 thus establishes a
relation between the existence of a large margin classier w  and the sparseness of
any solution found by the perceptron algorithm.
4 Main Result
In order to exploit the guaranteed sparseness of the solution of a kernel perceptron
we make use of the following lemma to be found in [8, 4].
Lemma 1 (Compression Lemma). Fix d 2 f1; : : : ; mg. For any measure P Z ,
the probability that m examples Z drawn iid according to P Z will yield a classier
 (Z) learned by the perceptron algorithm with k (Z)k 0 = d whose generalisation
error PXY

Y
 w(Z) ;  (X)

K  0
 is greater than " is at most
 m
d

(1 ") m d : (6)
Proof. Since we restrict the solution  (Z) with generalisation error greater than
" only to use d points Z d  Z but still to be consistent with the remaining set
Z n Z d , this probability is at most (1 ") m d for a xed subset Z d . The result
follows by the union bound over all m
d

subsets Z d . Intuitively, the consistency on
the m d unused training points witnesses the small generalisation error with high
probability.
If we set (6) to Æ
m and solve for " we have that with probability at most Æ
m over
the random draw of the training set Z the perceptron learning algorithm nds
a vector  such that kk 0 = d and whose generalisation error is greater than
" (m; d) = 1
m d ln m
d

+ ln (m) + ln 1
Æ
 . Thus by the union bound, if the per-
ceptron algorithm converges, the probability that the generalisation error of its
solution is greater than " (m; kk 0 ) is at most Æ. We have shown the following
sparsity bounds also to be found in [4].
Theorem 3 (Generalisation Error Bound for Perceptrons). For any measure
P Z , with probability at least 1 Æ over the random draw of the training set Z of
size m, if the perceptron learning algorithm converges to the vector  of coecients
then its generalisation error PXY

Y
 w (Z) ;  (X)

K  0
 is less than
1
m kk 0

ln
 m
kk 0

+ ln (m) + ln

1
Æ

: (7)
This theorem in itself constitutes a powerful result and can easily be adopted to
hold for a large class of learning algorithms including SVMs [4]. This bound often
outperforms margin bounds for practically relevant training set sizes, e.g. m <
100 000. Combining Theorem 2 and Theorem 3 thus gives our main result.

Theorem 4 (Margin Bound). For any measure P Z , with probability at least
1 Æ over the random draw of the training set Z of size m, if there exists a
vector   such that
  =
& 
R (X)
Z (  )
 2
'
 m
then the generalisation error PXY

Y
 w (Z) ;  (X)

K  0

of the classier 
found by the perceptron algorithm is less than
1
m  

ln
 m
 

+ ln (m) + ln
 1
Æ

: (8)
The most intriguing feature of this result is that the mere existence of a large
margin classier   is sucient to guarantee a small generalisation error for the
solution  of the perceptron although its attained margin Z () is likely to be
much smaller than Z (  ). It has long been argued that the attained margin Z ()
itself is the crucial quantity controlling the generalisation error of . In light of
our new result if there exists a consistent classier   with large margin we know
that there also exists at least one classier  with high sparsity that can eciently
be found using the perceptron algorithm. In fact, whenever the SVM appears to
be theoretically justied by a large observed margin, every solution found by the
perceptron algorithm has a small guaranteed generalisation error  mostly even
smaller than current bounds on the generalisation error of SVMs. Note that for
a given training sample Z it is not unlikely that by permutation of Z there exist
O m
 

many dierent consistent sparse classiers .
5 Impact on the Foundations of Support Vector Machines
Support vector machines owe their popularity mainly to their theoretical justica-
tion in the learning theory. In particular, two arguments have been put forward to
single out the solutions found by SVMs [14, p. 139]:
SVM (optimal hyperplanes) can generalise because
1. the expectation of the data compression is large.
2. the expectation of the margin is large.
The second reason is often justied by margin results (see [14, 12]) which bound
the generalisation of a classier  in terms of its own attained margin Z (). If
we require the slightly stronger condition that   < m
n ; n  4, then our bound (8)
for solutions of perceptron learning can be upper bounded by
1
m

  ln

2em
 

+ ln m n
n 1

+ ln

1
Æ n
n 1

;
which has to be compared with the PAC margin bound (see [12, 5])
2
m

64  log 2

2em
16 

log 2 (32m) + log 2 (2m) + log 2

1
Æ

:
Despite the fact that the former result also holds true for the margin Z (  ) (which
could loosely be upper bounded by (5))
 the PAC margin bound's decay (as a function of m) is slower by a log 2 (32m)
factor,

digit 0 1 2 3 4 5 6 7 8 9
perceptron 0:2 0:2 0:4 0:4 0:4 0:4 0:4 0:5 0:6 0:7
kk 0 740 643 1168 1512 1078 1277 823 1103 1856 1920
mistakes 844 843 1345 1811 1222 1497 960 1323 2326 2367
bound 6:7 6:0 9:8 12:0 9:2 10:5 7:4 9:4 14:3 14:6
SVM 0:2 0:1 0:4 0:4 0:4 0:5 0:3 0:4 0:5 0:6
kk 0 1379 989 1958 1900 1224 2024 1527 2064 2332 2765
bound 11:2 8:6 14:9 14:5 10:2 15:3 12:2 15:5 17:1 19:6
Table 1: Results of kernel perceptrons and SVMs on NIST (taken from [2, Table
3]). The kernel used was k (x; x 0 ) = (hx; x 0 i X + 1) 4 and m = 60 000. For both
algorithms we give the measured generalisation error (in %), the attained sparsity
and the bound value (in %, Æ = 0:05) of (7).
 for any m and almost any Æ the margin bound given in Theorem 4 guaran-
tees a smaller generalisation error.
 For example, using the empirical value    600 (see [14, p. 153]) in
the NIST handwritten digit recognition task and inserting this value into
the PAC margin bound, it would need the astronomically large number of
m > 410 743 386 to obtain a bound value of 0:112 as obtained by (3) for
the digit 0 (see Table 1).
With regard to the rst reason, it has been conrmed experimentally that SVMs nd
solutions which are sparse in the expansion coecients . However, there cannot
exist any distributionfree guarantee that the number of support vectors will in fact
be small 2 . In contrast, Theorem 2 gives an explicit bound on the sparsity in terms
of the achievable margin Z (  ). Furthermore, experimental results on the NIST
datasets show that the sparsity of solution found by the perceptron algorithm is
consistently (and often by a factor of two) greater than that of the SVM solution
(see [2, Table 3] and Table 1).
6 Conclusion
We have shown that the generalisation error of a very simple and ecient learning
algorithm for linear classiers  the perceptron algorithm  can be bounded by
a quantity involving the margin of the classier the SVM would have found on the
same training data using the same kernel. This result implies that the SVM solution
is not at all singled out as being superior in terms of provable generalisation error.
Also, the result indicates that sparsity of the solution may be a more fundamental
property than the size of the attained margin (since a large value of the latter
implies a large value of the former).
Our analysis raises an interesting question: having chosen a good kernel, correspond-
ing to a metric in which interclass distances are great and intraclass distances are
short, in how far does it matter which consistent classier we use? Experimental
2 Consider a distribution PXY on two parallel lines with support in the unit ball. Suppose
that their mutual distance is p
2. Then the number of support vectors equals the training
set size whereas the perceptron algorithm never uses more than two points by Theorem 2.
One could argue that it is the number of essential support vectors [13] that characterises
the data compression of an SVM (which would also have been two in our example). Their
determination, however, involves a combinatorial optimisation problem and can thus never
be performed in practical applications.

results seem to indicate that a vast variety of heuristics for nding consistent clas-
siers, e.g. kernel Fisher discriminant, linear programming machines, Bayes point
machines, kernel PCA & linear SVM, sparse greedy matrix approximation perform
comparably (see http://www.kernel-machines.org/).
Acknowledgements
This work was done while TG and RH were visiting the ANU Canberra. They
would like to thank Peter Bartlett and Jon Baxter for many interesting discussions.
Furthermore, we would like to thank the anonymous reviewer, Olivier Bousquet and
Matthias Seeger for very useful remarks on the paper.
References
[1] M. Aizerman, E. Braverman, and L. Rozonoer. Theoretical foundations of the po-
tential function method in pattern recognition learning. Automation and Remote
Control, 25:821837, 1964.
[2] Y. Freund and R. E. Schapire. Large margin classication using the perceptron
algorithm. Machine Learning, 1999.
[3] T. Friess, N. Cristianini, and C. Campbell. The Kernel-Adatron: A fast and sim-
ple learning procedure for Support Vector Machines. In Proceedings of the 15th
International Conference in Machine Learning, pages 188196, 1998.
[4] T. Graepel, R. Herbrich, and J. Shawe-Taylor. Generalisation error bounds for sparse
linear classiers. In Proceedings of the Thirteenth Annual Conference on Computa-
tional Learning Theory, pages 298303, 2000. in press.
[5] R. Herbrich. Learning Linear Classiers - Theory and Algorithms. PhD thesis, Tech-
nische Universität Berlin, 2000. accepted for publication by MIT Press.
[6] R. Herbrich and T. Graepel. A PAC-Bayesian margin bound for linear classiers:
Why SVMs work. In Advances in Neural Information System Processing 13, 2001.
[7] H. König. Eigenvalue Distribution of Compact Operators. Birkhäuser, Basel, 1986.
[8] N. Littlestone and M. Warmuth. Relating data compression and learnability. Tech-
nical report, University of California Santa Cruz, 1986.
[9] T. Mercer. Functions of positive and negative type and their connection with the
theory of integral equations. Transaction of London Philosophy Society (A), 209:415
446, 1909.
[10] A. Noviko. On convergence proofs for perceptrons. In Report at the Symposium
on Mathematical Theory of Automata, pages 2426, Politechnical Institute Brooklyn,
1962.
[11] M. Rosenblatt. Principles of neurodynamics: Perceptron and Theory of Brain Mech-
anisms. SpartanBooks, Washington D.C., 1962.
[12] J. Shawe-Taylor, P. L. Bartlett, R. C. Williamson, and M. Anthony. Structural risk
minimization over datadependent hierarchies. IEEE Transactions on Information
Theory, 44(5):19261940, 1998.
[13] V. Vapnik. Statistical Learning Theory. John Wiley and Sons, New York, 1998.
[14] V. Vapnik. The Nature of Statistical Learning Theory. Springer, second edition, 1999.
[15] G. Wahba. Support Vector Machines, Reproducing Kernel Hilbert Spaces and the ran-
domized GACV. Technical report, Department of Statistics, University of Wisconsin,
Madison, 1997. TRNO984.

