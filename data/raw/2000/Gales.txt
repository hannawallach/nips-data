Factored Semi­Tied Covariance Matrices
M.J.F. Gales
Cambridge University Engineering Department
Trumpington Street, Cambridge. CB2 1PZ
United Kingdom
mjfg@eng.cam.ac.uk
Abstract
A new form of covariance modelling for Gaussian mixture models and
hidden Markov models is presented. This is an extension to an efficient
form of covariance modelling used in speech recognition, semi­tied co­
variance matrices. In the standard form of semi­tied covariance matrices
the covariance matrix is decomposed into a highly shared decorrelating
transform and a component­specific diagonal covariance matrix. The use
of a factored decorrelating transform is presented in this paper. This fac­
toring effectively increases the number of possible transforms without in­
creasing the number of free parameters. Maximum likelihood estimation
schemes for all the model parameters are presented including the compo­
nent/transform assignment, transform and component parameters. This
new model form is evaluated on a large vocabulary speech recognition
task. It is shown that using this factored form of covariance modelling
reduces the word error rate.
1 Introduction
A standard problem in machine learning is to how to efficiently model correlations in multi­
dimensional data. Solutions should be efficient both in terms of number of model param­
eters and cost of the likelihood calculation. For speech recognition this is particularly
important due to the large number of Gaussian components used, typically in the tens of
thousands, and the relatively large dimensionality of the data, typically 30­60.
The following generative model has been used in speech recognition 1
x(ø) = w (1)
o(ø) = F
Ÿ x(ø)
v
–
(2)
where x(ø) is the underlying speech signal, F is the observation transformation matrix, w
is generated by a hidden Markov model (HMM) with diagonal covariance matrix Gaussian
1 This describes the static version of the generative model. The more general version is described
by replacing equation 1 by x(ø ) = Cx(ø \Gamma 1) +w.

mixture model (GMM) to model each state 2 and v is usually assumed to be generated by a
GMM, which is common to all HMMs. This differs from the static linear Gaussian models
presented in [7] in two important ways. First w is generated by either an HMM or GMM,
rather than a simple Gaussian distribution. The second difference is that the ``noise'' is
now restricted to the null space of the signal x(ø). This type of system can be considered
to have two streams. The first stream, the n 1 dimensions associated with x(ø), is the set
of discriminating, useful, dimensions. The second stream, the n 2 dimensions associated
with v, is the set of non­discriminating, nuisance, dimensions. Linear discriminant analy­
sis (LDA) and heteroscedastic LDA (HLDA) [5] are both based on this form of generative
model. When the dimensionality of the nuisance dimensions is reduced to zero this gener­
ative model becomes equivalent to a semi­tied covariance matrix system [3] with a single,
global, semi­tied class.
This generative model has a clear advantage during recognition compared to the standard
linear Gaussian models [2] in the reduction in the computational cost of the likelihood
calculation. The likelihood for component m may be computed as 3
p(o(ø ); ¯ (m) ; \Sigma (m)
diag ; F ) =
l(ø)
jdet(F )j
N ((F \Gamma1 ) [1] o(ø ); ¯ (m) ; \Sigma (m)
diag ) (3)
where ¯ (m) is the n 1 ­dimensional mean and \Sigma (m)
diag the diagonal covariance matrix of Gaus­
sian component m. l(ø) is the nuisance dimension likelihood which is independent of the
component being considered and only needs to be computed once for each time instance.
The initial normalisation term is only required during recognition when multiple trans­
forms are used. The dominant cost is a diagonal Gaussian computation for each compo­
nent, O(n 1 ) per component. In contrast a scheme such as factor analysis (a covariance
modelling scheme from the linear Gaussian model in [7]) has a cost of O(n 2
1 ) per compo­
nent (assuming there are n 1 factors). The disadvantage of this form of generative model is
that there is no simple expectation­maximisation (EM) [1] scheme for estimating the model
parameters. However, a simple iterative scheme is available [3].
For some tasks, such as speech recognition where there are many different ``sounds'' to be
recognised, it is unlikely that a single transform is sufficient to well model the data. To
reflect this there has been some work on using multiple feature­spaces [3, 2]. The stan­
dard approach for using multiple transforms is to assign each component, m, to a particular
transform, F (rm ) . To simplify the description of the new scheme only modifications to the
semi­tied covariance matrix scheme, where the nuisance dimension is zero, are considered.
The generative model is modified to be o(ø) = F (rm ) x(ø), where r m is the transform
class associated with the generating component, m, at time instance ø . The assignment
variable, r m , may either be determined by an ``expert'', for example using phonetic context
information, or it may be assigned in a maximum likelihood (ML) fashion [3]. Simply
2 Although it is not strictly necessary to use diagonal covariance matrices, these currently dominate
applications in speech recognition. w could also be generated by a simple GMM.
3 This paper uses the following convention: capital bold letters refer to matrices e.g. A, bold
letters refer to vectors e.g. b, and scalars are not bold e.g. c. When referring to elements of a matrix
or vector subscripts are used e.g. a i is the i th row of matrix A, a ij is the element of row i column
j of matrix A and b i is element i of vector b. Diagonal matrices are indicated by A diag . Where
multiple streams are used this is indicated, for example, by A [s] , this is a ns \Theta n matrix (n is the
dimensionality of the feature vector and ns is the size of stream s). Where subsets of the diagonal
matrices are specified the matrices are square, e.g. A diag[s] is ns \Theta ns square diagonal matrix. A T
is the transpose of the matrix and det(A) is the determinant of the matrix.

increasing the number of transforms increases the number of model parameters to be esti­
mated, hence reducing the robustness of the estimates. There is a corresponding increase in
the computational cost during recognition. In the limit there is a single transform per com­
ponent, the standard full­covariance matrix case. The approach adopted in this paper is to
factor the transform into multiple streams. Each component can then use a different trans­
form for each stream. Hence instead of using an assignment variable an assignment vector
is used. In order to maintain the efficient likelihood computation of equation 3, F (r)\Gamma1 ,
rather than F (r) , must be factored into rows. This is a partitioning of the feature space into
a set of observation streams. In common with other factoring schemes this dramatically in­
creases the effective number of transforms from which each component may select without
increasing the number of transform parameters. Though this paper only considers factoring
semi­tied covariance matrices the extension to the ``projection'' schemes presented in [2] is
straightforward.
This paper describes how to estimate the set of transforms and determine which subspaces
a particular component should use. The next section describes how to assign components
to transforms and, given this assignment, how to estimate the appropriate transforms. Some
initial experiments on a large vocabulary speech recognition task are presented in the fol­
lowing section.
2 Factored Semi­Tied Covariance Matrices
In order to factor semi­tied covariance matrices the inverse of the observation transforma­
tion for a component is broken into multiple streams. The feature space of each stream is
then determined by selecting from an inventory of possible transforms. Consider the case
where there are S streams. The effective full covariance matrix of component m, \Sigma (m) ,
may be written as \Sigma (m) = F (z (m) ) \Sigma (m)
diag F (z (m) )T , where the form of F (z (m) ) is restricted
so that 4
F (z (m) )\Gamma1 = A (z (m) ) =
2
6 6 6 4
A (z (m)
1 )
[1]
. . .
A (z (m)
S )
[S]
3
7 7 7 5 (4)
and z (m) is the S­dimensional assignment vector for component m. The complete set of
model parameters, M, consists of the standard model parameters, the component means,
variances, weights and, additionally, the set of transforms
n
A (1)
[s] ; : : : ; A (Rs )
[s]
o
for each
stream s (R s is the number of transforms associated with stream s) and the assignment
vector z (m) for each component. Note that the semi­tied covariance matrix scheme is the
case when S = 1. The likelihood is efficiently estimated by storing transformed observa­
tions for each stream transform, i.e. A (r)
[s] o(ø).
The model parameters are estimated using ML training on a labelled set of training data
O = fo(1); : : : ; o(T )g. The likelihood of the training data may be written as
p(OjM) =
X
\Theta
Y
ø
0
@ p(q(ø)jq(ø \Gamma 1))
X
m2`(ø)
w (m) p(o(ø ); ¯ (m) ; \Sigma (m)
diag ; A (z (m) ) )
1
A (5)
4 A similar factorisation has also been proposed in [4].

where \Theta is the set of all valid state sequences according to the transcription for the data,
q(ø) is the state at time ø of the current path, `(ø) is the set of Gaussian components be­
longing to state q(ø), and w (m) is the prior of component m. Directly optimising equation 5
is a very large optimisation task, as there are typically millions of model parameters. Alter­
natively, as is common with standard HMM training, an EM­based approach is used. The
posterior probability of a particular component, m, generating the observation at a given
time instance is denoted as fl m (ø). This may be simply found using the forward backward
algorithm [6] and the old set of model parameters “
M. The new set of model parameters
will be denoted as M. The estimation of the component priors and HMM transition ma­
trices are estimated in the standard fashion [6]. Directly optimising the auxiliary function
for the model parameters is computationally expensive [3] and does not allow the embed­
ding of the assignment process. Instead a simple iterative optimisation scheme is used as
follows:
1. Estimate the within class covariance matrix for each Gaussian component in the
system, W (m) , using the values of fl m (ø). Initialise the set of assignment vectors,
n
~
Z
o
=
\Phi z (1) ; : : : ; z (M)
\Psi
and the set of transforms for each stream
n
~
A
o
=
n
A (1)
[1] ; : : : ; A (R1 )
[1] ; : : : ; A (1)
[S] ; : : : ; A (RS )
[S]
o
.
2. Using the current estimates of the transforms and assignment vectors obtain the
ML estimate of the set of component specific diagonal covariance matrices incor­
porating the appropriate parameter tying as required. This set of parameters will
be denoted as
n
~
\Sigma
o
=
n
\Sigma (1)
diag ; : : : ; \Sigma (M)
diag
o
.
3. Estimate the new set of transforms,
n
~
A
o
, using the current set of component co­
variance matrices
n
~
\Sigma
o
and assignment vectors
n
~
Z
o
. The new auxiliary function
at this stage will be written as Q(M; “
M;
n
~
\Sigma
o
;
n
~
Z
o
).
4. Update the set of assignment variables for each component
n
~
Z
o
, given the current
set of model transforms,
n
~
A
o
.
5. Goto (2) until convergence, or an appropriate stopping criterion is satisfied. Oth­
erwise update
n
~
\Sigma
o
and the component means using the latest transforms and
assignment variables.
There are three distinct optimisation problems within this task. First the ML estimate of
the set of component specific diagonal covariance matrices is required. Second, the new
set of transforms must be estimated. Finally the new set of assignment vectors is required.
The ML estimates of the component specific variances (and means) under a transformation
is a standard problem, e.g. for the semi­tied case see [3] and is not described further. The
ML estimation of the transforms and assignment variables are described below.
The transforms are estimated in an iterative fashion. The proposed scheme is derived by
modifying the standard semi­tied covariance optimisation equation in [3]. A row by row

optimisation is used. Consider row i of stream p of transform r, a (r)
[p]i , the auxiliary function
may be written as (ignoring constant scalings and elements independent of a (r)
[p]i )
Q(M; “
M;
n
~
\Sigma
o
;
n
~
Z
o
) =
X
m
fi (m) log
`
(c (z (m) )
[p]i
a (z (m)
p )T
[p]i ) 2
'
\Gamma
X
s;r;j
a (r)
[s]j
K (srj) a (r)T
[s]j
where fi (m) =
P
ø fl m (ø),
K (srj) =
X
m:fz (m)
s =rg
W (m)
oe (m)2
diag[s]j
X
ø
fl m (ø) (6)
and c (z (m) )
[p]i is the cofactor of row i of stream p of transform A (z (m) ) . The gradient f (r)
[p]i ,
differentiating the auxiliary function with respect to a (r)
[p]i , is given by 5
f (r)
[p]i =
X
m:fz (m)
p =rg
8 !
: 2
fi (m) c (z (m) )
[p]i
c (z (m) )
[p]i
a (r)T
[p]i
9 =
; \Gamma 2a (r)
[p]i K (pri) (8)
The main cost for computing the gradient is calculating the cofactors for each component.
Having computed the gradient the Hessian may also be simply calculated as
H (r)
[p]i =
X
m:fz (m)
p =rg
8 !
: \Gamma2 fi (m) c (z (m) )T
[p]i c (z (m) )
[p]i
(c (z (m) )
[p]i a (r)T
[p]i ) 2
9 =
; \Gamma 2K (pri) (9)
The Hessian is guaranteed to be negative definite so the Newton direction must head to­
wards a maximum. At the t + 1 th iteration
a (r)
[p]i (t + 1) = a (r)
[p]i (t) \Gamma f (r)
[p]i H (r)\Gamma1
[p]i (10)
where the gradient and Hessian are based on the t th parameter estimates. In practice this
estimation scheme was highly stable.
The assignment for stream s of component m is found using a greedy search technique
based on ML estimation. Stream s of component m is assigned using
z (m)
s = arg max
r2Rs
8 !
:
0
@ jdet
i
A (u (srm) )
j
j 2
fi fi fidet
i
diag
i
A (r)
[s]
W (m) A (r)T
[s]
jjfi fi fi
1
A
9 =
; (11)
where the hypothesised assignment of factor stream s, u (srm) , is given by
u (srm)
j =
ae r; j = s
z (m)
j ; (otherwise)
(12)
5 When the standard semi­tied system is used (i.e. S = 1) the estimation of row, i has the closed
form solution
a
(r)
[1]i = c
(r)
[1]i K (1ri)\Gamma1
v u u t
/P
m:fz (m)
1
=rg fi (m)
c
(r)
[1]i K (1ri)\Gamma1 c
(r)T
[1]i
!
(7)

As the assignment is dependent on the cofactors, which themselves are dependent on the
other stream assignments for that component, an iterative scheme is required. In practice
this was found to converge rapidly.
3 Results and Discussion
An initial investigation of the use of factored semi­tied covariance matrices was carried
out on a large­vocabulary speaker­independent continuous­speech recognition task. The
recognition experiments were performed on the 1994 ARPA Hub 1 data (the H1 task), an
unlimited vocabulary task. The results were averaged over the development and evaluation
data. Note that no tuning on the ``development'' data was performed. The baseline sys­
tem used for the recognition task was a gender­independent cross­word­triphone mixture­
Gaussian tied­state HMM system. For details of the system see [8]. The total number of
phones (counting silence as a separate phone) was 46, from which 6399 distinct context
states were formed. The speech was parameterised into a 39­dimensional feature vector.
The set of baseline experiments with semi­tied covariance matrices (S = 1) used ``expert''
knowledge to determine the transform classes. Two sets were used. The first was based
on phone level transforms where all components of all states from the same phone shared
the same class (phone classes). The second used an individual transform per state (state
classes). In addition a global transform (global class) and a full­covariance matrix system
(comp class) were tested. Two systems were examined, a four Gaussian components per
state system and a twelve Gaussian component system. The twelve component system is
the standard system described in [8]. In both cases a diagonal covariance matrix system (la­
belled none) was generated in the standard HTK fashion [9]. These systems were then used
to generate the initial alignments to build the semi­tied systems. An additional iteration of
Baum­Welch estimation was then performed.
Three forms of assignment training were compared. The previously described expert sys­
tem and two ML­based schemes, standard and factored. The standard scheme used a single
stream (S = 1) which is similar to the scheme described in [3]. The factored scheme used
the new approach described in this paper with a separate stream for each of the elements of
the feature vector (S = 39).
Table 1: System performance on the 1994 ARPA H1 task
Transform Assignment Components
Classes Scheme 4 12
none --- 11.11 9.71
global --- 10.34 8.87
phone expert 10.04 8.86
state expert 9.20 8.84
comp --- 9.22 9.98
phone standard 9.73 8.62
phone factored 9.48 8.42
The results of the baseline semi­tied covariance matrix systems are shown in table 1. For the
four component system the full covariance matrix system achieved approximately the same
performance as that of the expert state semi­tied system. Both systems significantly (at the

95% level) outperformed the standard 12­component system (9.71%). The expert phone
system shows around an 9% degradation in performance compared to the state system,
but used less than a hundredth of the number of transforms (46 versus 6399). Using the
standard ML assignment scheme with initial phone classes, S = 1, reduced the error rate
of the phone system by around 3% over the expert system. The factored scheme, S = 39,
achieved further reductions in error rate. A 5% reduction in word error rate was achieved
over the expert system, which is significant at the 95% level.
Table 1 also shows the performance of the twelve component system. The use of a global
semi­tied transform significantly reduced the error rate by around 9% relative. Increasing
the number of transforms using the expert assignment showed no reduction in error rate.
Again using the phone level system and training the component transform assignments,
either the standard or the factored schemes, reduced the word error rate. Using the factored
semi­tied transforms (S = 39) significantly reduced the error rate, by around 5%, compared
to the expert systems.
4 Conclusions
This paper has presented a new form of semi­tied covariance, the factored semi­tied co­
variance matrix. The theory for estimating these transforms has been developed and im­
plemented on a large vocabulary speech recognition task. On this task the use of these
factored transforms was found to decrease the word error rate by around 5% over using a
single transform, or multiple transforms, where the assignments are expertly determined.
The improvement was significant at the 95% level. In future work the problems of deter­
mining the required number of transforms for each of the streams and how to determine the
appropriate dimensions will be investigated.
References
[1] A P Dempster, N M Laird, and D B Rubin. Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Society, 39:1--38, 1977.
[2] M J F Gales. Maximum likelihood multiple projection schemes for hidden Markov models. Tech­
nical Report CUED/F­INFENG/TR365, Cambridge University, 1999. Available via anonymous
ftp from: svr­ftp.eng.cam.ac.uk.
[3] M J F Gales. Semi­tied covariance matrices for hidden Markov models. IEEE Transactions
Speech and Audio Processing, 7:272--281, 1999.
[4] NK Goel and R Gopinath. Multiple linear transforms. In Proceedings ICASSP, 2001. To appear.
[5] N Kumar. Investigation of Silicon­Auditory Models and Generalization of Linear Discriminant
Analysis for Improved Speech Recognition. PhD thesis, John Hopkins University, 1997.
[6] L R Rabiner. A tutorial on hidden Markov models and selected applications in speech recogni­
tion. Proceedings of the IEEE, 77, February 1989.
[7] S Roweiss and Z Ghahramani. A unifying review of linear Gaussian models. Neural Computa­
tion, 11:305--345, 1999.
[8] P C Woodland, J J Odell, V Valtchev, and S J Young. The development of the 1994 HTK large
vocabulary speech recognition system. In Proceedings ARPA Workshop on Spoken Language
Systems Technology, pages 104--109, 1995.
[9] S J Young, J Jansen, J Odell, D Ollason, and P Woodland. The HTK Book (for HTK Version 2.0).
Cambridge University, 1996.

