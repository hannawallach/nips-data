Regularization with Dot-Product Kernels
Alex J. Smola, Zoltan L. 
Ovari, and Robert C. Williamson
Department of Engineering
Australian National University
Canberra, ACT, 0200
Abstract
In this paper we give necessary and suÆcient conditions under
which kernels of dot product type k(x; y) = k(x  y) satisfy Mer-
cer's condition and thus may be used in Support Vector Ma-
chines (SVM), Regularization Networks (RN) or Gaussian Pro-
cesses (GP). In particular, we show that if the kernel is analytic
(i.e. can be expanded in a Taylor series), all expansion coeÆcients
have to be nonnegative. We give an explicit functional form for the
feature map by calculating its eigenfunctions and eigenvalues.
1 Introduction
Kernel functions are widely used in learning algorithms such as Support Vector Ma-
chines, Gaussian Processes, or Regularization Networks. A possible interpretation
of their eects is that they represent dot products in some feature space F, i.e.
k(x; y) = (x)  (y) (1)
where  is a map from input (data) space X into F. Another interpretation is to
connect  with the regularization properties of the corresponding learning algorithm
[8]. Most popular kernels can be described by three main categories: translation
invariant kernels [9]
k(x; y) = k(x y); (2)
kernels originating from generative models (e.g. those of Jaakkola and Haussler, or
Watkins), and thirdly, dot-product kernels
k(x; y) = k(x  y): (3)
Since k inuences the properties of the estimates generated by any of the algorithms
above, it is natural to ask which regularization properties are associated with k.
In [8, 10, 9] the general connections between kernels and regularization properties
are pointed out, containing details on the connection between the Fourier spectrum
of translation invariant kernels and the smoothness properties of the estimates. In
a nutshell, the necessary and suÆcient condition for k(x y) to be a Mercer kernel
(i.e. be admissible for any of the aforementioned kernel methods) is that its Fourier
transform be nonnegative. This also allowed for an easy to check criterion for new
kernel functions. Moreover, [5] gave a similar analysis for kernels derived from
generative models.

Dot product kernels k(x  y), on the other hand, have been eluding further theo-
retical analysis and only a necessary condition [1] was found, based on geometrical
considerations. Unfortunately, it does not provide much insight into smoothness
properties of the corresponding estimate.
Our aim in the present paper is to shed some light on the properties of dot product
kernels, give an explicit equation how its eigenvalues can be determined, and, nally,
show that for analytic kernels that can be expanded in terms of monomials  n or
associated Legendre polynomials P d
n () [4], i.e.
k(x; y) = k(x  y) with k() =
1
X
n=0
a n  n or k() =
1
X
n=0
b n P d
n () (4)
a necessary and suÆcient condition is a n  0 for all n 2 N if no assumption
about the dimensionality of the input space is made (for nite dimensional spaces
of dimension d, the condition is that b n  0). In other words, the polynomial
series expansion in dot product kernels plays the role of the Fourier transform in
translation invariant kernels.
2 Regularization, Kernels, and Integral Operators
Let us briey review some results from regularization theory, needed for the fur-
ther understanding of the paper. Many algorithms (SVM, GP, RN, etc.) can be
understood as minimizing a regularized risk functional
R reg [f ] := R emp [f ] + 
 f ] (5)
where R emp is the training error of the function f on the given data,  > 0
and
 f ]
is the so-called regularization term. The rst term depends on the specic problem
at hand (classication, regression, large margin algorithms, etc.),  is generally
adjusted by some model selection criterion,
and
 f ] is a nonnegative functional
of f which models our belief which functions should be considered to be simple (a
prior in the Bayesian sense or a structure in a Structural Risk Minimization sense).
2.1 Regularization Operators
One possible interpretation of k is [8] that it leads to regularized risk functionals
where
 f ] = 1
2 kPfk 2 or equivalently hPk(x; ); Pk(y; )i = k(x; y): (6)
Here P is a regularization operator mapping functions f on X into a dot product
space (we choose L 2 (X)). The following theorem allows us to construct explicit
operators P and it provides a criterion whether a symmetric function k(x; y) is
suitable.
Theorem 1 (Mercer [3]) Suppose k 2 L1 (X 2 ) such that the integral operator
T k : L 2 (X) ! L 2 (X),
T k f() :=
Z
X
k(; x)f(x)d(x) (7)
is positive. Let  j 2 L 2 (X) be the eigenfunction of T k with eigenvalue  j 6= 0 and
normalized such that k j k L2 = 1 and let  j denote its complex conjugate. Then
1. ( j (T )) j 2 ` 1 .
2.  j 2 L1 (X) and sup j k j k L1 < 1.

3. k(x; x 0 ) =
P
j2N
 j  j (x) j (x 0 ) holds for almost all (x; x 0 ), where the series
converges absolutely and uniformly for almost all (x; x 0 ).
This means that by nding the eigensystem ( i ;  i ) of T k we can also determine
the regularization operator P via [8]
Pf =
1
X
i=1
a i
p
 i
 i for any f =
1
X
i=1
a i  i : (8)
The eigensystem ( i ;  i ) tells us which functions are considered \simple" in terms
of the operator P . Consequently, in order to determine the regularization properties
of dot product kernels we have to nd their eigenfunctions and eigenvalues.
2.2 Specic Assumptions
Before we diagonalize T k for a given kernel we have yet to specify the assumptions
we make about the measure  and the domain of integration X. Since a suitable
choice can drastically simplify the problem we try to keep as much of the symmetries
imposed by k(x  y) as possible. The predominant symmetry in dot product kernels
is rotation invariance. Therefore we set choose the unit ball in R d
X := U d := fxjx 2 R d and kxk 2  1g: (9)
This is a benign assumption since the radius can always be adjusted by rescaling
k(x  y) ! k((x)  (y)). Similar considerations apply to translation. In some cases
the unit sphere in R d is more amenable to our analysis. There we choose
X := S d 1 := fxjx 2 R
d and kxk 2 = 1g: (10)
The latter is a good approximation of the situation where dot product kernels
perform best | if the training data has approximately equal Euclidean norm (e.g.
in images or handwritten digits). For the sake of simplicity we will limit ourselves
to (10) in most of the cases.
Secondly we choose  to be the uniform measure on X. This means that we have to
solve the following integral equation: Find functions  i : L 2 (X) ! R together with
coeÆcients  i such that T k  i (x) :=
R
X k(x  y) i (y)dy =  i  i (x).
3 Orthogonal Polynomials and Spherical Harmonics
Before we can give eigenfunctions or state necessary and suÆcient conditions we
need some basic relations about Legendre Polynomials and spherical harmonics.
Denote by P n () the Legendre Polynomials and by P d
n () the associated Legendre
Polynomials (see e.g. [4] for details). They have the following properties
 The polynomials P n () and P d
n () are of degree n, and moreover P n := P 3
n
 The (associated) Legendre Polynomials form an orthogonal basis with
Z 1
1
P d
n ()P d
m ()(1  2 ) d 3
2 d = jS d 1 j
jS d 2 j
1
N(d; n)
Æ m;n : (11)
Here jS d 1 j = 2 d=2
(d=2) denotes the surface of S d 1 , and N(d; n) denotes
the multiplicity of spherical harmonics of order n on S d 1 , i.e. N(d; n) =
2n+d 2
n
n+d 3
n 1
 .

 This admits the orthogonal expansion of any analytic function k() on
[ 1; 1] into P d
n by
k() =
1
X
i=0
N(d; n)
jS d 2 j
jS d 1 j P d
n ()
Z 1
1
k( 0 )P d
n ( 0 )(1  0 2 ) d 3
2 d 0 : (12)
Moreover, the Legendre Polynomials may be expanded into an orthonormal basis
of spherical harmonics Y d
n;j by the Funk-Hecke equation (cf. e.g. [4]) to obtain
P d
n (x  y) = jS d 1 j
N(d; n)
N(d;n) X
j=1
Y d
n;j (x)Y d
n;j (y) (13)
where kxk = kyk = 1 and moreover
Z
Sd 1
Y d
n;j (x)Y d
n 0 ;j 0 (x)dx = Æ n;n 0 Æ j;j 0 : (14)
4 Conditions and Eigensystems on S d 1
Schoenberg [7] gives necessary and suÆcient conditions under which a function
k(x  y) dened on S d 1 satises Mercer's condition. In particular he proves the
following two theorems:
Theorem 2 (Dot Product Kernels in Finite Dimensions) A kernel k(x  y)
dened on S d 1 S d 1 satises Mercer's condition if and only if its expansion into
Legendre polynomials P d
n has only nonnegative coeÆcients, i.e.
k() =
1
X
i=0
b n P d
n () with b n  0: (15)
Theorem 3 (Dot Product Kernels in Innite Dimensions) A kernel k(xy)
dened on the unit sphere in a Hilbert space satises Mercer's condition if and only
if its Taylor series expansion has only nonnegative coeÆcients:
k() =
1
X
i=0
a n  n with a n  0: (16)
Therefore, all we have to do in order to check whether a particular kernel may be
used in a SV machine or a Gaussian Process is to look at its polynomial series
expansion and check the coeÆcients. This will be done in Section 5.
Before doing so note that (16) is a more stringent condition than (15). In other
words, in order to prove Mercer's condition for arbitrary dimensions it suÆces to
show that the Taylor expansion contains only positive coeÆcients. On the other
hand, in order to prove that a candidate of a kernel function will never satisfy
Mercer's condition, it is suÆcient to show this for (15) where P d
n = P n , i.e. for the
Legendre Polynomials.
We conclude this section with an explicit representation of the eigensystem of k(xy).
It is given by the following lemma:

Lemma 4 (Eigensystem of Dot Product Kernels) Denote by k(xy) a kernel
on S d 1  S d 1 satisfying condition (15) of Theorem 2. Then the eigensystem of k
is given by
	 n;j = Y d
n;j with eigenvalues  n;j = a n
jS d 1 j
N(d; n)
of multiplicity N(d; n): (17)
In other words, an
N(d;n) determines the regularization properties of k(x  y).
Proof Using the Funk-Hecke formula (13) we may expand (15) further into Spheri-
cal Harmonics Y d
n;j . The latter, however, are orthonormal, hence computing the dot
product of the resulting expansion with Y d
n;j (y) over S d 1 leaves only the coeÆcient
Y d
n;j (x) jSd 1 j
N(d;n) which proves that Y d
n;j are eigenfunctions of the integral operator T k .
In order to obtain the eigensystem of k(x  y) on U d we have to expand k into
k(x  y) =
P 1
m;n=0 (kxkkyk) m P d
n
 x
kxk  y
kyk

and expand 	 into 	(kxk)	
 x
kxk

.
The latter is very technical and is thus omitted. See [6] for details.
5 Examples and Applications
In the following we will analyze a few kernels and state under which conditions they
may be used as SV kernels.
Example 1 (Homogeneous Polynomial Kernels k(x; y) = (x  y) p ) It is well
known that this kernel satises Mercer's condition for p 2 N . We will show that for
p 62 N this is never the case.
Thus we have to show that (15) cannot hold for an expansion in terms of Legendre
Polynomials (d = 3). From [2, 7.126.1] we obtain for k(x; y) = jj p (we need jj to
make k well-dened).
Z 1
1
P n ()jj p d =
p
(p+ 1)
2 p 1 + p
2
n
2
 3
2 + p
2 + n
2
 if n even. (18)
For odd n the integral vanishes since P n ( ) = ( 1) n P n (). In order to satisfy
(15), the integral has to be nonnegative for all n. One can see that 1 + p
2
n
2

is the only term in (18) that may change its sign. Since the sign of the function
alternates with period 1 for x < 0 (and has poles for negative integer arguments) we
cannot nd any p for which n = 2b p
2 + 1c and n = 2d p
2 + 1e correspond to positive
values of the integral.
Example 2 (Inhomogeneous Polynomial Kernels k(x; y) = (x  y + 1) p )
Likewise we might conjecture that k() = (1 + ) p is an admissible kernel for all
p > 0. Again, we expand k in a series of Legendre Polynomials to obtain [2, 7.127]
Z 1
1
P n ()( + 1) p d = 2 p+1 2 (p + 1)
(p+ 2 + n)(p + 1 n)
: (19)
For p 2 N all terms with n > p vanish and the remainder is positive. For noninteger
p, however, (19) may change its sign. This is due to (p + 1 n). In particular,
for any p 62 N (with p > 0) we have (p+ 1 n) < 0 for n = dpe + 1. This violates
condition (15), hence such kernels cannot be used in SV machines either.

Example 3 (Vovk's Real Polynomial k(x; y) = 1 (xy) p
1 (xy) with p 2 N ) This
kernel can be written as k() =
P p 1
n=0  n , hence all the coeÆcients a i = 1 which
means that this kernel can be used regardless of the dimensionality of the input
space. Likewise we can analyze the an innite power series:
Example 4 (Vovk's Innite Polynomial k(x; y) = (1 (x  y)) 1 ) This kernel
can be written as k() =
P 1
n=0  n , hence all the coeÆcients a i = 1. It suggests poor
generalization properties of that kernel.
Example 5 (Neural Networks Kernels k(x; y) = tanh(a + (x  y))) It is a
longstanding open question whether kernels k() = tanh(a + ) may be used as SV
kernels, or, for which sets of parameters this might be possible. We show that is
impossible for any set of parameters.
The technique is identical to the one of Examples 1 and 2: we have to show that k
fails the conditions of Theorem 2. Since this is very technical (and is best done by
using computer algebra programs, e.g. Maple), we refer the reader to [6] for details
and explain for the simpler case of Theorem 3 how the method works. Expanding
tanh(a + ) into a Taylor series yields
tanh a +  1
cosh 2 a  2 tanh a
cosh 2 a
 3
3 (1 tanh 2 a)(1 3 tanh 2 a) +O( 4 ) (20)
Now we analyze (20) coeÆcient-wise. Since all of them have to be nonnegative we
obtain from the rst term a 2 [0; 1), the third term a 2 (1; 0], and nally from
the fourth term jaj 2 [arctanh 1
3 ; arctanh 1]. This leaves us with a 2 ;, hence under
no conditions on its parameters the kernel above satises Mercer's condition.
6 Eigensystems on U d
In order to nd the eigensystem of T k on U d we have to nd a dierent representation
of k where the radial part kxkkyk and the angular part  =

x
kxk  y
kyk

are factored
out separately. We assume that k(x  y) can be written as
k(x  y) =
1
X
n=0
 n (kxkkyk)P d
n () (21)
where  n are polynomials. To see that we can always nd such an expansion for
analytic functions, rst expand k in a Taylor series and then expand each coeÆcient
(kxkkyk) n into (kxkkyk) n
P n
j=0 c j (d; n)P d
j (). Rearranging terms into a series of
P d
j gives expansion (21). This allows us to factorize the integral operator into its
radial and its angular part. We obtain the following theorem:
Theorem 5 (Eigenfunctions of T k on U d ) For any kernel k with expansion
(21) the eigensystem of the integral operator T k on U d is given by
 n;j;l (x) = Y d
n;j

x
kxk

 n;l (kxk) (22)
with eigenvalues  n;j;l = jSd 1 j
N(d;n)  n;l , and multiplicity N(d; n), where ( n;l ;  n;l ) is
the eigensystem of the integral operator
Z 1
0
r d 1
x  n (r x r y ) n;l (r x )dr x =  n;l  n;l (r y ): (23)
In general, (23) cannot be solved analytically. However, the accuracy of numerically
solving (23) (nite integral in one dimension) is much higher than when diagonal-
izing T k directly.

Proof All we have to do is split the integral
R
Ud dx into
R 1
0 r d 1 dr
R
Sd 1
d
 More-
over note that since T k commutes with the group of rotations it follows from group
theory [4] that we may separate the angular and the radial part in the eigenfunc-
tions, hence use the ansatz (x) =


 x
kxk

(kxk).
Next apply the Funk-Hecke equation (13) to expand the associated Legendre
Polynomials P d
n into the spherical harmonics Y d
n;j . As in Lemma 4 this leads to the
spherical harmonics as the angular part of the eigensystem. The remaining radial
part is then (23). See [6] for more details.
This leads to the eigensystem of the homogeneous polynomial kernel k(x; y) =
(x  y) p : if we use (18) in conjunction with (12) to expand  p into a series of P d
n ()
we obtain an expansion of type (21) where all  n (r x r y ) / (r x r y ) p for n  p and
 n (r x r y ) = 0 otherwise. Hence, the only solution to (23) is  n (r) = r d , thus
 n;j (x) = kxk p Y d
n;j ( x
kxk ). Eigenvalues can be obtained in a similar way.
7 Discussion
In this paper we gave conditions on the properties of dot product kernels, under
which the latter satisfy Mercer's condition. While the requirements are relatively
easy to check in the case where data is restricted to spheres (which allowed us to
prove that several kernels never may be suitable SV kernels) and led to explicit
formulations for eigenvalues and eigenfunctions, the corresponding calculations on
balls are more intricate and mainly amenable to numerical analysis.
Acknowledgments: AS was supported by the DFG (Sm 62-1). The authors thank
Bernhard Scholkopf for helpful discussions.
References
[1] C. J. C. Burges. Geometry and invariance in kernel based methods. In B. Scholkopf,
C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods | Support
Vector Learning, pages 89{116, Cambridge, MA, 1999. MIT Press.
[2] I. S. Gradshteyn and I. M. Ryzhik. Table of integrals, series, and products. Academic
Press, New York, 1981.
[3] J. Mercer. Functions of positive and negative type and their connection with the
theory of integral equations. Philos. Trans. Roy. Soc. London, A 209:415{446, 1909.
[4] C. Muller. Analysis of Spherical Symmetries in Euclidean Spaces, volume 129 of
Applied Mathematical Sciences. Springer, New York, 1997.
[5] N. Oliver, B. Scholkopf, and A.J. Smola. Natural regularization in SVMs. In A.J.
Smola, P.L. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large
Margin Classiers, pages 51 { 60, Cambridge, MA, 2000. MIT Press.
[6] Z. Ovari. Kernels, eigenvalues and support vector machines. Honours thesis, Aus-
tralian National University, Canberra, 2000.
[7] I. Schoenberg. Positive denite functions on spheres. Duke Math. J., 9:96{108, 1942.
[8] A. Smola, B. Scholkopf, and K.-R. Muller. The connection between regularization
operators and support vector kernels. Neural Networks, 11:637{649, 1998.
[9] G. Wahba. Spline Models for Observational Data, volume 59 of CBMS-NSF Regional
Conference Series in Applied Mathematics. SIAM, Philadelphia, 1990.
[10] C. K. I. Williams. Prediction with Gaussian processes: From linear regression to
linear prediction and beyond. In M. I. Jordan, editor, Learning and Inference in
Graphical Models. Kluwer, 1998.

