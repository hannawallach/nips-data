Stagewise processing in error-correcting
codes and image restoration
K. Y. Michael Wong
Department of Physics, Hong Kong University of Science and Technology,
Clear Water Bay, Kowloon, Hong Kong
phkywong@ust.hk
Hidetoshi Nishimori
Department of Physics, Tokyo Institute of Technology,
Oh-Okayama, Meguro-ku, Tokyo 152-8551, Japan
nishi@stat.phys.titech.ac.jp
Abstract
We introduce stagewise processing in error-correcting codes and
image restoration, by extracting information from the former stage
and using it selectively to improve the performance of the latter
one. Both mean-eld analysis using the cavity method and sim-
ulations show that it has the advantage of being robust against
uncertainties in hyperparameter estimation.
1 Introduction
In error-correcting codes [1] and image restoration [2], the choice of the so-called
hyperparameters is an important factor in determining their performances. Hyper-
parameters refer to the coe∆cients weighing the biases and variances of the tasks.
In error correction, they determine the statistical signicance given to the parity-
checking terms and the received bits. Similarly in image restoration, they determine
the statistical weights given to the prior knowledge and the received data. It was
shown, by the use of inequalities, that the choice of the hyperparameters is opti-
mal when there is a match between the source and model priors [3]. Furthermore,
from the analytic solution of the innite-range model and the Monte Carlo simula-
tion of nite-dimensional models, it was shown that an inappropriate choice of the
hyperparameters can lead to a rapid degradation of the tasks.
Hyperparameter estimation is the subject of many studies such as the \evidence
framework" [4]. However, if the prior models the source poorly, no hyperparameters
can be reliable [5]. Even if they can be estimated accurately through steady-state
statistical measurements, they may uctuate when interfered by bursty noise sources
in communication channels. Hence it is equally important to devise decoding or
restoration procedures which are robust against the uncertainties in hyperparameter
estimation.
Here we introduce selective freezing to increase the tolerance to uncertainties in hy-

perparameter estimation. The technique has been studied for pattern reconstruc-
tion in neural networks, where it led to an improvement in the retrieval precision,
a widening of the basin of attraction, and a boost in the storage capacity [6]. The
idea is best illustrated for bits or pixels with binary states 1, though it can be eas-
ily generalized to other cases. In a nite temperature thermodynamic process, the
binary variables keep moving under thermal agitation. Some of them have smaller
thermal uctuations than the others, implying that they are more certain to stay
in one state than the other. This stability implies that they have a higher probabil-
ity to stay in the correct state for error-correction or image restoration tasks, even
when the hyperparameters are not optimally tuned. It may thus be interesting to
separate the thermodynamic process into two stages. In the rst stage we select
those relatively stable bits or pixels whose time-averaged states have a magnitude
exceeding a certain threshold. In the second stage we subsequently x (or freeze)
them in the most probable thermodynamic states. Thus these selectively frozen
bits or pixels are able to provide a more robust assistance to the less stable bits or
pixels in their search for the most probable states.
The two-stage thermodynamic process can be studied analytically in the mean-eld
model using the cavity method. For the more realistic cases of nite dimensions
in image restoration, simulation results illustrate the relevance of the innite-range
model in providing qualitative guidance. Detailed theory of selective freezing is
presented in [7].
2 Formulation
Consider an information source which generates data represented by a set of Ising
spins f i g, where  i = 1 and i = 1;    ; N . The data is generated according to the
source prior P s (f i g). For error-correcting codes transmitting unbiased messages,
all sequences are equally probable and P s (fg) = 2 N . For images with smooth
structures, the prior consists of ferromagnetic Boltzmann factors, which increase
the tendencies of the neighboring spins to stay at the same spin states, that is,
P s (fg) / exp
0
@  s
z
X
hiji
 i  j
1
A : (1)
Here hiji represents pairs of neighboring spins, z is the valency of each site. The
data is coded by constructing the codewords, which are the products of p spins
J 0
i 1 i p
=  i 1
    i p
for appropriately chosen sets of of indices fi 1 ;    ; i p g. Each spin
may appear in a number of p-spin codewords; the number of times of appearance
is called the valency z p . For conventional image restoration, codewords with only
p = 1 are transmitted, corresponding to the pixels in the image.
When the signal is transmitted through a noisy channel, the output consists of
the sets fJ i 1 i p
g and f i g, which are the corrupted versions of fJ 0
i 1 i p
g and f i g
respectively, and described by the output probability
P out (fJg; fgjfg) / exp

 J
X
J i 1 i p
 i 1
    i p
+  
X
 i  i

: (2)
According to Bayesian statistics, the posterior probability that the source sequence
is fg, given the outputs fJg and fg, takes the form
P (fgjfJg; fg) / exp
0
@  J
X
J i 1 i p
 i 1
    i p
+  
X
 i  i +  s
z
X
hiji
 i  j
1
A :
(3)

If the receiver at the end of the noisy channel does not have precise information on
 J ,   or  s , and estimates them as , h and m respectively, then the ith bit of
the decoded/restored information is given by sgnh i i, where
h i i = Tr i e Hfg
Tre Hfg ; (4)
and the Hamiltonian is given by
Hfg = 
X
J i 1 i p
 i 1
    i p
h
X
 i  i
m
z
X
hiji
 i  j : (5)
For the two-stage process of selective freezing, the spins evolve thermodynamically
as prescribed in Eq. (4) during the rst stage, and the thermal averages h i i of
the spins are monitored. Then we select those spins with jh i ij exceeding a given
threshold , and freeze them in the second stage of the thermodynamics. The
average of the spin ~  i in the second stage is then given by
h~ i i =
Tr~ i
Q
j

 h j i 2  2

∆ ~  j ;sgnh j i +  2 h j i 2

e ~
Hf~g
Tr
Q
j
  (h j i 2  2 ) ∆ ~
 j ;sgnh j i + ( 2 h j i 2 )
 e ~
Hf~g
; (6)
where  is the step function, ~
Hf~g is the Hamiltonian for the second stage, and
has the same form as Eq. (5) in the rst stage. One then regards sgnh~ i i as the ith
spin of the decoding/restoration process.
The most important quantity in selective freezing is the overlap of the decod-
ed/restored bit sgnh~ i i and the original bit  i averaged over the output probability
and the spin distribution. This is given by
M sf =
X

Y Z
dJ
Y Z
dP s (fg)P out (fJg; fgjfg) i sgnh~ i i: (7)
Following [3], we can prove that selective freezing cannot outperform the single-stage
process if the hyperparameters can be estimated precisely. However, the purpose
of selective freezing is rather to provide a relatively stable performance when the
hyperparameters cannot be estimated precisely.
3 Modeling error-correcting codes
Let us now suppose that the output of the transmission channel consists of only the
set of p-spin interactions fJ i 1 i p
g. Then h = 0 in the Hamiltonian (5), and we set
m = 0 for the case that all messages are equally probable. Analytical solutions are
available for the innite-range model in which the exchange interactions are present
for all possible pairs of sites. Consider the noise model in which J i 1 i p
is Gaussian
with mean p!j 0  i 1
    i p
=N p 1 and variance p!J 2 =2N p 1 . We can apply a gauge
transformation  i !  i  i and J i 1 i p
! J i 1 i p
 i 1
    i p
, and arrive at an equivalent
p-spin model with a ferromagnetic bias, where
P (J i 1 i p
) =
 N p 1
J 2 p!
 1=2
exp
"
N p 1
J 2 p!

J i 1 i p
p!
N p 1
j 0
 2
#
: (8)
The innite-range model is exactly solvable using the cavity method [8]. The
method uses a self-consistency argument to consider what happens when a spin
is added or removed from the system. The central quantity in this method is the

cavity eld, which is the local eld of a spin when it is added to the system, assuming
that the exchange couplings act only one-way from the system to the new spin (but
not from the spin back to the system). Since the exchange couplings feeding the
new spin have no correlations with the system, the cavity eld becomes a Gaussian
variable in the limit of large valency.
The thermal average of a spin, say spin 1, is given by
h 1 i = tanh h 1 ; (9)
where h 1 is the cavity eld obeying a Gaussian distribution, whose mean and vari-
ance are pj 0 m p 1 and pJ 2 q p 1 =2 respectively, where m and q are the magnetization
and Edwards-Anderson order parameter respectively, given by
m  1
N
X
i
h i i and q  1
N
X
i
h i i 2 : (10)
Applying self-consistently the cavity argument to all terms in Eq. (10), we can
obtain self-consistent equations for m and q.
Now we consider selective freezing. If we introduce a freezing threshold  so that
all spins with h i i 2 >  2 are frozen, then the freezing fraction f is given by
f  1
N
X
i
 h i i 2  2

: (11)
The thermal average of a dynamic spin in the second stage is related to the cavity
elds in both stages, say, for spin 1,
h~ 1 i = tanh 
n
~ h 1 + p
2 (p 1)J 2 r p 2  tr tanh h 1
o
; (12)
where ~ h 1 is the cavity eld in the second stage, r is the order parameter describing
the spin correlations of the two thermodynamic stages:
r  1
N
X
i
h i i

h~ i i

 2 h i i 2

+ sgnh i i

h i i 2  2
	
; (13)
 tr is the trans-susceptibility which describes the response of a spin in the second
stage to variations of the cavity eld in the rst stage, namely
 tr  1
N
X
i
@h~ i i
@h i
: (14)
The cavity eld ~ h 1 is a Gaussian variable. Its mean and variance are pj 0 ~
m p 1
and pJ 2 ~ q p 1 =2 respectively, where ~
m and ~
q are the magnetization and Edwards-
Anderson order parameter respectively, given by
~
m  1
N
X
i
 ( 2 h i i 2 )h~ i i + (h i i 2  2 )sgnh i i

; (15)
~
q  1
N
X
i
 ( 2 h i i 2 )h~ i i 2 + (h i i 2  2 )

: (16)
Furthermore, the covariance between h 1 and ~ h 1 is pJ 2 r p 1 =2, where r is given in
Eq. (13). Applying self-consistently the same cavity argument to all terms in Eqs.
(15), (16), (13) and (14), we arrive at the self-consistent equations for ~
m, ~
q, r and
 tr . The performance of selective freezing is measured by
M sf  1
N
X
i

( 2 h i i 2 )sgnh~ i i + (h i i 2  2 )sgnh i i

: (17)

0.0 0.2 0.4 0.6 0.8 1.0
T
0.83
0.85
0.87
0.89
0.91
0.93
Msf
f=0
f=0.7
f=0.8
f=0.9
0.3 0.6 0.9 1.2 1.5
T
0.92
0.93
0.94
0.95
Msf
f=0
f=0.7
f=0.8
f=0.9
Figure 1: The overlap M sf as a function of the decoding temperature T for various
given values of freezing fraction f . In this and the following gure, f = 0 corre-
sponds to one-stage decoding/restoration. (a) Theoretical results for p = 3, j 0 = 0:8
and J = 1; (b) results of Monte Carlo simulations for p = 2 and j 0 = J = 1.
In the example in Fig. 1(a), the overlap of the single-stage dynamics reaches its
maximum at the Nishimori point TN = J 2 =2j 0 as expected. We observe that the
tolerance against variations in T is enhanced by selective freezing both above and
below the optimal temperature (see especially f = 0:8). This shows that the region
of advantage for selective freezing is even broader than that discussed in [7], where
improvement is only observed above the optimal temperature.
The advantages of selective freezing are conrmed by Monte Carlo simulation-
s shown in Fig. 1(b). For one-stage dynamics, the overlap is maximum at the
Nishimori point (TN = 0:5) as expected. However, it deterriorates rather rapidly
when the decoding temperature increases. In contrast, selective freezing maintains
a more steady performance, especially when f = 0:9.
4 Modeling image restoration
In conventional image restoration problems, a given degraded image consists of the
set of pixels f i g, but not the set of exchange interactions fJ i 1 ;;i p
g. In this case,
 = 0 in the Hamiltonian (5). The pixels  i are the degraded versions of the source
pixels  i , corrupted by noise which, for convenience, is assumed to be Gaussian with
mean a i and variance  2 . In turn, the source pixels satisfy the prior distribution
in Eq. (1) for smooth images.
Analysis of the mean-eld model with extensive valency shows that selective freez-
ing performs as well as one-stage dynamics, but cannot outperform it. Nevertheless,
selective freezing provides a rather stable performance when the hyperparameters
cannot be estimated precisely. Hence we model a situation common in modern com-
munication channels carrying multimedia tra∆c, which are often bursty in nature.
Since burstiness results in intermittent interferences, we consider a distribution of
the degraded pixels with two Gaussian components, each with its own characteris-
tics,
P ( i j i ) = f 1
exp
h
1
2 2
1
( i a 1  i ) 2
i
p
2 2
1
+ (1 f 1 )
exp
h
1
2 2
2
( i a 2  i ) 2
i
p
2 2
2
: (18)

0.0 0.5 1.0 1.5 2.0
h
0.89
0.90
0.91
0.92
Msf
f=0
f=0.1
f=0.3
f=0.5
f=0.7
f=0.9
0 1 2 3 4 5 6 7
T m
0.82
0.84
0.86
0.88
0.90
0.92
Msf
f=0
f=0.3
f=0.5
f=0.7
f=0.9
f=0.95
Figure 2: (a) The performance of selective freezing with 2 components of Gaussian
noise at  s = 1:05, f 1 = 4f 2 = 0:8, a 1 = 5a 2 = 1 and  1 =  2 = 0:3, The restoration
agent operates at the optimal ratio m=h which assumes a single noise component
with the overall mean 0.84 and variance 0.4024. (b) Results of Monte Carlo sim-
ulations for the overlaps of selective freezing compared with that of the one-stage
dynamics for two-dimensional images generated at the source prior temperature
T s = 2:15.
Suppose the restoration agent operates at the optimal ratio of m=h which assumes
a single noise component. Then there will be a degradation of the quality of the
restored images. In the example in Fig. 2(a), the reduction of the overlap M sf
for selective freezing is much more modest than the one-stage process (f = 0).
Other cases of interest, in which the restoration agent operates on other imprecise
estimations, are discussed in [7]. All conrm the robustness of selective freezing.
It is interesting to study the more realistic case of two-dimensional images, since we
have so far presented analytical results for the mean-eld model only. As conrmed
by the results for Monte carlo simulations in Fig. 2(b), the overlaps of selective
freezing are much more steadier than that of the one-stage dynamics when the
decoding temperature changes. This steadiness is most remarkable for a freezing
fraction of f = 0:9.
5 Discussions
We have introduced a multistage technique for error-correcting codes and image
restoration, in which the information extracted from the former stage can be used
selectively to improve the performance of the latter one. While the overlap M sf
of the selective freezing is bounded by the optimal performance of the one-stage
dynamics derived in [3], it has the advantage of being tolerant to uncertainties in
hyperparameter estimation. This is conrmed by both analytical and simulational
results for mean-eld and nite-dimensional models. Improvement is observed both
above and below the optimal decoding temperature, superseding the observations
in [7]. As an example, we have illustrated its advantage of robustness when the
noise distribution is composed of more than one Gaussian components, such as in
the case of modern communication channels supporting multimedia applications.
Selective freezing can be generalized to more than two stages, in which spins that
remain relatively stable in one stage are progressively frozen in the following one.

It is expected that the performance can be even more robust.
On the other hand, we have a remark about the basic assumption of the cavity
method, namely that the addition or removal of a spin causes a small change in
the system describable by a perturbative approach. In fact, adding or removing a
spin may cause the thermal averages of other spins to change from below to above
the thresholds  (or vice versa). This change, though often small, induces a non-
negligible change of the thermal averages from fractional values to the frozen values
of 1 (or vice versa) in the second stage. The perturbative analysis of these changes
is only approximate. The situation is reminiscent of similar instabilities in other
disordered systems such as the perceptron, and are equivalent to Almeida-Thouless
instabilities in the replica method [9]. A full treatment of the problem would require
the introduction of a rough energy landscape [9], or the replica symmetry breaking
ansatz in the replica method [8]. Nevertheless, previous experiences on disordered
systems showed that the corrections made by a more complete treatment may not
be too large in the ordered phase. For example, simulational results in Figs. 1(b)
are close to the corresponding analytical results in [7].
In practical implementations of error-correcting codes, algorithms based on belief-
propagation methods are often employed [10]. It has recently been shown that
such decoded messages converge to the solutions of the TAP equations in the corre-
sponding thermodynamic system [11]. Again, the performance of these algorithms
are sensitive to the estimation of hyperparameters. We propose that the selective
freezing procedure has the potential to make these algorithms more robust.
Acknowledgments
This work was partially supported by the Research Grant Council of Hong Kong
(HKUST6157/99P).
References
[1] R. J. McEliece, The Theory of Information and Coding, Encyclopedia of Mathematics
and its Applications (Addison-Wesley, Reading, MA 1977).
[2] S. Geman and D. Geman, IEEE Trans. PAMI 6, 721 (1984).
[3] H. Nishimori and K. Y. M. Wong, Phys. Rev. E 60, 132 (1999).
[4] D. J. C. Mackay, Neural Computation 4, 415 (1992).
[5] J. M. Pryce and A. D. Bruce, J. Phys. A 28, 511 (1995).
[6] K. Y. M. Wong, Europhys. Lett. 36, 631 (1996).
[7] K. Y. M. Wong and H. Nishimori, submitted to Phys. Rev. E (2000).
[8] M. Mezard, G. Parisi, and V.A. Virasoro, Spin Glass Theory and Beyond (World
Scientic, Singapore 1987).
[9] K. Y. M. Wong, Advances in Neural Information Processing Systems 9, 302 (1997).
[10] B. J. Frey, Graphical Models for Machine Learning and Digital Communication (MIT
Press, 1998).
[11] Y. Kabashima and D. Saad, Europhys. Lett. 44, 668 (1998).

