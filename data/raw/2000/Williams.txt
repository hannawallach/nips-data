To appear in: Advances in Neural Information Processing Systems 13 eds. T. K. Leen, T. G.
Diettrich, V. Tresp. MIT Press (2001)
On a Connection between Kernel PCA
and Metric Multidimensional Scaling
Christopher K. I. Williams
Division of Informatics
The University of Edinburgh
5 Forrest Hill, Edinburgh EH1 2QL, UK
c.k.i.williams@ed.ac.uk
http://anc.ed.ac.uk
Abstract
In this paper we show that the kernel PCA algorithm of Scholkopf
et al (1998) can be interpreted as a form of metric multidimensional
scaling (MDS) when the kernel function k(x; y) is isotropic, i.e. it
depends only on jjx yjj. This leads to a metric MDS algorithm
where the desired conguration of points is found via the solution
of an eigenproblem rather than through the iterative optimization
of the stress objective function. The question of kernel choice is
also discussed.
1 Introduction
Suppose we are given n objects, and for each pair (i; j) we have a measurement
of the \dissimilarity" Æ ij between the two objects. In multidimensional scaling
(MDS) the aim is to place n points in a low dimensional space (usually Euclidean)
so that the interpoint distances d ij have a particular relationship to the original
dissimilarities. In classical scaling we would like the interpoint distances to be equal
to the dissimilarities. For example, classical scaling can be used to reconstruct a
map of the locations of some cities given the distances between them.
In metric MDS the relationship is of the form d ij  f(Æ ij ) where f is a specic
function. In this paper we show that the kernel PCA algorithm of Scholkopf et al
[7] can be interpreted as performing metric MDS if the kernel function is isotropic.
This is achieved by performing classical scaling in the feature space dened by the
kernel.
The structure of the remainder of this paper is as follows: In section 2 classical and
metric MDS are reviewed, and in section 3 the kernel PCA algorithm is described.
The link between the two methods is made in section 4. Section 5 describes ap-
proaches to choosing the kernel function, and we nish with a brief discussion in
section 6.

2 Classical and metric MDS
2.1 Classical scaling
Given n objects and the corresponding dissimilarity matrix, classical scaling is an
algebraic method for nding a set of points in space so that the dissimilarities are
well-approximated by the interpoint distances. The classical scaling algorithm is
introduced below by starting with the locations of n points, constructing a dis-
similarity matrix based on their Euclidean distances, and then showing how the
conguration of the points can be reconstructed (as far as possible) from the dis-
similarity matrix.
Let the coordinates of n points in p dimensions be denoted by x i , i = 1; : : : ; n. These
can be collected together in a n  p matrix X . The dissimilarities are calculated
by Æ 2
ij = (x i x j ) T (x i x j ). Given these dissimilarities, we construct the matrix
A such that a ij = 1
2 Æ 2
ij , and then set B = HAH , where H is the centering
matrix H = I n 1
n 11 T . With Æ 2
ij = (x i x j ) T (x i x j ), the construction of B
leads to b ij = (x i x) T (x j x), where x = 1
n
P n
i=1 x i . In matrix form we have
B = (HX)(HX) T , and B is real, symmetric and positive semi-denite. Let the
eigendecomposition of B be B = V V T , where  is a diagonal matrix and V is a
matrix whose columns are the eigenvectors of B. If p < n, there will be n p zero
eigenvalues 1 . If the eigenvalues are ordered  1   2      n  0, then B =
V p  p V T
p , where  p = diag( 1 ; : : : ;  p ) and V p is the n  p matrix whose columns
correspond to the rst p eigenvectors of B, with the usual normalization so that
the eigenvectors have unit length. The matrix ^
X of the reconstructed coordinates
of the points can be obtained as ^
X = V p  1
2
p , with B = ^
X ^
X T . Clearly from the
information in the dissimilarities one can only recover the original coordinates up
to a translation, a rotation and reections of the axes; the solution obtained for ^
X
is such that the origin is at the mean of the n points, and that the axes chosen by
the procedure are the principal axes of the ^
X conguration.
It may not be necessary to uses all p dimensions to obtain a reasonable approxi-
mation; a conguration ^
X in k-dimensions can be obtained by using the largest k
eigenvalues so that ^
X = V k  1
2
k . These are known as the principal coordinates of X
in k dimensions. The fraction of the variance explained by the rst k eigenvalues is
P k
i=1  i =
P n
i=1  i .
Classical scaling as explained above works on Euclidean distances as the dissimilar-
ities. However, one can run the same algorithm with a non-Euclidean dissimilarity
matrix, although in this case there is no guarantee that the eigenvalues will be
non-negative.
Classical scaling derives from the work of Schoenberg and Young and Householder
in the 1930's. Expositions of the theory can be found in [5] and [2].
2.1.1 Optimality properties of classical scaling
Mardia et al [5] (section 14.4) give the following optimality property of the classical
scaling solution.
1 In fact if the points are not in \general position" the number of zero eigenvalues will
be greater than n p. Below we assume that the points are in general position, although
the arguments can easily be carried through with minor modications if this is not the
case.

Theorem 1 Let X denote a conguration of points in R p , with interpoint distances
Æ 2
ij = (x i x j ) T (x i x j ). Let L be a p  p rotation matrix and set L = (L 1 ; L 2 ),
where L 1 is pk for k < p. Let ^
X = XL 1 , the projection of X onto a k-dimensional
subspace of R p , and let ^
d 2
ij = (^ x i ^
x j ) T (^ x i ^ x j ). Amongst all projections ^
X = XL 1 ,
the quantity  =
P
i;j (Æ 2
ij
^
d 2
ij ) is minimized when X is projected onto its principal
coordinates in k dimensions. For all i; j we have ^
d ij  Æ ij . The value of  for the
principal coordinate projection is  = 2n( k+1 + : : : +  p ).
2.2 Relationships between classical scaling and PCA
There is a well-known relationship between PCA and classical scaling; see e.g. Cox
and Cox (1994) section 2.2.7.
Principal components analysis (PCA) is concerned with the eigendecomposition of
the sample covariance matrix S = 1
n X T HX . It is easy to show that the eigenvalues
of nS are the p non-zero eigenvalues of B. To see this note that H 2 = H and
thus that nS = (HX) T (HX). Let v i be a unit-length eigenvector of B so that
Bv i =  i v i . Premultiplying by (HX) T yields
(HX) T (HX)(HX) T v i =  i (HX) T v i (1)
so we see that  i is an eigenvalue of nS. y i = (HX) T v i is the corresponding
eigenvector; note that y T
i y i =  i . Centering X and projecting onto the unit vector
^
y i =  1=2
i y i we obtain
HX^ y i =  1=2
i HX(HX) T v i =  1=2
i v i : (2)
Thus we see that the projection of X onto the eigenvectors of nS returns the classical
scaling solution.
2.3 Metric MDS
The aim of classical scaling is to nd a conguration of points ^
X so that the in-
terpoint distances d ij well approximate the dissimilarities Æ ij . In metric MDS this
criterion is relaxed, so that instead we require
d ij  f(Æ ij ); (3)
where f is a specied (analytic) function. For this denition see, e.g. Kruskal and
Wish [4] (page 22), where polynomial transformations are suggested.
A straightforward way to carry out metric MDS is to dene a error function (or
stress)
S =
P
i;j w ij (d ij f(Æ ij )) 2
P
i;j d 2
ij
; (4)
where the fw ij g are appropriately chosen weights. One can then obtain deriva-
tives of S with respect to the coordinates of the points that dene the d ij 's and
use gradient-based (or more sophisticated methods) to minimize the stress. This
method is known as least-squares scaling. An early reference to this kind of method
is Sammon (1969) [6], where w ij = 1=Æ ij and f is the identity function.
Note that if f(Æ ij ) has some adjustable parameters  and is linear with respect to  2 ,
then the function f can also be adapted and the optimal value for those parameters
given the current d ij 's can be obtained by (weighted) least-squares regression.
2 f can still be a non-linear function of its argument.

Critchley (1978) [3] (also mentioned in section 2.4.2 of Cox and Cox) carried out
metric MDS by running the classical scaling algorithm on the transformed dissim-
ilarities. Critchley suggests the power transformation f(Æ ij ) = Æ 
ij (for  > 0). If
the dissimilarities are derived from Euclidean distances, we note that the kernel
k(x; y) = jjx yjj  is conditionally positive denite (CPD) if   2 [1]. When the
kernel is CPD, the centered matrix will be positive denite. Critchley's use of the
classical scaling algorithm is similar to the algorithm discussed below, but crucially
the kernel PCA method ensures that the matrix B derived form the transformed
dissimilarities is non-negative denite, while this is not guaranteed by Critchley's
transformation for arbitrary .
A further member of the MDS family is nonmetric MDS (NMDS), also known as
ordinal scaling. Here it is only the relative rank ordering between the d's and the Æ's
that is taken to be important; this constraint can be imposed by demanding that
the function f in equation 3 is monotonic. This constraint makes sense for some
kinds of dissimilarity data (e.g. from psychology) where only the rank orderings
have real meaning.
3 Kernel PCA
In recent years there has been an explosion of work on kernel methods. For super-
vised learning these include support vector machines [8], Gaussian process predic-
tion (see, e.g. [10]) and spline methods [9]. The basic idea of these methods is to use
the \kernel trick". A point x in the original space is re-represented as a point (x)
in a NF -dimensional feature space 3 F , where (x) = ( 1 (x);  2 (x); : : : ; NF (x)).
We can think of each function  j () as a non-linear mapping. The key to the kernel
trick is to realize that for many algorithms, the only quantities required are of the
form 4 (x i ):(x j ) and thus if these can be easily computed by a non-linear function
k(x i ; x j ) = (x i ):(x j ) we can save much time and eort.
Scholkopf, Smola and Muller [7] used this trick to dene kernel PCA. One could
compute the covariance matrix in the feature space and then calculate its eigen-
vectors/eigenvalues. However, using the relationship between B and the sample
covariance matrix S described above, we can instead consider the n  n matrix K
with entries K ij = k(x i ; x j ) for i; j = 1; : : : ; n. If NF > n using K will be more
eÆcient than working with the covariance matrix in feature space and anyway the
latter would be singular.
The data should be centered in the feature space so that
P n
i=1 (x i ) = 0. This
is achieved by carrying out the eigendecomposition of ~
K = HKH which gives the
coordinates of the approximating points as described in section 2.2. Thus we see
that the visualization of data by projecting it onto the rst k eigenvectors is exactly
classical scaling in feature space.
4 A relationship between kernel PCA and metric MDS
We consider two cases. In section 4.1 we deal with the case that the kernel is
isotropic and obtain a close relationship between kernel PCA and metric MDS. If
the kernel is non-stationary a rather less close relationship is derived in section 4.2.
3 For some kernels NF = 1.
4 We denote the inner product of two vectors as either a:b or a T b.

4.1 Isotropic kernels
A kernel function is stationary if k(x i ; x j ) depends only on the vector  = x i x j . A
stationary covariance function is isotropic if k(x i ; x j ) depends only on the distance
Æ ij with Æ 2
ij =  : , so that we write k(x i ; x j ) = r(Æ ij ). Assume that the kernel is
scaled so that r(0) = 1. An example of an isotropic kernel is the squared exponential
or RBF (radial basis function) kernel k(x i ; x j ) = expf(x i x j ) T (x i x j )g, for
some parameter  > 0.
Consider the Euclidean distance in feature space ~
Æ 2
ij = ((x i ) (x j )) T ((x i )
(x j )). With an isotropic kernel this can be re-expressed as ~
Æ 2
ij = 2(1 r(Æ ij )).
Thus the matrix A has elements a ij = r(Æ ij ) 1, which can be written as A =
K 11 T . It can be easily veried that the centering matrix H annihilates 11 T , so
that HAH = HKH .
We see that the conguration of points derived from performing classical scaling
on K actually aims to approximate the feature-space distances computed as ~
Æ ij =
p
2(1 r(Æ ij )). As the ~ Æ ij 's are a non-linear function of the Æ ij 's this procedure
(kernel MDS) is an example of metric MDS.
Remark 1 Kernel functions are usually chosen to be conditionally positive denite,
so that the eigenvalues of the matrix ~
K will be non-negative. Choosing arbitrary
functions to transform the dissimilarities will not give this guarantee.
Remark 2 In nonmetric MDS we require that d ij  f(Æ ij ) for some monotonic
function f . If the kernel function r is monotonically decreasing then clearly 1 r
is monotonically increasing. However, there are valid isotropic kernel (covariance)
functions which are non-monotonic (e.g. the exponentially damped cosine r(Æ) =
e Æ cos(!Æ); see [11] for details) and thus we see that f need not be monotonic in
kernel MDS.
Remark 3 One advantage of PCA is that it denes a mapping from the original
space to the principal coordinates, and hence that if a new point x arrives, its
projection onto the principal coordinates dened by the original n data points can be
computed 5 . The same property holds in kernel PCA, so that the computation of the
projection of (x) onto the rth principal direction in feature space can be computed
using the kernel trick as
P n
i=1  r
i k(x; x i ), where  r is the rth eigenvector of ~
K (see
equation 4.1 in [7]). This projection property does not hold for algorithms that
simply minimize the stress objective function; for example the Sammon \mapping"
algorithm [6] does not in fact dene a mapping.
4.2 Non-stationary kernels
Sometimes non-stationary kernels (e.g. k(x i ; x j ) = (1 + x i :x j ) m for integer m)
are used. For non-stationary kernels we proceed as before and construct ~
Æ 2
ij =
((x i ) (x j )) T ((x i ) (x j )). We can again show that the kernel MDS procedure
operates on the matrix HKH . However, the distance ~ Æ ij in feature space is not a
function of Æ ij and so the relationship of equation 3 does not hold. The situation can
be saved somewhat if we follow Mardia et al (section 14.2.3) and relate similarities
5 Note that this will be, in general, dierent to the solution found by doing PCA on the
full data set of n + 1 points.

0 500 1000 1500 2000 2500
0
1
k
g
0
1
0
1
0
1
beta = 0
beta = 4
beta = 10
beta = 20
Figure 1: The plot shows  as a function of k for various values of  = =256 for
the USPS test set.
to dissimilarities through ~
Æ 2
ij = ~ c ii + ~ c jj 2~c ij , where ~
c ij denotes the similarity
between items i and j in feature space. Then we see that the similarity in feature
space is given by ~
c ij = (x i ):(x j ) = k(x i ; x j ). For kernels (such as polynomial
kernels) that are functions of x i :x j (the similarity in input space), we see then that
the similarity in feature space is a non-linear function of the similarity measured in
input space.
5 Choice of kernel
Having performed kernel MDS one can plot the scatter diagram (or Shepard dia-
gram) of the dissimilarities against the tted distances. We know that for each pair
the tted distance d ij  ~
Æ ij because of the projection property in feature space. The
sum of the residuals is given by 2n
P n
i=k+1  i where the f i g are the eigenvalues of
~
K = HKH . (See Theorem 1 above and recall that at most n of the eigenvalues of
the covariance matrix in feature space will be non-zero.) Hence the fraction of the
sum-squared distance explained by the rst k dimensions is  =
P k
i=1  i =
P n
i=1  i .
One idea for choosing the kernel would be to x the dimensionality k and choose
r() so that  is maximized. Consider the eect of varying  in the RBF kernel
k(x i ; x j ) = expf(x i x j ) T (x i x j )g: (5)
As  !1 we have ~
Æ 2
ij = 2(1 Æ(i; j)) (where Æ(i; j) is the Kronecker delta), which
are the distances corresponding to a regular simplex. Thus K ! I n , HKH = H
and  = k=(n 1). Letting  ! 0 and using e z ' 1 z for small , we can show
that K ij = 1 Æ 2
ij as  ! 0, and thus that the classical scaling solution is obtained
in this limit.
Experiments have been run on the US Postal Service database of handwritten digits,
as used in [7]. The test set of 2007 images was used. The size of each image is 1616
pixels, with the intensity of the pixels scaled so that the average variance over all 256
dimensions is 0:5. In Figure 1  is plotted against k for various values of  = =256.
By choosing an index k one can observe from Figure 1 what fraction of the variance
is explained by the rst k eigenvalues. The trend is that as  decreases more and

more variance is explained by fewer components, which ts in with the idea above
that the  ! 1 limit gives rise to the regular simplex case. Thus there does not
seem to be a non-trivial value of  which minimizes the residuals.
6 Discussion
The results above show that kernel PCA using an isotropic kernel function can be
interpreted as performing a kind of metric MDS. The main dierence between the
kernel MDS algorithm and other metric MDS algorithms is that kernel MDS uses
the classical scaling solution in feature space. The advantage of the classical scal-
ing solution is that it is computed from an eigenproblem, and avoids the iterative
optimization of the stress objective function that is used for most other MDS so-
lutions. The classical scaling solution is unique up to the unavoidable translation,
rotation and reection symmetries (assuming that there are no repeated eigenval-
ues). Critchley's work (1978) is somewhat similar to kernel MDS, but it lacks the
notion of a projection into feature space and does not always ensure that the matrix
B is non-negative denite.
We have also looked at the question of adapting the kernel so as to minimize the sum
of the residuals. However, for the case investigated this leads to a trivial solution.
Acknowledgements
I thank David Willshaw, Matthias Seeger and Amos Storkey for helpful conversations, and
the anonymous referees whose comments have helped improve the paper.
References
[1] C. Berg, J. P. R. Christensen, and P. Ressel. Harmonic Analysis on Semigroups.
Springer-Verlag, New York, 1984.
[2] T. F. Cox and M. A. A. Cox. Multidimensional Scaling. Chapman and Hall, London,
1994.
[3] F. Critchley. Multidimensionsal scaling: a short critique and a new method. In L. C. A
Corsten and J. Hermans, editors, COMPSTAT 1978. Physica-Verlag, Vienna, 1978.
[4] J. B. Kruskal and M. Wish. Multidimensional Scaling. Sage Publications, Beverly
Hills, 1978.
[5] Mardia, K. V. and Kent, J. T. and Bibby, J. M. Multivariate Analysis. Academic
Press, 1979.
[6] J. W. Sammon. A nonlinear mapping for data structure analysis. IEEE Trans. on
Computers, 18:401{409, 1969.
[7] B. Scholkopf, A. Smola, and K.-R. Muller. Nonlinear component analysis as a kernel
eigenvalue problem. Neural Computation, 10:1299{1319, 1998.
[8] V. N. Vapnik. The nature of statistical learning theory. Springer Verlag, New York,
1995.
[9] G. Wahba. Spline models for observational data. Society for Industrial and Applied
Mathematics, Philadelphia, PA, 1990. CBMS-NSF Regional Conference series in
applied mathematics.
[10] C. K. I. Williams and D. Barber. Bayesian classication with Gaussian processes.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342{1351,
1998.
[11] A. M. Yaglom. Correlation Theory of Stationary and Related Random Functions
Volume I:Basic Results. Springer Verlag, 1987.

