Sparsity of data representation of optimal kernel
machine and leave≠one≠out estimator
A. Kowalczyk
Chief Technology Office, Telstra
770 Blackburn Road, Clayton, Vic. 3168, Australia
(adam.kowalczyk@team.telstra.com)
Abstract
Vapnik's result that the expectation of the generalisation error of the opti≠
mal hyperplane is bounded by the expectation of the ratio of the number
of support vectors to the number of training examples is extended to a
broad class of kernel machines. The class includes Support Vector Ma≠
chines for soft margin classification and regression, and Regularization
Networks with a variety of kernels and cost functions. We show that key
inequalities in Vapnik's result become equalities once ``the classification
error'' is replaced by ``the margin error'', with the latter defined as an in≠
stance with positive cost. In particular we show that expectations of the
true margin error and the empirical margin error are equal, and that the
sparse solutions for kernel machines are possible only if the cost function
is ``partially'' insensitive.
1 Introduction
Minimization of regularized risk is a backbone of several recent advances in machine learn≠
ing, including Support Vector Machines (SVM) [13], Regularization Networks (RN) [5] or
Gaussian Processes [15]. Such a machine is typically implemented as a weighted sum of
a kernel function evaluated for pairs composed of a data vector in question and a number
of selected training vectors, so called support vectors. For practical machines it is desired
to have as few support vectors as possible. It has been observed empirically that SVM
solutions have often very few support vectors, or that they are sparse, while RN machines
are not. The paper shows that this behaviour is determined by the properties of the cost
function used (its partial insensitivity, to be precise).
Another motivation for interest in sparsity of solutions comes from celebrated result of
Vapnik [13] which links the number of support vectors to the generalization error of SVM
via a bound on leave≠one≠out estimator [9]. This result has been originally shown for a
special case of classification with hard margin cost function (optimal hyperplane). The
papers by Opper and Winther [10], Jaakkola and Haussler [6], and Joachims [7] extend
Vapnik's result in the direction of bounds for classification error of SVM's. The first of
those papers deals with the hard margin case, while the other two derive tighter bounds on
classification error of the soft margin SVMs with ffl≠insensitive linear cost.
In this paper we extend Vapnik's result in another direction. Firstly, we show that it holds
for to a wide range of kernel machines optimized for a variety of cost functions, for both

classification and regression tasks. Secondly, we find that Vapnik's key inequalities become
equalities once ``the misclassification error'' is replaced by ``the margin error'' (defined as
the rate of data instances incurring positive costs). In particular, we find that for margin
errors the following three expectations : (i) of the empirical risk, (ii) of the the true risk and
(iii) of the leave≠one≠out risk estimator are equal to each other. Moreover, we show that
they are equal to the expectation of the ratio of support vectors to the number of training
examples.
The main results are given in Section 2. Brief discussion of results is given in Section 3.
2 Main results
Given an l≠sample f(x 1 ; y 1 ); ::::; (x l ; y l )g of patterns x i 2 X ae R n and target values y i 2
Y ae R. The learning algorithms used by SVMs [13], RNs [5] or Gaussian Processes [15]
minimise the regularized risk functional of the form:
min
(f;b)2H\ThetaR
R reg [f; b] =
l
X
i=1
c(x i ; y i ; ∏ i [f; b]) +
ñ
2
jjf jj 2
H : (1)
Here H denotes a reproducing kernel Hilbert space (RKHS) [1], jj:jj H is the corresponding
norm, ñ ? 0 is a regularization constant, c : X \Theta Y \Theta R ! R + is a non≠negative cost
function penalising for the deviation ∏ i [f; b] = y i \Gamma ì
y i of the estimator ì
y i := f(x i ) + fib
from target y i at location x i , b 2 R is a constant (bias) and fi 2 f0; 1g is another constant
(fi = 0 is used to switch the bias off).
The important Representer Theorem [8, 4] states that the minimizer (1) has the expansion:
f(x) =
l
X
i=1
ff i k(x i ; x); (2)
where k : X \Theta X ! R is the kernel corresponding to the RKHS H. In the following
section we shall show that under general assumptions this expansion is unique.
If ff i 6= 0, then x i is called a support vector of f(:).
2.1 Unique Representer Theorem
We recall, that a function is called a real analytic function on a domain ae R q if for every
point of this domain the Taylor series for the function converges to this function in some
neighborhood of that point. 1
A proof of the following crucial Lemma is omitted due to lack of space.
Lemma 2.1. If ' : X ! R is an analytic function on an open connected subset X ae R n ,
then the subset ' \Gamma1 (0) ae X is either equal to X or has Lebesgue measure 0.
Analyticity is essential for the above result and the result does not hold even for functions
infinitely differentiable, in general. Indeed, for every closed subset V ae R n there exists
an infinitely differentiable function (C 1 ) on R n such that OE \Gamma1 (0) = V and there exist
closed subsets with positive Lebesgue measure and empty interior. Hence the Lemma, and
consequently the subsequent results, do not hold for the broader class of all C 1 functions.
1 Examples of analytic functions are polynomials. The ordinary functions such as sin(x), cos(x)
and exp(x) are examples of non≠polynomial analytic functions. The function /(x) := exp(\Gamma1=x 2 )
for x ? 0 and 0, otherwise, is an example of infinitely differentiable function of the real line but not
analytic (locally it is not equal to its Taylor series expansion at zero).

Standing assumptions. The following is assumed.
1. The set X ae R n is open and connected and either Y = f\Sigma1g (the case of classifi≠
cation) or Y ae R is an open segment (the case of regression).
2. The kernel k : X \Theta X ! R is a real analytic function on its domain.
3. The cost function ∏7!c(x; y; ∏) is convex, differentiable on R and c(x; y; 0) = 0
for every (x; y) 2 X \Theta Y . It can be shown that
c(x; y; ∏) ? 0 , @c
@∏ (x; y; ∏) 6= 0: (3)
4. l is a fixed integer, 1 ! l ü dim(H), and the training sample (x 1 ; y 1 ); :::; (x l ; y l )
is iid drawn from a continuous probability density p(x; y) on X \Theta Y .
5. The phrase ``with probability 1'' will mean with probability 1 with respect to the
selection of the training sample.
Note that standard polynomial kernel k(x; x 0 ) = (1 + x \Delta x 0 ) d , x; x 0 2 R n , satisfies the
above assumptions with dim(H) =
\Gamma n+d
d
\Delta . Similarly, the Gaussian kernel k(x; x 0 ) =
exp(\Gammajjx \Gamma x 0 jj 2 =oe) satisfies them with dim(H) = 1.
Typical cost functions such as the super≠linear loss functions c p (x; y; ∏) = (y∏) p
+ :=
(max(0; y∏)) p used for SVM classification, or c pffl (x; y; ∏) = (j∏j \Gamma ffl) p
+ used for SVM
regression, or the super≠linear loss c p (x; y; ∏) = j∏j p for p ? 1 for RN regression, satisfy
the above assumptions 2 . Similarly, variations of Huber robust loss [11, 14] satisfy those
assumptions.
The following result strengthens the Representer Theorem [8, 4]
Theorem 2.2. If l ü dimH, then both, the minimizer of the regularized risk (1) and its
expansion (2) are unique with probability 1.
Proof outline. Convexity of the functional (f; b)7!R reg [f; b] and its strict convexity with
respect to f 2 H implies the uniqueness of f 2 H minimizing the regularized risk (1);
cf.[3]. From the assumption that l ü dim H we derive the existence of ~ x 1 ; :::; ~
x l 2 X such
that the functions f(~x i ; :), i = 1; :::; l, are linearly independent. Equivalently, the following
Gram determinant is 6= 0:
OE(~x 1 ; :::; ~
x l ) := det[hk(~x i ; :); k(~x j ; :)i H ] 1üi;jül = det[k(~x i ; ~
x j )] 1üi;jül 6= 0:
Now Lemma 2.1 implies that OE(x 1 ; :::; x l ) 6= 0 with probability 1, since OE : X l ! R is an
analytic function. Hence functions k(x i ; :) are linearly independent and the expansion (2)
is unique with probability 1. Q.E.D.
2.2 Leave≠one≠out estimator
In this section the minimizer (1) for the whole data sequence of l≠training instances and
some other objects related to it will be additionally marked with superscript `(l)'. The
superscript `(lni)' will be used analogously to mark objects corresponding to the minimizer
of (1) for the reduced training sequence, with ith instance removed.
Lemma 2.3. With probability 1, for every i 2 f1; :::; lg:
ff (l)
i 6= 0 , c(x i ; y i ; ∏ i [f (l) ; b (l) ]) ? 0; (4)
ff (l)
i 6= 0 , c(x i ; y i ; ∏ i [f (lni) ; b (lni) ]) ? 0: (5)
2 Note that in general, if a function OE : R ! R is convex, differentiable and such that dOE=d∏(0) =
0, then the cost function c(x; y; ∏) := OE((∏)+) is convex and differentiable.

Proof outline. With probability 1, functions k(x j ; :), j = 1; :::; l, are linearly independent
(cf. the proof of Theorem 2.2) and there exists a feature map \Phi : X ! R l such that
vectors z j := \Phi(x j ), i = 1; :::; l are linearly independent, k(x j ; x) = z j \Delta \Phi(x) and
f (l) (x) = z (l) \Delta \Phi(x) + fib (l) for every x 2 X , where z (l) :=
P l
j=1 ff (l) j z j . The pair
(z (l) ; b (l) ) minimizes the function
~
R (l)
reg (z; b) :=
l
X
j=1
c(x j ; y j ; ~
∏ j (z; b)) +
ñ
2
jjzjj 2 (6)
where ~
∏ j (z; b) := y j \Gamma z \Delta z j \Gamma fib. This function is differentiable due to the standing
assumptions on the cost c. Hence, necessarily gradR reg = 0, at the minimum (z (l) ; b (l) ),
which due to the linear independence of vectors z j , gives
ff (l)
j = \Gamma 1
ñ
@c
@∏
(x j ; y j ; ~
∏ j (z (l) ; b (l) )) (7)
for every j = 1; :::; l. This equality combined with equivalence (3) proves (4).
Now we proceed to the proof of (5). Note that the pair (z (lni) ; b (lni) ), where z (lni) :=
P l
j 6=i ff (lni)
j z j , corresponds in the feature space to the minimizer (f (lni) ; b (lni) ) of the
reduced regularized risk:
~
R (lni)
reg (z; b) :=
l
X
j=1; j 6=i
c(x j ; y j ; ~
∏ j (z; b)) +
ñ
2
jjzjj 2 :
Sufficiency in (5). From (4) and characterization (7) of the critical point it follows immedi≠
ately that if ff (l)
i = 0, then the minimizers for the full and reduced data sets are identical.
Necessity in (5). A supposition of ff (l)
i 6= 0 and c(x i ; y i ; ∏ i [f (lni) ; b (lni) ]) = 0 leads to a
contradiction. Indeed, from (4), c(x i ; y i ; ~
∏ i (z (l) ; b (l) )) ? 0, hence:
~
R (l)
reg (z (lni) ; b (lni) ) = ~
R (lni)
reg (z (lni) ; b (lni) )
ü ~
R (lni)
reg (z (l) ; b (l) ) = ~
R (l)
reg (z (l) ; b (l) ) \Gamma c(x i ; y i ; ~
∏ i (z (l) ; b (l) ))
! ~
R (l)
reg (z (l) ; b (l) ) = min
(z;b)2R l \ThetaR
R (l)
reg (z; b):
This contradiction completes the proof. Q.E.D.
We say that x i is a sensitive support vector if ff (l)
i 6= 0 and f (l) 6= f (lni) , i.e., if its removal
from the training set changes the solution.
Corollary 2.4. Every support vector is sensitive with probability 1.
Proof. If ff i 6= 0, then the vector z (l) 62 Lin R (z 1 ; ::::; z i\Gamma1 ; z i+1 ; :::; z l ) since z (l)
has a non≠trivial component ff i z i in the direction of ith feature vector z i , while
z (lni) 2 Lin R (z 1 ; ::::; z i\Gamma1 ; z i+1 ; :::; z l ). Thus z (l) and z (lni) have different direc≠
tions in Lin R (z 1 ; :::; z l ) ae Z and there exists j 0 2 f1; :::; lg such that f (l) (x j 0 ) 6=
f (lni) (x j 0 ). Q.E.D.
We define the empirical risk and the expected (true) risk of margin error
Remp [f; b] :=
P l
i=1 I fc(x i ;y i ;∏ i [f;b])?0g
l =
#fi ; c(x i ; y i ; ∏ i [f; b]) ? 0g
l
;
R exp [f; b] := Prob[c(x; y; y \Gamma f(x) \Gamma fib) ? 0];

where (f; b) 2 H \Theta R, I f:g denotes the indicator function and # denotes the cardinality
(number of elements) of a set.
From the above Lemma we obtain immediately the following result:
Corollary 2.5. With probability 1:
#fi ; c(x i ; y i ; f (lni) (x i ) + fib (lni) ) ? 0g
l =
#fi ; ff (l)
i 6= 0g
l = Remp [f (l) ; b (l) ]:
There exist counter≠examples showing the phrase ``with probability 1'' above cannot be
omitted. The sum on L.H.S. above is the leave≠one≠out estimator of the risk of margin
error [14] for the minimizer of regularized risk (1). The above corollary shows that this
estimator is uniquely determined by the number of support vectors as well as the number
of training margin errors.
Now from the Lunts≠Brailovsky Theorem [14, Theorem 10.8] applied to the risk
Q(x; y; f; b) := I fc(x;y;y\Gammaf (x)\Gammafib?0g the following result is obtained.
Theorem 2.6.
E[R exp (f (l\Gamma1) ; b (l\Gamma1) )] = E[Remp (f (l) ; b (l) )] =
E[#fi ; ff (l)
i 6= 0g]
l
; (8)
where the first expectation is in the selection of training (l \Gamma 1)≠sample and the remaining
two are with respect to the selection of training l≠sample.
A cost function is called partially insensitive if there exists (x; y) 2 X \Theta Y and ∏ 1 6= ∏ 2
such that c(x; y; ∏ 1 ) = c(x; y; ∏ 2 ) = 0. Otherwise, the cost c is called sensitive. Typical
SVM cost functions are partially insensitive while typical RN cost functions are sensitive.
The following result can be derived from Theorem 2.6 and Lemma 2.3.
Corollary 2.7. If the number of support vectors is ! l with a probability ? 0, then the cost
function has to be partially insensitive.
Typical cost functions penalize for an allocation of a wrong sign, i.e.
8 (x;y;ìy)2X \ThetaY \ThetaR yìy ! 0 ) c(x; y; y \Gamma ì
y) ? 0: (9)
Let us define the risk of misclassification of the kernel machine ì
y(x) = f(x) + fib for
(f; b) 2 H \Theta R as R clas [f; b] := Prob[yìy(x) ! 0]. Assuming (9), we have R clas [f; b] ü
R exp [f; b]. Combining this observation with (8) we obtain an extension of Vapnik's result
[14, Theorem 10.5]:
Corollary 2.8. If condition (9) holds then
E[R clas (f (l\Gamma1) ; b (l\Gamma1) )] ü E[#fi ; ff (l)
i 6= 0g]
l
= E[Remp (f (l) ; b (l) )]: (10)
Note that the original Vapnik's result consists in an inequality analogous to the inequality
in the above condition for the specific case of classification by optimal hyperplanes (hard
margin support vector machines).
3 Brief Discussion of Results
Essentiality of assumptions. For every formal result in this paper and any of the standing
assumption there exists an example of a minimizer of (1) which violates the conclusions of
the result. In this sense all those assumptions are essential.
Linear combinations of admissible cost functions. Any weighted sum of cost func≠
tions satisfying our Standing Assumption 3 will satisfy this assumption as well, hence our

formalism will apply to it. An illustrative example is the following cost function for clas≠
sification c(x; y; ∏) =
P q
j C j (max(0; y(∏ \Gamma ffl j )) p j , where C j ? 0, ffl j ñ 0 and p j ? 1 are
constants and y 2 Y = f\Sigma1g.
Non≠differentiable cost functions. Our formal results can be extended with mi≠
nor modifications to the case of typical, non≠differentiable linear cost function such as
c = (y∏) + = max(0; y∏) for SVM classification, c = (j∏j \Gamma ffl) + for SVM regression and
to the classification with hard margins SVMs (optimal hyperplanes). Details are beyond
the scope of this paper. Note that the above linear cost functions can be uniformly approx≠
imated by differentiable cost functions, e.g. by Huber cost function [11, 14], to which our
formalism applies. This implies that our formalism ``applies approximately'' to the linear
loss case and some partial extension of it can be obtained directly using some limit argu≠
ments. However, using direct algebraic approach based on an evaluation of Kuhn≠Tucker
conditions one can come to stronger conclusions. Details will be presented elsewhere.
Theory of generalization. Equality of expectations of empirical and expected risk pro≠
vided by Theorem 2.6 implies that minimizers of regularized risk (1) are on average con≠
sistent. We should emphasize that this result holds for small training samples, of the size
l smaller than VC dimension of the function class, which is dim(H) + 1 in our case. This
should be contrasted with uniform convergence bounds [2, 13, 14] which are vacuous un≠
less l ?? VC dimension.
Significance of approximate solutions for RNs. Corollary 2.7 shows that sparsity of
solutions is practically not achievable for optimal RN solutions since they use sensitive
cost functions. This emphasizes the significance of research into approximately optimal
solution algorithms in such a case, cf. [12].
Application to selection of the regularization constant. The bound provided by Corol≠
lary 2.8 and the equivalence given by Theorem 2.6 can be used as a justification of a heuris≠
tic that the optimal value of regularization constant ñ is the one which minimizes the num≠
ber of margin errors (cf. [14]). This is especially appealing in the case of regression with
ffl≠insensitive cost, where the margin error has a straightforward interpretation of sample
being outside of the ffl≠tube.
Application to modelling of additive noise. Let us suppose that data is iid drawn form
the distribution of the form y = f(x) + ffl noise , where ffl noise is a random noise independent
of x, with 0 mean. Theorem 2.6 implies the following heuristic for approximation of the
noise distribution in the regression model y = f(x) + ffl noise :
Prob[ffl noise ? ffl] ﬂ #fi ; ff (l) 6= 0g
l
:
Here (f (l) ; b (l) ) is a minimizer of the regularized risk (1) with an ffl≠insensitive cost function,
i.e. such that c(x; y; ∏) ? 0 iff j∏j ? ffl.
Acknowledgement. The permission of the Chief Technology Officer, Telstra, to publish
this paper, is gratefully acknowledged.
References
[1] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathe≠
matical Society, 68:337 -- 404, 1950.
[2] P. Bartlett and J. Shave≠Taylor. Generalization performance of support vector ma≠
chines and other pattern classifiers. In B. Schòolkopf, et. al., eds., Advances in Kernel
Methods, pages 43--54, MIT Press, 1998.
[3] C. Burges and D. J. Crisp. Uniqueness of the SVM solution. In S. Sola et. al., ed.,
Adv. in Neural Info. Proc. Sys. 12, pages 144--152, MIT Press, 2000.

[4] D. Cox and F. O'Sullivan. Asymptotic analysis of penalized likelihood and related
estimators. Ann. Statist., 18:1676--1695, 1990.
[5] F. Girosi, M. Jones, and T. Poggio. Regularization theory and neural networks archi≠
tectures. Neural Computation, 7(2):219--269, 1995.
[6] T. Jaakkola and D. Haussler. Probabilistic kernel regression models. In Proc. Seventh
Work. on AI and Stat., San Francisco, 1999. Morgan Kaufman.
[7] T. Joachims. Estimating the Generalization Performance of an SVM Efficiently. In
Proc. of the International Conference on Machine Learning, 2000. Morgan Kaufman.
[8] G. Kimeldorf and G. Wahba. A correspondence between Bayesian estimation of
stochastic processes and smoothing by splines. Ann. Math. Statist., 41:495--502, 1970.
[9] A. Lunts and V. Brailovsky. Evaluation of attributes obtained in statistical decision
rules. Engineering Cybernetics, 3:98--109, 1967.
[10] M. Opper and O. Winther. Gaussian process classification and SVM: Mean field
results and leave≠one out estimator. In P. Bartlett, et. al eds., Advances in Large
Margin Classifiers, pages 301--316, MIT Press, 2000.
[11] A. Smola and B. Schòolkopf. A tutorial on support vector regression. Statistics and
Computing, 1998. In press.
[12] A. J. Smola and B. Schòolkopf. Sparse greedy matrix approximation for machine
learning. Typescript, March 2000.
[13] V. Vapnik. The Nature of Statistical Learning Theory. Springer Verlag, New York,
1995.
[14] V. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.
[15] C. K. I. Williams. Prediction with Gaussian processes: From linear regression to
linear prediction and beyond. In M. I. Jordan, editor, Learning and Inference in
Graphical Models. Kluwer, 1998.

