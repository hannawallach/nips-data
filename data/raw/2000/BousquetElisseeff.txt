Algorithmic Stability and Generalization
Performance
Olivier Bousquet
CMAP
Ecole Polytechnique
F-91128 Palaiseau cedex
FRANCE
bousquet@cmapx.polytechnique.fr
Andre Elissee 
Barnhill Technologies
6709 Waters Avenue
Savannah, GA 31406
USA
andre@barnhilltechnologies.com
Abstract
We present a novel way of obtaining PAC-style bounds on the gen-
eralization error of learning algorithms, explicitly using their stabil-
ity properties. A stable learner is one for which the learned solution
does not change much with small changes in the training set. The
bounds we obtain do not depend on any measure of the complexity
of the hypothesis space (e.g. VC dimension) but rather depend on
how the learning algorithm searches this space, and can thus be
applied even when the VC dimension is innite. We demonstrate
that regularization networks possess the required stability property
and apply our method to obtain new bounds on their generalization
performance.
1 Introduction
A key issue in computational learning theory is to bound the generalization error of
learning algorithms. Until recently, most of the research in that area has focused on
uniform a-priori bounds giving a guarantee that the dierence between the training
error and the test error is uniformly small for any hypothesis in a given class.
These bounds are usually expressed in terms of combinatorial quantities such as VC-
dimension. In the last few years, researchers have tried to use more rened quantities
to either estimate the complexity of the search space (e.g. covering numbers [1])
or to use a posteriori information about the solution found by the algorithm (e.g.
margin [11]). There exist other approaches such as observed VC dimension [12], but
all are concerned with structural properties of the learning systems. In this paper
we present a novel way of obtaining PAC bounds for specic algorithms explicitly
using their stability properties. The notion of stability, introduced by Devroye
and Wagner [4] in the context of classication for the analysis of the Leave-one-
out error and further rened by Kearns and Ron [8] is used here in the context
of regression in order to get bounds on the empirical error rather than the leave-
one-out error. This method has the nice advantage of providing bounds that do
 This work was done while the author was at Laboratoire ERIC, Universite Lumiere
Lyon 2, 5 avenue Pierre Mendes-France, F-69676 Bron cedex, FRANCE

not depend on any complexity measure of the search space (e.g. VC-dimension or
covering numbers) but rather on the way the algorithm searches this space. In that
respect, our approach can be related to Freund's [7] where the estimated size of the
subset of the hypothesis space actually searched by the algorithm is used to bound
its generalization error. However Freund's result depends on a complexity term
which we do not have here since we are not looking separately at the hypotheses
considered by the algorithm and their risk.
The paper is structured as follows: next section introduces the notations and the
denition of stability used throughout the paper. Section 3 presents our main result
as a PAC-like theorem. In Section 4 we will prove that regularization networks are
stable and apply the main result to obtain bounds on their generalization ability.
A discussion of the results will be presented in Section 5.
2 Notations and Denitions
X and Y being respectively an input and an output space, we consider a learning
set S = fz 1 = (x 1 ; y 1 ); ::; z m = (x m ; ym )g of size m in Z = X  Y drawn i.i.d. from
an unknown distribution D. A learning algorithm is a function L from Z m into
Y X mapping a learning set S onto a function fS from X to Y. To avoid complex
notations, we consider only deterministic algorithms. It is also assumed that the
algorithm A is symmetric with respect to S, i.e. for any permutation over the
elements of S, fS yields the same result. Furthermore, we assume that all functions
are measurable and all sets are countable which does not limit the interest of the
results presented here.
The empirical error of a function f measured on the training set S is:
Rm (f) = 1
m
m
X
i=1
c(f; z i )
c : Y X X Y ! R + being a cost function. The risk or generalization error can be
written as:
R(f) = E zD [c(f; z)]
The study we describe here intends to bound the dierence between empirical and
generalization error for specic algorithms. More precisely, our goal is to bound for
any  > 0, the term
PSD m [jR m (f S ) R(fS )j > ] (1)
Usually, learning algorithms cannot output just any function in Y X but rather pick
a function fS in a set F ( Y X representing the structure or the architecture or the
model. Classical VC theory deals with structural properties and aims at bounding
the following quantity:
PSD m
"
sup
f2F
jRm (f) R(f)j > 
#
(2)
This applies to any algorithm using F as a hypothesis space and a bound on this
quantity directly implies a similar bound on (1). However, classical bounds require
the VC dimension of F to be nite and do not use information about algorithmic
properties. For a set F, there exists many ways to search it which may yield dierent
performance. For instance, multilayer perceptrons can be learned by a simple back-
propagation algorithm or combined with a weight decay procedure. The outcome

of the algorithm belongs in both cases to the same set of functions, although their
performance can be dierent.
VC theory was initially motivated by empirical risk minimization (ERM) in which
case the uniform bounds on the quantity (2) give tight error bounds. Intuitively,
the empirical risk minimization principle relies on a uniform law of large numbers.
Because it is not known in advance, what will be the minimum of the empirical
risk, it is necessary to study the dierence between empirical and generalization
error for all possible functions in F. If, now, we do not consider this minimum, but
instead, we focus on the outcome of a learning algorithm A, we may then know a
little bit more what kind of functions will be obtained. This limits the possibilities
and restricts the supremum over all the functions in F to the possible outcomes of
the algorithm. An algorithm which always outputs the null function does not need
to be studied by a uniform law of large numbers.
Let's introduce a notation for modied training sets: if S denotes the initial
training set, S = fz 1 ; : : : ; z i 1 ; z i ; z i+1 ; : : : ; z mg, then S i denotes the training
set after z i has been replaced by a dierent training example z 0
i , that is S i =
fz 1 ; : : : ; z i 1 ; z 0
i ; z i+1 ; : : : ; z mg. Now, we dene a notion of stability for regression.
Denition 1 (Uniform stability) Let S = fz 1 ; : : : ; z m g be a training set, S i =
Snz i be the training set where instance i has been removed and A a symmetric
algorithm. We say that A is -stable if the following holds:
8S 2 Z m ; 8z 0
i ; z 2 Z; jc(f S ; z) c(f S i ; z)j   (3)
This condition expresses that for any possible training set S and any replacement
example z 0
i , the dierence in cost (measured on any instance in Z) incurred by the
learning algorithm when training on S and on S i is smaller than some constant .
3 Main result
A stable algorithm,i.e. -stable with a small , has the property that replacing one
element in its learning set does not change much its outcome. As a consequence,
the empirical error, if thought as a random variable, should have a small variance.
Stable algorithms can then be good candidates for their empirical error to be close
to their generalization error. This assertion is formulated in the following theorem:
Theorem 2 Let A be a -stable algorithm, such that 0  c(f S ; z)  M , for all
z 2 Z and all learning set S. For all  > 0, for any m  8M 2
 2 , we have:
PSD m [jR m (f S ) R(fS )j > ]  64Mm + 8M 2
m 2
(4)
and for any m  1,
PSD m [jR m (f S ) R(fS )j >  + ]  2 exp
 m 2
2(m +M) 2

(5)
Notice that this theorem gives tight bounds when the stability  is of the order
of 1=m. It will be proved in next section that regularization networks satisfy this
requirement.
In order to prove theorem 2, one has to study the random variable X = R(fS )
Rm (f S ), which can be done using two dierent approaches. The rst one (corre-
sponding to the exponential inequality) uses a classical martingale inequality and is

detailed below. The second one is a bit more technical and requires to use standard
proof techniques such as symmetrization. Here we only briey sketch this proof and
refer the reader to [5] for more details.
Proof of inequality (5) We use the following theorem :
Theorem 3 (McDiarmid [9]). Let Y 1 ; : : : ; Yn be n i.i.d. random variables taking
values in a set A, and assume that F : A n ! R satises for 1  i  n:
sup
y1 ;::: ;yn ;y 0
i 2A
jF (y 1 ; : : : ; yn ) F (y 1 ; : : : ; y i 1 ; y 0
i ; y i+1 ; : : : ; yn )j  c i
then
P [jF (Y 1 ; : : : ; Yn ) E[F (Y 1 ; : : : ; Yn )]j > ]  2e 2 2 = P n
i=1 c 2
i
In order to apply theorem 3, we have to bound the expectation of X . We begin
with a useful lemma:
Lemma 1 For any symmetric learning algorithm we have for all 1  i  m:
ESD m [R(fS ) Rm (f S )] = E S;z 0
i D m+1 [c(f S ; z 0
i ) c(f S i ; z 0
i )]
Proof: Notice that
ESD m [Rm (f S )] = 1
m
m
X
j=1
ESD m [c(f S ; z j )] = ESD m [c(f S ; z i )] ; 8i 2 f1; : : : ; mg
by symmetry and the i.i.d. assumption. Now, by simple renaming of z i as z 0
i we get
ESD m [Rm (f S )] = E S i D m [c(f S i ; z 0
i )] = E S;z 0
i D m+1 [c(f S i ; z 0
i )]
which, with the observation that
ESD m [R(fS )] = E S;z 0
i D m+1 [c(f S ; z 0
i )]
concludes the proof. 
Using the above lemma and the fact that A is -stable, we easily get:
ESD m [R(fS ) Rm (f S )]  E S;z 0
i D m+1 [] = 
We now have to compute the constants c i appearing in theorem 3.
We have
jR(fS ) R(f S i )j  E zD [jc(f S ; z) c(f S i ; z)j]  
and
jRm (f S ) Rm (f S i )j  1
m
X
j 6=i
jc(f S ; z j ) c(f S i ; z j )j + 1
m jc(f S ; z i ) c(f S i ; z 0
i )j
 2M
m
+ 
Theorem 3 applied to R(fS ) Rm (f S ) then gives inequality (5).
Sketch of the proof of inequality (4) Recall Chebyshev's inequality :
P (jX j  )  E[X 2 ]
 2
; (6)
for any random variable X . In order to apply this inequality, we have to bound
E[X 2 ]. This can be done with a similar reasoning as for the expectation. Calcu-
lations are however more complex and we do not describe them here. For more
details, see [5]. The result is the following:
E  [X 2 ]  M 2 =m + 8M
which with (6) gives inequality (4) and concludes the proof.

4 Stability of Regularization Networks
4.1 Denitions
Regularization networks have been introduced in machine learning by Poggio and
Girosi [10]. The relationship between these networks and the Support Vector Ma-
chines, as well as their Bayesian interpretation, make them very attractive. We
consider a training set S = f(x 1 ; y 1 ); : : : ; (x m ; ym )g with x i 2 R d and y i 2 R, that
is we are in the regression setting. The regularization network technique consists
in nding a function f : R d ! R in a space H which minimizes the following
functional:
C(f) = 1
m
m
X
j=1
(f(x j ) y j ) 2 + kfk 2
H (7)
where kfk 2
H denotes the norm in the space H . In this framework, H is chosen to
be a reproducing kernel Hilbert space (rkhs), which is basically a functional space
endowed with a dot product 1 . A rkhs is dened by a kernel function, that is a
symmetric function k : R d  R d ! R which we will assume to be bounded by a
constant  in what follows 2 . In particular the following property will hold :
jf(x)j  kfkH kkkH  kfkH (8)
4.2 Stability study
In this section, we show that regularization networks are, furthermore, stable as
soon as  is not too small.
Theorem 4 For regularization networks with kkkH   and (f(x) y) 2  M ,
R(fS )  Rm (f S ) + 4M
m + 4M
s  2 2
 2 + 4
 + 2
 ln(2=∆)
m (9)
and
R(fS )  Rm (f S ) + 2M
s  64
 + 2
 1
m∆ (10)
Proof: Let's denote by fS the minimizer of C. Let's dene
C i (f) = 1
m
m
X
j 6=i
(f(x j ) y j ) 2 + 1
m
(f(x 0
i ) y 0
i ) 2 + kfk 2
H
Let f S i be the minimizer of C i and let g denote the dierence f S i fS . By simple
algebra, we have for t 2 [0; 1]
C(fS ) C(fS + tg) = 2t
m
m
X
j=1
(f S (x j ) y j )g(x j ) 2t < fS ; g > +t 2 A(g)
1 We do not detail here the properties of such a space and refer the reader to [2, 3] for
additional details.
2 Once again we do not give full detail of the denition of appropriate kernel functions
and refer the reader to [3].

where A(g) which is not explicitly written here is the factor of t 2 . Similarly we have
C i (f S i ) C i (f S i tg) = 2t
m
X
j 6=i
(f S i (x j ) y j )g(x j )
+ 2t
m (f S i (x 0
i ) y 0
i )g(x 0
i ) + 2t < f S i ; g > +t 2 A i (g)
By optimality, we have
C(fS ) C(fS + tg)  0 and C i (f S i ) C i (f S i tg)  0
thus, summing those inequalities, dividing by t=m and making t ! 0, we get
2
X
j 6=i
g(x j ) 2 2(f S (x i ) y i )g(x i ) + 2(f S i (x 0
i ) y 0
i )g(x 0
i ) + 2mkgk 2
H  0
which gives
mkgk 2
H  (f S (x i ) y i )g(x i ) (f S (x 0
i ) y 0
i )g(x 0
i )  2
p
MkgkH
using (8). We thus obtain
kf S i fS kH  2
p
M=(m) (11)
and also
8x; y j(f S (x) y) 2 (f S i (x) y) 2 j  2
p
M jf S (x) f S i (x)j  4M=(m)
We thus proved that the minimization of C[f ] is a 4M
m -stable procedure which
allows to apply theorem 2. .
4.3 Discussion
These inequalities are both of interest since the range where they are tight is dier-
ent. Indeed, (10) has a poor dependence on ∆ which makes it deteriorate when high
condence is sought for. However, (9) can give high condence bounds but will be
looser when  is small.
Moreover, results exposed by Evgeniou et al. [6] indicate that the optimal depen-
dence of  with m is obtained for m = O(ln ln m). If we plug this into the above
bounds, we can notice that (9) does not converge as m !1. It may be conjectured
that the poor estimation of the variance coming from the martingale method in Mc-
Diarmid's inequality is responsible for this eect, but a ner analysis is required to
fully understand this phenomenon.
One of the interests of these results is to provide a mean for choosing the  parameter
by minimizing the right hand side of the inequality. Usually, it is determined with
a validation set: some of the data is not used during learning and  is chosen such
that the error of fS over the validation set is minimized. The drawback of this
approach is to reduce the amount of data available for learning.
5 Conclusion and future work
We have presented a new approach to get bounds on the generalization performance
of learning algorithms which makes use of specic properties of these algorithms.
The bounds we obtain do not depend on the complexity of the hypothesis class but
on a measure of how stable the algorithm's output is with respect to changes in the
training set.

Although this work has focused on regression, we believe that it can be extended
to classication, in particular by making the stability requirement less demanding
(e.g. stability in average instead of uniform stability). Future work will also aim
at nding other algorithms that are stable or can be appropriately modied to ex-
hibit the stability property. At last, a promising application of this work could be
the model selection problem where one has to tune parameters of the algorithms
(e.g.  and parameters of the kernel for regularization networks). Instead of using
cross-validation, one could measure how stability is inuenced by the various pa-
rameters of interest and plug these measures into theorem 2 to derive bounds on
the generalization error.
Acknowledgments
We would like to thank G. Lugosi, S. Boucheron and O. Chapelle for interesting
discussions on stability and concentration inequalities. Many thanks to A. Smola
and to the anonymous reviewers who helped improve the readability.
References
[1] N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. Scale-sensitive dimensions,
uniform convergence and learnability. Journal of the ACM, 44(4):615{631, 1997.
[2] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68:337{404,
1950.
[3] M. Atteia. Hilbertian Kernels and splines functions. Studies in computational math-
ematics 4. North-Holland, 1992.
[4] L.P. Devroye and T.J. Wagner. Distribution-free performance bounds for potential
function rules. IEEE Trans. on Information Theory, 25(5):202{207, 1979.
[5] A. Elissee. A study about algorithmic stability and its relation to generalization
performances. Technical report, Laboratoire ERIC, Univ. Lyon 2, 2000.
[6] T. Evgeniou, M. Pontil, and T. Poggio. A unied framework for regularization net-
works and support vector machines. Technical Memo AIM-1654, Massachusetts In-
stitute of Technology, Articial Intelligence Laboratory, December 1999.
[7] Y. Freund. Self bounding learning algorithms. In Proceedings of the 11th Annual Con-
ference on Computational Learning Theory (COLT-98), pages 247{258, New York,
July 24{26 1998. ACM Press.
[8] M. Kearns and D. Ron. Algorithmic stability and sanity-check bounds for leave-one-
out cross-validation. Neural Computation, 11(6):1427{1453, 1999.
[9] C. McDiarmid. On the method of bounded dierences. In Surveys in Combinatorics,
pages 148{188. Cambridge University Press, Cambridge, 1989.
[10] T. Poggio and F. Girosi. Regularization algorithms for learning that are equivalent
to multilayer networks. Science, 247:978{982, 1990.
[11] J. Shawe-Taylor, P. L. Bartlett, R. C. Williamson, and M. Anthony. A framework for
structural risk minimization. In Proc. 9th Annu. Conf. on Comput. Learning Theory,
pages 68{76. ACM Press, New York, NY, 1996.
[12] J. Shawe-Taylor and R. C. Williamson. Generalization performance of classiers in
terms of observed covering numbers. In Paul Fischer and Hans Ulrich Simon, edi-
tors, Proceedings of the 4th European Conference on Computational Learning The-
ory (Eurocolt-99), volume 1572 of LNAI, pages 274{284, Berlin, March 29{31 1999.
Springer.

