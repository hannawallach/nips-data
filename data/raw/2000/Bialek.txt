Stability and noise in biochemical switches
William Bialek
NEC Research Institute
4 Independence Way
Princeton, New Jersey 08540
bialek@research.nj.nec.com
Abstract
Many processes in biology, from the regulation of gene expression in
bacteria to memory in the brain, involve switches constructed from
networks of biochemical reactions. Crucial molecules are present in
small numbers, raising questions about noise and stability. Analysis
of noise in simple reaction schemes indicates that switches stable for
years and switchable in milliseconds can be built from fewer than
one hundred molecules. Prospects for direct tests of this prediction,
as well as implications, are discussed.
1 Introduction
The problem of building a reliable switch arises in several di#erent biological con­
texts. The classical example is the switching on and o# of gene expression during
development [1], or in simpler systems such as phage # [2]. It is likely that the cell
cycle should also be viewed as a sequence of switching events among discrete states,
rather than as a continuously running clock [3]. The stable switching of a specific
class of kinase molecules between active and inactive states is believed to play a role
in synaptic plasticity, and by implication in the maintenance of stored memories
[4]. Although many details of mechanism remain to be discovered, these systems
seem to have several common features. First, the stable states of the switches are
dissipative, so that they reflect a balance among competing biochemical reactions.
Second, the total number of molecules involved in the construction of the switch is
not large. Finally, the switch, once flipped, must be stable for a time long compared
to the switching time, perhaps---for development and for memory---even for a time
comparable to the life of the organism. Intuitively we might expect that systems
with small numbers of molecules would be subject to noise and instability [5], and
while this is true we shall see that extremely stable biochemical switches can in fact
be built from a few tens of molecules. This has interesting implications for how we
think about several cellular processes, and should be testable directly.
Many biological molecules can exist in multiple states, and biochemical switches
use this molecular multistability so that the state of the switch can be `read out' by
sampling the states (or enzymatic activities) of individual molecules. Nonetheless,
these biochemical switches are based on a network of reactions, with stable states
that are collective properties of the network dynamics and not of any individual
molecule. Most previous work on the properties of biochemical reaction networks
has involved detailed simulation of particular kinetic schemes [6], for example in
discussing the kinase switch that is involved in synaptic plasticity [7]. Even the
problem of noise has been discussed heuristically in this context [8]. The goal in

the present analysis is to separate the problem of noise and stability from other
issues, and to see if it is possible to make some general statements about the limits
to stability in switches built from a small number of molecules. This e#ort should
be seen as being in the same spirit as recent work on bacterial chemotaxis, where
the goal was to understand how certain features of the computations involved in
signal processing can emerge robustly from the network of biochemical reactions,
independent of kinetic details [9].
2 Stochastic kinetic equations
Imagine that we write down the kinetic equations for some set of biochemical re­
actions which describe the putative switch. Now let us assume that most of the
reactions are fast, so that there is a single molecular species whose concentration
varies more slowly than all the others. Then the dynamics of the switch essentially
are one dimensional, and this simplification allows a complete discussion using stan­
dard analytical methods. In particular, in this limit there are general bounds on
the stability of switches, and these bounds are independent of (incompletely known)
details in the biochemical kinetics. It should be possible to make progress on multi­
dimensional versions of the problem, but the point here is to show that there exists
a limit in which stable switches can be built from small numbers of molecules.
Let the number of molecules of the `slow species' be n. All the di#erent reactions
can be broken into two classes: the synthesis of the slow species at a rate f(n)
molecules per second, and its degradation at a rate g(n) molecules per second; the
dependencies on n can be complicated because they include the e#ects of all other
species in the system. Then, if we could neglect fluctuations, we would write the
e#ective kinetic equation
dn
dt
= f(n) - g(n). (1)
If the system is to function as a switch, then the stationarity condition f(n) = g(n)
must have multiple solutions with appropriate local stability properties.
The fact that molecules are discrete units means that we need to give the chemical
kinetic Eq. (1) another interpretation. It is the mean field approximation to a
stochastic process in which there is a probability per unit time f(n) of making the
transition n # n+1, and a probability per unit time g(n) of the opposite transition
n # n - 1. Thus if we consider the probability P (n, t) for there being n molecules
at time t, this distribution obeys the evolution (or `master') equation
#P (n, t)
#t
= f(n - 1)P (n - 1, t) + g(n + 1)P (n + 1, t) - [f(n) + g(n)]P (n, t),(2)
with obvious corrections for n = 0, 1. We are interested in the e#ects of stochasticity
for n not too small. Then 1 is small compared with typical values of n, and we can
approximate P (n, t) as being a smooth function of n. We can expand Eq. (2) in
derivatives of the distribution, and keep the leading terms:
#P (n, t)
#t
= #
#n
½ [g(n) - f(n)]P (n, t) + 1
2
#
#n
[f(n) + g(n)]P (n, t) ¾ . (3)
This is analogous to the di#usion equation for a particle moving in a potential, but
this analogy works only if allow the e#ective temperature to vary with the position
of the particle.
As with di#usion or Brownian motion, there is an alternative to the di#usion equa­
tion for P (n, t) and this is to write an equation of motion for n(t) which supplements

Eq. (1) by the addition of a random or Langevin force #(t):
dn
dt
= f(n) - g(n) + #(t), (4)
##(t)#(t # )# = [f(n) + g(n)]#(t - t # ). (5)
From the Langevin equation we can also develop the distribution functional for
the probability of trajectories n(t). It should be emphasized that all of these ap­
proaches are equivalent provided that we are careful to treat the spatial variations
of the e#ective temperature [10]. 1 In one dimension this complication does not im­
pede solving the problem. For any particular kinetic scheme we can compute the
e#ective potential and temperature, and kinetic schemes with multiple stable states
correspond to potential functions with multiple minima.
3 Noise induced switching rates
We want to know how the noise term destabilizes the distinct stable states of the
switch. If the noise is small, then by analogy with thermal noise we expect that there
will be some small jitter around the stable states, but also some rate of spontaneous
jumping between the states, analogous to thermal activation over an energy barrier
as in a chemical reaction. This jumping rate should be the product of an ``attempt
frequency''---of order the relaxation rate in the neighborhood of one stable state---
and a ``Boltzmann factor'' that expresses the exponentially small probability of
going over the barrier. For ordinary chemical reactions this Boltzmann factor is
just exp(-F + /kB T ), where F + is the activation free energy. If we want to build
a switch that can be stable for a time much longer than the switching time itself,
then the Boltzmann factor has to provide this large ratio of time scales.
There are several ways to calculate the analog of the Boltzmann factor for the
dynamics in Eq. (4). The first step is to make more explicit the analogy with
Brownian motion and thermal activation. Recall that Brownian motion of an over­
damped particle is described by the Langevin equation
#
dx
dt
= -V # (x) + #(t), (6)
where # is drag coe#cient of the particle, V (x) is the potential, and the noise force
has correlations ##(t)#(t # )# = 2#T #(t - t # ), where T is the absolute temperature
measured in energy units so that Boltzmann's constant is equal to one. Comparing
with Eq. (4), we see that our problem is equivalent to a particle with # = 1
in an e#ective potential V e# (n) such that V #
e# (n) = g(n) - f(n), at an e#ective
temperature T e# (n) = [f(n) + g(n)]/2.
If the temperature were uniform then the equilibrium distribution of n would be
P eq (n) # exp[-V e# (n)/T e# ]. With nonuniform temperature the result is (up to
1 In a review written for a biological audience, McAdams and Arkin [11] state that
Langevin methods are unsound and can yield invalid predictions precisely for the case
of bistable reaction systems which interests us here; this is part of their argument for
the necessity of stochastic simulation methods as opposed to analytic approaches. Their
reference for the failure of Langevin methods [12], however, seems to consider only Langevin
terms with constant spectral density, thus ignoring (in the present language) the spatial
variations of e#ective temperature. For the present problem this would mean replacing
the noise correlation function [f(n) + g(n)]#(t - t # ) in Eq. (5) by Q#(t - t # ) where Q is
a constant. This indeed is wrong, and is not equivalent to the master equation. On the
other hand, if the arguments of Refs. [11, 12] were generally correct, they would imply that
Langevin methods could not used for the description of Brownian motion with a spatially
varying temperature, and this would be quite a surprise.

weakly varying prefactors)
P eq (n) # exp[-U(n)] (7)
U(n) = Z n
0
dy
V #
e# (y)
T e# (y) . (8)
One way to identify the Boltzmann factor for spontaneous switching is then to
compute the relative equilibrium occupancy of the stable states (n 0 and n 1 ) and the
unstable ``transition state'' at n # . The result is that the e#ective activation energy
for transitions from a stable state at n = n 0 to the stable state at n = n 1 > n 0 is
F + (n 0 # n 1 ) = 2kB T Z n#
n0
dn
g(n) - f(n)
g(n) + f(n)
, (9)
where n # is the unstable point, and similarly for the reverse transition,
F + (n 1 # n 0 ) = 2kB T Z n1
n#
dn
f(n) - g(n)
g(n) + f(n)
. (10)
An alternative approach is to note that the distribution of trajectories n(t) includes
locally optimal paths that carry the system from each stable point up to the tran­
sition state; the e#ective activation free energy can then be written as an integral
along these optimal paths. The use of optimal path ideas in chemical kinetics has
a long history, going back at least to Onsager. A discussion in the spirit of the
present one is Ref. [13]. For equations of the general form
dn
dt
= -V #
e# (n) + #(t), (11)
with ##(t)#(t # )# = 2T e# (t)#(t-t # ), the probability distribution for trajectories P [n(t)]
can be written as [10]
P [n(t)] = exp (-S[n(t)]) (12)
S[n(t)] = 1
4 Z dt
1
T e# (t) [ —
n(t) + V #
e# (n(t))] 2
-
1
2 Z dtV ##
e# (n(t)). (13)
If the temperature T e# is small, then the trajectories that minimize the action
should be determined primarily by minimizing the first term in Eq. (13), which is
# 1/T e# . Identifying the e#ective potential and temperature as above, the relevant
term is
1
2 Z dt
[ —
n - f(n) + g(n)] 2
f(n) + g(n)
= 1
2 Z dt
—
n 2
f(n) + g(n)
+ 1
2 Z dt
[f(n) - g(n)] 2
f(n) + g(n)
- Z dt —
n f(n) - g(n)
f(n) + g(n)
. (14)
We are searching for trajectories which take n(t) from a stable point n 0 where
f(n 0 ) = g(n 0 ) through the unstable point n # where f and g are again equal but the
derivative of their di#erence (the curvature of the potential) has changed sign. For
a discussion of the analogous quantum mechanical problem of tunneling in a double
well, see Ref. [14]. First we note that along any trajectory from n 0 to n # we can
simplify the third term in Eq. (14):
Z dt —
n
f(n) - g(n)
f(n) + g(n)
= Z n#
n0
dn
f(n) - g(n)
f(n) + g(n)
. (15)

This term thus depends on the endpoints of the trajectory and not on the path, and
therefore cannot contribute to the structure of the optimal path. In the analogy
to mechanics, the first two terms are equivalent to the (Euclidean) action for a
particle with position dependent mass in a potential; this means that along extremal
trajectories there is a conserved energy
E = 1
2
—
n 2
f(n) + g(n) -
1
2
[f(n) - g(n)] 2
f(n) + g(n)
. (16)
At the endpoints of the trajectory we have —
n = 0 and f(n) = g(n), and so we are
looking for zero energy trajectories, along which
—
n(t) = ±[f(n(t)) - g(n(t))]. (17)
Substituting back into Eq. (14), and being careful about the signs, we find once
again Eq's. (9,10).
Both the `transition state' and the optimal path method involve approximations,
but if the noise is not too large the approximations are good and the results of
the two methods agree. Yet another approach is to solve the master equation (2)
directly, and again one gets the same answer for the switching rate when the noise
is small, as expected since all the di#erent approaches are all equivalent if we make
consistent approximations. It is much more work to find the prefactors of the rates,
but we are concerned here with orders of magnitude, and hence the prefactors aren't
so important.
4 Interpretation
The crucial thing to notice in this calculation is that the integrands in Eq's. (9,10)
are bounded by one, so the activation energy (in units of the thermal energy kB T )
is bounded by twice the change in the number of molecules. Translating back to
the spontaneous switching rates, the result is that the noise driven switching time
is longer than the relaxation time after switching by a factor that is bounded,
spontaneous switching time
relaxation time < exp(#n), (18)
where #n is the change in the number of molecules required to go from one stable
`switched' state to the other. Imagine that we have a reaction scheme in which the
di#erence between the two stable states corresponds to roughly 25 molecules. Then
it is possible to have a Boltzmann factor of up to exp(25) # 10 10 . Usually we think
of this as a limit to stability: with 25 molecules we can have a Boltzmann factor of no
more than # 10 10 . But here I want to emphasize the positive statement that there
exist kinetic schemes in which just 25 molecules would be su#cient to have this level
of stability. This corresponds to years per millisecond: with twenty five molecules,
a biochemical switch that can flip in milliseconds can be stable for years. Real
chemical reaction schemes will not saturate this bound, but certainly such stability
is possible with roughly 100 molecules. The genetic switch in # phage operates with
roughly 100 copies of the repressor molecules, and even in this simple system there
is extreme stability: the genetic switch is flipped spontaneously only once in 10 5
generations of the host bacterium [2]. Kinetic schemes with greater cooperativity
get closer to the bound, achieving greater stability for the same number of molecules.
In electronics, the construction of digital elements provides insulation against fluc­
tuations on a microscopic scale and allows a separation between the logical and
physical design of a large system. We see that, once a cell has access to several tens
of molecules, it is possible to construct `digital' switch elements with dynamics that
are no longer significantly a#ected by microscopic fluctuations. Furthermore, weak

interactions of these molecules with other cellular components cannot change the
basic `states' of the switch, although these interactions can couple state changes to
other events.
The importance of this `digitization' on the scale of 10-100 molecules is illustrated
by di#erent models for pattern formation in development. In the classical model
due to Turing, patterns are expressed by spatial variations in the concentration of
di#erent molecules, and patterns arise because uniform concentrations are rendered
unstable through the combination of nonlinearities in the kinetics with the di#erent
di#usion constants of di#erent substances. In this picture, the spatial structure of
the pattern is linked directly to physical properties of the molecules. An alternative
that each spatial location is labelled by a set of discrete possible states, and patterns
evolve out of the `automaton' rules by which each location changes state in relation
to the neighboring states. In this picture states and rules are more abstract, and the
dynamics of pattern formation is really at a di#erent level of description from the
molecular dynamics of chemical reactions and di#usion. Reliable implementations of
automaton rules apparently are accessible as soon as the relevant chemical reactions
involve a few dozen molecules.
Biochemical switches have been reconstituted in vitro, but I am not aware of any
attempts to verify that stable switching is possible with small numbers of molecules.
It would be most interesting to study model systems in which one could confine and
monitor su#ciently few molecules that it becomes possible to observe spontaneous
switching, that is the breakdown of stability. Although genetic switches have cer­
tain advantages, even the simplest systems would require full enzymatic apparatus
for gene expression (but see Ref. [16] for recent progress on controllable in vitro
expression systems). 2 Kinase switches are much simpler, since they can be con­
structed from just a few proteins and can be triggered by calcium; caged calcium
allows for an optical pulse to serve as input.
At reasonable protein concentrations, 10 - 100 molecules are found in a volume of
roughly 1 (µm) 3 . Thus it should be possible to fabricate an array of `cells' with
linear dimensions ranging from 100 nm to 10 µm, such that solutions of kinase
and accessory proteins would switch stably in the larger cells but exhibit instability
and spontaneous switching in the smaller cells. The state of the switch could be
read out by including marker proteins that would serve as substrates of the kinase
but have, for example, fluorescence lines that are shifted by phosphorylation, or by
having fluorescent probes on the kinase itself; transitions of single enzyme molecules
should be observable [15].
A related idea would be to construct vesicles containing ligand gate ion channels
which can conduct calcium, and then have inside the vesicle enzymes for synthesis
and degradation of the ligand which are calcium sensitive. The cGMP channels of
rod photoreceptors are an example, and in rods the cyclase synthesizing cGMP is
calcium sensitive, but the sign is wrong to make a switch [17]; presumably this could
solved by appropriate mixing and matching of protein components from di#erent
cells. In such a vesicle the di#erent stable states would be distinguished by di#erent
2 Note also that reactions involving polymer synthesis (mRNA from DNA or protein
from mRNA) are not `elementary' reactions in the sense described by Eq. (2). Synthesis
of a single mRNA molecule involves thousands of steps, each of which occurs (conditionally)
at constant probability per unit time, and so the noise in the overall synthesis reaction is
very di#erent. If the synthesis enzymes are highly processive, so that the polymerization
apparatus incoporates many monomers into the polymer before `backing up' or falling o#
the template, then synthesis itself involves a delay but relatively little noise; the dominant
source of noise becomes the assembly and disassembly of the polymerization complex.
Thus there is some subtlety in trying to relate a simple model to the complex sequence
of reactions involved in gene expression. On the other hand a detailed simulation is
problematic, since there are so many di#erent elementary steps with unknown rates. This
combination of circumstances would make experiments on a minimal, in vitro genetic
switch espcially interesting.

levels of internal calcium (as with adaptation states in the rod), and these could
be read out optically using calcium indicators; caged calcium would again provide
an optical input to flip the switch. Amusingly, a close packed array of such vesicles
with # 100 nm dimension would provide an optically addressable and writable
memory with storage density comparable to current RAM, albeit with much slower
switching.
In summary, it should be possible to build stable biochemical switches from a few
tens of molecules, and it seems likely that nature makes use of these. To test our
understanding of stability we have to construct systems which cross the threshold for
observable instabilities, and this seems accessible experimentally in several systems.
Acknowledgments
Thanks to M. Dykman, J. J. Hopfield, and A. J. Libchaber for helpful discussions.
References
1. J. M. W. Slack, From Egg to Embryo: Determinative Events in Early De­
velopment (Cambridge University Press, Cambridge, 1983); P. A. Lawrence,
The Making of a Fly: The Genetics of Animal Design (Blackwell Science,
Oxford, 1992).
2. M. Ptashne, A Genetic Switch: Phage # and Higher Organisms, 2nd Edi­
tion (Blackwell, Cambridge MA, 1992); A. D. Johnson, A. R. Poteete, G.
Lauer, R. T. Sauer, G. K. Ackers, and M. Ptashne, Nature 294, 217--223
(1981).
3. A. W. Murray, Nature 359, 599--604 (1992).
4. S. G. Miller and M. B. Kennedy, Cell 44, 861--870 (1986); M. B. Kennedy,
Ann. Rev. Biochem. 63, 571--600 (1994).
5. E. Schr˜odinger, What is Life? (Cambridge University Press, Cambridge,
1944).
6. H. H. McAdams and A. Arkin, Ann. Rev. Biophys. Biomol. Struct. 27,
199--224 (1998); U. S. Bhalla and R. Iyengar, Science 283, 381--387 (1999).
7. J. E. Lisman, Proc. Nat. Acad. Sci. (USA) 82, 3055--3057 (1985).
8. J. E. Lisman and M. A. Goldring, Proc. Nat. Acad. Sci. (USA) 85,
5320--5324 (1988).
9. N. Barkai and S. Leibler, Nature 387, 913--917 (1997).
10. J. Zinn­Justin, Quantum Field Theory and Critical Phenomena (Clarendon
Press, Oxford, 1989).
11. H. H. McAdams and A. Arkin, Trends Genet. 15, 65--69 (1999).
12. F. Baras, M. Malek Mansour and J. E. Pearson, J. Chem. Phys. 105,
8257--8261 (1996).
13. M. I. Dykman, E. Mori, J. Ross, and P. M. Hunt, J. Chem. Phys. 100,
5735--5750 (1994).
14. S. Coleman, Aspects of Symmetry (Cambridge University Press, Cambridge,
1975).
15. H. P. Lu, L. Xun, and X. S. Xie, Science 282, 1877--1882 (1998); T. Ha,
A. Y. Ting, J. Liang, W. B. Caldwell, A. A. Deniz, D. S. Chemla, P. G.
Schultz, and S. Weiss, Proc. Nat. Acad. Sci. (USA) 96, 893--898 (1999).
16. G. V. Shivashankar, S. Liu & A. J. Libchaber, Appl. Phys. Lett. 76,
3638--3640 (2000).
17. F. Rieke and D. A. Baylor, Revs. Mod. Phys. 70, 1027--1036 (1998).

