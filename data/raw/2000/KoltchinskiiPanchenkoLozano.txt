Some new bounds on the generalization error of
combined classifiers
Vladimir Koltchinskii
Department of Mathematics and Statistics
University of New Mexico
Albuquerque, NM 87131­1141
vlad@math.unm.edu
Dmitriy Panchenko
Department of Mathematics and Statistics
University of New Mexico
Albuquerque, NM 87131­1141
panchenk@math.unm.edu
Fernando Lozano
Department of Electrical and Computer Engineering
University of New Mexico
Albuquerque, NM 87131
flozano@eece.unm.edu
Abstract
In this paper we develop the method of bounding the generalization error
of a classifier in terms of its margin distribution which was introduced in
the recent papers of Bartlett and Schapire, Freund, Bartlett and Lee. The
theory of Gaussian and empirical processes allow us to prove the margin
type inequalities for the most general functional classes, the complexity
of the class being measured via the so called Gaussian complexity func­
tions. As a simple application of our results, we obtain the bounds of
Schapire, Freund, Bartlett and Lee for the generalization error of boost­
ing. We also substantially improve the results of Bartlett on bounding
the generalization error of neural networks in terms of l 1 norms of the
weights of neurons. Furthermore, under additional assumptions on the
complexity of the class of hypotheses we provide some tighter bounds,
which in the case of boosting improve the results of Schapire, Freund,
Bartlett and Lee.
1 Introduction and margin type inequalities for general functional
classes
Let (X; Y ) be a random couple, where X is an instance in a space S and Y 2 f1; 1g is
a label. Let G be a set of functions from S into R: For g 2 G; sign(g(X)) will be used as
a predictor (a classifier) of the unknown label Y: If the distribution of (X; Y ) is unknown,
then the choice of the predictor is based on the training data (X 1 ; Y 1 ); : : : ; (Xn ; Yn ) that
consists of n i.i.d. copies of (X; Y ): The goal of learning is to find a predictor ^ g 2 G (based
on the training data) whose generalization (classification) error PfY ^ g(X)  0g is small
enough. We will first introduce some probabilistic bounds for general functional classes
and then give several examples of their applications to bounding the generalization error of
boosting and neural networks. We omit all the proofs and refer an interested reader to [5].

Let (S; A; P ) be a probability space and let F be a class of measurable functions from
(S; A) into R: Let fX k g be a sequence of i.i.d. random variables taking values in
(S; A) with common distribution P: Let Pn be the empirical measure based on the sample
(X 1 ; : : : ; Xn ); Pn := n 1
P n
i=1 Æ X i ; where Æ x denotes the probability distribution con­
centrated at the point x: We will denote Pf :=
R
S fdP; Pn f :=
R
S fdPn ; etc. In what
follows, ` 1 (F) denotes the Banach space of uniformly bounded real valued functions on
F with the norm kY kF := sup f2F jY (f)j; Y 2 ` 1 (F): Define
Gn (F) := E kn 1
n
X
i=1
g i Æ X i kF = E sup
f2F
  n 1
n
X
i=1
g i f(X i )
  ;
where fg i g is a sequence of i.i.d. standard normal random variables, independent of fX i g:
We will call n 7! Gn (F) the Gaussian complexity function of the class F : One can find in
the literature (see, e.g. [11]) various upper bounds on such quantities as Gn (F) in terms of
entropies, VC­dimensions, etc.
We give below a bound in terms of margin cost functions (compare to [6, 7]) and Gaussian
complexities.
Let  = f' k : R ! Rg 1
k=1 be a class of Lipschitz functions such that (1 + sgn( x))=2 
' k (x) for all x 2 R and all k: For each ' 2 ; L(') will denote it's Lipschitz constant.
Theorem 1 For all t > 0;
P
n
9f 2 F : Pff  0g > inf
k1
h
Pn' k (f) +
p
2L(' k )Gn (F)
+
 log k
n
 1=2 i
+
t + 2
p
n
o
 2 expf2t 2 g:
Let us consider a special family of cost functions. Assume that ' is a fixed nonincreasing
Lipschitz function from R into R such that '(x)  (1+sgn( x))=2 for all x 2 R: One can
easily observe that L('(=Æ))  L(')Æ 1 : Applying Theorem 1 to the class of Lipschitz
functions  := f'(=Æ k ) : k  0g; where Æ k := 2 k ; we get the following result.
Theorem 2 For all t > 0;
P
n
9f 2 F : Pff  0g > inf
Æ2[0;1]
h
Pn'( f
Æ ) +
2
p
2L(')
Æ Gn (F)
+
 log log 2 (2Æ 1 )
n
 1=2 i
+
t + 2
p
n
o
 2 expf2t 2 g:
In [5] an example was given which shows that, in general, the order of the factor Æ 1 in the
second term of the bound can not be improved.
Given a metric space (T ; d); we denote H d (T ; ") the "­entropy of T with respect to d;
i.e. H d (T ; ") := log N d (T ; "); where N d (T ; ") is the minimal number of balls of radius
" covering T : The next theorem improves the previous results under some additional as­
sumptions on the growth of random entropies H dPn ;2 (F ; ): Define for  2 (0; 1]
Æ n (; f) := sup
n
Æ 2 (0; 1) : Æ  Pff  Æg  n 1+ 
2
o
and
^
Æ n (; f) := sup
n
Æ 2 (0; 1) : Æ  Pn ff  Æg  n 1+ 
2
o
:
We call Æ n (; f) and ^
Æ n (; f); respectively, the ­margin and the empirical ­margin of f:

Theorem 3 Suppose that for some  2 (0; 2) and for some constant D > 0
H dPn ;2 F ; u

 Du  ; u > 0 a:s: (1)
Then for any   2
2+ ; for some constants A; B > 0 and for all large enough n
P
n
8f 2 F : A 1 ^
Æ n (; f)  Æ n (; f)  A ^
Æ n (; f)
o
 1 B(log 2 log 2 n) exp
n
n

2 =2
o
:
This implies that with high probability for all f 2 F
Pff  0g  c(n 1 =2 ^ Æ n (; f)  ) 1 :
The bound of Theorem 2 corresponds to the case of  = 1: It is easy to see from the
definitions of  margins that the quantity (n 1 =2 ^ Æ n (; f)  ) 1 increases in  2 (0; 1]:
This shows that the bound in the case of  < 1 is tighter. Further discussion of this
type of bounds and their experimental study in the case of convex combinations of simple
classifiers is given in the next section.
2 Bounding the generalization error of convex combinations of
classifiers
Recently, several authors ([1, 8]) suggested a new class of upper bounds on generalization
error that are expressed in terms of the empirical distribution of the margin of the predictor
(the classifier). The margin is defined as the product Y ^
g(X): The bounds in question are
especially useful in the case of the classifiers that are the combinations of simpler classifiers
(that belong, say, to a class H). One of the examples of such classifiers is provided by the
classifiers obtained by boosting [3, 4], bagging [2] and other voting methods of combining
the classifiers. We will now demonstrate how our general results can be applied to the case
of convex combinations of simple base classifiers.
We assume that ~
S := Sf1; 1g and ~
F := f ~
f : f 2 Fg; where ~
f(x; y) := yf(x): P will
denote the distribution of (X; Y ); Pn the empirical distribution based on the observations
((X 1 ; Y 1 ); : : : ; (Xn ; Yn )): It is easy to see that Gn ( ~
F) = Gn (F): One can easily see
that if F := conv(H); where H is a class of base classifiers, then Gn (F) = Gn (H):
These easy observations allow us to obtain useful bounds for boosting and other methods
of combining the classifiers. For instance, we get in this case the following theorem that
implies the bound of Schapire, Freund, Bartlett and Lee [8] when H is a VC­class of sets.
Theorem 4 Let F := conv(H); where H is a class of measurable functions from (S; A)
into R: For all t > 0;
P
n
9f 2 F : Pfyf(x)  0g > inf
Æ2[0;1]
h
Pn'( yf(x)
Æ ) +
2
p
2
Æ Gn (H)
+
 log log 2 (2Æ 1 )
n
 1=2 i
+
t + 2
p
n
o
 2 expf2t 2 g:
In particular, if H is a VC--class of classifiers h : S 7! f1; 1g (which means that the class
of sets ffx : h(x) = +1g : h 2 Hg is a Vapnik--Chervonenkis class) with VC--dimension
V (H), we have with some constant C > 0; Gn (H)  C(V (H)=n) 1=2 : This implies that
with probability at least 1 
Pfyf(x)  0g  inf
Æ2(0;1]
h
Pn fyf(x)  Æg +
C
Æ
r
V (H)
n
+

+
 log log 2 (2Æ 1 )
n
 1=2 i
+
q
1
2 log 2
 + 2
p
n
;
which slightly improves the bound obtained previously by Schapire, Freund, Bartlett and
Lee [8].
Theorem 3 provides some improvement of the above bounds on generalization error of
convex combinations of base classifiers. To be specific, consider the case when H is a
VC­class of classifiers. Let V := V (H) be its VC­dimension. A well known bound (going
back to Dudley) on the entropy of the convex hull (see [11], p. 142) implies that
H dPn ;2 (conv(H); u)  sup
Q2P(S)
H dQ;2 (conv(H); u)  Du
2(V 1)
V :
It immediately follows from Theorem 3 that for all   2(V 1)
2V 1
and for some constants
C; B
P
n
9f 2 conv(H) : Pf ~
f  0g >
C
n 1 =2^ Æ n (; f) 
o
 B log 2 log 2 n exp
n 1
2
n

2
o
;
where
^
Æ n (; f) := sup
n
Æ 2 (0; 1) : Æ  Pn f(x; y) : yf(x)  Æg  n 1+ 
2
o
:
This shows that in the case when the VC­dimension of the base is relatively small the
generalization error of boosting and some other convex combinations of simple classifiers
obtained by various versions of voting methods becomes better than it was suggested by the
bounds of Schapire, Freund, Bartlett and Lee. One can also conjecture that the remarkable
generalization ability of these methods observed in numerous experiments can be related
to the fact that the combined classifier belongs to a subset of the convex hull for which
the random entropy H dPn ;2 is much smaller than for the whole convex hull (see [9, 10] for
improved margin type bounds in a much more special setting).
To demonstrate the improvement provided by our bounds over previous results, we show
some experimental evidence obtained for a simple artificially generated problem, for which
we are able to compute exactly the generalization error as well as the ­margins.
We consider the problem of learning a classifier consisting of the indicator function of the
union of a finite number of intervals in the input space S = [0; 1]. We used the Adaboost
algorithm [4] to find a combined classifier using as base class H = f[0; b] : b 2 [0; 1]g [
f[b; 1] : b 2 [0; 1]g (i.e. decision stumps). Notice that in this case V = 2, and according to
the theory values of gamma in (2=3; 1) should result in tighter bounds on the generalization
error.
For our experiments we used a target function with 10 equally spaced intervals, and a sam­
ple size of 1000, generated according to the uniform distribution in [0; 1]. We ran Adaboost
for 500 rounds, and computed at each round the generalization error of the combined clas­
sifier and the bound C(n 1 =2 ^
Æ n (; f)  ) 1 for different values of . We set the constant
C to one.
In figure 1 we plot the generalization error and the bounds for  = 1; 0:8 and 2=3. As
expected, for  = 1 (which corresponds roughly to the bounds in [8]) the bound is very
loose, and as  decreases, the bound gets closer to the generalization error. In figure 2
we show that by reducing further the value of  we get a curve even closer to the actual
generalization error (although for  = 0:2 we do not get an upper bound). This seems to
support the conjecture that Adaboost generates combined classifiers that belong to a subset
of of the convex hull of H with a smaller random entropy. In figure 3 we plot the ratio
^
Æ n (; f)=Æn (; f) for  = 0:4; 2=3 and 0:8 against the boosting iteration. We can see that
the ratio is close to one in all the examples indicating that the value of the constant A in
theorem 3 is close to one in this case.

0 50 100 150 200 250 300 350 400 450 500
0
0.2
0.4
0.6
0.8
1
1.2
boosting round
Figure 1: Comparison of the generalization error (thicker line) with (n 1 =2 ^
Æ n (; f)  ) 1
for  = 1; 0:8 and 2=3 (thinner lines, top to bottom).
0 50 100 150 200 250
0
0.05
0.1
0.15
0.2
0.25
0.3
boosting round
Figure 2: Comparison of the generalization error (thicker line) with (n 1 =2 ^
Æ n (; f)  ) 1
for  = 0:5; 0:4 and 0:2 (thinner lines, top to bottom).
0 50 100 150 200 250 300 350 400 450 500
0
0.5
1
1.5
2
0 50 100 150 200 250 300 350 400 450 500
1
1.1
1.2
1.3
1.4
0 50 100 150 200 250 300 350 400 450 500
1
1.05
1.1
1.15
Figure 3: Ratio ^
Æ n (; f)=Æn (; f) versus boosting round for  = 0:4; 2=3; 0:8 (top to
bottom)

3 Bounding the generalization error in neural network learning
We turn now to the applications of the bounds of previous section in neural network learn­
ing. Let H be a class of measurable functions from (S; A) into R: Given a sigmoid 
from R into [ 1; 1] and a vector w := (w 1 ; : : : ; wn ) 2 R n ; let N ;w (u 1 ; : : : ; un ) :=
(
P n
i=1 w j u j ): We call the function N ;w a neuron with weights w and sigmoid : For
w 2 R n ; kwk `1 :=
P n
i=1 jw i j: Let  j : j  1 be functions from R into [ 1; 1]; satisfying
the Lipschitz conditions:
j j (u)  j (v)j  L j ju vj; u; v 2 R:
Let fA j g be a sequence of positive numbers. We define recursively classes of neural net­
works with restrictions on the weights of neurons (j below is the number of layers):
H 0 = H; H j (A 1 ; : : : ; A j ) :=
:=
n
N  j ;w (h 1 ; : : : ; hn ) : n  0; h i 2 H j 1 (A 1 ; : : : ; A j 1 ); w 2 R n ; kwk `1  A j
o [
[
H j 1 (A 1 ; : : : ; A j 1 ):
Theorem 5 For all t > 0 and for all l  1
P
n
9f 2 H l (A 1 ; : : : ; A l ) : Pf ~
f  0g > inf
Æ2(0;1]
h
Pn'(
~
f
Æ
) +
1
Æ
l
Y
k=1
(2L j A j + 1)Gn (H)+
+
 log log 2 (2Æ 1 )
n
 1=2
i
+
t + 2
p
n
o
 2 expf2t 2 g
Remark. Bartlett [1] obtained a similar bound for a more special class H and with larger
constants. In the case when A j  A; L j  L (the case considered by Bartlett) the ex­
pression in the right hand side of his bound includes (AL) l(l+1)=2
Æ l ; which is replaced in our
bound by (AL) l
Æ : These improvement can be substantial in applications, since the above
quantities play the role of complexity penalties.
Finally, it is worth mentioning that the theorems of Section 1 can be applied also to bound­
ing the generalization error in multi­class problems. Namely, we assume that the labels
take values in a finite set Y with card(Y) =: L: Consider a class ~
F of functions from
~
S := S  Y into R: A function f 2 ~
F predicts a label y 2 Y for an example x 2 S iff
f(x; y) > max
y 0 6=y
f(x; y 0 ):
The margin of an example (x; y) is defined as
m f (x; y) := f(x; y) max
y 0 6=y
f(x; y 0 );
so f misclassifies the example (x; y) iff m f (x; y)  0: Let
F := ff(; y) : y 2 Y ; f 2 ~
Fg:
The next result follows from Theorem 2.
Theorem 6 For all t > 0;
P
n
9f 2 ~
F : Pfm f  0g > inf
Æ2(0;1]
h
Pn fm f  Æg +
4
p
2L(2L 1)
Æ Gn (F)+
+
 log log 2 (2Æ 1 )
n
 1=2 i
+
t + 2
p
n
o
 2 expf2t 2 g:

References
[1] Bartlett, P. (1998) The Sample Complexity of Pattern Classification with Neural Net­
works: The Size of the Weights is More Important than the Size of the Network. IEEE
Transactions on Information Theory, 44, 525­536.
[2] Breiman, L. (1996). Bagging Predictors. Machine Learning,26(2), 123­140.
[3] Freund Y. (1995) Boosting a weak learning algorithm by majority. Information and
Computation,121,2,256­285.
[4] Freund Y. and Schapire, R.E. (1997) A decision­theoretic generalization of on­line
learning and an application to boosting. Journal of Computer and System Sciences,
55(1),119­139.
[5] Koltchinskii, V. and Panchenko, D. (2000) Empirical margin distributions and bound­
ing the generalization error of combined classifiers, preprint.
[6] Mason, L., Bartlett, P. and Baxter, J. (1999) Improved Generalization through Explicit
Optimization of Margins. Machine Learning , 0, 1­11.
[7] Mason, L., Baxter, J., Bartlett, P. and Frean, M. (1999) Functional Gradient Tech­
niques for Combining Hypotheses. In: Advances in Large Margin Classifiers. Smola,
Bartlett, Sch˜olkopf and Schnurmans (Eds), to appear.
[8] Schapire, R., Freund, Y., Bartlett, P. and Lee, W. S. (1998) Boosting the Margin: A
New Explanation of Effectiveness of Voting Methods. Ann. Statist., 26, 1651­1687.
[9] Shawe--Taylor, J. and Cristianini, N. (1999) Margin Distribution Bounds on Gener­
alization. In: Lecture Notes in Artificial Intelligence, 1572. Computational Learning
Theory, 4th European Conference, EuroCOLT'99, 263--273.
[10] Shawe--Taylor, J. and Cristianini, N. (1999) Further Results on the Margin Distribu­
tion. Proc. of COLT'99, 278--285.
[11] van der Vaart, A. and Wellner, J. (1996) Weak convergence and Empirical Processes.
With Applications to Statistics. Springer­Verlag, New York.

