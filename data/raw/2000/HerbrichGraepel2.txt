A PAC-Bayesian Margin Bound for Linear
Classiers: Why SVMs work
Ralf Herbrich
Statistics Research Group
Computer Science Department
Technical University of Berlin
ralfh@cs.tu-berlin.de
Thore Graepel
Statistics Research Group
Computer Science Department
Technical University of Berlin
guru@cs.tu-berlin.de
Abstract
We present a bound on the generalisation error of linear classiers
in terms of a rened margin quantity on the training set. The
result is obtained in a PACBayesian framework and is based on
geometrical arguments in the space of linear classiers. The new
bound constitutes an exponential improvement of the so far tightest
margin bound by Shawe-Taylor et al. [8] and scales logarithmically
in the inverse margin. Even in the case of less training examples
than input dimensions suciently large margins lead to non-trivial
bound values and  for maximum margins  to a vanishing com-
plexity term. Furthermore, the classical margin is too coarse a
measure for the essential quantity that controls the generalisation
error: the volume ratio between the whole hypothesis space and
the subset of consistent hypotheses. The practical relevance of the
result lies in the fact that the well-known support vector machine
is optimal w.r.t. the new bound only if the feature vectors are all of
the same length. As a consequence we recommend to use SVMs on
normalised feature vectors only  a recommendation that is well
supported by our numerical experiments on two benchmark data
sets.
1 Introduction
Linear classiers are exceedingly popular in the machine learning community due
to their straight-forward applicability and high exibility which has recently been
boosted by the so-called kernel methods [13]. A natural and popular framework
for the theoretical analysis of classiers is the PAC (probably approximately cor-
rect) framework [11] which is closely related to Vapnik's work on the generalisation
error [12]. For binary classiers it turned out that the growth function is an ap-
propriate measure of complexity and can tightly be upper bounded by the VC
(Vapnik-Chervonenkis) dimension [14]. Later, structural risk minimisation [12] was
suggested for directly minimising the VC dimension based on a training set and an
a priori structuring of the hypothesis space.
In practice, e.g. in the case of linear classiers, often a thresholded real-valued func-

tion is used for classication. In 1993, Kearns [4] demonstrated that considerably
tighter bounds can be obtained by considering a scale-sensitive complexity measure
known as the fat shattering dimension. Further results [1] provided bounds on the
Growth function similar to those proved by Vapnik and others [14, 6]. The popular-
ity of the theory was boosted by the invention of the support vector machine (SVM)
[13] which aims at directly minimising the complexity as suggested by theory.
Until recently, however, the success of the SVM remained somewhat obscure because
in PAC/VC theory the structuring of the hypothesis space must be independent of
the training data  in contrast to the data-dependence of the canonical hyperplane.
As a consequence Shawe-Taylor et.al. [8] developed the luckiness framework, where
luckiness refers to a complexity measure that is a function of both hypothesis and
training sample.
Recently, David McAllester presented some PAC-Bayesian theorems [5] that bound
the generalisation error of Bayesian classiers independently of the correctness of the
prior and regardless of the underlying data distribution  thus fullling the basic
desiderata of PAC theory. In [3] McAllester's bounds on the Gibbs classier were
extended to the Bayes (optimal) classier. The PAC-Bayesian framework provides
a posteriori bounds and is thus closely related in spirit to the luckiness framework 1 .
In this paper we give a tight margin bound for linear classiers in the PAC-Bayesian
framework. The main idea is to identify the generalisation error of the classier h of
interest with that of the Bayes (optimal) classier of a (point-symmetric) subset Q
that is summarised by h. We show that for a uniform prior the normalised margin
of h is directly related to the volume of a large subset Q summarised by h. In
particular, the result suggests that a learning algorithm for linear classiers should
aim at maximising the normalised margin instead of the classical margin. In Section
2 and 3 we review the basic PAC-Bayesian theorem and show how it can be applied
to single classiers. In Section 4 we give our main result and outline its proof. In
Section 5 we discuss the consequences of the new result for the application of SVMs
and demonstrate experimentally that in fact a normalisation of the feature vectors
leads to considerably superior generalisation performance.
We denote ntuples by italic bold letters (e.g. x = (x 1 ; : : : ; xn )), vectors by roman
bold letters (e.g. x), random variables by sans serif font (e.g. X) and vector spaces
by calligraphic capitalised letters (e.g. X ). The symbols P; E; I and ` n
2 denote a
probability measure, the expectation of a random variable, the indicator function
and the normed space (2norm) of sequences of length n, respectively.
2 A PAC Margin Bound
We consider learning in the PAC framework. Let X be the input space, and let Y =
f1;+1g. Let a labelled training sample z = (x; y) 2 (X  Y) m = Z m be drawn
iid according to some unknown probability measure P Z = P YjX PX . Furthermore for
a given hypothesis space H  Y X we assume the existence of a true hypothesis
h  2 H that labelled the data
P YjX=x (y) = I y=h  (x) : (1)
We consider linear hypotheses
H = fhw : x 7! sign (hw;  (x)i K ) j w 2 W g ; W = fw 2 K j kwk K = 1g ; (2)
1 In fact, even Shawe-Taylor et.al. concede that ... a Bayesian might say that luckiness
is just a complicated way of encoding a prior. The sole justication for our particular way
of encoding is that it allows us to get the PAC like results we sought... [9, p. 4].

where the mapping  : X ! K  ` n
2 maps 2 the input data to some feature space
K and kwk K = 1 leads to a one-to-one correspondence of hypotheses hw to their
parameters w. From the existence of h  we know that there exists a version space
V (z)  W ,
V (z) = fw 2 W j 8 (x; y) 2 z : hw (x) = yg :
Our analysis aims at bounding the true risk R [w] of consistent hypotheses hw ,
R [w] = PXY (hw (X) 6= Y) :
Since all classiers w 2 V (z) are indistinguishable in terms of number of errors
committed on the given training set z let us introduce the concept of the margin
 z (w) of a classier w, i.e.
 z (w) = min
(x i ;y i )2z
y i hw; x i i K
kwk K
: (3)
The following theorem due to Shawe-Taylor et al. [8] bounds the generalisation
errors R [w] of all classier w 2 V (z) in terms of the margin  z (w).
Theorem 1 (PAC margin bound). For all probability measures P Z such that
PX (k (X)k K  &) = 1, for any Æ > 0 with probability at least 1 Æ over the ran-
dom draw of the training set z, if we succeed in correctly classifying m samples z
with a linear classier w achieving a positive margin  z (w) >
p
32& 2 =m then the
generalisation R [w] of w is bounded from above by
2
m

 (w) log 2

8em
 (w)

log 2 (32m) + ln

m 2
Æ

;  (w) =
& 
8&
 z (w)
 2
'
: (4)
As the bound on R [w] depends linearly on  2
z (w) we see that Theorem 1 provides
a theoretical foundation of all algorithms that aim at maximising  z (w), e.g. SVMs
and Boosting [13, 7].
3 PAC-Bayesian Analysis
We rst present a result [5] that bounds the risk of the generalised Gibbs clas-
sication strategy Gibbs W (z) by the measure PW (W (z)) on a consistent subset
W (z)  V (z). This average risk is then related via the Bayes-Gibbs lemma to the
risk of the Bayes classication strategy Bayes W (z) on W (z). For a single consistent
hypothesis w 2 W it is then necessary to identify a consistent subset Q (w) such
that the Bayes strategy Bayes Q(w) on Q (w) always agrees with w. Let us dene
the Gibbs classication strategy Gibbs W (z) w.r.t. the subset W (z)  V (z) by
Gibbs W (z) (x) = hw (x) ; w  PWjW2W (z) : (5)
Then the following theorem [5] holds for the risk of Gibbs W (z) .
Theorem 2 (PAC-Bayesian bound for subsets of classiers). For any mea-
sure PW and any measure P Z , for any Æ > 0 with probability at least 1 Æ over
the random draw of the training set z for all subsets W (z)  V (z) such that
PW (W (z)) > 0 the generalisation error of the associated Gibbs classication strat-
egy Gibbs W (z) is bounded from above by
R

Gibbs W (z)

 1
m

ln

1
PW (W (z))

+ 2 ln (m) + ln

1
Æ

+ 1

: (6)
2 For notational simplicity we sometimes abbreviate  (x) by x which should not be
confused with the sample x of training objects.

Now consider the Bayes classier Bayes W (z) ,
Bayes W (z) (x) = sign EWjW2W (z) [h W (x)]

;
where the expectation EWjW2W (z) is taken over a cut-o posterior given by com-
bining the PAC-likelihood (1) and the prior PW .
Lemma 1 (Bayes-Gibbs Lemma). For any two measures PW and PXY and any
set W  W
PXY (BayesW (X) 6= Y)  2  PXY (Gibbs W (X) 6= Y) : (7)
Proof. (Sketch) Consider only the simple PAC setting we need. At all those points
x 2 X at which BayesW is wrong by denition at least half of the classiers w 2 W
under consideration make a mistake as well.
The combination of Lemma 1 with Theorem 2 yields a bound on the risk of
Bayes W (z) . For a single hypothesis w 2 W let us nd a (Bayes-admissible) subset
Q (w) of version space V (z) such that Bayes Q(w) on Q (w) agrees with w on every
point in X .
Denition 1 (Bayes-admissibility). Given the hypothesis space in (2) and a
prior measure PW over W we call a subset Q (w)  W Bayes admissible w.r.t. w
and PW if and only if
8x 2 X : hw (x) = Bayes Q(w) (x) :
Although dicult to achieve in general the following geometrically plausible lemma
establishes Bayes-admissibility for the case of interest.
Lemma 2 (Bayes-admissibility for linear classiers). For uniform mea-
sure PW over W each ball Q (w) = fv 2 W j kw vk K  r g is Bayes admissible
w.r.t. its centre w.
Please note that by considering a ball Q (w) rather than just w we make use of
the fact that w summarises all its neighbouring classiers v 2 Q (w). Now using a
uniform prior PW the normalised margin
z (w) = min
(x i ;y i )2z
y i hw; x i i K
kwk K kx i k K
; (8)
quanties the relative volume of classiers summarised by w and thus allows us
to bound its risk. Note that in contrast to the classical margin  z (see 3) this
normalised margin is a dimensionless quantity and constitutes a measure for the
relative size of the version space invariant under rescaling of both weight vectors w
and feature vectors x i .
4 A PAC-Bayesian Margin Bound
Combining the ideas outlined in the previous section allows us to derive a gener-
alisation error bound for linear classiers w 2 V (z) in terms of their normalised
margin z (w).

Figure 1: Illustration of the volume ratio for the classier at the north pole. Four
training points shown as grand circles make up version space  the polyhedron
on top of the sphere. The radius of the cap of the sphere is proportional to the
margin z , which only for constant kx i k K is maximised by the SVM.
Theorem 3 (PAC-Bayesian margin bound). Suppose K  ` n
2 is a given
feature space of dimensionality n. For all probability measures P Z , for any
Æ > 0 with probability at least 1 Æ over the random draw of the training set
z, if we succeed in correctly classifying m samples z with a linear classier w
achieving a positive margin z (w) > 0 then the generalisation error R [w] of
w is bounded from above by
2
m
 
d ln
 
1
1
p
1 2
z (w)
!
+ 2 ln (m) + ln

1
Æ

+ 2
!
: (9)
where d = min (m; n).
Proof. Geometrically the hypothesis space W is the unit sphere in R n (see Figure
1). Let us assume that PW is uniform on the unit sphere as suggested by symmetry.
Given the training set z and a classier w all classiers v 2 Q (w)
Q (w) =
n
v 2 W
   hw; vi K >
p
1 2
z (w)
o
(10)
are within V (z) (For a proof see [2]). Such a set Q (w) is Bayes-admissible by
Lemma 2 and hence we can use PW (Q (w)) to bound the generalisation error of
w. Since PW is uniform, the value ln (PW (Q (w))) is simply the logarithm of the
volume ratio between the surface of the unit sphere and the surface of all v fullling
equation (10). In [2] it is shown that this ratio is exactly given by
ln
0
B @
R 2
0 sin n 2 () d
R arccos
p
1 2
z (w)

0 sin n 2 () d
1
C A :
It can be shown that this ratio is tightly bounded from above by
n ln
 
1
1
p
1 2
z (w)
!
+ ln (2) :

10 20 30 40
0.040
0.050
0.060
0.070
p
generalisation
error
10 20 30 40
0.16
0.18
0.20
0.22
p
generalisation
error
(a) (b)
Figure 2: Generalisation errors of classiers learned by an SVM with (dashed line)
and without (solid line) normalisation of the feature vectors x i . The error bars
indicate one standard deviation over 100 random splits of the data sets. The two
plots are obtained on the (a) thyroid and (b) sonar data set.
With ln (2) < 1 we obtain the desired result. Note that m points maximally span
an mdimensional space and thus we can marginalise over the remaining n m
dimensions of feature space K. This gives d = min (m; n).
An appealing feature of equation (9) is that for z (w) = 1 the bound reduces
to 2
m (2 ln (m) ln (Æ) + 2) with a rapid decay to zero as m increases. In case of
margins z (w) > 0:91 the troublesome situation of d = m, which occurs e.g. for
RBF kernels, is compensated for. Furthermore, upper bounding 1=(1
p
1 ) by
2= we see that Theorem 3 is an exponential improvement of Theorem 1 in terms
of the attained margins. It should be noted, however, that the new bound depends
on the dimensionality of the input space via d = min (m; n).
5 Experimental Study
Theorem 3 suggest the following learning algorithm: given a version space V (z)
(through a given training set z) nd the classier w that maximises z (w). This
algorithm, however, is given by the SVM only if the training data in feature space
K are normalised. We investigate the inuence of such a normalisation on the
generalisation error in the feature space K of all monomials up to the pth degree
(well-known from handwritten digit recognition, see [13]). Since the SVM learning
algorithm as well as the resulting classier only refer to inner products in K, it
suces to use an easy-to-calculate kernel function k : X  X ! R such that for all
x; x 0 2 X , k (x; x 0 ) = h (x) ;  (x 0 )i K , given in our case by the polynomial kernel
8p 2 N : k (x; x 0 ) = (hx; x 0 i X + 1) p :
Earlier experiment have shown [13] that without normalisation too large values
of p may lead to overtting. We used the UCI [10] data sets thyroid (d = 5,
m = 140, m test = 75) and sonar (d = 60, m = 124, m test = 60) and plotted
the generalisation error of SVM solutions (estimated over 100 dierent splits of the
data set) as a function of p (see Figure 2). As suggested by Theorem 3 in almost all
cases the normalisation improved the performance of the support vector machine
solution at a statistically signicant level. As a consequence, we recommend:
When training an SVM, always normalise your data in feature space.

Intuitively, it is only the spatial direction of both weight vector and feature vectors
that determines the classication. Hence the dierent lengths of feature vectors in
the training set should not enter the SVM optimisation problem.
6 Conclusion
The PAC-Bayesian framework together with simple geometrical arguments yields
the so far tightest margin bound for linear classiers. The role of the normalised
margin z in the new bound suggests that the SVM is theoretically justied only for
input vectors of constant length. We hope that this result is recognised as a useful
bridge between theory and practice in the spirit of Vapnik's famous statement:
Nothing is more practical than a good theory
Acknowledgements We would like to thank David McAllester, John Shawe-
Taylor, Bob Williamson, Olivier Chapelle, John Langford, Alex Smola and Bernhard
Schölkopf for interesting discussions and useful suggestions on earlier drafts.
References
[1] N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. Scale sensitive dimensions,
uniform convergence and learnability. Journal of the ACM, 44(4):615631, 1997.
[2] R. Herbrich. Learning Linear Classiers - Theory and Algorithms. PhD thesis, Tech-
nische Universität Berlin, 2000. accepted for publication by MIT Press.
[3] R. Herbrich, T. Graepel, and C. Campbell. Bayesian learning in reproducing kernel
Hilbert spaces. Technical report, Technical University of Berlin, 1999. TR 99-11.
[4] M. J. Kearns and R. Schapire. Ecient distribution-free learning of probabilistic
concepts. Journal of Computer and System Sciences, 48(2):464497, 1993.
[5] D. A. McAllester. Some PAC Bayesian theorems. In Proceedings of the Eleventh An-
nual Conference on Computational Learning Theory, pages 230234, Madison, Wis-
consin, 1998.
[6] N. Sauer. On the density of families of sets. Journal of Combinatorial Theory, Series
A, 13:145147, 1972.
[7] R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. Boosting the margin: A
new explanation for the eectiveness of voting methods. In Proceedings of the 14th
International Conference in Machine Learning, 1997.
[8] J. Shawe-Taylor, P. L. Bartlett, R. C. Williamson, and M. Anthony. Structural risk
minimization over datadependent hierarchies. IEEE Transactions on Information
Theory, 44(5):19261940, 1998.
[9] J. Shawe-Taylor and R. C. Williamson. A PAC analysis of a Bayesian estimator.
Technical report, Royal Holloway, University of London, 1997. NC2TR1997013.
[10] UCI. University of California Irvine: Machine Learning Repository, 1990.
[11] L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134
1142, 1984.
[12] V. Vapnik. Estimation of Dependences Based on Empirical Data. Springer, 1982.
[13] V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.
[14] V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. Theory of Probability and its Application, 16(2):264281,
1971.

