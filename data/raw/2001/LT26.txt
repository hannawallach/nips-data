Information Geometrical Framework for
Analyzing Belief Propagation Decoder
Shiro Ikeda
Kyushu Inst. of Tech., & PRESTO, JST
Wakamatsu, Kitakyushu, Fukuoka, 808­0196 Japan
shiro@brain.kyutech.ac.jp
Toshiyuki Tanaka
Tokyo Metropolitan Univ.
Hachioji, Tokyo, 192­0397 Japan
tanaka@eei.metro­u.ac.jp
Shun­ichi Amari
RIKEN BSI
Wako, Saitama, 351­0198 Japan
amari@brain.riken.go.jp
Abstract
The mystery of belief propagation (BP) decoder, especially of the turbo
decoding, is studied from information geometrical viewpoint. The loopy
belief network (BN) of turbo codes makes it difficult to obtain the true
``belief'' by BP, and the characteristics of the algorithm and its equilib­
rium are not clearly understood. Our study gives an intuitive understand­
ing of the mechanism, and a new framework for the analysis. Based on
the framework, we reveal basic properties of the turbo decoding.
1 Introduction
Since the proposal of turbo codes[2], they have been attracting a lot of interests because
of their high performance of error correction. Although the thorough experimental results
strongly support the potential of this iterative decoding method, the mathematical back­
ground is not sufficiently understood. McEliece et al.[5] have shown its relation to the
Pearl's BP, but the BN for the turbo decoding is loopy, and the BP solution gives only an
approximation.
The problem of the turbo decoding is a specific example of a general problem of marginaliz­
ing an exponential family distribution. The distribution includes higher order correlations,
and its direct marginalization is intractable. But the partial model with a part of the corre­
lations, can be marginalized with BP algorithm exactly, since it does not have any loop. By
collecting and exchanging the BP results of the partial models, the true ``belief'' is approxi­
mated. This structure is common among various iterative methods, such as Gallager codes,
Beth’e approximation in statistical physics[4], and BP for loopy BN.
We investigate the problem from information geometrical viewpoint[1]. It gives a new
framework for analyzing these iterative methods, and shows an intuitive understanding of
them. Also it reveals a lot of basic properties, such as characteristics of the equilibrium, the
condition of stability, the cost function related to the decoder, and the decoding error. In this

paper, we focus on the turbo decoding, because its structure is simple, but the framework
is general, and the main results can be generalized.
2 Information Geometrical Framework
2.1 Marginalization, MPM Decoding, and Belief
Let us consider a distribution of x = (x 1 ; \Delta \Delta \Delta ; xN ) T which is defined as follows
p(x) = C exp(c 0 (x) + c 1 (x) + \Delta \Delta \Delta + c K (x)); (1)
where, c 0 (x) is the linear function of fx i g, and each c k (x) is the higher order correlations
of fx i g. The problem of turbo codes and similar iterative methods are to marginalize this
distribution. Let \Pi denote the operator of marginalization as, \Pi ffip(x) def
=
Q N
i=1 p(x i ). The
marginalization is equivalent to take the expectation of x as
j
def
=
X
x
xp(x); j = (j 1 ; \Delta \Delta \Delta ; jN ) T :
In the case of MPM (maximization of the posterior marginals) decoding, x i 2 f\Gamma1; +1g
and the sign of each j i is the decoding result. In the belief network, x i 2 f0; 1g and j i is
the belief. In these iterative methods, the marginalization of eq.(1) is not tractable, but the
marginalization of the following distribution is tractable.
p r (x; ¸) = exp (c 0 (x) + c r (x) + ¸ \Delta x \Gamma ' r (¸)) ; r = 1; \Delta \Delta \Delta ; K; ¸ 2 R N : (2)
Each p r (x; ¸) includes only one of the fc k (x)g in eq.(1), and additional parameter ¸ is
used to adjust linear part of x. The iterative methods are exchanging information through
¸ for each p r , and finally approximate \Pi ffip(x).
2.2 The Case of Turbo Decoding
Turbo Encoder
x
Encoder 1 y 1
Encoder 2 y 2
­
­
­
­ ­
­
­ ­ x; y 1 ; y 2
BSC(oe) ­ ~
x; ~
y 1 ~ y 2
­
Turbo Decoder
Decoder 1 ¸ 2
Decoder 2
¸ 1
~
x; ~
y 1
~ x; ~
y 2
­
­
oe
oe
­
oe
Figure 1: Turbo codes
In the case of turbo codes, x is the information bits, from which the turbo encoder generates
two sets of parity bits, y 1 = (y 11 ; \Delta \Delta \Delta ; y 1L ) T , and y 2 = (y 21 ; \Delta \Delta \Delta ; y 2L ) T , y 1j ; y 2j 2
f\Gamma1; +1g (Fig.1). Each parity bit is expressed as the form
Q
i x i , where the product is
taken over a subset of f1; \Delta \Delta \Delta ; Ng. The codeword (x; y 1 ; y 2 ) is then transmitted over a
noisy channel, which we assume BSC (binary symmetric channel) with flipping probability
oe ! 1=2. The receiver observes (~ x; ~
y 1 ; ~
y 2 ), ~ x i ; ~
y 1j ; ~ y 2j 2 f\Gamma1; +1g.
The ultimate goal of the turbo decoding is the MPM decoding of x based on p(xj~ x; ~
y 1 ; ~
y 2 ).
Since the channel is memoryless, the following relation holds
p(~ x; ~
y 1 ; ~
y 2 jx) = exp (fi ~
x \Delta x + fi ~
y 1 \Delta y 1 + fi ~
y 2 \Delta y 2 \Gamma (N + 2L)/(fi))
fi ? 0; oe =
1
2
(1 \Gamma tanh fi); /(fi) def
= ln(e fi + e \Gammafi ):

By assuming the uniform prior on x, the posterior distribution is given as follows
p(xj~ x; ~
y 1 ; ~
y 2 ) =
p(~ x; ~
y 1 ; ~
y 2 jx)
P
x p(~ x; ~
y 1 ; ~
y 2 jx)
= C exp (fi ~
x \Delta x + fi ~
y 1 \Delta y 1 + fi ~
y 2 \Delta y 2 )
= C exp (c 0 (x) + c 1 (x) + c 2 (x)) :
(3)
Here C is the normalizing factor, and c 0 (x) = fi ~
x\Deltax, c r (x) = fi ~
y r \Deltay r (r = 1; 2).
Equation(3) is equivalent to eq.(1), where K = 2. When N is large, marginalization of
p(xj~ x; ~
y 1 ; ~
y 2 ) is intractable since it needs summation over 2 N terms. Turbo codes uti­
lize two decoders which solve the MPM decoding of p r (x; ¸) (r = 1; 2) in eq.(2). The
distribution is derived from p(~ x; ~
y r jx) and the prior of x which has the form of
!(x; ¸) = exp(¸ \Delta x \Gamma /(¸)):
!(x; ¸) is a factorizable distribution. The marginalization of p(~ x; ~
y r jx) is feasible since
its BN is loop free. The parameter ¸ serves as the window of exchanging the information
between the two decoders. The MPM decoding is approximated by updating ¸ iteratively
in ``turbo'' like way.
2.3 Information Geometrical View of MPM Decoding
Let us consider the family of all the probability distributions over x. We denote it by S,
which is defined as
S =
(
p(x)
fi fi fip(x) ? 0; x 2 f\Gamma1; +1g N ;
X
x
p(x) = 1
)
:
We consider an e--flat submanifold M 0 in S. This is the submanifold of p 0 (x; `) defined
as
M 0 =
\Phi p 0 (x; `) = exp (c 0 (x) + ` \Delta x \Gamma ' 0 (`)) j` = (` 1 ; \Delta \Delta \Delta ; ` N ) T 2 R N \Psi : (4)
Since c 0 (x) = fi ~
x \Delta x, every distribution of M 0 can be rewritten as follows
p 0 (x; `) = exp (c 0 (x) + ` \Delta x \Gamma ' 0 (`)) = exp ((fi ~
x + `) \Delta x \Gamma ' 0 (`)) :
It shows that every distribution of M 0 is decomposable, or factorizable. From the informa­
tion geometry[1], we have the following theorem of m--projection.
Theorem 1. Let M be an e--flat submanifold in S, and let q(x)2S. The point in M that
minimizes the KL­divergence from q(x) to M , is denoted by,
\Pi M ffiq(x) = argmin
p(x)2M
D[q(x); p(x)];
and is called the m--projection of q(x) to M . The m--projection is unique.
It is easy to show that the marginalization corresponds to the m--projection to M 0 [7]. Since
MPM decoding and marginalization is equivalent, MPM decoding is also equivalent to the
m--projection to M 0 .
2.4 Information Geometry of Turbo Decoding
Let ßM ffiq(x) denote the parameters in M of the m--projected distribution,
ßM ffiq(x) = argmin
`2R N
D[q(x); p(x; `)]:
The turbo decoding process is written as follows,

1. Let ¸ t
1 = 0 for t = 0, and t = 1.
2. Project p 2 (x; ¸ t
1 ) onto M 0 as ` = ßM0 ffip 2 (x; ¸ t
1 ), and calculate ¸ t+1
2 by
¸ t+1
2 = ßM0 ffip 2 (x; ¸ t
1 ) \Gamma ¸ t
1 :
3. Project p 1 (x; ¸ t+1
2 ) onto M 0 as ` = ßM0 ffip 1 (x; ¸ t+1
2 ), and calculate ¸ t+1
1 by
¸ t+1
1 = ßM0 ffip 1 (x; ¸ t+1
2 ) \Gamma ¸ t+1
2 :
4. If ßM0 ffip 1 (x; ¸ t+1
2 ) 6= ßM0 ffip 2 (x; ¸ t+1
1 ), go to step 2.
The turbo decoding approximates the estimated parameter ` \Lambda , the projection of
p(xj~ x; ~
y 1 ; ~
y 2 ) onto M 0 , as ` \Lambda = ¸ \Lambda
1 + ¸ \Lambda
2 , where the estimated distribution is
p 0 (x; ` \Lambda ) = exp (c 0 (x) + ¸ \Lambda
1 \Delta x + ¸ \Lambda
2 \Delta x \Gamma ' 0 (¸ \Lambda
1 + ¸ \Lambda
2 )) : (5)
An intuitive understanding of the turbo decoding is as follows. In step 2, (¸ \Lambda
2 \Delta x) in eq.(5) is
replaced with c 2 (x). The distribution becomes p 2 (x; ¸ \Lambda
1 ), and ¸ \Lambda
2 is estimated by projecting
it onto M 0 . In step 3, (¸ \Lambda
1 \Delta x) in eq.(5) is replaced with c 1 (x), and ¸ \Lambda
1 is estimated by m--
projection of p 1 (x; ¸ \Lambda
2 ).
We now define the submanifold corresponding to each decoder,
M r =
\Phi p r (x; ¸) = exp (c 0 (x) + c r (x) + ¸ \Delta x \Gamma ' r (¸))j¸ = (¸ 1 ; \Delta \Delta \Delta ; ¸ N ) T 2 R N
\Psi
r = 1; 2:
¸ is the coordinate system of M r . M r is also an e--flat submanifold. M 1 6=M 2 and
M r 6= M 0 hold because c r (x) includes cross terms of x and c 1 (x)6=c 2 (x) in general.
The information geometrical view of the turbo decoding is schematically shown in Fig.2.
3 The Properties of Belief Propagation Decoder
3.1 Equilibrium
When the the turbo decoding converges, equilibrium solution defines three important dis­
tributions, p 1 (x; ¸ \Lambda
1 ), p 2 (x; ¸ \Lambda
2 ), and p 0 (x; ` \Lambda ). They satisfy the following two conditions:
1: \Pi ffip 1 (x; ¸ \Lambda
2 ) = \Pi ffip 2 (x; ¸ \Lambda
1 ) = p 0 (x; ` \Lambda ): (6)
2: ` \Lambda = ¸ \Lambda
1 + ¸ \Lambda
2 : (7)
Let us define a manifold M(`) as
M(`) =
(
p(x)
fi fi fip(x) 2 S;
X
x
p(x)x =
X
x
p 0 (x; `)x
)
:
From its definition, for any p(x)2M(`), the expectation of x is the same, and its m--
projection to M 0 coincides with p 0 (x; `). This is an m--flat submanifold[1], and we call
M(`) an equimarginal submanifold. Since eq.(6) holds, p 0 (x; ` \Lambda ); p 1 (x; ¸ \Lambda
2 ); p 2 (x; ¸ \Lambda
1 ) 2
M(` \Lambda ) is satisfied.
Let us define an e--flat version of the submanifold as E(` \Lambda ), which connects p 0 (x; ` \Lambda ),
p 1 (x; ¸ \Lambda
2 ), and p 2 (x; ¸ \Lambda
1 ) in log­linear manner
E(` \Lambda ) =
(
p(x) = Cp 0 (x;` \Lambda ) t 0 p 1 (x;¸ \Lambda
2 ) t 1 p 2 (x;¸ \Lambda
1 ) t 2
fi fi fi
2
X
r=0
t r = 1
)
:
Since eq.(7) holds, p(xj~ x; ~
y 1 ; ~
y 2 ) is included in the E(`). It can be proved by taking
t 0 = \Gamma1, t 1 = t 2 = 1.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . .
. . . . . .
. . . . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . .
. . .
. . . . .
. . .
. . . .
. . .
. . . .
. . .
. . . .
. . . .
. . .
. . .
. .
. . .
. .
. . .
. .
. . .
. . .
. . .
. . .
. .
. . .
. .
. .
. . .
. .
. .
. . .
. . .
. . .
. .
. .
. .
. .
. .
. .
. .
. .
. .
. .
. .
. . .
. .
. .
. .
. .
. .
. .
. . .
. .
. . .
. .
. .
. .
. .
.
. .
. . .
. . .
. .
.
. .
. .
. .
. .
. .
. .
. .
. .
. .
. .
.
. .
. .
. .
. .
. .
. .
. .
. .
. .
. .
.
. .
. .
. .
.
. .
. .
. .
. .
. .
. . .
. .
. .
. .
. .
. . .
. .
. .
. .
. .
.
. .
.
. .
. .
. .
. .
. .
. .
. .
. . .
. .
. .
. .
. . .
. . .
. . .
. . .
. .
. . .
. .
. .
. .
. . .
. .
. . .
. . .
. .
. . .
. .
. . .
. .
. . .
. .
. . .
. . .
. .
. . .
. .
. . .
. . .
. . . .
. . .
. . .
. . . .
. . . .
. . . .
. . .
. . . . .
. . . .
. . . .
. . . .
. . . . .
. . . .
. . . . . .
. . . . . . .
. . . . .
. . . . . . .
. . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. .. .. . . . .. .. .. .. .. .. .. .. . .. .. ... ... ... ... ... ... ... ... ... ....... ....... ........ ........ ............. ......... .......... ......... ..... ... ... ... .... ... ... ... .... .. .. .. . .. .. .. .. .. .. .. .. . . .. .. . . . . .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
S
m\Gammaprojection
m\Gammaprojection
j j j j j j j j
i i i i1
s
s
M 1
fi ~
x
¸ \Lambda
2 p 1 (x; ¸ \Lambda
2 )
j j j j j j j j
i i i i1 Q Qs
Q Qsi i i i1
M 0
fi ~
x
¸ \Lambda
1
¸ \Lambda
2
¸ \Lambda
1
¸ \Lambda
2
ßM0 ffip 2 (¸ \Lambda
1 )
s s
j j j j j j j j
Q Qs
s
s M 2
fi ~
x
¸ \Lambda
1
p 2 (x; ¸ \Lambda
1 )
Figure 2: Turbo decoding
S
M(` \Lambda
)
E(` \Lambda
)
M 0
M 1
M 2
p 0 (x; ` \Lambda
)
True MPM solution
p 1 (x; ¸ \Lambda
2 )
p 2 (x; ¸ \Lambda
1 )
p(xj~ x; ~
y 1 ; ~
y 2 )
Figure 3: M(` \Lambda ) and E(` \Lambda )
Theorem 2. When the turbo decoding procedure converges, the convergent probability
distributions p 0 (x; ` \Lambda ), p 1 (x; ¸ \Lambda
2 ), and p 2 (x; ¸ \Lambda
1 ) belong to equimarginal submanifold
M(` \Lambda ), while its e--flat version E(` \Lambda ) includes these three distributions and also the pos­
terior distribution p(xj~ x; ~
y 1 ; ~
y 2 ) (Fig.3).
If M(` \Lambda ) includes p(xj~ x; ~
y 1 ; ~
y 2 ), p(x; ` \Lambda ) is the true marginalization of p(xj~ x; ~
y 1 ; ~
y 2 ).
However, M(` \Lambda ) does not necessarily include p(xj~ x; ~
y 1 ; ~
y 2 ). This fact means that
p(xj~ x; ~
y 1 ; ~
y 2 ) and p 0 (x; ` \Lambda ) are not necessarily equimarginal, which is the origin of the
decoding error.
3.2 Condition of Stability
The expectation parameters are defined as follows with ' 0 in eq.(4) and ' r in eq.(2)
j 0 (`) def
=
X
x
xp 0 (x; `) = @ ` ' 0 (`); j r (¸) def
=
X
x
xp r (x; ¸) = @ ¸ ' r (¸) r = 1; 2:
Equation (6) is rewritten as follows with these parameters,
j 0 (` \Lambda ) = j 1 (¸ \Lambda
2 ) = j 2 (¸ \Lambda
1 ):
We give a sufficiently small perturbation ffi to ¸ \Lambda
1 and apply one turbo decoding step. The
m--projection from p 2 (x; ¸ \Lambda + ffi) to M 0 gives,
j 0 (` \Lambda + \Delta`) = j 2 (¸ \Lambda
1 + ffi)
\Delta` = G 0 (` \Lambda ) \Gamma1 G 2 (¸ \Lambda
1 )ffi:
Here, G 0 (`) is the Fisher information matrix of p 0 (x; `), and G r (¸) is that of p r (x; ¸),
(r = 1; 2). Note that G 0 (`) is a diagonal matrix. The Fisher information matrix is defined
as follows
G 0 (`) = @ `` 0 ' 0 (`) = @ ` j 0 (`); G r (¸) = @ ¸¸ 0 ' r (¸) = @ ¸ j r (¸); r = 1; 2:
¸ 2 in step 2 will be,
¸ 2 = ¸ \Lambda
2 +
\Gamma G 0 (` \Lambda ) \Gamma1 G 2 (¸ \Lambda
1 ) \Gamma I N
\Delta
ffi:
Here, I N is an identity matrix of size N . Following the same line for step 3, we derive the
theorem which coincides with the result of Richardson[6].

Theorem 3. Let – i be the eigenvalues of the matrix T defined as
T =
\Gamma
G 0 (` \Lambda ) \Gamma1 G 1 (¸ \Lambda
2 ) \Gamma I N
\Delta \Gamma
G 0 (` \Lambda ) \Gamma1 G 2 (¸ \Lambda
1 ) \Gamma I N
\Delta
:
When j– i j ! 1 holds for all i, the equilibrium point is stable.
3.3 Cost Function and Characteristics of Equilibrium
We give the cost function which plays an important role in turbo decoding.
F(¸ 1 ; ¸ 2 ) = ' 0 (`) \Gamma (' 1 (¸ 2 ) + ' 2 (¸ 1 )):
Here, ` = ¸ 1 + ¸ 2 . This function is identical to the ``free energy'' defined in [4].
Theorem 4. The equilibrium state ¸ \Lambda
1 ; : : : ; ¸ \Lambda
K is the critical point of F .
Proof. Direct calculation gives @ ¸1 F = j 0 (`) \Gamma j 2 (¸ 1 ), @ ¸2 F = j 0 (`) \Gamma j 1 (¸ 2 ). For the
equilibrium, j 0 (` \Lambda ) = j 1 (¸ \Lambda
2 ) = j 2 (¸ \Lambda
1 ) holds, and the proof is completed.
When (¸ t+1
r \Gamma ¸ t
r ) is small,
`
¸
t+1
1
¸ t+1
2
'
\Gamma
`
¸ t
1
¸ t
2
'
' \Gamma
` O G 0 (`) \Gamma1
G 0 (`) \Gamma1 O
'` @ ¸1 F
@ ¸2 F
'
:
This shows how the algorithm works, but it does not give the characteristics of the equilib­
rium point. The Hessian of F is
H =
`
@ ¸1 ¸1 F @ ¸1 ¸2 F
@ ¸2 ¸1 F @ ¸2 ¸2 F
'
=
`
G 0 \Gamma G 1 G 0
G 0 G 0 \Gamma G 2
'
:
And by transforming the variables as, ` = ¸ 1 + ¸ 2 and š = ¸ 1 \Gamma ¸ 2 , we have
`
@ `` F @ `š F
@ š` F @ šš F
'
=
1
4
`
4G 0 (`) \Gamma (G 1 +G 2 ) (G 1 \Gamma G 2 )
(G 1 \Gamma G 2 ) \Gamma(G 1 +G 2 )
'
:
Most probably, @ `` F is positive definite but @ šš F is always negative, and F is generally
saddle at equilibrium.
3.4 Perturbation Analysis
For the following discussion, we define a distribution p(x; `; v) as
p(x; `; v) = exp (c 0 (x) + ` \Delta x + v \Delta c(x) \Gamma '(`; v)) ; v = (v 1 ; v 2 ) T ;
'(`; v) = ln
X
x
exp (c 0 (x) + ` \Delta x + v \Delta c(x)) ; c(x)
def
= (c 1 (x); c 2 (x)) T :
This distribution includes p 0 (x; `) (v = 0), p(xj~ x; ~
y 1 ; ~
y 2 ) (` = 0, v = 1), and p r (x; ¸)
(` = ¸, v = e r ), where 1 = (1; 1) T , e 1 = (1; 0) T , and e 2 = (0; 1) T . The expectation
parameter j(`; v) is defined as,
j(`; v) = @ ` '(`; v) =
X
x
xp(x; `; v):
Let us consider M(` \Lambda ), where every distribution p(x; `; v)2M(` \Lambda ) has the same expecta­
tion parameter, that is, j(`; v) = j(` \Lambda ) holds. Here, we define, j(` \Lambda ) = j(` \Lambda ; 0). From
the Taylor expansion, we have,
j i (`; v) = j i (` \Lambda ) +
X
j
@ j j i (` \Lambda )\Delta` j +
X
r
@ r j i (` \Lambda )v r + 1
2
X
r;s
@ r @ s j i (` \Lambda )v r v s
+
X
j;r
@ r @ j j i (` \Lambda )v r \Delta` j +
1
2
X
k;l
@ k @ l j i (` \Lambda )\Delta` k \Delta` l +O(kvk 3 ) +O(k\Delta`k 3 ):
(8)

The indexes fi; j; k; lg are for `, fr; sg are for v, and \Delta` def
=` \Gamma ` \Lambda . After adding some
definitions, that is, j i (`; v) = j i (` \Lambda ), and @ j j i (` \Lambda ) = g ij (` \Lambda ), where fg ij g is the Fisher
information matrix of p(x; ` \Lambda ; 0) which is a diagonal matrix, we substitute \Delta` i with func­
tion of v r up to its 2nd order, and neglect the higher orders of v r . And we have,
\Delta` i '\Gammag ii
X
r
A ir v r \Gamma
g ii
2
X
r;s
i
@ r \Gamma
X
k
g kk A kr @ k
ji
@ s \Gamma
X
j
g jj A js @ j
j
j i (` \Lambda )v r v s ;
(9)
where, g ii = 1=g ii , and A ir = @ v r j i (` \Lambda ).
Let v = e 1 , and since p(x; `; e 1 ) = p 1 (x; `) 2 M(` \Lambda ) holds, ` = ¸ \Lambda
2 and \Delta` =
¸ \Lambda
2 \Gamma ` \Lambda = \Gamma¸ \Lambda
1 . Also when we put v = e 2 , \Delta` = \Gamma¸ \Lambda
2 holds. From eq.(9), we have the
following result,
\Gamma¸ i;\Lambda
r ' \Gamma g ii A ir \Gamma
g ii
2
i
@ r \Gamma
X
k
g kk A kr @ k
ji
@ r \Gamma
X
j
g jj A jr @ j
j
j i (` \Lambda ): (10)
Next, let v = 1, and we consider p(x; ¯
`; 1) 2 M(` \Lambda ), where ¯
` is the parameter which
satisfies this equation. Since p(x; 0; 1) = p(xj~ x; ~
y 1 ; ~
y 2 ) is not necessarily included in
M(` \Lambda ), ¯
` is generally not equal to 0. From eq.(9),
¯ ` i \Gamma ` i;\Lambda ' \Gammag ii
X
r
A ir \Gamma
g ii
2
X
r
i
@ r \Gamma
X
k
g kk A kr @ k
ji
@ r \Gamma
X
j
g jj A jr @ j
j
j i (` \Lambda ):
From the condition ` \Lambda = ¸ i
1 + ¸ i
2 and eq.(10), we have the following approximation,
¯
` i ' \Gamma
g ii
2
X
r 6=s
i
@ r \Gamma
X
k
g kk A kr @ k
ji
@ s \Gamma
X
j
g jj A js @ j
j
j i (` \Lambda ):
This result gives the approximation accuracy of the BP decoding. Let the true belief be
jMPM , and we evaluate the difference between jMPM and j(` \Lambda ) on M 0 . The result is
summarized in the following theorem.
Theorem 5. The true expectation of x, which is jMPM = j(0; 1), is approximated as,
jMPM ' j(` \Lambda ) +
1
2
X
r 6=s
i
@ r \Gamma
X
k
g kk A kr @ k
ji
@ s \Gamma
X
j
g jj A js @ j
j
j(` \Lambda ): (11)
Where j(` \Lambda ) is the solution of the turbo decoding.
Equation (11) is related to the m--embedded--curvature of E(` \Lambda ) (Fig.3). The result can be
extended to general case where K–3 [3, 8].
4 Discussion
We have shown a new framework for understanding and analyzing the belief propagation
decoder.
Since the BN of turbo codes is loopy, we don't have enough theoretical results for BP
algorithm, while a lot of experiments show that it works surprisingly well in such cases.
The mystery of the BP decoders is summarized in 2 points, the approximation accuracy
and the convergence property.
Our results elucidate the mathematical background of the BP decoding algorithm. The
information geometrical structure of the equilibrium is summarized in Theorem 2. It shows

the e--flat submanifold E(` \Lambda ) plays an important role. Furthermore, Theorem 5 shows that
the relation between E(` \Lambda ) and the m--flat submanifold M(` \Lambda ) causes the decoding error,
and the principal component of the error is the curvature of E(` \Lambda ). Since the curvature
strongly depends on the codeword, we can control it by the encoder design. This shows a
room for improvement of the ``near optimum error correcting code''[2].
For the convergent property, we have shown the energy function, which is known as Beth’e
free energy[4, 9]. Unfortunately, the fixed point of the turbo decoding algorithm is gener­
ally a saddle of the function, which makes further analysis difficult. We have only shown a
local stability condition, and the global property is one of our future works.
This paper gives a first step to the information geometrical understanding of the belief
propagation decoder. The main results are for the turbo decoding, but the mechanism is
common with wider class, and the framework is valid for them. We believe further study
in this direction will lead us to better understanding and improvements of these methods.
Acknowledgments
We thank Chiranjib Bhattacharyya who gave us the opportunity to face this problem. We
are also grateful to Yoshiyuki Kabashima and Motohiko Isaka for useful discussions.
References
[1] S. Amari and H. Nagaoka. (2000) Methods of Information Geometry, volume 191 of
Translations of Mathematical Monographs. American Mathematical Society.
[2] C. Berrou and A. Glavieux. (1996) Near optimum error correcting coding and decod­
ing: Turbo­codes. IEEE Transactions on Communications, 44(10):1261--1271.
[3] S. Ikeda, T. Tanaka, and S. Amari. (2001) Information geometry of turbo codes and
low­density parity­check codes. submitted to IEEE transaction on Information Theory.
[4] Y. Kabashima and D. Saad. (2001) The TAP approach to intensive and extensive con­
nectivity systems. In M. Opper and D. Saad, editors, Advanced Mean Field Methods --
Theory and Practice, chapter 6, pages 65--84. The MIT Press.
[5] R. J. McEliece, D. J. C. MacKay, and J.­F. Cheng. (1998) Turbo decoding as an in­
stance of Pearl's ``belief propagation'' algorithm. IEEE Journal on Selected Areas in
Communications, 16(2):140--152.
[6] T. J. Richardson. (2000) The geometry of turbo­decoding dynamics. IEEE Transac­
tions on Information Theory, 46(1):9--23.
[7] T. Tanaka. (2001) Information geometry of mean­field approximation. In M. Opper and
D. Saad, editors, Advanced Mean Field Methods -- Theory and Practice, chapter 17,
pages 259--273. The MIT Press.
[8] T. Tanaka, S. Ikeda, and S. Amari. (2002) Information­geometrical significance of
sparsity in Gallager codes. in T. G. Dietterich et al. (eds.), Advances in Neural Infor­
mation Processing Systems, vol. 14 (this volumn), The MIT Press.
[9] J. S. Yedidia, W. T. Freeman, and Y. Weiss. (2001) Bethe free energy, Kikuchi approx­
imations, and belief propagation algorithms. Technical Report TR2001--16, Mitsubishi
Electric Research Laboratories.

