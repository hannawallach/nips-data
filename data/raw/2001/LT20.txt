Distribution of Mutual Information
Marcus Hutter
IDSIA, Galleria 2, CH-6928 Manno-Lugano, Switzerland
marcus@idsia.ch http://www.idsia.ch/  marcus
Abstract
The mutual information of two random variables { and | with joint
probabilities f ij g is commonly used in learning Bayesian nets as
well as in many other elds. The chances  ij are usually estimated
by the empirical sampling frequency n ij =n leading to a point es-
timate I(n ij =n) for the mutual information. To answer questions
like \is I(n ij =n) consistent with zero?" or \what is the probability
that the true mutual information is much larger than the point es-
timate?" one has to go beyond the point estimate. In the Bayesian
framework one can answer these questions by utilizing a (second
order) prior distribution p() comprising prior information about
. From the prior p() one can compute the posterior p(jn), from
which the distribution p(I jn) of the mutual information can be cal-
culated. We derive reliable and quickly computable approximations
for p(I jn). We concentrate on the mean, variance, skewness, and
kurtosis, and non-informative priors. For the mean we also give an
exact expression. Numerical issues and the range of validity are
discussed.
1 Introduction
The mutual information I (also called cross entropy) is a widely used information
theoretic measure for the stochastic dependency of random variables [CT91, Soo00].
It is used, for instance, in learning Bayesian nets [Bun96, Hec98], where stochasti-
cally dependent nodes shall be connected. The mutual information dened in (1)
can be computed if the joint probabilities f ij g of the two random variables { and |
are known. The standard procedure in the common case of unknown chances  ij is
to use the sample frequency estimates n ij
n instead, as if they were precisely known
probabilities; but this is not always appropriate. Furthermore, the point estimate
I( n ij
n ) gives no clue about the reliability of the value if the sample size n is nite.
For instance, for independent { and |, I() =0 but I( n ij
n ) =O(n 1=2 ) due to noise
in the data. The criterion for judging dependency is how many standard deviations
I( n ij
n ) is away from zero. In [KJ96, Kle99] the probability that the true I() is
greater than a given threshold has been used to construct Bayesian nets. In the
Bayesian framework one can answer these questions by utilizing a (second order)

prior distribution p(),which takes account of any impreciseness about . From the
prior p() one can compute the posterior p(jn), from which the distribution p(I jn)
of the mutual information can be obtained.
The objective of this work is to derive reliable and quickly computable analytical
expressions for p(I jn). Section 2 introduces the mutual information distribution,
Section 3 discusses some results in advance before delving into the derivation. Since
the central limit theorem ensures that p(I jn) converges to a Gaussian distribution
a good starting point is to compute the mean and variance of p(I jn). In section
4 we relate the mean and variance to the covariance structure of p(jn). Most
non-informative priors lead to a Dirichlet posterior. An exact expression for the
mean (Section 6) and approximate expressions for the variance (Sections 5) are
given for the Dirichlet distribution. More accurate estimates of the variance and
higher central moments are derived in Section 7, which lead to good approximations
of p(I jn) even for small sample sizes. We show that the expressions obtained in
[KJ96, Kle99] by heuristic numerical methods are incorrect. Numerical issues and
the range of validity are briey discussed in section 8.
2 Mutual Information Distribution
We consider discrete random variables { 2 f1;:::;rg and | 2 f1;:::;sg and an i.i.d.
random process with samples (i;j)2f1;:::;rgf1;:::;sg drawn with joint probability
 ij . An important measure of the stochastic dependence of { and | is the mutual
information
I() =
r
X
i=1
s
X
j=1
 ij log  ij
 i+ +j
=
X
ij
 ij log ij
X
i
 i+ log i+
X
j
+j log+j : (1)
log denotes the natural logarithm and  i+ =
P
j  ij and +j =
P
i  ij are marginal
probabilities. Often one does not know the probabilities  ij exactly, but one has a
sample set with n ij outcomes of pair (i;j). The frequency ^  ij := n ij
n may be used as
a rst estimate of the unknown probabilities. n:=
P
ij n ij is the total sample size.
This leads to a point (frequency) estimate I(^) =
P
ij
n ij
n log n ij n
n i+n+j
for the mutual
information (per sample).
Unfortunately the point estimation I(^) gives no information about its accuracy.
In the Bayesian approach to this problem one assumes a prior (second order) prob-
ability density p() for the unknown probabilities  ij on the probability simplex.
From this one can compute the posterior distribution p(jn)/p()
Q
ij  n ij
ij (the n ij
are multinomially distributed). This allows to compute the posterior probability
density of the mutual information. 1
p(I jn) =
Z
∆(I() I)p(jn)d rs  (2)
2 The ∆() distribution restricts the integral to  for which I()= I . For large sam-
1
I() denotes the mutual information for the specic chances , whereas I in the context
above is just some non-negative real number. I will also denote the mutual information
random variable in the expectation E[I] and variance Var[I]. Expectaions are always w.r.t.
to the posterior distribution p(jn).
2 Since 0I()Imax with sharp upper bound Imax :=minflogr;logsg, the integral may
be restricted to
R Imax
0
, which shows that the domain of p(Ijn) is [0;Imax ]:

ple size n !1, p(jn) is strongly peaked around  = ^
 and p(I jn) gets strongly
peaked around the frequency estimate I = I(^). The mean E[I ] =
R 1
0
Ip(I jn) dI =
R I()p(jn)d rs  and the variance Var[I ] =E[(I E[I]) 2 ] =E[I 2 ] E[I ] 2 are of cen-
tral interest.
3 Results for I under the Dirichlet P(oste)rior
Most 3 non-informative priors for p() lead to a Dirichlet posterior distribution
p(jn) /
Q
ij  n ij 1
ij with interpretation n ij = n 0
ij +n 00
ij , where n 0
ij are the number
of samples (i;j), and n 00
ij comprises prior information (1 for the uniform prior, 1
2 for
Jereys' prior, 0 for Haldane's prior, 1
rs for Perks' prior [GCSR95]). In principle
this allows to compute the posterior density p(I jn) of the mutual information. In
sections 4 and 5 we expand the mean and variance in terms of n 1 :
E[I ] =
X
ij
n ij
n log n ij n
n i+ n+j + (r 1)(s 1)
2n + O(n 2 ); (3)
Var[I ] = 1
n
X
ij
n ij
n

log n ij n
n i+ n+j
 2 1
n
 X
ij
n ij
n log n ij n
n i+ n+j
 2
+ O(n 2 ):
The rst term for the mean is just the point estimate I(^). The second term is
a small correction if n r s. Kleiter [KJ96, Kle99] determined the correction by
Monte Carlo studies as minf r 1
2n ; s 1
2n g. This is wrong unless s or r are 2. The
expression 2E[I ]=n they determined for the variance has a completely dierent
structure than ours. Note that the mean is lower bounded by const:
n +O(n 2 ), which
is strictly positive for large, but nite sample sizes, even if { and | are statistically
independent and independence is perfectly represented in the data (I(^) = 0). On
the other hand, in this case, the standard deviation =
p
Var(I) 1
n E[I ] correctly
indicates that the mean is still consistent with zero.
Our approximations (3) for the mean and variance are good if rs
n is small. The
central limit theorem ensures that p(I jn) converges to a Gaussian distribution with
mean E[I ] and variance Var[I ]. Since I is non-negative it is more appropriate to
approximate p(I j) as a Gamma (= scaled  2 ) or log-normal distribution with mean
E[I ] and variance Var[I ], which is of course also asymptotically correct.
A systematic expansion in n 1 of the mean, variance, and higher moments is possible
but gets arbitrarily cumbersome. The O(n 2 ) terms for the variance and leading
order terms for the skewness and kurtosis are given in Section 7. For the mean it
is possible to give an exact expression
E[I ] = 1
n
X
ij
n ij [ (n ij + 1)  (n i+ + 1)  (n+j + 1) +  (n + 1)] (4)
with  (n+1)= +
P n
k=1
1
k = logn+O( 1
n ) for integer n. See Section 6 for details
and more general expressions for   for non-integer arguments.
There may be other prior information available which cannot be comprised in a
Dirichlet distribution. In this general case, the mean and variance of I can still be
3 But not all priors which one can argue to be non-informative lead to Dirichlet poste-
riors. Brand [Bra99] (and others), for instance, advocate the entropic prior p()/e H() .

related to the covariance structure of p(jn), which will be done in the following
Section.
4 Approximation of Expectation and Variance of I
In the following let ^  ij :=E[ ij ]. Since p(jn) is strongly peaked around = ^  for
large n we may expand I() around ^
 in the integrals for the mean and the variance.
With  ij := ij ^  ij and using
P
ij  ij =1=
P
ij ^  ij we get for the expansion of (1)
I() = I(^) +
X
ij
log
 ^  ij
^  i+ ^ +j

 ij +
X
ij
 2
ij
2^ ij
X
i
 2
i+
2^ i+
X
j
 2
+j
2^ +j
+O( 3 ): (5)
Taking the expectation, the linear term E[ ij ] =0 drops out. The quadratic terms
E[ ij  kl ] =Cov( ij ; kl ) are the covariance of  under distribution p(jn) and are
proportional to n 1 . It can be shown that E[ 3 ] n 2 (see Section 7).
E[I ] = I(^) + 1
2
X
ijkl
 ∆ ik ∆ jl
^  ij
∆ ik
^
 i+
∆ jl
^
+j

Cov( ij ;  kl ) +O(n 2 ): (6)
The Kronecker delta ∆ ij is 1 for i =j and 0 otherwise. The variance of I in leading
order in n 1 is
Var I() = E[(I E[I]) 2 ] +
= E
2
6 4
0
@ X
ij
log
 ^
 ij
^  i+ ^
+j

 ij
1
A
2
3
7 5 =
=
X
ijkl
log ^
 ij
^
 i+ ^
+j log ^
 kl
^  k+ ^ +l Cov( ij ;  kl ); (7)
where +
= means = up to terms of order n 2 . So the leading order variance and
the leading and next to leading order mean of the mutual information I() can be
expressed in terms of the covariance of  under the posterior distribution p(jn).
5 The Second Order Dirichlet Distribution
Noninformative priors for p() are commonly used if no additional prior information
is available. Many non-informative choices (uniform, Jereys', Haldane's, Perks',
... prior) lead to a Dirichlet posterior distribution:
p(jn) = 1
N(n)
Y
ij
 n ij 1
ij ∆( ++ 1) with normalization
N(n) =
Z Y
ij
 n ij 1
ij ∆( ++ 1)d rs  =
Q
ij (n ij )
(n) ; (8)
where is the Gamma function, and n ij =n 0
ij +n 00
ij , where n 0
ij are the number of
samples (i;j), and n 00
ij comprises prior information (1 for the uniform prior, 1
2 for
Jereys' prior, 0 for Haldane's prior, 1
rs for Perks' prior). Mean and covariance of
p(jn) are
^  ij := E[ ij ] = n ij
n ; Cov( ij ;  kl ) = 1
n + 1 (^ ij ∆ ik ∆ jl ^  ij ^
 kl ) (9)

Inserting this into (6) and (7) we get after some algebra for the mean and variance
of the mutual information I() up to terms of order n 2 :
E[I ] = J + (r 1)(s 1)
2(n + 1) + O(n 2 ); (10)
Var[I ] = 1
n + 1 (K J 2 ) + O(n 2 ); (11)
J :=
X
ij
n ij
n log n ij n
n i+ n+j = I(^); (12)
K :=
X
ij
n ij
n

log n ij n
n i+ n+j
 2
: (13)
J and K (and L, M , P , Q dened later) depend on ^
 ij = n ij
n only, i.e. are O(1)
in n. Strictly speaking we should expand 1
n+1 = 1
n +O(n 2 ), i.e. drop the +1, but
the exact expression (9) for the covariance suggests to keep the +1. We compared
both versions with the exact values (from Monte-Carlo simulations) for various
parameters . In most cases the expansion in 1
n+1 was more accurate, so we suggest
to use this variant.
6 Exact Value for E[I]
It is possible to get an exact expression for the mean mutual information E[I ] under
the Dirichlet distribution. By noting that xlogx = d
d x  j =1 , (x = f ij ; i+ ; +j g),
one can replace the logarithms in the last expression of (1) by powers. From (8) we
see that E[( ij )  ] = (n ij +)(n)
(n ij )(n+) . Taking the derivative and setting =1 we get
E[ ij log  ij ] = d
d E[( ij )  ] =1 = 1
n
X
ij
n ij [ (n ij + 1)  (n + 1)]:
The   function has the following properties (see [AS74] for details)
 (z) = d log (z)
dz
=
0 (z)
(z) ;  (z + 1) = log z + 1
2z
1
12z 2
+O( 1
z 4
);
 (n) =  +
n 1
X
k=1
1
k
;  (n + 1
2 ) =  + 2 log 2 + 2
n
X
k=1
1
2k 1 : (14)
The value of the Euler constant  is irrelevant here, since it cancels out. Since the
marginal distributions of  i+ and +j are also Dirichlet (with parameters n i+ and
n+j ) we get similarly
E[ i+ log  i+ ] = 1
n
X
i
n i+ [ (n i+ + 1)  (n + 1)];
E[+j log +j ] = 1
n
X
j
n+j [ (n+j + 1)  (n + 1)]:
Inserting this into (1) and rearranging terms we get the exact expression 4
E[I ] = 1
n
X
ij
n ij [ (n ij + 1)  (n i+ + 1)  (n+j + 1) +  (n + 1)] (15)
4 This expression has independently been derived in [WW93].

For large sample sizes,  (z+1) logz and (15) approaches the frequency estimate
I(^) as it should be. Inserting the expansion  (z+1)= logz+ 1
2z +::: into (15) we
also get the correction term (r 1)(s 1)
2n of (3).
The presented method (with some renements) may also be used to determine an
exact expression for the variance of I(). All but one term can be expressed in terms
of Gamma functions. The nal result after dierentiating w.r.t.  1 and  2 can be
represented in terms of   and its derivative   0 . The mixed term E[( i+ ) 1 ( +j ) 2 ]
is more complicated and involves conuent hypergeometric functions, which limits
its practical use [WW93].
7 Generalizations
A systematic expansion of all moments of p(I jn) to arbitrary order in n 1 is possible,
but gets soon quite cumbersome. For the mean we already gave an exact expression
(15), so we concentrate here on the variance, skewness and the kurtosis of p(I jn).
The 3 rd and 4 th central moments of  under the Dirichlet distribution are
E[ a  b  c ] = 2
(n + 1)(n + 2) [2^ a ^
 b ^  c ^  a ^
 b ∆ bc ^  b ^  c ∆ ca ^
 c ^
 a ∆ ab + ^  a ∆ ab ∆ bc ]
(16)
E[ a  b  c  d ] = 1
n 2 [3^ a ^
 b ^
 c ^  d ^
 c ^
 d ^
 a ∆ ab ^  b ^  d ^  a ∆ ac ^
 b ^
 c ^  a ∆ ad (17)
^  a ^
 d ^
 b ∆ bc ^
 a ^  c ^  b ∆ bd ^
 a ^
 b ^
 c ∆ cd
+^ a ^
 c ∆ ab ∆ cd + ^
 a ^  b ∆ ac ∆ bd + ^
 a ^  b ∆ ad ∆ bc ] +O(n 3 )
with a=ij, b=kl;:::2f1;:::;rgf1;:::;sg being double indices, ∆ ab =∆ ik ∆ jl ;::: ^
 ij = n ij
n .
Expanding  k = ( ^
) k in E[ a  b :::] leads to expressions containing E[ a  b :::],
which can be computed by a case analysis of all combinations of equal/unequal
indices a;b;c;::: using (8). Many terms cancel leading to the above expressions. They
allow to compute the order n 2 term of the variance of I(). Again, inspection of
(16) suggests to expand in [(n+1)(n+2)] 1 , rather than in n 2 . The variance in
leading and next to leading order is
Var[I ] = K J 2
n + 1 + M + (r 1)(s 1)( 1
2 J) Q
(n + 1)(n + 2) +O(n 3 ) (18)
M :=
X
ij
 1
n ij
1
n i+
1
n+j + 1
n

n ij log n ij n
n i+ n+j ; (19)
Q := 1
X
ij
n 2
ij
n i+ n+j : (20)
J and K are dened in (12) and (13). Note that the rst term K J 2
n+1 also contains
second order terms when expanded in n 1 . The leading order terms for the 3 rd and
4 th central moments of p(I jn) are
E[(I E[I]) 3 ] = 2
n 2 [2J 3 3KJ + L] + 3
n 2 [K + J 2 P ] +O(n 3 );
L :=
X
ij
n ij
n

log n ij n
n i+ n+j
 3
; P :=
X
i
nJ 2
i+
n i+
+
X
j
nJ 2
+j
n+j ;

J i+ :=
X
j
n ij
n
log n ij n
n i+ n+j
; J+j :=
X
i
n ij
n
log n ij n
n i+ n+j
;
E[(I E[I]) 4 ] = 3
n 2 [K J 2 ] 2 +O(n 3 );
from which the skewness and kurtosis can be obtained by dividing by Var[I ] 3=2
and Var[I ] 2 respectively. One can see that the skewness is of order n 1=2 and the
kurtosis is 3+O(n 1 ). Signicant deviation of the skewness from 0 or the kurtosis
from 3 would indicate a non-Gaussian I . They can be used to get an improved
approximation for p(I jn) by making, for instance, an ansatz
p(I jn) / (1 + ~ bI + ~
cI 2 )  p 0 (I j ~
; ~
 2 )
and tting the parameters ~ b, ~ c, ~ , and ~
 2 to the mean, variance, skewness, and
kurtosis expressions above. p 0 is the Normal or Gamma distribution (or any other
distribution with Gaussian limit). From this, quantiles p(I > I  jn) :=
R 1
I p(I jn) dI ,
needed in [KJ96, Kle99], can be computed. A systematic expansion of arbitrarily
high moments to arbitrarily high order in n 1 leads, in principle, to arbitrarily
accurate estimates.
8 Numerics
There are short and fast implementations of  . The code of the Gamma function in
[PFTV92], for instance, can be modied to compute the   function. For integer and
half-integer values one may create a lookup table from (14). The needed quantities
J , K, L, M , and Q (depending on n) involve a double sum, P only a single sum,
and the r+s quantities J i+ and J+j also only a single sum. Hence, the computation
time for the (central) moments is of the same order O(rs) as for the point estimate
(1). \Exact" values have been obtained for representative choices of  ij , r, s,
and n by Monte Carlo simulation. The  ij := x ij =x++ are Dirichlet distributed,
if each x ij follows a Gamma distribution. See [PFTV92] how to sample from a
Gamma distribution. The variance has been expanded in rs
n , so the relative error
Var[I]approx Var[I]exact
Var[I]exact of the approximation (11) and (18) are of the order of rs
n and
( rs
n ) 2 respectively, if { and | are dependent. If they are independent the leading
term (11) drops itself down to order n 2 resulting in a reduced relative accuracy
O( rs
n ) of (18). Comparison with the Monte Carlo values conrmed an accurracy in
the range ( rs
n ) 1:::2 . The mean (4) is exact. Together with the skewness and kurtosis
we have a good description for the distribution of the mutual information p(I jn) for
not too small sample bin sizes n ij . We want to conclude with some notes on useful
accuracy. The hypothetical prior sample sizes n 00
ij =f0; 1
rs ; 1
2 ;1g can all be argued to
be non-informative [GCSR95]. Since the central moments are expansions in n 1 ,
the next to leading order term can be freely adjusted by adjusting n 00
ij 2 [0:::1]. So
one may argue that anything beyond leading order is free to will, and the leading
order terms may be regarded as accurate as we can specify our prior knowledge.
On the other hand, exact expressions have the advantage of being safe against
cancellations. For instance, leading order of E[I ] and E[I 2 ] does not su∆ce to
compute the leading order of Var[I ].

Acknowledgements
I want to thank Ivo Kwee for valuable discussions and Marco Zaalon for encour-
aging me to investigate this topic. This work was supported by SNF grant 2000-
61847.00 to Jurgen Schmidhuber.
References
[AS74] M. Abramowitz and I. A. Stegun, editors. Handbook of mathematical functions.
Dover publications, inc., 1974.
[Bra99] M. Brand. Structure learning in conditional probability models via an entropic
prior and parameter extinction. Neural Computation, 11(5):1155{1182, 1999.
[Bun96] W. Buntine. A guide to the literature on learning probabilistic networks from
data. IEEE Transactions on Knowledge and Data Engineering, 8:195{210,
1996.
[CT91] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley Series
in Telecommunications. John Wiley & Sons, New York, NY, USA, 1991.
[GCSR95] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data Analysis.
Chapman, 1995.
[Hec98] D. Heckerman. A tutorial on learning with Bayesian networks. Learnig in
Graphical Models, pages 301{354, 1998.
[KJ96] G. D. Kleiter and R. Jirousek. Learning Bayesian networks under the control
of mutual information. Proceedings of the 6th International Conference on
Information Processing and Management of Uncertainty in Knowledge-Based
Systems (IPMU-1996), pages 985{990, 1996.
[Kle99] G. D. Kleiter. The posterior probability of Bayes nets with strong dependences.
Soft Computing, 3:162{173, 1999.
[PFTV92] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical
Recipes in C: The Art of Scientic Computing. Cambridge University Press,
Cambridge, second edition, 1992.
[Soo00] E. S. Soo. Principal information theoretic approaches. Journal of the Ameri-
can Statistical Association, 95:1349{1353, 2000.
[WW93] D. R. Wolf and D. H. Wolpert. Estimating functions of distributions from A
nite set of samples, part 2: Bayes estimators for mutual information, chi-
squared, covariance and other statistics. Technical Report LANL-LA-UR-93-
833, Los Alamos National Laboratory, 1993. Also Santa Fe Insitute report
SFI-TR-93-07-047.

