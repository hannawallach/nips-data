Asymptotic Universality for Learning
Curves of Support Vector Machines
M. Opper 1 R. Urbanczik 2
1 Neural Computing Research Group
School of Engineering and Applied Science
Aston University, Birmingham B4 7ET, UK.
opperm@aston.ac.uk
2 Institut Fur Theoretische Physik,
Universitat Wurzburg Am Hubland, D-97074 Wurzburg, Germany
robert@physik.uni-wuerzburg.de.
Abstract
Using methods of Statistical Physics, we investigate the r^ole
of model complexity in learning with support vector machines
(SVMs). We show the advantages of using SVMs with kernels
of innite complexity on noisy target rules, which, in contrast to
common theoretical beliefs, are found to achieve optimal general-
ization error although the training error does not converge to the
generalization error. Moreover, we nd a universal asymptotics of
the learning curves which only depend on the target rule but not
on the SVM kernel.
1 Introduction
Powerful systems for data inference, like neural networks implement complex input-
output relations by learning from example data. The price one has to pay for the
exibility of these models is the need to choose the proper model complexity for a
given task, i.e. the system architecture which gives good generalization ability for
novel data. This has become an important problem also for support vector machines
[1]. The main advantage of SVMs is that the learning task is a convex optimization
problem which can be reliably solved even when the example data require the tting
of a very complicated function. A common argument in computational learning
theory suggests that it is dangerous to utilize the full exibility of the SVM to learn
the training data perfectly when these contain an amount of noise. By tting more
and more noisy data, the machine may implement a rapidly oscillating function
rather than the smooth mapping which characterizes most practical learning tasks.
Its prediction ability could be no better than random guessing in that case. Hence,
modications of SVM training [2] that allow for training errors were suggested to be
necessary for realistic noisy scenarios. This has the drawback of introducing extra
model parameters and spoils much of the original elegance of SVMs.

Surprisingly, the results of this paper show that the picture is rather dierent in
the important case of high dimensional data spaces. Using methods of Statistical
Physics, we show that asymptotically, the SVM achieves optimal generalization
ability for noisy data already for zero training error. Moreover, the asymptotic rate
of decay of the generalization error is universal, i.e. independent of the kernel that
denes the SVM. These results have been published previously only in a physics
journal [3].
As is well known, SVMs classify inputs y using a nonlinear mapping into a feature
vector 	(y) which is an element of a Hilbert space. Based on a training set of m
inputs x  and their desired classications   , SVMs construct the maximum margin
hyperplane P in the feature space. P can be expressed as a linear combination of
the feature vectors 	(x  ), and to classify an input y, that is to decide on which side
of P the image 	(y) lies, one basically has to evaluate inner products 	(x  )  	(y).
For carefully chosen mappings 	 and Hilbert spaces, inner products 	(x)  	(y)
can be evaluated e∆ciently using a kernel function k(x; y) = 	(x)  	(y), without
having to individually calculate the feature vectors 	(x) and 	(y). In this manner
it becomes computationally feasible to use very high and even innite dimensional
feature vectors.
This raises the intriguing question whether the use of a very high dimensional
feature space may typically be helpful. So far, recent results [4, 5] obtained by
using methods of Statistical Mechanics (which are naturally well suited for analysing
stochastic models in high dimensional spaces), have been largely negative in this
respect. They suggest (as one might perhaps expect) that it is rather important to
match the complexity of the kernel to the target rule. The analysis in [4] considers
the case of N-dimensional inputs with binary components and assumes that the
target rule giving the correct classication  of an input x is obtained as the sign
of a function t(x) which is polynomial in the input components and of degree L.
The SVM uses a kernel which is a polynomial of the inner product x  y in input
space of degree K  L, and the feature space dimension is thus O(N K ). In this
scenario it is shown, under mild regularity condition on the kernel and for large N ,
that the SVM generalizes well when the number of training examples m is on the
order of N L . So the scale of the learning curve is determined by the complexity
of the target rule and not by the kernel. However, considering the rate with which
the generalization error approaches zero one nds the optimal N L =m decay only
when K is equal to L and the convergence is substantially slower when K > L. So
it is important to match the complexity of the kernel to the target rule and using
a large value of K is only justied if L is assumed large and if one can use on the
order of N L examples for training.
In this paper we show that the situation is very dierent when one considers the
arguably more realistic scenario of a target rule corrupted by noise. Now one can no
longer use K = L since no separating hyperplane P will exist when m is su∆ciently
large compared to N L . However when K > L, this plane will exist and we will show
that it achieves optimal generalization performance in the limit that N L =m is small.
Remarkably, the asymptotic rate of decay of the generalization error is independent
of the kernel in this case and a general characterization of the asymptote in terms of
properties of the target rule is possible. In a second step we show that under mild
regularity conditions these ndings also hold when k(x; y) is an arbitrary function
of x  y or when the kernel is a function of the Euclidean distance jx yj. The latter
type of kernels is widely used in practical applications of SVMs.

2 Learning with Noise: Polynomial Kernels
We begin by assuming a polynomial kernel k(x; y) = f(x  y) where f(z) =
P K
k=0 c k z k is of degree K. Denoting by  a multi-index  = ( 1 ; : : : ; N ) with  i 2
N 0 , we set x  =
p
jj!
Q N
i=1
x  i
i
p  i !
and the degree of x  is jj =
P N
i=1  i . The kernel
can then be described by features 	  (x) = p c jj x  since k(x; y) =
P
 	  (x)	  (y),
where the summation runs over all multi-indices of degree up to K. To assure that
the features are real, we assume that the coe∆cients c k in the kernel are nonneg-
ative. A hyperplane in feature space is parameterized by a weight vector w with
components w  , and if 0 <   w  	(x  ), a point (x  ;   ) of the training set lies on
the correct side of the plane. To express that the plane P has maximal distance to
the points of the training set, we choose an arbitrary positive stability parameter 
and require that the weight vector w  of P minimize w  w subject to the constraints
 <   w  	(x  ), for  = 1; : : : ; m.
2.1 The Statistical Mechanics Formulation
Statistical Mechanics allows to analyze a variety of learning scenarios exactly in the
\thermodynamic limit" of high input dimensionality, when the data distributions
are simple enough. In this approach, one computes a partition function which
serves as a generating function for important averages such as the generalization
error. To dene the partition problem for SVMs one rst analyzes a soft version of
the optimization problem characterized by an inverse temperature . One considers
the partition function
Z =
Z
dw e 1
2 ww
m
Y
=1
(  w  	(x  ) ); (1)
where the SVM constraints are enforced strictly using the Heaviside step function
. Properties of w  can be computed from ln Z and taking the limit  !1.
To model the training data, we assume that the random and independent input
components have zero mean and variance 1=N . This scaling assures that the vari-
ance of w  	(x  ) stays nite in the large N limit. For the target rule we assume
that its deterministic part is given by the polynomial t(x) =
P

p c jj B  x  with
real parameters B  . The magnitude of the contribution of each degree k to the
value of t(x) is measured by the quantities
T k = c k
1
N k
X
;jj=k
B 2
 (2)
where N k = ( N+k 1
k ) is the number of terms in the sum. The degree of t(x) is L
and lower than K, so TL > 0 and TL+1 = : : : = TK = 0. Note, that this denition
of t(x) ensures that any feature necessary for computing t(x) is available to the
SVM. For brevity we assume that the constant term in t(x) vanishes (T 0 = 0) and
the normalization is
P
k T k = 1.
2.2 The Noise Model
In the deterministic case the label of a point x would simply be the sign of t(x).
Here we consider a nondeterministic rule and the output label is obtained using a
random variable  u 2 f1; 1g parameterized by a scalar u. The observable instances
of the rule, and in particular the elements of the training set, are then obtained by

independently sampling the random variable (x;  t(x) ). Simple examples are additive
noise,  t(x) = sgn(t(x) + ), or multiplicative noise,  t(x) = sgn(t(x)), where  is
a noise term independent of x. In general, we will assume that the noise does not
systematically corrupt the deterministic component, formally
1 > Prob ( u = sgn(u)) >
1
2 for all u: (3)
So sgn(t(x)) is the best possible prediction of the output label of x, and the minimal
achievable generalization error is  min = h( t(x) t(x) )i x . In the limit of many
input dimensions N , a central limit argument yields that for a typical target rule
 min = 2h( u) ^
(u)i u , where u is zero mean and unit variance Gaussian. The
function ^
 will play a considerable r^ole in the sequel. It is a symmetrized form of
the probability p(u) that  u is equal to 1, ^
(u) = 1
2 (p(u) + 1 p( u)).
2.3 Order Parameter Equations
One now evaluates the average of ln Z (Eq. 1) over random drawings of training
data for large N in terms of the order parameters
Q
=
 (w  	(x)) 2

x

w ; q =
D
(hwi w  	(x)) 2
E
x
and
r = Q 1
2 hhw  	(x)i w B  	(x)i x : (4)
Here the thermal average over w refers to the Gibbs distribution (1). For the large
N limit, a scaling of the training set size m must be specied, for which we make
the generic Ansatz m = N l , where l = 1; : : : ; L. Focusing on the limit of large ,
where the density on the weight vectors converges to a delta peak and q ! Q, we
introduce the rescaled order parameter  = (Q q)=S l , with
S l = f(1)
l
X
i=0
c i : (5)
Note that this scaling with S l is only possible since the degree K of the kernel
f(x  y) is greater than l, and thus S l 6= 0. Finally, we obtain an expression for f l =
lim !1 lim N!1 hhln Zii S l =(N l ), where the double brackets denote averaging over
all training sets of size m. The value of f l results from extremizing, with respect to
r; q and , the function
f l (r; q; ) =
q


^
( u)G

ru +
p
1 r 2 v

p q

u;v
q
2
 S l
c l
1
 1
  
1 r 2
( 1)T l S l =c l +
P l
i=1 T i
!
(6)
where G(z) = (z)z 2 , and u; v are independent Gaussian random variables with
zero mean and unit variance.
Since the stationary value of f l is nite, hhw   w  ii is of the order N l . So the
higher order components of w  are small, (w 
 ) 2  1 for jj > l; although these
components play a crucial r^ole in ensuring that a hyperplane separating the training
points exists even for large . But the key quantity obtained from Eq. (6) is the
stationary value of r which determines the generalization error of the SVM via
 g = h ^
( u)(ru +
p
1 r 2 v)i u;v , and in particular  g =  min for r = 1.

2.4 Universal Asymptotics
We now specialize to the case that l equals L, the degree of the polynomial t(x) in
the target rule. So m = NL and for large , after some algebra, Eq. (6) yields
r = 1 A(q  )
4B(q  ) 2
1

(7)
where B(q) =
D
^
(y) y + = p q)
 E
y
and A(q) =
D
^ (y) y + = p q

y + = p q
 2
E
y
. Further q  = arg min q qA(q), and con-
sidering the derivatives of qA(q) for q ! 0 and q ! 1, one may show that
condition (3) assures that qA(q) does have a minimum.
Equation (7) shows that optimal generalization performance is achieved on this scale
in the limit of large . Remarkably, as long as K > L, the asymptote is invariant
to the choice of the kernel since A(q) and B(q) are dened solely in terms of the
target rule.
3 Extension to other Kernels
Our next goal is to understand cases where the kernel is a general function of the
inner product or of the distance between the vectors. We still assume that the
target rule is of nite complexity, i.e. dened by a polynomial and corrupted by
noise and that the number of examples is polynomial in N . Remarkably, the more
general kernels then reduce to the polynomial case in the thermodynamic limit.
Since it is di∆cult to nd a description of the Hilbert space for k(x; y) which is useful
for a Statistical Physics calculation, our starting point is the dual representation:
The weight vector w  dening the maximal margin hyperplane P can be written
as a linear combination of the feature vectors 	(x  ) and hence w   	(y) = (y),
where
(y) =
m
X
=1
   k(x  ; y): (8)
By standard results of convex optimization theory the   are uniquely dened by the
Kuhn-Tucker conditions   0,   (x  )   (for all patterns), further requiring
that for positive  the second of the two inequalities holds as an equality. One also
nds that w   w  =
P m
=1  and for a polynomial kernel we thus obtain a bound
on
P m
=1  since w   w  is O(m).
We rst consider kernels (x  y), with a general continuous function  of the inner
product, and assume that  can be approximated by a polynomial f in the sense
that (1) = f(1) and (z) f(z) = O(z K ) for z ! 0. Now, with a probability
approaching 1 with increasing N , the magnitude of x  x  is smaller than, say, N 1=3
for all dierent indices  and  as long as m is polynomial in N . So, considering Eq.
(8), for large N the functions (z) and f(z) will only be evaluated in a small region
around zero and at z = 1 when used as kernels of a SVM trained on m = NL
examples. Using the fact that
P m
=1   = O(m) we conclude that for large N and
K > 3L the solution of the Kuhn-Tucker conditions for f converges to the one for
. So Eqs. (6,7) can be used to calculate the generalization error for  by setting
 l =  (l) (0)=l! for l = 1; : : : ; L, when  is an analytic function. Note that results in
[4] assure that  l  0 if the kernel (x  y) is positive denite for all input dimensions
N . Further, the same reduction to the polynomial case will hold in many instances
where  is not analytical but just su∆ciently smooth close to 0.

3.1 RBF Kernels
We next turn to radial basis function (RBF) kernels where k(x; y) depends only
on the Euclidean distance between two inputs, k(x; y) = (jx yj 2 ). For binary
input components (x i = N 1=2 ) this is just the inner product case since (jx
yj 2 ) = (2 2x  y). However, for more general input distributions, e.g. Gaussian
input components, the uctuations of jxj around its mean value 1 have the same
magnitude as x  y even for large N , and an equivalence with inner product kernels
is not evident.
Our starting point is the observation that any kernel (jx yj 2 ) which is positive
denite for all input dimensions N is a positive mixture of Gaussians [6]. More
precisely (z) =
R 1
0 e k 2 z da(k) where the transform a(k) is nondecreasing. For
the special case of a single Gaussian one easily obtains features 	  by rewriting
(jx yj 2 ) = e jx yj 2 =2 = e jxj 2 =2 e xy e jyj 2 =2 . Expanding the kernel e xy into
polynomial features, yields the features 	  (x) = e jxj 2 =2 x  =
p
jj! for (jx yj 2 ).
But, for a generic weight vector w in feature space,
w  	(x) =
X

w  	  (x) = e 1
2
jxj 2
X

w 
x 
p
jj!
(9)
is of order 1, and thus for large N the uctuations of jxj can be neglected.
This line of argument can be extended to the case that the kernel is a nite mixture
of Gaussians, (z) =
P n
i=1 a i e  2
i
z=2 with positive coe∆cients a i . Applying the
reasoning for a single Gaussian to each term in the sum, one obtains a doubly
indexed feature vector with components 	 ;i (x) = e  2
i jxj 2 =2 (a i  2jj
i =jj!) 1=2 x  . It
is then straightforward to adapt the calculation of the partition function (Eq. 1 -
6) to the doubly indexed features, showing that the kernel (jx yj 2 ) yields the
same generalization behavior as the inner product kernel (2 2x  y). Based on the
calculation, we expect the same equivalence to hold for general radial basis function
kernels, i.e. an innite mixture of Gaussians, even if it would be involved to prove
that the limit of many Gaussians commutes with the large N limit.
4 Simulations
To illustrate the general results we rst consider a scenario where a linear target rule,
corrupted by additive Gaussian noise, is learned using dierent transcendental RBF
kernels (Fig. 1). While Eq. (7) predicts that the asymptote of the generalization
error does not depend on the kernel, remarkably, the dependence on the kernel
is very weak for all values of . In contrast, the generalization error depends
substantially on the nature of the noise process. Figure 2 shows the nding for
a quadratic rule with additive noise for the cases that the noise is Gaussian and
binary. In the Gaussian case a 1= decay of  g to  min is found, whereas for binary
noise the decay is exponential in . Note that in both cases the order parameter r
approaches 1 as 1=.
5 Summary
The general characterization of learning curves obtained in this paper demonstrates
that support vector machines with high order or even transcendental kernels have
denitive advantages when the training data is noisy. Further the calculations lead-
ing to Eq. (6) show that maximizing the margin is an essential ingredient of the

approach: If one just chooses any hyperplane which classies the training data cor-
rectly, the scale of the learning curve is not determined by the target rule and far
more examples are needed to achieve good generalization. Nevertheless our ndings
are at odds with many of the current theoretical motivations for maximizing the
margin which argue that this minimizes the eective Vapnik Chervonenkis dimen-
sion of the classier and thus ensures fast convergence of the training error to the
generalization error [1, 2]. Since we have considered hard margins, in contrast to the
generalization error, the training error is zero, and we nd no convergence between
the two quantities. But close to optimal generalization is achieved since maximizing
the margin biases the SVM to explain as much as possible of the data in terms of a
low order polynomial. While the Statistical Physics approach used in this paper is
only exactly valid in the thermodynamic limit, the numerical simulations indicate
that the theory is already a good approximation for a realistic number of input
dimensions.
We thank Rainer Dietrich for useful discussion and for giving us his code for the sim-
ulations. The work of M.O. was supported by the EPSRC (grant no. GR/M81601)
and the British Council (ARC project 1037); R.U. was supported by the DFG and
the DAAD.
References
[1] C. Cortes and V. Vapnik. , Machine Learning, 20:273-297, 1995.
[2] N. Cristianini and J. Shawe-Taylor. Support Vector Machines. Cambridge Uni-
versity Press, 2000.
[3] M. Opper and R. Urbanczik. Phys. Rev. Lett., 86:4410{4413, 2001.
[4] R. Dietrich, M. Opper, and H. Sompolinsky. Phys. Rev. Lett., 82:2975 {2978,
1999.
[5] S. Risau-Gusman and M. Gordon. Phys. Rev. E, 62:7092{7099, 2000.
[6] I. Schoenberg. Anal. Math, 39:811{841, 1938.

0 5 10 15 20
0.1
0.2
0.3
 g
 min
 = P=N
(A) 2
(B) 4
(C) 3
(D) 
(E)
Figure 1: Linear target rule corrupted by additive Gaussian noise  (hi =
0,
  2
 =
1=16) and learned using dierent kernels. The curves are the theoretical prediction;
symbols show simulation results for N = 600 with Gaussian inputs and error bars
are approximately the size of the symbols. (A) Gaussian kernel, (z) = e kz with
k = 2=3. (B) Wiener kernel given by the nonanalytic function (z) = e c p z . We
chose c  0:065 so that the theoretical prediction for this case coincides with (A).
(C) Gaussian kernel with k = 1=20, the inuence of the parameter change on the
learning curve is minimal. (D) Perceptron, (z) = z. Above  c  7:5 vanishing
training error cannot be achieved and the SVM is undened. (E) Kernel invariant
asymptote for (A,B,C).
0 2 4 6 8
0
0.1
0.2
0.3
0.4
0.5
1 10 19
0.01
0.1
0.19
 g  min

 g
 min
 = P=N 2
Figure 2: A noisy quadratic rule (T 1 = 0; T 2 = 1) learned using the Gaussian kernel
with k = 1=20. The upper curve (simulations ) is for additive Gaussian noise as
in Fig. 1. The lower curve (simulations ) is for binary noise,   s with equal
probability. We chose s  0:20 so that the value of  min is the same for the two
noise processes. The inset shows that  g decays as 1= for Gaussian noise, whereas
an exponential decay is found in the binary case. The dashed curves are the kernel
invariant asymptotes. The simulations are for N = 90 with Gaussian inputs, and
standard errors are approximately the size of the symbols.

