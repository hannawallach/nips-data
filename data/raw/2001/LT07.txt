On the Generalization Ability
of On­line Learning Algorithms
Nicol‘ o Cesa­Bianchi
DTI, University of Milan
via Bramante 65
26013 Crema, Italy
cesa­bianchi@dti.unimi.it
Alex Conconi
DTI, University of Milan
via Bramante 65
26013 Crema, Italy
conconi@dti.unimi.it
Claudio Gentile
DSI, University of Milan
via Comelico 39
20135 Milano, Italy
gentile@dsi.unimi.it
Abstract
In this paper we show that on­line algorithms for classification and re­
gression can be naturally used to obtain hypotheses with good data­
dependent tail bounds on their risk. Our results are proven without re­
quiring complicated concentration­of­measure arguments and they hold
for arbitrary on­line learning algorithms. Furthermore, when applied to
concrete on­line algorithms, our results yield tail bounds that in many
cases are comparable or better than the best known bounds.
1 Introduction
One of the main contributions of the recent statistical theories for regression and classi­
fication problems [21, 19] is the derivation of functionals of certain empirical quantities
(such as the sample error or the sample margin) that provide uniform risk bounds for all the
hypotheses in a certain class. This approach has some known weak points. First, obtaining
tight uniform risk bounds in terms of meaningful empirical quantities is generally a dif­
ficult task. Second, searching for the hypothesis minimizing a given empirical functional
is often computationally expensive and, furthermore, the minimizing algorithm is seldom
incremental (if new data is added to the training set then the algorithm needs be run again
from scratch).
On­line learning algorithms, such as the Perceptron algorithm [17], the Winnow algo­
rithm [14], and their many variants [16, 6, 13, 10, 2, 9], are general methods for solving
classification and regression problems that can be used in a fully incremental fashion. That
is, they need (in most cases) a short time to process each new training example and adjust
their current hypothesis. While the behavior of these algorithms is well understood in the
so­called mistake bound model [14], where no assumptions are made on the way the train­
ing sequence is generated, there are fewer results concerning how to use these algorithms
to obtain hypotheses with small statistical risk.
Littlestone [15] proposed a method for obtaining small risk hypotheses from a run of an
arbitrary on­line algorithm by using a cross validation set to test each one of the hypotheses
generated during the run. This method does not require any convergence property of the on­
line algorithm and provides risk tail bounds that are sharper than those obtainable choosing,
for instance, the hypothesis in the run that survived the longest. Helmbold, Warmuth,

and others [11, 6, 8] showed that, without using any cross­validation sets, one can obtain
expected risk bounds (as opposed to the more informative tail bounds) for a hypothesis
randomly drawn among those generated during the run.
In this paper we prove, via refinements and extensions of the previous analyses, that on­
line algorithms naturally lead to good data­dependent tail bounds without employing the
complicated concentration­of­measure machinery needed by other frameworks [19]. In
particular we show how to obtain, from an arbitrary on­line algorithm, hypotheses whose
risk is close to m=t with high probability (Theorems 2 and 3), where t is the amount of
training data and m is a data­dependent quantity measuring the cumulative loss of the on­
line algorithm on the actual training data. When applied to concrete algorithms, the loss
bound m translates into a function of meaningful data­dependent quantities. For classifi­
cation problems, the mistake bound for the p­norm Perceptron algorithm yields a tail risk
bound in terms of the empirical distribution of the margins --- see (4). For regression prob­
lems, the square loss bound for ridge regression yields a tail risk bound in terms of the
eigenvalues of the Gram matrix --- see (5).
2 Preliminaries and notation
Let X ; Y be arbitrary sets and Z = X  Y . An example is a pair (x; y), where x is an
instance belonging to X and y 2 Y is the label associated with x. Random variables will
be denoted in upper case and their realizations will be in lower case. We let Z be the
pair of random variables (X; Y ), where X and Y take values in X and Y , respectively.
Throughout the paper, we assume that data are generated i.i.d. according to an unknown
probability distribution over Z . All probabilities and expectations will be understood with
respect to this underlying distribution. We use the short­hand Z t to denote the vector­
valued random variable (Z 1 ; : : : ; Z t ).
A hypothesis h is any (measurable) mapping from instances x 2 X to predictions h(x) 2
D, where D is a given decision space. The risk of h is defined by er(h) = E [`(h(X); Y )],
where ` : D  Y ! R is a nonnegative loss function. Unless otherwise specified, we will
assume that ` takes values in [0; L] for some known 0 < L < 1. The on­line algorithms we
investigate are defined within a well­known mathematical model, which is a generalization
of a learning model introduced by Littlestone [14] and Angluin [1]. Let a training sequence
z t = ((x 1 ; y 1 ); : : : ; (x t ; y t )) 2 (X  Y) t be fixed. In this learning model, an on­line
algorithm processes the examples in z t one at a time in trials, generating a sequence of
hypotheses h 0 ; h 1 ; : : : ; h t . At the beginning of the i­th trial, the algorithm receives the
instance x i and uses its current hypothesis h i 1 to compute a prediction h i 1 (x i ) 2 D
for the label y i associated with x i . Then, the true value of the label y i is disclosed and
the algorithm suffers a loss `(h i 1 (x i ); y i ), measuring how bad is the prediction h i 1 (x i )
for the label y i . Before the next trial begins, the algorithm generates a new hypothesis h i
which may or may not be equal to h i 1 . We measure the algorithm's performance on z t by
its cumulative loss
m(z t
) =
t
X
i=1
`(h i 1 (x i ); y i ):
In our analysis, we will write M and H 0 ; : : : ; H t when we want to stress the fact that the
cumulative loss and the hypotheses of the on­line algorithm are functions of the random
sample Z t . In particular, throughout the paper H 0 will denote the (deterministic) initial
hypothesis of an arbitrary on­line algorithm and, for each 1  i  t, H i will be a random
variable denoting the i­th hypothesis of the on­line algorithm and such that the value of
H i (Z 1 ; : : : ; Z t ) does not change upon changes in the values of Z i+1 ; : : : ; Z t .
Our goal is to relate the risk of the hypotheses produced by an on­line algorithm running
on an i.i.d. sequence Z t to the cumulative loss M(Z t ) of the algorithm on that sequence.

The cumulative loss M(Z t ) will be our key empirical (data­dependent) quantity. Via our
analysis we will obtain bounds of the form
P
 
er(f(H 0 ; : : : ; H t ))  M(Z t )
t + c
r
1
t ln
1
Æ
!
 Æ ;
where f(H 0 ; : : : ; H t ) is a specific function of the sequence of hypotheses H 0 ; : : : ; H t
produced by the algorithm, and c is a suitable positive constant. We will see that for spe­
cific on­line algorithms the ratio M(Z t )=t can be further bounded in terms of meaningful
empirical quantities.
Our method centers on the following simple concentration lemma about bounded losses.
Lemma 1 Let ` be an arbitrary bounded loss satisfying 0  `  L. Let an arbitrary
on­line algorithm output (not necessarily distinct) hypotheses H 0 ; : : : ; H t when it is run
on Z t . Then for any 0 < Æ  1 we have
P
1
t
t
X
i=1
er(H i 1 )  M
t + L
r
2
t ln
1
Æ
!
 Æ :
Proof. For each i = 1; : : : ; t, set V i 1 = er(H i 1 ) ` (H i 1 (X i ); Y i ). We have
1
t
t
X
i=1
V i 1 =
1
t
t
X
i=1
er(H i 1 )
M
t
:
Furthermore, L  V i 1  L, since ` takes values in [0; L]. Also,
E [V i 1 j F i 1 ] = er(h i 1 ) E [` (h i 1 (X i ); Y i ) j F i 1 ] ; = 0
where F i 1 denotes the ­algebra generated by Z 1 ; : : : ; Z i 1 . A direct application of the
Hoeffding­Azuma inequality [3] to the bounded random variables V 0 ; : : : ; V t 1 proves the
lemma. 2
3 Concentration for convex losses
In this section we investigate the risk of the average hypothesis
h def
= 1
t
t
X
i=1
h i 1 ;
where h 0 ; h 1 ; : : : ; h t are the hypotheses generated by some on­line algorithm run on t
training examples. 1 The average hypothesis generates valid predictions whenever the deci­
sion space D is convex.
Theorem 2 Let D be convex and ` : DY ! [0; L] be convex in the first argument. Let an
arbitrary on­line algorithm for ` output (not necessarily distinct) hypotheses H 0 ; : : : ; H t
when the algorithm is run on Z t . Then for any 0 < Æ < 1 the following holds
P er(H)  M
t + L
r
2
t ln
1
Æ
!
 Æ :
1 Notice that the last hypothesis h t
is not used in this average.

Proof. Since ` is convex in the first argument, by Jensen's inequality we have ` h(x); y
 
1
t
P t
i=1
` (h i 1 (x); y) : Taking expectation with respect to (X; Y ) yields er(h) 
1
t
P t
i=1
er(h i 1 ). Using the last inequality along with Lemma 1 yields the thesis. 2
This theorem, which can be viewed as the tail bound version of the expected bound in [11],
implies that the risk of the average hypothesis is close to m(z t )=t for ``most'' samples
z t . On the other hand, note that it is unlikely that
P t
i=1 er(H i 1 )=t concentrates around
E [M ]=t, at least without taking strong assumptions on the underlying on­line algorithm.
An application of Theorem 2 will be shown is Section 5. Here we just note that by applying
this theorem to the Weighted Majority algorithm [16], we can prove a version of [5, The­
orem 4] for the absolute loss without resorting to sophisticated concentration inequalities
(details in the full paper).
4 Penalized risk estimation for general losses
If the loss function ` is nonconvex (such as the 0­1 loss) then the risk of the average hy­
pothesis cannot be bounded in the way shown in the previous section. However, the risk
of the best hypothesis, among those generated by the on­line algorithm, cannot be higher
than the average risk of the same hypotheses. Hence, Lemma 1 immediately tells us that,
under no conditions on the loss function other than boundedness, for most samples z t at
least one of the hypotheses generated has risk close to m(z t )=t. In this section we give
a technique (Lemma 4) that, using a penalized risk estimate, finds with high probability
such a hypothesis. The argument used is a refinement of Littlestone's method [15]. Unlike
Littlestone's, our technique does not require a cross validation set. Therefore we are able
to obtain bounds on the risk whose main term is m(z t )=t, where t is the size of the whole
set of examples available to the learning algorithm (i.e., training set plus validation set in
Littlestone's paper). Similar observations are made in [4], though the analysis there does
actually refer only to randomized hypotheses with 0­1 loss (namely, to absolute loss).
Let us define the penalized risk estimate of hypothesis h i by
m i
t i + c Æ (t i) ;
where t i is the length of the suffix z i+1 ; : : : ; z t of the training sequence that the on­line
algorithm had not seen yet when h i was generated, m i is the cumulative loss of h i on that
suffix, and
c Æ (x) =
r
1
2 x ln
t(t + 1)
Æ :
Our algorithm chooses the hypothesis b h = h i  , where
i  = argmin
0it 1
 m i
t i + c Æ (t i)

:
For the sake of simplicity, we will restrict to losses ` with range [0; 1]. However, it should
be clear that losses taking values in arbitrary bounded real interval can be handled using
techniques similar to those shown in Section 3. We prove the following result.
Theorem 3 Let an arbitrary on­line algorithm output (not necessarily distinct) hypotheses
H 0 ; : : : ; H t when it is run on Z t . Then, for any 0 < Æ  1, the hypothesis b
H chosen using
the penalized risk estimate based on c Æ=2 satisfies
P er( b
H) > M
t + 5
r
1
t ln
2(t + 1)
Æ
!
 Æ :

The proof of this theorem is based on the two following technical lemmas.
Lemma 4 Let an arbitrary on­line algorithm output (not necessarily distinct) hypotheses
H 0 ; : : : ; H t when it is run on Z t . Then for any 0 < Æ < 1 the following holds:
P

er( b
H) > min
0it 1
(er(H i ) + 2 c Æ (t i))

 Æ :
Proof. Let I  = argmin 0it 1 (er(H i ) + 2 c Æ (t i)) : Let further H  = H I  , M  =
M I  , and set for brevity
b e i =
M i
t i ;
b e  =
M 
t I  :
For any fixed " > 0 we have
P er( b
H) > er(H  ) + "


t 1
X
i=0
P (be i + c Æ (t i)  b e  + c Æ (t I  ) ; er(H i ) > er(H  ) + ") : (1)
Now, if
b e i + c Æ (t i)  b e  + c Æ (t I  )
holds then either
b e i  er(H i ) c Æ (t i)
or
b e  > er(H  ) + c Æ (t I  )
or
er(H i ) er(H  ) < 2 c Æ (t I  )
hold. Hence for any fixed i we can write
P b
e i + c Æ (t i)  be  + c Æ (t I  ) ; er(H i ) > er(H  ) + "

 P (be i  er(H i ) c Æ (t i) ; er(H i ) > er(H  ) + ")
+ P (be  > er(H  ) + c Æ (t I  ) ; er(H i ) > er(H  ) + ")
+ P (er(H i ) er(H  ) < 2 c Æ (t I  ) ; er(H i ) > er(H  ) + ")
 P (be i  er(H i ) c Æ (t i)) + P (be  > er(H  ) + c Æ (t I  )) (2)
+ P (er(H i ) er(H  ) < 2 c Æ (t I  ) ; er(H i ) > er(H  ) + ") : (3)
Probability (3) is zero if " = 2 c Æ (t I  ). Hence, plugging (2) into (1) we can write
P

er( b
H) > er(H  ) + 2 c Æ (t I  )


t 1
X
i=0
P (be i  er(H i ) c Æ (t i)) + t P (be  > er(H  ) + c Æ (t I  ))
 Æ
t + 1
+ t
t 1
X
i=0
P (be i  er(H i ) + c Æ (t i))
 Æ
t + 1
+
Æt
t + 1
= Æ;
where in the last two inequalities we applied Chernoff­Hoeffding bounds. 2

Lemma 5 Let an arbitrary on­line algorithm output (not necessarily distinct) hypotheses
H 0 ; : : : ; H t when it is run on Z t . Then for any 0 < Æ < 1 the following holds:
P
 
min
0it 1
(er(H i ) + 2 c Æ (t i))  M
t +
r
2
t ln
1
Æ + 4
r
1
t ln
t + 1
Æ
!
 Æ :
Proof. We have
min
0it 1
(er(h i ) + 2 c Æ (t i))  1
t
t 1
X
i=0
(er(h i ) + 2 c Æ (t i))
=
1
t
t 1
X
i=0
er(h i ) +
2
t
t 1
X
i=0
s
1
2(t i) ln
t(t + 1)
Æ
< 1
t
t 1
X
i=0
er(h i ) +
2
t
t 1
X
i=0
r
1
t i ln
t + 1
Æ
 1
t
t 1
X
i=0
er(h i ) + 4
r
1
t ln
t + 1
Æ ;
where the last inequality follows from
P t
i=1
p
1=i  2
p
t. Therefore
P min
0it 1
(er(H i ) + 2 c Æ (t i))  M
t +
r
2
t ln 1
Æ + 4
r
1
t ln
t + 1
Æ
!
 P
1
t
t 1
X
i=0
er(H i )  M
t +
r
2
t ln
1
Æ
!
 Æ;
by Lemma 1 (with L = 1). 2
Proof (of Theorem 3). The proof follows by combining Lemma 4 and Lemma 5, and by
overapproximating the square root terms therein. 2
5 Applications
For the sake of concreteness we now sketch two generalization bounds which can be ob­
tained through a direct application of our techniques.
The p­norm Perceptron algorithm [10, 9] is a linear threshold algorithm which keeps in the
i­th trial a weight vector  i 1 2 R n . On instance x i 2 X = fx 2 R n : jjxjj p  1g, the
algorithm predicts by h i 1 (x i ) = sign(g( i 1 )x i ) 2 f1;+1g, where g() = r 1
2 jjjj 2
p
and p  2. If the algorithm's prediction is wrong (i.e., if h i 1 (x i ) 6= y i ) then the algorithm
performs the weight update  i    i 1 + y i x i . Notice that p = 2 yields the classical
Perceptron algorithm [17]. On the other hand, p = (log n) gets an algorithm which
performs like a multiplicative algorithm, such as the Normalized Winnow algorithm [10].
Applying Theorem 3 to the bound on the number M of mistakes for the p­norm Perceptron
algorithm shown in [9], we immediately obtain that, with probability at least 1 Æ with
respect to the draw of the training sample Z t , the risk er( b
H) of the penalized estimator b
H
is at most
1
t

D  (u; Z t
) +
(p 1)
 2 +
1

q
(p 1) D  (u; Z t )

+ 5
r
1
t ln
2(t + 1)
Æ (4)

for any  > 0 and for any u such that jjujj p=(p 1)  1. The margin­based quantity
D  (u; z t ) =
P t
i=1 maxf0; 1 y i u  x i =g is called soft margin in [20] and accounts for
the distribution of margin values achieved by the examples in z t with respect to hyperplane
u. Traditional data­dependent bounds using uniform convergence methods (e.g., [19]) are
typically expressed in terms of the sample margin jfi : y i u  x i  gj=t, i.e., in terms of
the fraction of training points whose margin is at most . The ratio D  (u; z t )=t occurring
in (4) has a similar flavor, though the two ratios are, in general, incomparable.
We remark that bound (4) does not have the extra log factors appearing in the analyses
based on uniform convergence. Furthermore, it is significantly better than the bound in [20]
whenever D  =t is constant, which typically occurs when the data sequence is not linearly
separable.
As a second application, we consider the ridge regression algorithm [12] for square loss.
Assume X = R n and Y = [ Y; +Y ]. This algorithm computes at the beginning of the
i­th trial the vector w = w i 1 which minimizes a
2 jjwjj 2
2 +
P i 1
j=1
1
2 (y j w  x j ) 2 , where
a > 0. On instance x i the algorithm predicts with h i 1 (x i ) =  Y (w i 1  x i ), where  Y is
the ``clipping'' function  Y (x) = Y if x  Y ,  Y (x) = Y if x  Y and  Y (x) = x
if Y  x  Y . The losses 1
2 (y i h i 1 (x i )) 2 are thus bounded by 2 Y 2 . We can apply
Theorem 2 to the bound on the cumulative loss M for ridge regression (see [22, 2]) and
obtain that, with probability at least 1 Æ with respect to the draw of the training sample
Z t , the risk er(H) of the average hypothesis estimator H is at most
1
t
 
a
2
jjujj 2
2 +M(u;Z t ) + 2 Y 2
 
ln
     aI +
t
X
i=1
X i X >
i
     n ln a
!!
+ 2 Y 2
r
2
t ln
1
Æ
(5)
for any u 2 R n , where M(u;Z t ) =
P t
i=1
1
2 (Y i u  X i ) 2 , jAj denotes the determinant
of matrix A, I is the n­dimensional identity matrix and A > is the transpose of A. 2 Let us
denote by X t the matrix whose columns are the data vectors x i , i = 1; : : : ; t. Then simple
linear algebra shows that
ln
  aI +
P t
i=1
X i X >
i
   n ln a = ln
  aI +X t X >
t
   n ln a =
P n
i=1 ln (1 +  i =a) ;
where the  i 's are the eigenvalues of X t X >
t . The nonzero eigenvalues of X t X >
t are the
same as the nonzero eigenvalues of the Gram matrix X >
t
X t . Risk bounds in terms of
the eigenvalues of the Gram matrix were also derived in [23]; we defer to the full paper a
comparison between these results and ours. Finally, our bound applies also to kernel ridge
regression [18] by replacing the eigenvalues of X >
t X t with the eigenvalues of the kernel
Gram matrix K(x i ; x j ), 1  i; j  t, where K is the kernel being considered.
References
[1] Angluin, D. Queries and concept learning, Machine Learning, 2(4), 319­342, 1988.
[2] Azoury, K., and Warmuth, M. K. Relative loss bounds for on­line density estimation
with the exponential family of distributions, Machine Learning, 43:211--246, 2001.
[3] K. Azuma. Weighted sum of certain dependend random variables. Tohoku Mathe­
matical Journal, 68, 357--367, 1967.
2 Using a slightly different linear regression algorithm, Forster and Warmuth [7] have proven a
sharper bound on the expected relative loss. In particular, they have exhibited an algorithm computing
hypothesis H = H(Z t ) such that in expectation (over Z t ) the relative risk er(H) minu2R n E[`(u 
X; Y )] is bounded by nY 2 =t.

[4] A. Blum, A. Kalai, and J. Langford. Beating the hold­out: bounds for k­fold and
progressive cross­validation. In 12th COLT, pages 203--208, 1999.
[5] S. Boucheron, G. Lugosi, and P. Massart. A sharp concentration inequality with
applications. Random Structures and Algorithms, 16, 277--292, 2000.
[6] N. Cesa­Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K.
Warmuth. How to use expert advice. Journal of the ACM, 44(3), 427--485, 1997.
[7] J. Forster, and M. K. Warmuth. Relative expected instantaneous loss bounds. 13th
COLT, 90--99, 2000.
[8] Y. Freund and R. Schapire. Large margin classification using the perceptron algo­
rithm. Machine Learning, 37(3), 277--296, 1999.
[9] C. Gentile The robustness of the p­norm algorithms. Manuscript, 2001. An extended
abstract (co­authored with N. Littlestone) appeared in 12th COLT, 1--11, 1999.
[10] A. J. Grove, N. Littlestone, and D. Schuurmans. General convergence results for
linear discriminant updates, Machine Learning, 43(3), 173--210, 2001.
[11] D. Helmbold and M. K. Warmuth. On weak learning. JCSS, 50(3), 551--573, June
1995.
[12] A. Hoerl, and R. Kennard, Ridge regression: biased estimation for nonorthogonal
problems. Technometrics, 12, 55--67, 1970.
[13] J. Kivinen and M. K. Warmuth. Additive versus exponentiated gradient updates for
linear prediction. Information and Computation, 132(1), 1--64, 1997.
[14] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear­
threshold algorithm. Machine Learning, 2, 285--318, 1988.
[15] N. Littlestone. From on­line to batch learning. In 2nd COLT, 269--284, 1989.
[16] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information
and Computation, 108(2), 212--261, 1994.
[17] F. Rosenblatt. Principles of neurodynamics: Perceptrons and the theory of brain
mechanisms. Spartan Books, Washington, D.C., 1962.
[18] C. Saunders, A. Gammerman, and V. Vovk. Ridge Regression Learning Algorithm in
Dual Variables, In 15th ICML, 1998.
[19] J. Shawe­Taylor, P. Bartlett, R. Williamson, and M. Anthony, Structural Risk Mini­
mization over Data­dependent Hierarchies. IEEE Trans. IT, 44, 1926--1940, 1998.
[20] J. Shawe­Taylor and N. Cristianini, On the generalization of soft margin algorithms,
2000. NeuroCOLT2 Tech. Rep. 2000­082, http://www.neurocolt.org.
[21] V.N. Vapnik, Statistical learning theory. J. Wiley and Sons, NY, 1998.
[22] V. Vovk, Competitive on­line linear regression. In NIPS*10, 1998. Also: Tech. Rep.
Department of Computer Science, Royal Holloway, University of London, CSD­TR­
97­13, 1997.
[23] R. C. Williamson, J. Shawe­Taylor, B. Sch˜olkopf and A. J. Smola, Sample Based
Generalization Bounds, IEEE Trans. IT, to appear.

