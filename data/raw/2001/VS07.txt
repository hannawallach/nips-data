Grouping and dimensionality reduction by
locally linear embedding
Marzia Polito
Division of Physics, Mathematics and Astronomy
California Institute of Technology
Pasadena, CA, 91125
polito@caltech.edu
Pietro Perona
Division of Engeneering and Applied Mathematics
California Institute of Technology
Pasadena, CA, 91125
perona@caltech.edu
Abstract
Locally Linear Embedding (LLE) is an elegant nonlinear
dimensionality-reduction technique recently introduced by Roweis
and Saul [2]. It fails when the data is divided into separate groups.
We study a variant of LLE that can simultaneously group the data
and calculate local embedding of each group. An estimate for the
upper bound on the intrinsic dimension of the data set is obtained
automatically.
1 Introduction
Consider a collection of N data points X i 2 R D . Suppose that, while the dimension
D is large, we have independent information suggesting that the data are distributed
on a manifold of dimension d << D. In many circumstances it is benecial to
calculate the coordinates Y i 2 R d of the data on the lower-dimensional manifold,
both because the shape of the manifold may yield some insight in the process that
produced the data, and because it is cheaper to store and manipulate the data when
it is embedded in fewer dimensions. How can we compute such coordinates?
Principal component analysis (PCA) is a classical technique which works well when
the data lie close to a at manifold [1]. Elegant methods for dealing with data that
is distributed on curved manifolds have been recently proposed [3, 2]. We study
one of them, Locally Linear Embedding (LLE) [2], by Roweis and Saul. While LLE
is not designed to handle data that are disconnected, i.e. separated into groups,
we show that a simple variation of the method will handle this situation correctly.
Furthermore, both the number of groups and the upper bound on the intrinsic
dimension of the data may be estimated automatically, rather than being given
a-priori.

2 Locally linear embedding
The key insight inspiring LLE is that, while the data may not lie close to a glob-
ally linear manifold, it may be approximately locally linear, and in this case each
point may be approximated as a linear combination of its nearest neighbors. The
coeÆcients of this linear combination carries the vital information for constructing
a lower-dimensional linear embedding.
More explicitly: consider a data set fX i g i=1:::;N 2 R D . The local linear structure
can be easily encoded in a sparse N by N matrix W , proceeding as follows.
The rst step is to choose a criterion to determine the neighbors of each point.
Roweis and Saul chose an integer number K and pick, for every point, the K points
nearest to it. For each point X i then, they determine the linear combination of its
neighbors which best approximates the point itself. The coeÆcients of such linear
combinations are computed by minimizing the quadratic cost function:
(W ) =
X
i
jX i
N
X
j=1
W ij X j j 2 (1)
while enforcing the constraints W ij = 0 if X j is not a neighbor of X i , and
P N
j=1 W ij = 1 for every i; these constraints ensure that the approximation of
X i  ^
X i =
P N
j=1 W ij X j lies in the aÆne subspace generated by the K nearest
neighbors of X i , and that the solution W is translation-invariant. This least square
problem may be solved in closed form [2].
The next step consists of calculating a set fY i g i=1;:::;N of points in R d , reproducing
as faithfully as possible the local linear structure encoded in W . This is done
minimizing a cost function
(Y ) =
N
X
i=1
jY i
N
X
j=1
W ij Y j j 2 (2)
To ensure the uniqueness of the solution two constraint are imposed: translation
invariance by placing the center of gravity of the data in the origin, i.e.
P
i Y i = 0,
and normalized unit covariance of the Y i 's, i.e. 1
N
P N
i=1 Y
i
 Y i = I .
Roweis and Saul prove that (Y ) = tr(Y T MY ), where M is dened as
M = (I W ) T (I W ):
The minimum of the function (Y ) for the d-th dimensional representation is then
obtained with the following recipe. Given d, consider the d + 1 eigenvectors asso-
ciated to the d + 1 smallest eigenvalues of the matrix M . Then discard the very
rst one. The rows of the matrix Y whose columns are given by such d eigenvectors
give the desired solution. The rst eigenvector is discarded because it is a vector
composed of all ones, with 0 as eigenvalue. As we shall see, this is true when the
data set is `connected'.
2.1 Disjoint components
In LLE every data point has a set of K neighbors. This allows us to partition of
the whole data set X into K-connected components, corresponding to the intuitive
visual notion of dierent `groups' in the data set.
We say that a partition X = [ i U i is ner than a partition X = [ j V j if every U i
is contained in some V j . The partition in K-connected components is the nest

10 20 30 40 50 60 70
0
10
20
30
40
50
0 10 20 30 40 50 60 70
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
10 20 30 40 50 60 70 80 90 100
-10
0
10
20
30
40
50
60
0 10 20 30 40 50 60 70
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
Figure 1: (Top-left) 2D data X i distributed along a curve (the index i increases
from left to right for convenience). (Top-right) Coordinates Y i of the same points
calculated by LLE with K = 10 and d = 1. The x axis represents the index
i and the y axis represents Y i . This is a good parametrization which recognizes
the intrinsically 1-dimensional structure of the data. (Bottom-left) As above, the
data is now disconnected, i.e. points in dierent groups do not share neighbors.
(Bottom-right) One-dimensional LLE calculated on the data (dierent symbols used
for points belonging to the dierent groups). Notice that the Y i 's are not a good
representation of the data any longer since they are constant within each group.
partition of the data set such that if two points have at least one neighbor in
common, or one is a neighbor of the other, then they belong to the same component.
Note that for any two points in the same component, we can nd an ordered se-
quence of points having them as endpoints, such that two consecutive points have
at least one neighbor in common. A set is K-connected if it contains only one
K-connected component.
Consider data that is not K-connected, then LLE does not compute a good
parametrization, as illustrated in Figure 1.
2.2 Choice of d.
How is d chosen? The LLE method [2] is based on the assumption that d is known.
What if we do not know it in advance? If we overestimate d it then LLE behaves
pathologically.
Let us consider a straight line, drawn in R 3 . Figure 2 shows what happens if d
is chosen equal to 1 and to 2. When the choice is 2 (right) then LLE `makes up'
information and generates a somewhat arbitrary 2D curve.
As an eect of the covariance constraint, the representation curves the line, the

0 10 20 30 40 50 60 70 80
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
-0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
Figure 2: Coordinates Y i calculated for data X i distributed along a straight line in
R D = R 3 when the dimension d is chosen as d = 1 (Left), and d = 2 (Right). The
index i is indicated along the x axis (Left) and along the 2D curve (Right).
curvature can be very high, and even locally we possibly completely lose the lin-
ear structure. The problem is, we chosed the wrong target dimension. The one-
dimensional LLE works in fact perfectly (see Figure 2, left).
PCA provides a principled way of estimating the intrinsic dimensionality of the
data: it corresponds to the number of large singular values of the covariance matrix
of the data. Is such an estimate possible with LLE as well?
3 Dimensionality detection: the size of the eigenvalues
In the example of Figure 2 the two dimensional representation of the data (d = 2)
is clearly the `wrong' one, since the data lie in a one-dimensional linear subspace.
In this case the unit covariance constraint in minimizing the function (Y ) is not
compatible with the linear structure. How could one have obtained the correct
estimate of d? The answer is that d + 1 should be less or equal to the number of
eigenvalues of M that are close to zero.
Proposition 1. Assume that the data X i 2 R D is K-connected and that it is
locally at, i.e. there exists a corresponding set Y i 2 R d for some d > 0 such that
Y i =
P
j W ij Y j (zero-error approximation), the set fY i g has rank d, and has the
origin as center of gravity:
P N
i=1 Y i = 0. Call z the number of zero eigenvalues of
the matrix M . Then d < z.
Proof. By construction the N vector composed of all 1's is a zero-eigenvector of
M . Moreover, since the Y i are such that the addends of  have zero error, then the
matrix Y , which by hypothesis has rank d, is in the kernel of I W and hence in
the kernel of M . Due to the center of gravity constraint, all the columns of Y are
orthogonal to the all 1's vector. Hence M has at least d + 1 zero eigenvalues. 
Therefore, in order to estimate d, one may count the number z of zero eigenvalues
of M and choose any d < z. Within this range, smaller values of d will yield more
compact representations, while larger values of d will yield more expressive ones,
i.e. ones that are most faithful to the original data.
What happens in non-ideal conditions, i.e. when the data are not exactly locally
at, and when one has to contend with numerical noise? The appendix provides an
argument showing that the statement in the proposition is robust with respect to

0 10 20 30 40 50 60 70 80
10 16
10 14
10 12
10 10
10
10
10
10
10 0
10 2
2nd eigenvalue
1st eigenvalue
0 10 20 30 40 50 60 70 80
10 16
10 14
10 12
10 10
10 8
10 6
10 4
10 2
10
10
2nd eigenvalue
1st eigenvalue
Figure 3: (Left) Eigenvalues for the straight-line data X i used for Figure 2. (Right)
Eigenvalues for the curve data shown in the top-left panel of Figure 1. In both cases
the two last eigenvalue are orders of magnitude smaller than the other eigenvalues,
indicating a maximal dimension d = 1 for the data.
noise, i.e. numerical errors and small deviations from the ideal locally at data will
result in small deviations from the ideal zero-value of the rst d + 1 eigenvalues,
where d is used here for the `intrinsic' dimension of the data. This is illustrated in
Figure 3.
In Figure 4 we describe the successful application of the dimensionality detection
method on a data set of synthetically generated grayscale images.
4 LLE and grouping
In the rst example (2.1) we pointed out the limits of LLE when applied to multiple
components of data. It appears then that a grouping procedure should always
preceed LLE. The data would be rst split into its component groups, each one
of which should be then analyzed with LLE. A deeper analysis of the algorithm
though, suggests that grouping and LLE could actually be performed at the same
time.
Proposition 2. Suppose the data set fX i g i=1;:::;N 2 R D is partitioned into m K-
connected components. Then there exists an m-dimensional eigenspace of M with
zero eigenvalue which admits a basis fv i g i=1;:::;m where the v i have entries that are
either `1' or `0'. More precisely: each v i corresponds to one of the groups of the
data and takes value v i;j = 1 for j in the group, v i;j = 0 for j not in the group.
Proof. Without loss of generality, assume that the indexing of the data X i is such
that the weight matrix W , and consequentely the matrix M , are block-diagonal
with m blocks, each block corresponding to one of the groups of data. This is
achieved by a permutation of indices, which will not eect any further step of our
algorithm. As a direct consequence of the row normalization of W , each block of
M has exactly one eigenvector composed of all ones, with eigenvalue 0. Therefore,
there is an m-dimensional eigenspace with eigenvalue 0, and there exist a basis of
it, each vector of which has value 1 on a certain component, 0 otherwise. 
Therefore one may count the number of connected components by computing the
eigenvectors of M corresponding to eigenvalue 0, and counting the number m of
those vectors v i whose components take few discrete values (see Figure 6). Each
index i may be assigned to a group by clustering based on the value of v 1 ; : : : ; vm .

3 5 10
10
10
10
 
10
 
10
 
0.06 0.04 0.02 0.02 0.04 0.06
0.06
0.04
0.02
0
0.02
0.04
0.06
Figure 4: (Left) A sample from a data set of N=1000, 40 by 40 grayscale images,
each one thought as a point in a 1600 dimensional vector space. In each image,
a slightly blurred line separates a dark from a bright portion. The orientation of
the line and its distance from the center of the image are variable. (Middle) The
non-zero eigenvalues of M. LLE is performed with K=20. The 2nd and 3rd smallest
eigenvalues are of smaller size than the others, giving an upper bound of 2 on the
intrinsic dimension of the data set. (Right) The 2-dimensional LLE representation.
The polar coordinates, after rescaling, are the distance of the dividing line from the
center and its orientation.
0.08 0.06 0.04 0.02 0.02 0.04 0.06 0.08
0.08
0.06
0.04
0.02
0
0.02
0.04
0.06
0.08
0.1
0.12
4 9 10 11
10
10
10
 
10
 
0.08 0.06 0.04 0.02 0 0.02 0.04 0.06 0.08
Figure 5: The data set is analogous to the one used above (N=1000, 40 by 40
grayscale images, LLE performed with K=20). The orientation of the line dividing
the dark from the bright portion is now only allowed to vary in two disjoint intervals.
(Middle) The non-zero eigenvalues of M. (Left and Right) The 3rd and 5th (resp.
4th and 6th) eigenvectors of M are used for the LLE representation of the rst (resp.
the second) K-component.

0 10 20 30 40 50 60 70 80
0
0.2
0.4 1
0 10 20 30 40 50 60 70 80
0
0.1
0.2
2
0 10 20 30 40 50 60 70 80
-0.4
-0.2
0
3
0 10 20 30 40 50 60 70 80
-0.5
0
0.5
4
0 10 20 30 40 50 60 70 80
-0.5
0
0.5 5
0 10 20 30 40 50 60 70 80
-0.5
0
0.5
6
0 10 20 30 40 50 60 70 80
10 18
10 16
10 14
10 12
10 10
10 8
10 6
10 4
10 2
10
10
1st, 2nd and 3rd eigenvalues
4th, 5th and 6th eigenvalues
Figure 6: (Left) The last six eigenvectors of M for the broken parabola of Figure 1
shown, top to bottom, in reverse order of magnitude of the corresponding eigenvalue.
The x axis is associated to the index i. (Right) The eigenvalues of the same (log
scale). Notice that the last six are practically zero. The eigenvectors corresponding
to the three last eigenvalues have discrete values indicating that the data is split in
three groups. There are z=6 zero-eigenvalues indicating that the dimension of the
data is d  z=m 1 = 1.
In the Appendix (A) we show that such a process is robust with respect to numerical
noise. It is also robust to small perturbations of the block-diagonal structure of M
(see Figure 7). This makes the use of LLE for grouping purposes convenient. Should
the K-connected components be completely separated, the partition would be easily
obtained via a more eÆcient graph-search algorithm.
The proof is carried out for ordered indices as in Fig. 3 but it is invariant under
index permutation.
The analysis of Proposition 1 may be extended to the dimension of each of the
m groups according to Proposition 2. Therefore, in the ideal case, we will nd z
zero-eigenvalues of M which, together with the number m obtained by counting
the discrete-valued eigenvectors may be used to estimate the maximal d using z 
m(d + 1). This behavior may be observed experimentally, see Figures 6 and 5.
5 Conclusions
We have examined two diÆculties of the Locally Linear Embedding method [2] and
shown that, in a neighborhood of ideal conditions, they may be solved by a careful
exam of eigenvectors of the matrix M that are associated to very small eigenvalues.
More specically: the number of groups in which the data is partitioned corresponds
to the number of discrete-valued eigenvectors, while the maximal dimension d of
the low-dimensional embedding may be obtained by dividing the number of small
eigenvalues by m and subtracting 1.
Both the groups and the low-dimensional embedding coordinates may be computed
from the components of such eigenvectors.
Our algorithms have mainly been tested on synthetically generated data. Further
investigation on real data sets is necessary in order to validate our theoretical results.

20 40 60 80 100 120
0
10
20
30
40
50
60
70
80
90
0 20 40 60 80 100 120
10 16
10 14
10 12
10 10
10
10
10
10
10
10
10
1st and 2nd eigenvalues
3rd and 4th eigenvalues
Figure 7: (Left) 2D Data X i distributed along a broken parabola. Nevertheless,
for K=14, the components are not completely K-disconnected (a dierent symbol is
used for the neighbors of the leftmost point on the rightmost component). (Right)
The set of eigenvalues for M. A set of two almost-zero eigenvalues and a set of two
of small size are visible.
References
[1] C. Bishop, Neural Networks for Pattern Recognition, Oxford Univ. Press,
(1995).
[2] S.T. Roweis, L.K.Saul, Science, 290, p. 2323-2326, (2000).
[3] J. Tenenbaum, V. de Silva, J. Langford, Science, 290, p. 2319-2323, (2000).
A Appendix
In Proposition 2 of Section 4 we proved that during the LLE procedure we can
automatically detect the number of K-connected components, in case there is no
noise. Similarly, in Proposition 1 of Section 3 we proved that under ideal conditions
(no noise, locally at data), we can determine an estimate for the intrinsic dimension
of the data. Our next goal is to establish a certain robustness of these results in
the case there is numerical noise, or the components are not completely separated,
or the data is not exactly locally at.
In general, suppose we have a non degenerate matrix A, and an orthonormal basis
of eigenvectors v 1 ; :::; v m , with eigenvalues  1 ; ::: m . As a consequence of a small
perturbation of the matrix into A + dA, we will have eigenvectors v i + dv i with
eigenvalues  i + d i . The unitary norm constraint makes sure that dv i is orthog-
onal to v i and could be therefore written as dv i =
P
k 6=i
 ik v k . Using again the
orthonormality, one can derive expressions for the perturbations of  i and v i :
d i = < v i ; dAv i >
 ij ( i  j ) = < v j ; dAv i > :
This shows that if the perturbation dA has order , then the perturbations d and
 ij are also of order . Notice that we are not interested in perturbations  ij within
the eigenspace of eigenvalue 0, but rather those orthogonal to it, and therefore
 i 6=  j .

