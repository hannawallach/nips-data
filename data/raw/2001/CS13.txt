Reinforcement Learning and Time
Perception | a Model of Animal
Experiments
J. L. Shapiro
Department of Computer Science
University of Manchester
Manchester, M13 9PL U.K.
jls@cs.man.ac.uk
John Wearden
Department of Psychology
University of Manchester
Manchester, M13 9PL U.K.
Abstract
Animal data on delayed-reward conditioning experiments shows a
striking property | the data for dierent time intervals collapses
into a single curve when the data is scaled by the time interval.
This is called the scalar property of interval timing. Here a simple
model of a neural clock is presented and shown to give rise to the
scalar property. The model is an accumulator consisting of noisy,
linear spiking neurons. It is analytically tractable and contains
only three parameters. When coupled with reinforcement learning
it simulates peak procedure experiments, producing both the scalar
property and the pattern of single trial covariances.
1 Introduction
An aspect of delayed-reward reinforcement learning problem which has a long his-
tory of study in animal experiments, but has been overlooked by theorists, is the
learning of the expected time to the reward. In a number of animal experiments,
animals need to wait a given time interval after a stimulus before performing an
action in order to receive the reward. In order to be able to do this, the animal
requires an internal clock or mechanism for perceiving time intervals, as well as a
learning system which can tackle more familiar aspects of delayed reward reinforce-
ment learning problem. In this paper it is shown that a simple connectionist model
of an accumulator used to measure time duration, coupled to a standard TD()
reinforcement learning rule reproduces the most prominent features of the animal
experiments.
The reason it might be desirable for a learner to learn the expected time to receive
a reward is that it allows it to perform the action for an appropriate length of
time. An example described by Grossberg and Merrill [4] and modeled in animal
experiments by Gibbon and Church [3] is foraging. An animal which had no sense of
the typical time to nd food might leave too often, thereby spending an inordinate
amount of time ying between patches. Alternatively it could remain in a depleted
patch and starve. The ability to learn times to rewards is an important aspect of

intelligent behavior more generally.
1.1 Peak Procedure Experiments
A typical type of experiment which investigates how animals learn the time between
stimulus and reward is the peak procedure. In this, the animal is trained to respond
after a given time interval t r has elapsed. Some stimulus (e.g. a light) is presented
which stays on during the trial. The animal is able to respond at any time. The
animal receives a reward for the rst response after the length of time t r . The trial
ends when the animal receives the reward.
On some trials, however, no reward is given even when the animal responds appro-
priately. This is to see when the animal would stop responding. What happens
in non-reward trials is that the animal typically will start responding at a certain
time, will respond for a period, and then stop responding. Responses averaged over
many trials, however, give a smooth curve. The highest response is at the time
interval t r , and there is variation around this. The inaccuracy in the response (as
measured by the standard deviation in the average response curves for non-reward
trials) is also proportional to the time interval. In other words, the ratio of the
standard deviation to the mean response time (the coe∆cient of variation) is a
constant independent of the time interval.
A more striking property of the timing curves is scalar property, of which the above
are two consequences. When the average response rate for non-reward trials is
multiplied by the time interval and plotted against the relative time (time divided
by the time interval) the data from dierent time intervals collapse onto one curve.
This strong form of the scalar property can be expressed mathematically as follows.
Let T be the actual time since the start of the trial and ~
T be subjective time.
Subjective time is the time duration which the animal perceives to have occurred,
(or at least appears to perceive judging from its behavior). The experiments show
that ~
T varies for a given T . This variation can be expressed as a conditional
probability, the probability of acting as though the time is ~
T given that the actual
time is T , which is written P ( ~
T jT ). The fact that the data collapses implies this
probability depends on T and ~
T in a special way,
P ( ~
T jT )  1
T
P inv
  ~
T
T
!
: (1)
Here P inv is the function which describes the shape of the scaled curves. Thus, time
acts as a scale factor. This is a strong and striking result. This has been seen in
many species, including rats, pigeons, turtles; humans will show similar results if the
time intervals are short or if they are prevented from counting through distracting
tasks. For reviews of interval timing phenomena, see [5] and [3].
A key question which remains unanswered is: what is the origin of the scalar prop-
erty. Since the scalar property is ubiquitous, it may be revealing something fun-
damental about the nature of an internal clock or time perception system. This
is especially true if there are only a few known mechanisms which generate this
phenomenon. It is well known that any model based on the accumulation of in-
dependent errors, such as a clock with a variable pulse-rate, does not produce the
scalar property. In such a model it would be the ratio of the variance to the mean
response time which would be independent of the time interval (a consequence of
the law of large numbers). In section 2, a simple stochastic process will be presented
which gives rise to scalar timing. In section 3 simulations of the model on the peak

procedure are presented. The model reproduces experimental results on the mean
responses and the covariation between responses on non-reward trials.
2 The model
2.1 An accumulator network of spiking neurons
Here it is shown that a simple connectionist model of an accumulator can give rise
to the strong scalar property. The network consists of noisy, linear, spiking neurons
which are connected in a random, spatially homogeneous way. The network encodes
time as the total activity in the network which grows during the measured time
interval. Psychological aspects of the model will be presented elsewhere [8]
The network consists of N identical neurons. The connectivity between neurons
is random and dened by a connection matrix C ij which is random and sparse.
The connection strength is the same between all connected neurons. An important
parameter is the fan-out of the ith neuron C i ; its average across the network is
denoted C. Time is in discrete units of size  , the time required for a spike produced
by a neuron to invoke a spike in a connected neuron. There is no refractory period.
The neurons are linear | the expected number of spikes produced by a neuron is
 times the number of pre-synaptic spikes. Let a i (t) denote the number of spikes
produced by neuron i at time t. This obeys
a i (t + ) =
h i (t)
X
=1
 + I i (t); (2)
where h i (t) is the number of spikes feeding into neuron i, h i (t) =
P
j C ji x j (t). I i (t)
is the external input at i, and  is a random variable which determines whether a
pre-synaptic spike invokes one in a connected neuron. The mean of  is  and the
variance is denoted  2
 . So the spikes behave independently; saturation eects are
ignored. The total activity of the network is
n(t) =
N
X
i=1
a i (t): (3)
At each time-step, the number of spikes will grow due to the fan-out of the neurons.
At the same time, the number of spikes will shrink due to the fact that a spike
invokes another spike with a probability less than 1. An essential assumption of
this work is that these two processes balance each other, C = 1.
Finally, in order for this network to act as an accumulator, it receives statistically
stationary input during the time interval which is being measured, so I(t) is only
present during the measured interval and statistically stationary then.
2.2 Derivation of the strong scalar property
Here it is shown that the network activity obeys equation (1). Let y be the scaled
network activity,
y(t) = n(t)=t: (4)
The goal here is the derive the probability distribution for y as a function of time,
P (yjt). In order to do this, we use the cumulant generating function (or character-
istic function). For any probability distribution, (x), the generating function for

cumulants is,
G(s) = log
Z
 (x) exp(sx)dx


1
X
i=0
s i
i!
 i (5)
(6)
where
 is the domain of (x),  i is the ith cumulant of (x), and s is just a dummy
variable. Taking the nth derivative of G(s) with respect to s and setting s to 0 gives
 i . Cumulants are like moments, see [1] for some denitions and properties.
We will derive a recursion relation for the cumulant generating function for y(t),
denoted G y (s; t). Let G  (s) denote the generating function for the distribution of 
and G I (s) denote the generating functions for the distribution of inputs I(t). These
latter two are assumed to be stationary, hence there is no time-dependence. From
equation 2 it follows that,
G y (s; t + ) = G I
 s
t + 

+ 1
N
X
i
G y

tC i G 
 s
t + 

; t

: (7)
In deriving the above, it was assumed that the activity at each node is statistically
the same, and that the fan-out at i is uncorrelated with the activity at i (this
requires a su∆ciently sparsely connectivity, i.e. no tight loops).
Dierentiating the last equation n times with respect to s and setting s to zero
produces a set recursion relations for the cumulants of y, denoted n . It is necessary
to take terms only up to rst order in 1=t to nd the xed point distribution. The
recursion relations to this order are
 1 (t + ) =

1 
t

 1 (t) + m I
t + 
(8)
n (t + ) =

1 n 
t

n (t) + 1
t
n(n 1)
2 C 2
 n 1 (t)
+ O
 1
t 2

; n > 1: (9)
The above depends upon the mean total input activity m I  G 0
I (0) the average
fan-out C, and the variance in the noise ,  2
  G 00
 (0). In general it would depend
upon the fan-out times the mean of the noise , but that is 1 by assumption. Higher
order statistics in C and  only contribute to terms which are higher order in 1=t.
The above equations converge to a xed point, which shows that n(t)=t has a time-
independent distribution for large t. The xed point is found to be
G y (s; 1) =
1
X
n=0
s n
n! n (1) = 2m I
 2
log

1  2

2 s

: (10)
Equation 10 is the generating function for a gamma distribution,
P (xja; b) = exp( x=b)x a 1
b a (a) (11)
with
a = 2m I
C 2

; b = C 2

2 : (12)
Corrections to the xed point are O(1=t).
What this shows is that for large t, the distribution of neural activity, n is scalar,
P (njt) = 1
t P
 n
t
ja; b

; (13)
with a and b dened above.

2.3 Reinforcement learning of time intervals
The above model represents a way for a simple connectionist system to measure a
time interval. In order to model behavior, the system must learn to association the
external stimulus and the clock with the response and the reward. To do this, some
additional components are needed.
The ith stimulus is represented by a signal s i . The output of the accumulator trig-
gers a set of clock nodes which convert the quantity or activity encoding of time
used by the accumulator into a \spatial code" in which particular nodes represent
dierent network activities. This was done because it is di∆cult to use the accu-
mulator activity directly, as this takes a wide range of values. Each clock node
responds to a particular accumulator activity. The output of the ith clock node
at time t is denoted X i (t); it is one if the activity is i, zero otherwise. It would
be more reasonable to use a coarse coding, but this ne-grained encoding is partic-
ularly simple. The components of the learning model are shown schematically in
gure 1.
X i (t)
V j (t)
Stimulus
s i accumulator
w ij
A ij
Figure 1: The learning model. The accumulator feeds into a bank of clock nodes, X i ,
which are tuned to accumulator activities. The response V j is triggered by simulta-
neous presence of both the stimulus s i and the appropriate clock node. Solid lines
denote weights which are xed; dashed lines show weights which learn according to
the TD() learning rule.
The stimulus and the clock nodes feed into response nodes. The output of the jth
response node, V j (t) is given by
V j (t) =
X
i
w ij X i (t) +A ij s i  j : (14)
Here  is a threshold, A ij is the association between the stimulus and the response,
and w ij is the association between a clock node and the response. Both the stimulus
and the appropriate clock node must be present in order for there to be a reasonable
probability of a response. The response probability is V j (t), unless that is negative,
in which case there is no response, or is greater than 1, in which case there is
denitely a response.
Both A ij and the w's learn via a TD- learning rule. TD- is an important learning

rule for modeling associative conditioning; it has been used to model aspects of
classical conditioning including Pavlovian conditioning and blocking. For example,
a model which is very eective at modeling Pavlovian eye-blink experiments and
other classical conditioning results has been proposed by Moore et. al. [6] building
on the model of Sutton, Barto, and Desmond (see description in [7]). This model
represents time using a tapped delay line; at each time-step, a dierent node in the
delay line is activated. Time acts as one of the conditioned stimuli. The conditioned
stimsing temporal dierence (TD) reinforcement learning is associated with the
response through the unconditioned stimulus. These authors did not attempt to
model the scalar property, and in their model time is represented accurately by
the system. The model presented here is similar to these models. The clock nodes
play the role of the tapped delay-line nodes in that model. However, here they
are stimulated by the accumulator rather than each other, and they will follow a
stochastic trajectory due to the uctuating nature of the accumulator
The learning rule for w ij couples to an \eligibility trace" for the clock nodes X i (t)
which takes time to build up and decays after the node is turned o. They obey
the following equations,
X i (t + ) = X i (t) + (1 )

X i (t) X i (t)

: (15)
The standard TD- learning parameters,  and  are used, see [9]. The learning
equations are
w ij = ∆(t + )X i (t); (16)
A ij = ∆(t + )s i ; (17)
∆(t) = R(t) +  V j (t) V j (t ): (18)
Here  is a learning rate, ∆ is the temporal dierence component, R(t) is the re-
inforcement. The outputs V j at both times use the current value of the weights.
The threshold is set to a constant value ( 1 in the simulations). It would make no
dierence if a eligibility trace were used for the stimulus s i , because that was held
on during the learning.
3 Simulations
The model has been used to simulate peak procedure. In the simulations, the model
is forced to respond for the rst set of trials (50 trials in the simulations); otherwise
the model would never respond. This could represent shaping in real experiments.
After that the model learns using reward trials for an additional number of trials
(150 trials in these simulations). The system is then run for 1000 trials, every
10th trial is a non-reward trial; the system continues to learn during these trials.
Figure 2 shows average over non-reward trials for dierent time intervals. The scalar
property clearly holds.
Gibbon and Church [3] have argued that the covariation between trials is a useful
diagnostic to distinguish models of scalar timing. The methodology which they
proposed is to t the results of single non-reward trials from peak procedure exper-
iments to a break-run-break pattern of response The animal is assumed to respond
at a low rate until a start time is reached. The animal then responds at a high rate
until a stop time is reached, whence it returns to the low response rate. The covari-
ation between the start and stop times between trials is measured and compared to
those predicted by theory.
The question Gibbon and Church asked was, how does the start and stop time co-
vary across trials. For example, if the animal starts responding early, does it stop

0 100 200 300 400 500 600 700 800 900
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
PSfrag replacements
t r = 40
t r = 80
t r = 160
t r = 320
time
response
rate
time/t r
t r (response rate) 0.5 1 1.5 2 2.5 3
0
5
10
15
20
25
30
35
PSfrag replacements
t r = 40
t r = 80
t r = 160
t r = 320
time
response rate
time/t r
t
r
(response
rate)
Figure 2: Left) Average response of the spatially encoded network for non-reward
trials. The accumulator parameters are: m I = 10, C 2 = 1 (Poisson limit); learning
parameters are  = 0:75;  = 1, learning rate  is 0.5. Right) Relative time plotted
against response rate times time interval for reinforcement times of 40 , 80 , 160 ,
240; and 320 . All experiments are averages over 100 non-reward trials, which
were every 10 trial in 1000 learning trials.
responding early, as though it has a shifted estimate of the time interval? Or does
it stop responding late, as though it has a more liberal view about what constitutes
the particular interval. The covariance between start and stop parameters addresses
this question.
Comparable experiments can be carried out on the model proposed here. The
procedure used is described in [2]. Figure 3 shows a comparison with data from
reference [2] with simulations. The pattern of covariation found in the simulations
is qualitatively similar to that of the animal data. The interesting quantity is the
correlation between the start time and the spread (dierence between stop and start
times). This is negative in both.
1 2 3 4 5 6
-0.5
0
0.5
1
covariance
1 2 3 4 5 6
-0.5
0
0.5
1
covariance
Figure 3: Left) Covariances across individual trials in experiments on rats. Data is
taken from Table 2 of reference [2] averaged over the four conditions. The covari-
ances are shown in the following order: 1. start-stop, 2. start-spread, 3. spread-
middle, 4. start-middle, 5. stop-spread, 6. stop-middle. The black, gray, and white
bars are for times of reinforcement t r of 15, 30, and 60 seconds respectively. Right)
Covariances across individual trials simulated by the model. The reinforcement
times are 40 , 80; and 160 . The covariances are given in the same order as in left
gure.

4 Conclusion
Previous models of interval timing fail to explain its most striking feature | the
collapse of the data when scaled by the time interval. We have presented a sim-
ple model of an accumulator clock based on spiking, noisy, linear neurons which
produces this eect. It is a simple model, analytically tractable, based on a driven
branching process. The parameters are:  | the time for a spike on one neuron
to excite spikes on connected neurons, m I | the average number of spikes excited
externally at each short time interval  , and the variance of the spike transmission
process, which in this model is  2
 . A weakness of this model is that it requires
ne-tuning of a pair of parameters, so that the expected number of spikes grows in
with external excitation only.
Once a scalar clock is produced, simple reinforcement learning can be used to asso-
ciate the clock signal with appropriate responses. A set of intermediate clock nodes
was used to encode time. TD- reinforcement learning between the intermediate
nodes at reinforcement and an eligibility trace simulates peak procedure and the
individual trial covariances.
References
[1] M. Abramowitz and I. A. Stegun, editors. Handbook of Mathematical Functions. New
York: Dover Publications, 1967.
[2] Russell M. Church, Walter H. Meck, and John Gibbon. Application of scalar timing
theory to individual trials. Journal of Experimental Psychology { Animal Behavior
Processes, 20(2):135{155, 1994.
[3] John Gibbon and Russell M. Church. Representation of time. Cognition, 37:23{54,
1990.
[4] Stephen Grossberg and John W. L. Merrill. A neural network model of adaptively
timed reinforcement learning and hippocampal dynamics. Cognitive Brain Research,
1:3{38, 1992.
[5] S. C. Hinton and W. H. Meck. How time ies: Functional and neural mechansims
of interval timing. In C. M. Bradshaw and E. Szadabi, editors, Time and Behaviour:
Psychological and Neurobehavioural Analyses. Amsterdam: Elsevier Science, 1997.
[6] J. W. Moore, J. E. Desmond, and N. E. Berthier. Adaptively timed conditioned
responses and the cerebellum: A neural network approach. Biological Cybernetics,
62:17{28, 1989.
[7] John W. Moore, Neil D. Berthier, and Diana E. J. Blazis. Classical eye-blink condition-
ing: Brain systems and implementation of a computational model. In Michael Gabriel
and John Moore, editors, Learning and Computational Neuroscience: Foundations of
Adaptive Networks, A Bradford Book, pages 359{387. The MIT Press, 1990.
[8] J. L. Shapiro and John Wearden. Modelling scalar timing by an accumulator network
of spiking neurons. In preparation, 2001.
[9] Richard S. Sutton and Andrew G. Barto. Reinforcment Learning: An Introduction. A
Bradford Book. The MIT Press, 1998.

