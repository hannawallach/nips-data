On the Concentration of Spectral
Properties
John Shawe-Taylor
Royal Holloway, University of London
john@cs.rhul.ac.uk
Nello Cristianini
BIOwulf Technologies
nello@support-vector.net
Jaz Kandola
Royal Holloway, University of London
jaz@cs.rhul.ac.uk
Abstract
We consider the problem of measuring the eigenvalues of a ran-
domly drawn sample of points. We show that these values can be
reliably estimated as can the sum of the tail of eigenvalues. Fur-
thermore, the residuals when data is projected into a subspace is
shown to be reliably estimated on a random sample. Experiments
are presented that conrm the theoretical results.
1 Introduction
A number of learning algorithms rely on estimating spectral data on a sample of
training points and using this data as input to further analyses. For example in
Principal Component Analysis (PCA) the subspace spanned by the rst k eigen-
vectors is used to give a k dimensional model of the data with minimal residual,
hence forming a low dimensional representation of the data for analysis or clus-
tering. Recently the approach has been applied in kernel dened feature spaces
in what has become known as kernel-PCA [5]. This representation has also been
related to an Information Retrieval algorithm known as latent semantic indexing,
again with kernel dened feature spaces [2].
Furthermore eigenvectors have been used in the HITS [3] and Google's PageRank [1]
algorithms. In both cases the entries in the eigenvector corresponding to the maxi-
mal eigenvalue are interpreted as authority weightings for individual articles or web
pages.
The use of these techniques raises the question of how reliably these quantities can
be estimated from a random sample of data, or phrased dierently, how much data is
required to obtain an accurate empirical estimate with high condence. Ng et al. [6]
have undertaken a study of the sensitivity of the estimate of the rst eigenvector to
perturbations of the connection matrix. They have also highlighted the potential
instability that can arise when two eigenvalues are very close in value, so that their
eigenspaces become very diÆcult to distinguish empirically.
The aim of this paper is to study the error in estimation that can arise from the
random sampling rather than from perturbations of the connectivity. We address

this question using concentration inequalities. We will show that eigenvalues esti-
mated from a sample of size m are indeed concentrated, and furthermore the sum
of the last m k eigenvalues is subject to a similar concentration eect, both re-
sults of independent mathematical interest. The sum of the last m k eigenvalues
is related to the error in forming a k dimensional PCA approximation, and hence
will be shown to justify using empirical projection subspaces in such algorithms as
kernel-PCA and latent semantic kernels.
The paper is organised as follows. In section 2 we give the background results and
develop the basic techniques that are required to derive the main results in section
3. We provide experimental verication of the theoretical ndings in section 4,
before drawing our conclusions.
2 Background and Techniques
We will make use of the following results due to McDiarmid. Note that E S is the
expectation operator under the selection of the sample.
Theorem 1 (McDiarmid [4]) Let X 1 ; : : : ; Xn be independent random variables tak-
ing values in a set A, and assume that f : A n ! R, and f i : A n 1 ! R satisfy for
1  i  n
sup
x1 ;:::;x n
jf(x 1 ; : : : ; xn ) f i (x 1 ; : : : ; x i 1 ; x i+1 ; : : : ; xn )j  c i ;
then for all  > 0,
P fjf(X 1 ; : : : ; Xn ) Ef(X 1 ; : : : ; Xn )j > g  2 exp
 2 2
P n
i=1 c 2
i

Theorem 2 (McDiarmid [4]) Let X 1 ; : : : ; Xn be independent random variables tak-
ing values in a set A, and assume that f : A n ! R, for 1  i  n
sup
x1 ;:::;x n ;^x i
jf(x 1 ; : : : ; xn ) f(x 1 ; : : : ; x i 1 ; ^
x i ; x i+1 ; : : : ; xn )j  c i ;
then for all  > 0,
P fjf(X 1 ; : : : ; Xn ) Ef(X 1 ; : : : ; Xn )j > g  2 exp
 2 2
P n
i=1 c 2
i

We will also make use of the following theorem characterising the eigenvectors of a
symmetric matrix.
Theorem 3 (Courant-Fischer Minimax Theorem) If M 2 R mm is symmet-
ric, then for k = 1; : : : ; m,
 k (M) = max
dim(T)=k
min
06=v2T
v 0 Mv
v 0 v = min
dim(T)=m k+1
max
06=v2T
v 0 Mv
v 0 v ;
with the extrama achieved by the corresponding eigenvector.
The approach adopted in the proofs of the next section is to view the eigenvalues as
the sums of squares of residuals. This is applicable when the matrix is positive semi-
denite and hence can be written as an inner product matrix M = X 0 X , where X 0 is
the transpose of the matrix X containing the m vectors x 1 ; : : : ; xm as columns. This
is the nite dimensional version of Mercer's theorem, and follows immediately if we
take X = V
p
, where M = V V 0 is the eigenvalue decomposition of M . There
may be more succinct ways of representing X , but we will assume for simplicity (but
without loss of generality) that X is a square matrix with the same dimensions as
M . To set the scene, we now present a short description of the residuals viewpoint.

The starting point is the singular value decomposition of X = UV 0 , where U and
V are orthonormal matrices and  is a diagonal matrix containing the singular
values (in descending order). We can now reconstruct the eigenvalue decomposition
of M = X 0 X = V U 0 UV 0 = V V 0 , where  =  2 . But equally we can construct
a matrix N = XX 0 = UV 0 V U 0 = UU 0 , with the same eigenvalues as M .
As a simple example consider now the rst eigenvalue, which by Theorem 3 and the
above observations is given by
 1 (M) = max
06=v2R m
v 0 Nv
v 0 v
= max
06=v2R m
v 0 XX 0 v
v 0 v
= max
06=v2R m
kv 0 Xk 2
v 0 v
= max
06=v2R m
m
X
j=1
kP v (x j )k 2 =
m
X
j=1
kx j k 2 min
06=v2R m
m
X
j=1
kP ?
v (x j )k 2
where P v (x) (P ?
v (x)) is the projection of x onto the space spanned by v (space
perpendicular to v), since kxk 2 = kP v (x)k 2 + kP ?
v (x)k 2 . It follows that the rst
eigenvector is characterised as the direction for which sum of the squares of the
residuals is minimal.
Applying the same line of reasoning to the rst equality of Theorem 3, delivers the
following equality
 k = max
dim(V )=k
min
06=v2V
m
X
j=1
kP v (x j )k 2 : (1)
Notice that this characterisation implies that if v k is the k-th eigenvector of N , then
 k =
m
X
j=1
kP v k (x j )k 2 ; (2)
which in turn implies that if V k is the space spanned by the rst k eigenvectors,
then
k
X
i=1
 i =
m
X
j=1
kP Vk (x j )k 2 =
m
X
j=1
kx j k 2
m
X
j=1
kP ?
Vk (x j )k 2 ; (3)
where P V (x) (P ?
V (x)) is the projection of x into the space V (space perpendicular
to V ). It readily follows by induction over the dimension of V that we can equally
characterise the sum of the rst k and last m k eigenvalues by
k
X
i=1
 i = max
dim(V )=k
m
X
j=1
kP V (x j )k 2 =
m
X
j=1
kx j k 2 min
dim(V )=k
m
X
j=1
kP ?
V (x j )k 2 ;
m
X
i=k+1
 i =
m
X
j=1
kx j k 2
k
X
i=1
 i = min
dim(V )=k
m
X
j=1
kP ?
V (x j )k 2 : (4)
Hence, as for the case when k = 1, the subspace spanned by the rst k eigenvalues
is characterised as that for which the sum of the squares of the residuals is minimal.
Frequently, we consider all of the above as occurring in a kernel dened feature
space, so that wherever we have written x j we should have put (x j ), where  is
the corresponding projection.
3 Concentration of eigenvalues
The previous section outlined the relatively well-known perspective that we now
apply to obtain the concentration results for the eigenvalues of positive semi-denite

matrices. The key to the results is the characterisation in terms of the sums of
residuals given in equations (1) and (4).
Theorem 4 Let K(x; z) be a positive semi-denite kernel function on a space X,
and let  be a distribution on X. Fix natural numbers m and 1  k < m and let
S = (x 1 ; : : : ; xm ) 2 X m be a sample of m points drawn according to . Then for
all  > 0,
P fj 1
m  k (S) E S [ 1
m  k (S)]j  g  2 exp
 2 2 m
R 4

;
where  k (S) is the k-th eigenvalue of the matrix K(S) with entries K(S) ij =
K(x i ; x j ) and R 2 = max x2X K(x; x).
Proof : The result follows from an application of Theorem 1 provided
sup
S
j 1
m  k (S) 1
m  k (S n fx i g)j  R 2 =m:
Let ^
S = S n fx i g and let V ( ^
V ) be the k dimensional subspace spanned by the rst
k eigenvectors of K(S) (K( ^
S)). Using equation (1) we have
 k (S)  min
v2 ^
V
m
X
j=1
kP v (x j )k 2  min
v2 ^
V
X
j 6=i
kP v (x j )k 2 =  k ( ^
S)
 k ( ^
S)  min
v2V
X
j 6=i
kP v (x j )k 2  min
v2V
m
X
j=1
kP v (x j )k 2 R 2 =  k (S) R 2
Surprisingly a very similar result holds when we consider the sum of the last m k
eigenvalues.
Theorem 5 Let K(x; z) be a positive semi-denite kernel function on a space X,
and let  be a distribution on X. Fix natural numbers m and 1  k < m and let
S = (x 1 ; : : : ; xm ) 2 X m be a sample of m points drawn according to . Then for
all  > 0,
P fj 1
m  >k (S) E S [ 1
m  >k (S)]j  g  2 exp
 2 2 m
R 4

;
where  >k (S) is the sum of all but the largest k eigenvalues of the matrix K(S) with
entries K(S) ij = K(x i ; x j ) and R 2 = max x2X K(x; x).
Proof : The result follows from an application of Theorem 1 provided
sup
S
j 1
m  >k (S) 1
m  >k (S n fx i g)j  R 2 =m:
Let ^
S = S n fx i g and let V ( ^
V ) be the k dimensional subspace spanned by the rst
k eigenvectors of K(S) (K( ^
S)). Using equation (4) we have
 >k (S) 
m
X
j=1
kP ?
^
V (x j )k 2 
X
j 6=i
kP ?
^
V (x j )k 2 +R 2 =  >k ( ^
S) +R 2
 >k ( ^
S) 
X
j 6=i
kP ?
V (x j )k 2 =
m
X
j=1
kP ?
V (x j )k 2 kP ?
V (x i )k 2   >k (S)

Our next result concerns the concentration of the residuals with respect to a xed
subspace. For a subspace V and training set S, we introduce the notation

P V (S) = 1
m
m
X
i=1
kP V (x i )k 2 :
Theorem 6 Let  be a distribution on X. Fix natural numbers m and a subspace
V and let S = (x 1 ; : : : ; xm ) 2 X m be a sample of m points drawn according to .
Then for all  > 0,
P fj 
P V (S) E S [ 
P V (S)]j  g  2 exp
  2 m
2R 4

:
Proof : The result follows from an application of Theorem 2 provided
sup
S;^x i
j 
P V (S) 
P (S n fx i g [ f^x i )j  R 2 =m:
Clearly the largest change will occur if one of the points x i and ^
x i is lies in the
subspace V and the other does not. In this case the change will be at most R 2 =m.
4 Experiments
In order to test the concentration results we performed experiments with the Breast
cancer data using a cubic polynomial kernel. The kernel was chosen to ensure that
the spectrum did not decay too fast.
We randomly selected 50% of the data as a `training' set and kept the remaining
50% as a `test' set. We centered the whole data set so that the origin of the feature
space is placed at the centre of gravity of the training set. We then performed an
eigenvalue decomposition of the training set. The sum of the eigenvalues greater
than the k-th gives the sum of the residual squared norms of the training points
when we project onto the space spanned by the rst k eigenvectors. Dividing this by
the average of all the eigenvalues (which measures the average square norm of the
training points in the transformed space) gives a fraction residual not captured in
the k dimensional projection. This quantity was averaged over 5 random splits and
plotted against dimension in Figure 1 as the continuous line. The error bars give
one standard deviation. The Figure 1a shows the full spectrum, while Figure 1b
shows a zoomed in subwindow. The very tight error bars show clearly the very tight
concentration of the sums of tail of eigenvalues as predicted by Theorem 5.
In order to test the concentration results for subsets we measured the residuals of
the test points when they are projected into the subspace spanned by the rst k
eigenvectors generated above for the training set. The dashed lines in Figure 1 show
the ratio of the average squares of these residuals to the average squared norm of the
test points. We see the two curves tracking each other very closely, indicating that
the subspace identied as optimal for the training set is indeed capturing almost
the same amount of information in the test points.
5 Conclusions
The paper has shown that the eigenvalues of a positive semi-denite matrix gener-
ated from a random sample is concentrated. Furthermore the sum of the last m k
eigenvalues is similarly concentrated as is the residual when the data is projected
into a xed subspace.

-50 0 50 100 150 200 250 300 350
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Projection Dimensionality
Residual
Error
(a)
0 10 20 30 40 50 60 70 80 90 100
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
Projection Dimensionality
Residual
Error
(b)
Figure 1: Plots of the fraction of the average squared norm captured in the subspace
spanned by the rst k eigenvectors for dierent values of k. Continuous line is
fraction for training set, while the dashed line is for the test set. (a) shows the full
spectrum, while (b) zooms in on an interesting portion.

Experiments are presented that conrm the theoretical predictions on a real world
dataset. The results provide a basis for performing PCA or kernel-PCA from a
randomly generated sample, as they conrm that the subset identied by the sample
will indeed `generalise' in the sense that it will capture most of the information in
a test sample.
Further research should look at the question of how the space identied by a sub-
sample relates to the eigenspace of the underlying kernel operator.
References
[1] S. Brin and L. Page. The anatomy of a large-scale hypertextual (web) search en-
gine. In Proceedings of the Seventh International World Wide Web Conference,
1998.
[2] Nello Cristianini, Huma Lodhi, and John Shawe-Taylor. Latent semantic kernels
for feature selection. Technical Report NC-TR-00-080, NeuroCOLT Working
Group, http://www.neurocolt.org, 2000.
[3] J. Kleinberg. Authoritative sources in a hyperlinked environment. In Proceedings
of 9th ACM-SIAM Symposium on Discrete Algorithms, 1998.
[4] C. McDiarmid. On the method of bounded dierences. In Surveys in Combina-
torics 1989, pages 148{188. Cambridge University Press, 1989.
[5] S. Mika, B. Scholkopf, A. Smola, K.-R. Muller, M. Scholz, and G. Ratsch.
Kernel PCA and de-noising in feature spaces. In Advances in Neural Information
Processing Systems 11, 1998.
[6] Andrew Y. Ng, Alice X. Zheng, and Michael I. Jordan. Link analysis, eigenvec-
tors and stability. In To appear in the Seventeenth International Joint Confer-
ence on Articial Intelligence (IJCAI-01), 2001.

