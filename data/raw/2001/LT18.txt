Algorithmic Luckiness
Ralf Herbrich
Microsoft Research Ltd.
CB3 0FB Cambridge
United Kingdom
rherb@microsoft.com
Robert C. Williamson
Australian National University
Canberra 0200
Australia
Bob.Williamson@anu.edu.au
Abstract
In contrast to standard statistical learning theory which studies
uniform bounds on the expected error we present a framework that
exploits the specic learning algorithm used. Motivated by the
luckiness framework [8] we are also able to exploit the serendipity
of the training sample. The main dierence to previous approaches
lies in the complexity measure; rather than covering all hypothe-
ses in a given hypothesis space it is only necessary to cover the
functions which could have been learned using the xed learning
algorithm. We show how the resulting framework relates to the
VC, luckiness and compression frameworks. Finally, we present an
application of this framework to the maximum margin algorithm
for linear classiers which results in a bound that exploits both the
margin and the distribution of the data in feature space.
1 Introduction
Statistical learning theory is mainly concerned with the study of uniform bounds
on the expected error of hypotheses from a given hypothesis space [9, 1]. Such
bounds have the appealing feature that they provide performance guarantees for
classiers found by any learning algorithm. However, it has been observed that
these bounds tend to be overly pessimistic. One explanation is that only in the
case of learning algorithms which minimise the training error it has been proven
that uniformity of the bounds is equivalent to studying the learning algorithm's
generalisation performance directly.
In this paper we present a theoretical framework which aims at directly studying the
generalisation error of a learning algorithm rather than taking the detour via the
uniform convergence of training errors to expected errors in a given hypothesis space.
In addition, our new model of learning allows the exploitation of the fact that we
serendipitously observe a training sample which is easy to learn by a given learning
algorithm. In that sense, our framework is a descendant of the luckiness framework
of Shawe-Taylor et al. [8]. In the present case, the luckiness is a function of a given
learning algorithm and a given training sample and characterises the diversity of
the algorithms solutions. The notion of luckiness allows us to study given learning
algorithms at many dierent perspectives. For example, the maximum margin
algorithm [9] can either been studied via the number of dimensions in feature space,

the margin of the classier learned or the sparsity of the resulting classier. Our
main results are two generalisation error bounds for learning algorithms: one for
the zero training error scenario and one agnostic bound (Section 2). We shall
demonstrate the usefulness of our new framework by studying its relation to the
VC framework, the original luckiness framework and the compression framework of
Littlestone and Warmuth [6] (Section 3). Finally, we present an application of the
new framework to the maximum margin algorithm for linear classiers (Section 4).
The detailed proofs of our main results can be found in [5].
We denote vectors using bold face, e.g. x = (x 1 ; : : : ; xm ) and the length of this vector
by jxj, i.e. jxj = m. In order to unburden notation we use the shorthand notation
z [i:j] := (z i ; : : : ; z j ) for i  j. Random variables are typeset in sans-serif font. The
symbols PX , EX [f (X)] and Idenote a probability measure over X, the expectation of
f () over the random draw of its argument x and the indicator function, respectively.
The shorthand notation Z (1) := [ 1
m=1 Z m denotes the union of all mfold Cartesian
products of the set Z . For any m 2 N we dene I m  f1; : : : ; mg m as the set of all
permutations of the numbers 1; : : : ; m,
I m := f(i 1 ; : : : ; i m ) 2 f1; : : : ; mg m j 8j 6= k : i j 6= i k g :
Given a 2mvector i 2 I 2m and a sample z 2 Z 2m we dene  i : f1; : : : ; 2mg !
f1; : : : ; 2mg by  i (j) := i j and  i (z) by  i (z) := z  i (1) ; : : : ; z  i (2m)

.
2 Algorithmic Luckiness
Suppose we are given a training sample z = (x; y) 2 (X  Y) m
= Z m of size
m 2 N independently drawn (iid) from some unknown but xed distribution PXY =
P Z together with a learning algorithm A : Z (1) ! Y X . For a predened loss
l : Y  Y ! [0; 1] we would like to investigate the generalisation error G l [A; z] :=
R l [A (z)] inf h2Y X R l [h] of the algorithm where the expected error R l [h] of h is
dened by
R l [h] := EXY [l (h (X) ; Y)] :
Since inf h2Y X R l [h] (which is also known as the Bayes error) is independent of A
it suces to bound R l [A (z)]. Although we know that for any xed hypothesis h
the training error
b
R l [h; z] :=
1
jzj
X
(x i ;y i )2z
l (h (x i ) ; y i )
is with high probability (over the random draw of the training sample z 2 Z (1) )
close to R l [h], this might no longer be true for the random hypothesis A (z). Hence
we would like to state that with only small probability (at most Æ), the expected
error R l [A (z)] is larger than the training error b
R l [A (z) ; z] plus some sample and
algorithm dependent complexity " (A; z; Æ),
P Z m

R l [A (Z)] > b
R l [A (Z) ; Z] + " (A; Z; Æ)

< Æ : (1)
In order to derive such a bound we utilise a modied version of the basic lemma of
Vapnik and Chervonenkis [10].
Lemma 1. For all loss functions l : Y Y ! [0; 1], all probability measures P Z , all
algorithms A and all measurable formulas  : Z m ! ftrue; falseg, if m" 2 > 2 then
P Z m

R l [A (Z)] > b
R l [A (Z) ; Z] + "

^  (Z)

<
2P Z 2m
 b
R l

A Z [1:m]

; Z [(m+1):2m]

> b
R l

A Z [1:m]

; Z [1:m]

+
"
2

^  Z [1:m]
 
| {z }
J(Z)
:

Proof (Sketch). The probability on the r.h.s. is lower bounded by the proba-
bility of the conjunction of event on the l.h.s. and Q (z)  R l

A z [1:m]

b
R l

A z [1:m]

; z (m+1):2m

< "
2 . Note that this probability is over z 2 Z 2m . If
we now condition on the rst m examples, A z [1:m]

is xed and therefore by an
application of Hoeding's inequality (see, e.g. [1]) and since m" 2 > 2 the additional
event Q has probability of at least 1
2 over the random draw of (z m+1 ; : : : ; z 2m ).
Use of Lemma 1  which is similar to the approach of classical VC analysis 
reduces the original problem (1) to the problem of studying the deviation of the
training errors on the rst and second half of a double sample z 2 Z 2m of size
2m. It is of utmost importance that the hypothesis A z [1:m]
 is always learned
from the rst m examples. Now, in order to fully exploit our assumptions of the
mutual independence of the double sample z 2 Z 2m we use a technique known
as symmetrisation by permutation: since P Z 2m is a product measure, it has the
property that P Z 2m (J (Z)) = P Z 2m (J ( i (Z))) for any i 2 I 2m . Hence, it suces
to bound the probability of permutations  i such that J ( i (z)) is true for a given
and xed double sample z. As a consequence thereof, we only need to count the
number of dierent hypotheses that can be learned by A from the rst m examples
when permuting the double sample.
Denition 1 (Algorithmic luckiness). Any function L that maps an algorithm
A : Z (1) ! Y X and a training sample z 2 Z (1) to a real value is called an algorith-
mic luckiness. For all m 2 N, for any z 2 Z 2m , the lucky set HA (L; z)  Y X is the
set of all hypotheses that are learned from the rst m examples z  i (1) ; : : : ; z  i (m)

when permuting the whole sample z whilst not decreasing the luckiness, i.e.
HA (L; z) :=

A z  i (1) ; : : : ; z  i (m)

j i 2 IA (L; z)
	
; (2)
where
IA (L; z) :=

i 2 I 2m
  L A; z  i (1) ; : : : ; z  i (m)

 L (A; (z 1 ; : : : ; z m ))
	
: (3)
Given a xed loss function l : Y  Y ! [0; 1] the induced loss function set
L l (HA (L; z)) is dened by
L l (HA (L; z)) := f(x; y) 7! l (h (x) ; y) j h 2 HA (L; z)g :
For any luckiness function L and any learning algorithm A, the complexity of the
double sample z is the minimal number N 1 (; L l (HA (L; z)) ; z) of hypotheses ^ h 2
Y X needed to cover L l (HA (L; z)) at some predened scale  , i.e. for any hypothesis
h 2 HA (L; z) there exists a ^ h 2 Y X such that
1
2m
2m
X
i=1
  l (h (x i ) ; y i ) l

^ h (x i ) ; y i
     : (4)
To see this note that whenever J ( i (z)) is true (over the random draw of permu-
tations) then there exists a function ^ h which has a dierence in the training errors
on the double sample of at least "
2 + 2 . By an application of the union bound we
see that the number N 1 (; L l (HA (L; z)) ; z) is of central importance. Hence, if we
are able to bound this number over the random draw of the double sample z only
using the luckiness on the rst m examples we can use this bound in place of the
worst case complexity sup z2Z 2m N 1 (; L l (HA (L; z)) ; z) as usually done in the VC
framework (see [9]).

Denition 2 (!smallness of L). Given an algorithm A : Z (1) ! Y X and a loss
l : Y Y ! [0; 1] the algorithmic luckiness function L is !small at scale  2 R + if
for all m 2 N, all Æ 2 (0; 1] and all P Z
P Z 2m N 1 (; L l (HA (L; Z)) ; Z) > ! L A; Z [1:m]
 ; l; m; Æ; 

| {z }
S(Z)
< Æ :
Note that if the range of l is f0; 1g then N 1 1
2m ; L l (HA (L; z)) ; z

equals the num-
ber of dichotomies on z incurred by L l (HA (L; z)).
Theorem 1 (Algorithmic luckiness bounds). Suppose we have a learning
algorithm A : Z (1) ! Y X and an algorithmic luckiness L that is !small at
scale  for a loss function l : Y  Y ! [0; 1]. For any probability measure P Z ,
any d 2 N and any Æ 2 (0; 1], with probability at least 1 Æ over the random
draw of the training sample z 2 Z m of size m, if ! (L (A; z) ; l; m; Æ=4; )  2 d
then
R l [A (z)]  b
R l [A (z) ; z] +
s
8
m

d + log 2

4
Æ

+ 4 : (5)
Furthermore, under the above conditions if the algorithmic luckiness L is !
small at scale 1
2m for a binary loss function l (; ) 2 f0; 1g and b
R l [A (z) ; z] = 0
then
R l [A (z)]  2
m

d + log 2
 4
Æ

: (6)
Proof (Compressed Sketch). We will only sketch the proof of equation (5); the proof
of (6) is similar and can be found in [5]. First, we apply Lemma 1 with  (z) 
! (L (A; z) ; l; m; Æ=4; )  2 d . We now exploit the fact that
P Z 2m (J (Z)) = P Z 2m (J (Z) ^ S (Z))
| {z }
P Z 2m (S(Z))
+P Z 2m (J (Z) ^ :S (Z))
<
Æ
4
+ P Z 2m (J (Z) ^ :S (Z)) ;
which follows from Denition 2. Following the above-mentioned argument it suf-
ces to bound the probability of a random permutation  I (z) that J ( I (z)) ^
:S ( I (z)) is true for a xed double sample z. Noticing that  (z) ^ :S (z) )
N 1 (; L l (HA (L; z)) ; z)  2 d we see that we only consider swappings  i for which
N 1 (; L l (HA (L;  i (z))) ;  i (z))  2 d . Thus let us consider such a cover of
size not more than 2 d . By (4) we know that whenever J ( i (z)) ^ :S ( i (z))
is true for a swapping i then there exists a hypothesis ^ h 2 Y X in the cover
such that b
R l
h
^ h; ( I (z)) [(m+1):2m]
i b
R l
h
^ h; ( I (z)) [1:m]
i
> "
2 + 2 . Using the
union bound and Hoeding's inequality for a particular choice of P I shows that
P I (J ( I (z)) ^ :S ( I (z)))  Æ
4 which nalises the proof.
A closer look at (5) and (6) reveals that the essential dierence to uniform bounds
on the expected error is within the denition of the covering number: rather than
covering all hypotheses h in a given hypothesis space H  Y X for a given double
sample it suces to cover all hypotheses that can be learned by a given learning
algorithm from the rst half when permuting the double sample. Note that the
usage of permutations in the denition of (2) is not only a technical matter; it
fully exploits all the assumptions made for the training sample, namely the training
sample is drawn iid.

3 Relationship to Other Learning Frameworks
In this section we present the relationship of algorithmic luckiness to other learning
frameworks (see [9, 8, 6] for further details of these frameworks).
VC Framework If we consider a binary loss function l (; ) 2 f0; 1g and assume
that the algorithm A selects functions from a given hypothesis space H  Y X then
L (A; z) = VCDim (H) is a !small luckiness function where
!

L 0 ; l; m; Æ; 1
2m



2em
L 0
 L0
: (7)
This can easily be seen by noticing that the latter term is an upper bound on
max z2Z 2m jf(l (h (x 1 ) ; y 1 ) ; : : : ; l (h (x 2m ) ; y 2m )) : h 2 Hgj (see also [9]). Note that
this luckiness function neither exploits the particular training sample observed nor
the learning algorithm used.
Luckiness Framework Firstly, the luckiness framework of Shawe-Taylor et al. [8]
only considered binary loss functions l and the zero training error case. In this work,
the luckiness ~
L is a function of hypothesis and training samples and is called ~
!
small if the probability over the random draw of a 2m sample z that there exists a
hypothesis h with ~ !( ~
L(h; (z 1 ; : : : ; z m )); Æ) < N 1 ( 1
2m ; f(x; y) 7! l (g (x) ; y) j ~
L (g; z) 
~
L (h; z)g; z), is smaller than Æ. Although similar in spirit, the classical luckiness
framework does not allow exploitation of the learning algorithm used to the same
extent as our new luckiness. In fact, in this framework not only the covering number
must be estimable but also the variation of the luckiness ~
L itself. These dierences
make it very dicult to formally relate the two frameworks.
Compression Framework In the compression framework of Littlestone and
Warmuth [6] one considers learning algorithms A which are compression schemes,
i.e. A (z) = R (C (z)) where C (z) selects a subsample z  z and R : Z (1) ! Y X
is a permutation invariant reconstruction function. For this class of learning algo-
rithms, the luckiness L (A; z) = jC (z)j is !small where ! is given by (7). In
order to see this we note that (3) ensures that we only consider permutations  i
where C( i (z))  jC (z)j, i.e. we use not more than L training examples from
z 2 Z 2m . As there are exactly 2m
d

distinct choices of d training examples from
2m examples the result follows by application of Sauer's lemma [9]. Disregarding
constants, Theorem 1 gives exactly the same bound as in [6].
4 A New Margin Bound For Support Vector Machines
In this section we study the maximum margin algorithm for linear classiers, i.e. A :
Z (1) ! H where H := fx 7! h (x) ; wi j w 2 Kg and  : X ! K  ` n
2 is known
as the feature mapping. Let us assume that l (h (x) ; y) = l 0 1 (h (x) ; y) := I yh(x)0 .
Classical VC generalisation error bounds exploit the fact that VCDim (H ) = n and
(7). In the luckiness framework of Shawe-Taylor et al. [8] it has been shown that we
can use fat H ( z (w))  ( z (w)) 2 (at the price of an extra log 2 (32m) factor) in
place of VCDim (H ) where  z (w) = min (x i ;y i )2z y i h (x i ) ; wi = kwk is known as
the margin. Now, the maximum margin algorithm nds the weight vector wMM that
maximises  z (w). It is known that wMM can be written as a linear combination of
the  (x i ). For notational convenience, we shall assume that A : Z (1) ! R (1) maps

to the expansion coecients  such that kw k = 1 where w :=
P jzj
i=1  i  (x i ).
Our new margin bound follows from the following theorem together with (6).
Theorem 2. Let  i (x) be the smallest  > 0 such that f (x 1 ) ; : : : ;  (x m )g
can be covered by at most i balls of radius less than or equal . Let z (w) be
dened by z (w) := min (x i ;y i )2z
y i h(x i );wi
k(x i )kkwk . For the zero-one loss l 0 1 and
the maximum margin algorithm A, the luckiness function
L (A; z) = min
8 <
: i 2 N
      i 
 
 i (x)
P m
j=1 jA (z) j j
z wA(z)

! 2
9 =
; ; (8)
is !-small at scale 1=2m w.r.t. the function
!

L 0 ; l; m; Æ; 1
2m

=

2em
L 0
 2L0
: (9)
Proof (Sketch). First we note that by a slight renement of a theorem of Makovoz
[7] we know that for any z 2 Z m there exists a weight vector ~
w =
P m
i=1 ~
 i  (x i )
such that   ~
w wA(z)
  2
 2
z wA(z)

(10)
and ~
 2 R m has no more than L (A; z) non-zero components. Although only
wA(z) is of unit length, one can show that (10) implies that

 wA(z) ; ~
w= k ~
wk


q
1 2
z wA(z)

:
Using equation (10) of [4] this implies that ~
w correctly classies z 2 Z m . Consider
a xed double sample z 2 Z 2m and let k 0 := L (A; (z 1 ; : : : ; z m )). By virtue of (3)
and the aforementioned argument we only need to consider permutations  i such
that there exists a weight vector ~
w =
P m
j=1 ~
 j  (x j ) with no more than k 0 non-zero
~
 j . As there are exactly 2m
d
 distinct choices of d 2 f1; : : : ; k 0 g training examples
from the 2m examples z there are no more than (2em=k 0 ) k0 dierent subsamples
to be used in ~
w. For each particular subsample z  z the weight vector ~
w is a
member of the class of linear classiers in a k 0 (or less) dimensional space. Thus,
from (7) it follows that for the given subsample z there are no more (2em=k 0 ) k0
dierent dichotomies induced on the double sample z 2 Z 2m . As this holds for any
double sample, the theorem is proven.
There are several interesting features about this margin bound. Firstly, observe
that
P m
j=1 jA (z) j j is a measure of sparsity of the solution found by the maximum
margin algorithm which, in the present case, is combined with margin. Note that
for normalised data, i.e. k ()k = constant, the two notion of margins coincide,
i.e. z (w) =  z (w). Secondly, the quantity  i (x) can be considered as a measure
of the distribution of the mapped data points in feature space. Note that for all
i 2 N,  i (x)   1 (x)  max j2f1;:::;mg k (x j )k. Supposing that the two class-
conditional probabilities P XjY=y are highly clustered,  2 (x) will be very small. An
extension of this reasoning is useful in the multi-class case; binary maximum margin
classiers are often used to solve multi-class problems [9]. There appears to be also
a close relationship of  i (x) with the notion of kernel alignment recently introduced
in [3]. Finally, one can use standard entropy number techniques to bound  i (x) in
terms of eigenvalues of the inner product matrix or its centred variants. It is worth
mentioning that although our aim was to study the maximum margin algorithm the

above theorem actually holds for any algorithm whose solution can be represented
as a linear combination of the data points.
5 Conclusions
In this paper we have introduced a new theoretical framework to study the gen-
eralisation error of learning algorithms. In contrast to previous approaches, we
considered specic learning algorithms rather than specic hypothesis spaces. We
introduced the notion of algorithmic luckiness which allowed us to devise data de-
pendent generalisation error bounds. Thus we were able to relate the compression
framework of Littlestone and Warmuth with the VC framework. Furthermore, we
presented a new bound for the maximum margin algorithm which not only exploits
the margin but also the distribution of the actual training data in feature space.
Perhaps the most appealing feature of our margin based bound is that it natu-
rally combines the three factors considered important for generalisation with linear
classiers: margin, sparsity and the distribution of the data. Further research is
concentrated on studying Bayesian algorithms and the relation of algorithmic luck-
iness to the recent ndings for stable learning algorithms [2].
Acknowledgements This work was done while RCW was visiting Microsoft Re-
search Cambridge. This work was also partly supported by the Australian Research
Council. RH would like to thank Olivier Bousquet for stimulating discussions.
References
[1] M. Anthony and P. Bartlett. A Theory of Learning in Articial Neural Networks.
Cambridge University Press, 1999.
[2] O. Bousquet and A. Elissee. Algorithmic stability and generalization performance. In
T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information
Processing Systems 13, pages 196202. MIT Press, 2001.
[3] N. Cristianini, A. Elissee, and J. Shawe-Taylor. On optimizing kernel alignment.
Technical Report NC2-TR-2001-087, NeuroCOLT, http://www.neurocolt.com, 2001.
[4] R. Herbrich and T. Graepel. A PAC-Bayesian margin bound for linear classiers:
Why SVMs work. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances
in Neural Information Processing Systems 13, pages 224230, Cambridge, MA, 2001.
MIT Press.
[5] R. Herbrich and R. C. Williamson. Algorithmic luckiness. Technical report, Microsoft
Research, 2002.
[6] N. Littlestone and M. Warmuth. Relating data compression and learnability. Tech-
nical report, University of California Santa Cruz, 1986.
[7] Y. Makovoz. Random approximants and neural networks. Journal of Approximation
Theory, 85:98109, 1996.
[8] J. Shawe-Taylor, P. L. Bartlett, R. C. Williamson, and M. Anthony. Structural risk
minimization over data-dependent hierarchies. IEEE Transactions on Information
Theory, 44(5):19261940, 1998.
[9] V. Vapnik. Statistical Learning Theory. John Wiley and Sons, New York, 1998.
[10] V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative fre-
quencies of events to their probabilities. Theory of Probability and its Applications,
16(2):264281, 1971.

