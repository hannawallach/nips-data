Means, Correlations and Bounds
M.A.R. Leisink and H.J. Kappen
Department of Biophysics
University of Nijmegen, Geert Grooteplein 21
NL 6525 EZ Nijmegen, The Netherlands
fmartijn,bertg@mbfys.kun.nl
Abstract
The partition function for a Boltzmann machine can be bounded
from above and below. We can use this to bound the means and
the correlations. For networks with small weights, the values of
these statistics can be restricted to non­trivial regions (i.e. a subset
of [\Gamma1; 1]). Experimental results show that reasonable bounding
occurs for weight sizes where mean field expansions generally give
good results.
1 Introduction
Over the last decade, bounding techniques have become a popular tool to deal with
graphical models that are too complex for exact computation. A nice property of
bounds is that they give at least some information you can rely on. For instance,
one may find that a correlation is definitely between 0:4 and 0:6. An ordinary ap­
proximation might be more accurate, but in practical situations there is absolutely
no warranty for that.
The best known bound is probably the mean field bound, which has been described
for Boltzmann machines in [1] and later for sigmoid belief networks in [2]. Apart
from its bounding properties, mean field theory is a commonly used approximation
technique as well. Recently this first order bound was extended to a third order
approximation for Boltzmann machines and sigmoid belief networks in [3] and [4],
where it was shown that this particular third order expansion is still a bound.
In 1996 an upper bound for Boltzmann machines was described in [5] and [6]. In the
same articles the authors derive an upper bound for a special case of sigmoid belief
networks: the two­layered networks. In this article we will focus solely on Boltzmann
machines, but an extension to sigmoid belief networks is quite straightforward.
This article is organized as follows: In section 2 we start with the general theory
about bounding techniques. Later in that section the upper and lower bound are
briefly described. For a full explanation we refer to the articles mentioned before.
The section is concluded by explaining how these bounds on the partition function
can be used to bound means and correlations. In section 3 results are shown for
fully connected Boltzmann machines, where size of weights and thresholds as well as
network size are varied. In section 4 we present our conclusions and outline possible
extensions.

2 Theory
There exists a general method to create a class of polynomials of a certain order,
which all bound a function of interest, f 0 (x). Such a class of order 2n can be
found if the 2n­th order derivative of f 0 (x), written as f 2n (x), can be bounded by
a constant. When this constant is zero, the class is actually of order 2n\Gamma1. It turns
out that this class is parameterized by n free parameters.
Suppose we have a function b 2k for some integer k which bounds the function f 2k
from below (an upper bound can be written as a lower bound by using the negative
of both functions). Thus
8 x f 2k (x) – b 2k (x) (1)
Now construct the primitive functions f 2k\Gamma1 and b 2k\Gamma1 such that f 2k\Gamma1 (¯) =
b 2k\Gamma1 (¯) for a free to choose value for ¯. This constraint can always be achieved by
adding an appropriate constant to the primitive function b 2k\Gamma1 . It is easy to prove
that ae
f 2k\Gamma1 (x) Ÿ b 2k\Gamma1 (x) for x Ÿ ¯
f 2k\Gamma1 (x) – b 2k\Gamma1 (x) for x – ¯ (2)
or in shorthand notation f 2k\Gamma1 (x) 7 b 2k\Gamma1 (x).
If we repeat this procedure and construct the primitive functions f 2k\Gamma2 and b 2k\Gamma2
such that f 2k\Gamma2 (¯) = b 2k\Gamma2 (¯) for the same ¯, one can verify that
8 x f 2k\Gamma2 (x) – b 2k\Gamma2 (x) (3)
Thus given a bound f 2k (x) – b 2k (x) we can construct a class of bounding functions
for f 2k\Gamma2 parameterized by ¯.
Since we assumed f 2n (x) can be bounded from below by a constant, we can apply the
procedure n times and we finally find f 0 (x) – b 0 (x), where b 0 (x) is parameterized
by n free parameters. This procedure can be found in more detail in [4].
2.1 A third order lower bound for Boltzmann machines
Boltzmann machines are stochastic neural networks with N binary valued neurons,
s i , which are connected by symmetric weights w ij . Due to this symmetry the
probability distribution is a Boltzmann­Gibbs distribution which is given by (see
also [7])
p (~s j`; w) = 1
Z
exp
0
@ 1
2
X
ij
w ij s i s j +
X
i
` i s i
1
A = 1
Z
exp (\GammaE (~s; `; w)) (4)
where the ` i are threshold values and
Z (`; w) =
X
all ~s
exp (\GammaE (~s; `; w)) (5)
is the normalization known as the partition function.
This partition function is especially important, since statistical quantities such as
means and correlations can be directly derived from it. For instance, the means can
be computed as
hs n i =
X
all ~s
p (~s j`; w) s n =
X
all ~s=s n
p (~s; s n =+1j`; w) \Gamma p (~s; s n = \Gamma1j`; w)
= Z+ (`; w) \Gamma Z \Gamma (`; w)
Z (`; w)
(6)

where Z+ and Z \Gamma are partition functions over a network with s n clamped to +1
and \Gamma1, respectively.
This explains why the objective of almost any approximationmethod is the partition
function given by equation 5. In [3] and [4] it is shown that the standard mean field
lower bound can be obtained by applying the linear bound
8 x;¯ e x – e ¯ (1 + x \Gamma ¯) (7)
to all exponentially many terms in the sum. Since ¯ may depend on ~s, one can
choose ¯ (~s ) = ¯ i s i + ¯ 0 , which leads to the standard mean field equations, where
the ¯ i turn out to be the local fields.
Moreover, the authors show that one can apply the procedure of `upgrading bounds'
(which is described briefly at the beginning of this section) to equation 7, which
leads to the class of third order bounds for e x . This is achieved in the following
way:
8 x;š f 2 (x) = e x – e š (1 + x \Gamma š) = b 2 (x)
f 1 (x) = e x 7 e ¯ + e š
`
(1 + ¯ \Gamma š) (x \Gamma ¯) + 1
2 (x \Gamma ¯) 2
'
= b 1 (x) (8)
8 x;¯;– f 0 (x) = e x – e ¯
ae
1 + x \Gamma ¯ + e –
` 1 \Gamma –
2 (x \Gamma ¯) 2 + 1
6 (x \Gamma ¯) 3
'oe
= b 0 (x)
with – = š \Gamma ¯.
In principle, this third order bound could be maximized with respect to all the free
parameters, but here we follow the suggestion made in [4] to use a mean field opti­
mization, which is much faster and generally almost as good as a full optimization.
For more details we refer to [4].
2.2 An upper bound
An upper bound for Boltzmann machines has been described in [5] and [6] 1 . Basi­
cally, this method uses a quadratic upper bound on log cosh x, which can easily be
obtained in the following way:
f 2 (x) = 1 \Gamma tanh 2 x Ÿ 1 = b 2 (x)
f 1 (x) = tanh x ? x \Gamma ¯ + tanh ¯ = b 1 (x) (9)
f 0 (x) = log cosh x Ÿ
1
2 (x \Gamma ¯) 2 + (x \Gamma ¯) tanh ¯ + log cosh ¯ = b 0 (x)
Using this bound, one can derive
Z (`; w) =
X
all ~s
exp
0
@ 1
2
X
ij
w ij s i s j +
X
i
` i s i
1
A
=
X
all ~s=s n
2 exp
/
log cosh
/ X
i
w ni s i + ` n
!!
exp
0
@ 1
2
X
ij 6=n
w ij s i s j +
X
i6=n
` i s i
1
A
Ÿ
X
all ~s=s n
exp
0
@ 1
2
X
ij 6=n
w 0
ij s i s j +
X
i6=n
` 0
i s i + k
1
A = e k \Delta Z (` 0 ; w 0 ) (10)
1 Note: The articles referred to, use s i 2 f0; 1g instead of the +1=\Gamma1 coding used here.

where k is a constant and ` 0 and w 0 are thresholds and weights in a reduced network
given by
w 0
ij = w ij + w ni w nj
` 0
ij = ` i +w ni (` n \Gamma ¯n + tanh¯n ) (11)
k = 1
2 (` n \Gamma ¯n + tanh ¯n ) 2
\Gamma
1
2 tanh 2 ¯n + log 2 cosh ¯n
Hence, equation 10 defines a recursive relation, where each step reduces the network
by one neuron. Finally, after N steps, an upper bound on the partition function is
found 2 . We did a crude minimization with respect to the free parameters ¯. A more
sophisticated method can probably be found, but this is not the main objective of
this article.
2.3 Bounding means and correlations
The previous subsections showed very briefly how we can obtain a lower bound, Z L ,
and an upper bound, Z U , for any partition function. We can use this in combination
with equation 6 to obtain a bound on the means:
hs n i L = Z L
+ \Gamma Z U
\Gamma
X
Ÿ hs n i Ÿ
Z U
+ \Gamma Z L
\Gamma
Y
= hs n i U (12)
where X = Z U if the nominator is positive and X = Z L otherwise. For Y it is the
opposite. The difference, hs n i U
\Gamma hs n i L , is called the bandwidth.
Naively, we can compute the correlations similarly to the means using
hs n s m i = Z++ + Z \Gamma\Gamma \Gamma Z+\Gamma \Gamma Z \Gamma+
Z
(13)
where the partition function is computed for all combinations s n s m . Generally,
however, this gives poor results, since we have to add four bounds together, which
leads to a bandwidth which is about twice as large as for the means. We can
circumvent this by computing the correlations using
hs n s m i = Z (`; wjsn = s m ) \Gamma Z (`; wjsn = \Gammas m )
Z
(14)
where we allow the sum in the partition functions to be taken over s n , but fix s m
either to s n or its negative. Finally, the computation of the bounds hs n s m i L and
hs n s m i U is analogue to equation 12.
There exists an alternative way to bound the means and correlations. One can write
hs n i = Z+ \Gamma Z \Gamma
Z+ + Z \Gamma
= Z+ =Z \Gamma \Gamma 1
Z+ =Z \Gamma + 1 = z \Gamma 1
z + 1 = f (z) (15)
with z = Z+ =Z \Gamma , which can be bounded by
Z L
+
Z U
\Gamma
Ÿ z Ÿ
Z U
+
Z L
\Gamma
(16)
Since f (z) is a monotonically increasing function of z, the bounds on hs n i are given
by applying this function to the left and right side of equation 16. The correlations
can be bounded similarly. It is still unknown whether this algorithm would yield
better results than the first one, which is explored in this article.
2 The original articles show that it is not necessary to do all the N steps. However,
since this is based on mixing approximation techniques with exact calculations, it is not
used here as it would hide the real error the approximation makes.

0 0.2 0.4 0.6 0.8 1
10
10.5
11
11.5
12
12.5
13
s w
log
Z
Exact
Mean field lower bound
Upper bound
Third order lower bound
Figure 1: Comparison of 1) the mean field lower bound, 2) the upper bound and
3) the third order lower bound with the exact log partition function. The network
was a fully connected Boltzmann machine with 14 neurons and oe ` = 0:2. The size
of the weights is varied on the x­axis. Each point was averaged over 100 networks.
3 Results
In all experiments we used fully connected Boltzmann machines of which the thresh­
olds and weights both were drawn from a Gaussian with zero mean and standard
deviation oe ` and oe w =
p
N , respectively, where N is the network size. This is the so
called sk­model (see also [8]). Generally speaking, the mean field approximation
breaks down for oe ` = 0 and oe w ? 0:5, whereas it can be proven that any expansion
based approximation is inaccurate when oe w ? 1 (which is the radius of convergence
as in [9]). If oe ` 6= 0 these maximum values are somewhat larger.
In figure 1 we show the logarithm of the exact partition function, the first order
or mean field bound, the upper bound (which is roughly quadratic) and the third
order lower bound. The weight size is varied along the horizontal axis. One can
see clearly that the mean field bound is not able to capture the quadratic form of
the exact partition function for small weights due to its linear behaviour. The error
made by the upper and third order lower bound is small enough to make non­trivial
bounds on the means and correlations.
An example of this bound is shown in figure 2 for the specific choice oe ` = oe w = 0:4.
For both the means and the correlations a histogram is plotted for the upper and
lower bounds computed with equation 12. Both have an average bandwidth of
0:132, which is a clear subset of the whole possible interval of [\Gamma1; 1].
In figure 3 the average bandwidth is shown for several values of oe ` and oe w . For
bandwidths of 0:01, 0:1 and 1 a line is drawn. We conclude that almost everywhere
the bandwidth is non­trivially reduced and reaches practically useful values for oe w
less than 0:5. This is more or less equivalent to the region where the mean field
approximation performs well. That approximation, however, gives no information
on how close it actually is to the exact value, whereas the bounding method limits
it to a definite region.

-0.2 -0.1
20
40
60
80
0 0.1 0.2
20
40
60
80
Means
Distance to exact
-0.2 -0.1
100
200
300
400
500
600
Correlations
0 0.1 0.2
100
200
300
400
500
600
Distance to exact
Figure 2: For the specific choice oe ` = oe w = 0:4 thirty fully connected Boltzmann
machines with 14 neurons were initialized and the bounds were computed. The two
left panels show the distance between the lower bound and the exact means (left)
and similarly for the upper bound (right). The right two panels show the distances
of both bounds for the correlations.
0
0.5
1
1.5
2
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
s w
s q
0.01
0.1
1
0
0.5
1
1.5
2
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
s w
s q
0.01
0.1
1
Figure 3: In the left panel the average bandwidth is colour coded for the means,
where oe ` and oe w are varied in ten steps along the axes. The right panel shows
the same for the correlations. For each oe ` ; oe w thirty fully connected Boltzmann
machines were initialized and the bounds on all the means and correlations were
computed. For three specific bandwidths a line is drawn.

0 10 20 30 40
0
0.002
0.004
0.006
0.008
0.01
s q =0.3
s w =0.1
Bandwidth
0 10 20 30 40
0
0.1
0.2
0.3
0.4
s q =0.3
s w =0.3
Network size 0 10 20 30 40
0
0.5
1
1.5
2
s q =0.3
s w =0.5
Figure 4: For oe w = 0:1, 0:3 and 0:5 the bandwidth for the correlations is shown
versus the network size. oe ` = 0:3 in all cases, but the plots are nearly the same for
other values. Please note the different scales for the y­axis. A similar graph for the
means is not shown here, but it is roughly the same. The solid line is the average
bandwidth over all correlations, whereas the dashed lines indicate the minimumand
maximum bandwidth found.
Unfortunately, the bounds have the unwanted property that the error scales badly
with the size of the network. Although this makes the bounds unsuitable for very
large networks, there is still a wide range of networks small enough to take advan­
tage of the proposed method and still much too large to be treated exactly. The
bandwidth versus network size is shown in figure 4 for three values of oe w . Obviously,
the threshold of practical usefulness is reached earlier for larger weights.
Finally, we remark that the computation time for the upper bound is O
\Gamma
N 4
\Delta and
O
\Gamma
N 3
\Delta
for the mean field and third order lower bound. This is not shown here.
4 Conclusions
In this article we combined two already existing bounds in such a way that not only
the partition function of a Boltzmann machine is bounded from both sides, but also
the means and correlations. This may seem superfluous, since there exist already
several powerful approximation methods. Our method, however, can be used apart
from any approximation technique and gives at least some information you can rely
on. Although approximation techniques might do a good job on your data, you
can never be sure about that. The method outlined in this paper ensures that the
quantities of interest, the means and correlations, are restricted to a certain region.
We have seen that, generally speaking, the results are useful for weight sizes where
an ordinary mean field approximation performs well. This makes the method ap­
plicable to a large class of problems. Moreover, since many architectures are not
fully connected, one can take advantage of that structure. At least for the upper
bound it is shown already that this can improve computation speed and tightness.
This would partially cancel the unwanted scaling with the network size.
Finally, we would like to give some directions for further research. First of all, an
extension to sigmoid belief networks can easily be done, since both a lower and an
upper bound are already described. The upper bound, however, is only applicable to
two layer networks. A more general upper bound can probably be found. Secondly
one can obtain even better bounds (especially for larger weights) if the general
constraint
8nm \Gamma 1 + jhs n i + hs m ij Ÿ hs n s m i Ÿ 1 \Gamma jhs n i \Gamma hs m ij (17)
is taken into account. This might even be extended to similar constraints, where
three or more neurons are involved.

Acknowledgements
This research is supported by the Technology Foundation stw, applied science
devision of nwo and the technology programme of the Ministry of Economic Affairs.
References
[1] C. Peterson and J. Anderson. A mean field theory learning algorithm for neural net­
works. Complex systems, 1:995--1019, 1987.
[2] S.K. Saul, T.S. Jaakkola, and M.I. Jordan. Mean field theory for sigmoid belief net­
works. Journal of Artificial Intelligence Research, 4:61--76, 1996.
[3] Martijn A.R. Leisink and Hilbert J. Kappen. A tighter bound for graphical models. In
Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural
Information Processing Systems 13, pages 266--272. MIT Press, 2001.
[4] Martijn A.R. Leisink and Hilbert J. Kappen. A tighter bound for graphical models.
Neural Computation, 13(9), 2001. To appear.
[5] T. Jaakkola and M.I. Jordan. Recursive algorithms for approximating probabilities in
graphical models. MIT Comp. Cogn. Science Technical Report 9604, 1996.
[6] Tommi S. Jaakkola and Michael I. Jordan. Computing upper and lower bounds on
likelihoods in intractable networks. In Proceedings of the Twelfth Annual Conference
on Uncertainty in Artificial Intelligence (UAI--96), pages 340--348, San Francisco, CA,
1996. Morgan Kaufmann Publishers.
[7] D. Ackley, G. Hinton, and T. Sejnowski. A learning algorithm for Boltzmann machines.
Cognitive Science, 9:147--169, 1985.
[8] D. Sherrington and S. Kirkpatrick. Solvable model of a spin­glass. Physical Review
Letters, 35(26):1793--1796, 12 1975.
[9] T. Plefka. Convergence condition of the tap equation for the infinite­ranged ising spin
glass model. J.Phys.A: Math.Gen., 15:1971--1978, 1981.

