On the Convergence of Leveraging 
Gunnar R˜ atsch, y Sebastian Mika z and Manfred K. Warmuth x
y RSISE, Australian National University, Canberra, ACT 0200 Australia
z Fraunhofer FIRST, Kekul’estr. 7, 12489 Berlin, Germany
x University of California at Santa Cruz, CA 95060, USA
raetsch@csl.anu.edu.au, mika@first.fhg.de, manfred@cse.ucsc.edu
Abstract
We give an unified convergence analysis of ensemble learning meth­
ods including e.g. AdaBoost, Logistic Regression and the Least­Square­
Boost algorithm for regression. These methods have in common that
they iteratively call a base learning algorithm which returns hypotheses
that are then linearly combined. We show that these methods are related
to the Gauss­Southwell method known from numerical optimization and
state non­asymptotical convergence results for all these methods. Our
analysis includes ` 1 ­norm regularized cost functions leading to a clean
and general way to regularize ensemble learning.
1 Introduction
We show convergence rates of ensemble learning methods such as AdaBoost [10], Logistic
Regression (LR) [11, 5] and the Least­Square (LS) regression algorithm called LS­Boost
[12]. These algorithms have in common that they iteratively call a base learning algorithm
L (also called weak learner) on a weighted training sample. The base learner is expected
to return in each iteration t a hypothesis ^ h t from some hypothesis set of weak hypotheses
H that has small weighted training error. This is the weighted number of false predictions
in classification and weighted estimation error in regression. These hypotheses are then
linearly combined to form the final hypothesis f ^
 (x) =
P
t ^
 t ^
h t (x); in classification one
uses the sign of f ^
 for prediction. The hypothesis coefficient ^
 t is determined at iteration
t, such that a certain objective is minimized or approximately minimized, and is fixed for
later iterations. Here we will work out sufficient conditions on the base learning algorithm
to achieve linear convergence to the minimum of an associated loss function G. This means
that for any starting condition the minimum can be reached with precision  > 0 in only
O(log(1=)) iterations.
Relation to Previous Work In the original work on AdaBoost it has been shown that
the optimization objective (which is an upper bound on the training error) converges ex­
ponentially fast to zero, if the base learner is consistently better than random guessing, i.e.
its weighted training error  is always smaller than some constant  with  < 1
2 . In this
case the convergence is known to be linear (i.e. exponentially decreasing) [10]. One can
easily show that this is the case when the data is separable: 1 If the data is not separable, the
 Supported by DFG grants MU 987/1­1, JA 379/9­1 and NSF grant CCR 9821087; we gratefully
acknowledge help from B. Borchers, P. Spellucci, R. Israel and S. Lemm. This work has been done,
while G. R˜atsch was at Fraunhofer FIRST, Berlin.
1 We call the data separable, if there exists  such that f (x) separates the training examples.

weighted training error  cannot be upper bounded by a constant smaller 1
2 , otherwise one
could use AdaBoost to find a separation using the aforementioned convergence result. 2
For AdaBoost and Logistic Regression it has been shown [5] that they generate a combined
hypothesis asymptotically minimizing a loss functional G only depending on the output of
the combined hypothesis f . This holds for the non­separable case; however, the assumed
conditions in [5] on the performance of the base learner are rather strict and can usually
not be satisfied in practice. Although the analysis in [5] holds in principle for any strictly
convex cost function of Legendre­type (e.g. [24], p. 258, and [1]), one needs to show the
existence of a so­called auxiliary function [7, 5] for each cost function other than the expo­
nential or the logistic loss. This can indeed be done [cf. 19, Section 4.2], but in any case
only leads to asymptotical results. In the present work we can also show rates of conver­
gence.
In an earlier attempt to show the convergence of such methods for arbitrary loss functions
[17], one needed to assume that the hypothesis coefficients ^
 t are upper bounded by a
rather small constant. For this case it has been shown that the algorithm asymptotically
converges to a combined hypothesis minimizing G. However, since the ^
 t 's need to be
small, the algorithm requires many iterations to achieve this goal.
In [9] it has been shown that for loss functions which are (essentially) exponentially de­
creasing (including the loss functions of AdaBoost and Logistic regression), the loss is
O(1=
p
t) in the first ~ t iterations and afterwards O( ~
t t ). This implies linear convergence.
However, this only holds, if the loss reaches zero, i.e. if the data is separable. In our work
we do not need to assume separability.
An equivalent optimization problem for AdaBoost has also been considered in a paper that
predates the formulation of AdaBoost [4]. This optimization problem concerns the likeli­
hood maximization for some exponential family of distributions. In this work convergence
is proven for the general non­separable case, however, only for the exponential loss, i.e. for
the case of AdaBoost. 3 The framework set up in this paper is more general and we are able
to treat any strictly convex loss function.
In this paper we propose a family of algorithms that are able to generate a combined hy­
pothesis f converging to the minimum of G[f ] (if it exists), which is a functional depending
on the outputs of the function f evaluated on the training set. Special cases are AdaBoost,
Logistic Regression and LS­Boost. While assuming mild conditions on the base learning
algorithm and the loss function G, we can show linear convergence rates [15] (beginning
in the first iteration) of the type G[f t+1 ] G[f  ]  (G[f t ] G[f  ]) for some fixed
 2 [0; 1). This means that the difference to the minimum loss converges exponentially
fast to zero (in the number of iterations). A similar convergence has been proven for Ada­
Boost in the special case of separable data [10], although the constant  shown in [10] can
be considerable smaller [see also 9]. To prove the convergence of leveraging, we exploit
results of Luo & Tseng [16] for a variant of the Gauss­Southwell method known from nu­
merical optimization.
Since in practice the hypothesis set H can be quite large, ensemble learning algorithms
without any regularization often suffer from overfitting [22, 12, 2, 19]. Here, the com­
plexity can only be controlled by the size of the base hypothesis set or by early stopping
after a few iterations. However, it has been shown that shrinkage regularization implied
by penalizing some norm of the hypothesis coefficients is the favorable strategy [6, 12].
We therefore extend our analysis to the case of ` 1 ­norm regularized loss functions. With
a slight modification this leads to a family of converging algorithms that e.g. includes the
Leveraged Vector Machine [25] and a variant of LASSO [26].
In the following section we briefly review AdaBoost, Logistic Regression, and LS­Boost
and cast them in a common framework. In Sec. 3 we present our main results. After re­
2 This can also be seen when analyzing a certain linear program in the dual domain (cf. [23])
3 We will expand on this connection in the full paper (see also [14, 19]).

lating these results to leveraging algorithms, we present an extension to regularized cost
functions in Sec. 4 and finally conclude.
2 Leveraging algorithms revisited
We first briefly review some of the most well known leveraging algorithms for classification
and regression. For more details see e.g. [10, 11, 12, 8]. We work with Alg. 1 as a template
for a generic leveraging algorithm, since these algorithms have the same algorithmical
structure. Finally, we will generalize the problem and extend the notation.
AdaBoost & Logistic Regression are designed for classification tasks. In each iteration
they call a base learning algorithm on the training set S = f(x 1 ; y 1 ); : : : ; (xn ; yn )g  X 
f1;+1g (cf. step 3a in Alg. 1). Here a weighting d t = [d t
1 ; : : : ; d t
N ] on the sample is used
that is recomputed in each iteration t. The base learner is expected to return a hypothesis
^
h t from some hypothesis space 4 H := fh j j h j : X 7! f1;+1g; j = 1; : : : ; Jg that
has small weighted classification error 5  t =
P N
n=1 jd t
n jI(y n 6= ^ h t (xn )) [10, 11], where
I(true) = 1 and I(false) = 0. It is more convenient to work with the edge of ^
h t , which is
defined as  t = 1 2 t =
P N
n=1 d t
n
^ h t (xn ). After selecting the hypothesis, its weight ^
 t
is computed such that it minimizes a certain functional (cf. step 3b). For AdaBoost this is
G AB (^) =
XN
n=1
exp
n
yn

^
 ^ h t (xn ) + f t 1 (xn )
o
(1)
and for Logistic Regression it is
G LR (^) =
XN
n=1
log
n
1 + exp

yn (^ ^ h t (xn ) + f t 1 (xn ))
o
; (2)
where f t 1 is the combined hypothesis of the previous iteration given by f t 1 (xn ) =
P t 1
r=1 ^
 r ^ h r (xn ). For AdaBoost it has been shown that ^
 t minimizing (1) can be com­
puted analytically [3]. This is true because we assumed that the hypotheses are binary
valued. Similarly, for LR there exists an analytic solution of (2). The weighting d on the
sample is updated based on the new combined hypothesis f t (xn ) = ^
 ^ h t (xn ) + f t 1 (xn ):
d t+1
n = yn exp( yn f t (xn )) and d t+1
n = yn exp( ynf t (xn ))
1+exp( ynf t (xn )) ; for AdaBoost and Logistic
Regression, respectively.
Least­Square­Boost is an algorithm to solve regression tasks. In this case S =
f(x 1 ; y 1 ); : : : ; (xn ; yn )g  X Y , Y  R and H := fh j j h j : X 7! Y ; j = 1; : : : ; Jg. It
works in a similar way as AdaBoost and LR. It first selects a hypothesis solving
^ h t = argmin
^ h2H
1
2
XN
n=1

d t
n
^ h(xn )
 2
; (3)
and then finds the hypothesis weight ^
 t by minimizing the squared error of the new com­
bined hypothesis:
G LS (^) =
1
2
XN
n=1

yn f t 1 (xn ) ^
 ^ h t (xn )
 2
: (4)
The ``weighting'' of the sample is computed as d t+1
n = yn f t (xn ), which is the residual
of f t [12]. In a second version of LS­Boost, the base hypothesis and its weight are found
simultaneously by solving [12]:
[ ^
h t ; ^
 t ] = argmin
^
2R; ^ h2H
1
2
XN
n=1

yn f t 1 (xn ) ^
 ^ h(xn )
 2
(5)
Since in (5) one reaches a lower loss function value than with (3) and (4), it might be the
favorable strategy.
4 Notice that H always contains only a finite number of different hypotheses when evaluated on
the training set and is effectively finite [2].
5 Different from common convention, we include the yn in dn to make the presentation simpler.

Algorithm 1 -- A Leveraging algorithm for the loss function G.
1. Input: S = h(x1 ; y1 ); : : : ; (xN ; yN )i, No. of Iterations T , Loss function G : R N ! R
2. Initialize: f0  0, d 1
n = g'(yn ; f0 (xn)) for all n = 1 : : : N
3. Do for t = 1; : : : ; T ,
(a) Train classifier on fS; d t g and obtain hypothesis ^ h t : X ! Y
(b) Set ^
 t = argmin 2R G[f t +  ^ h t ]
(c) Update f t+1 = f t + ^
 t ^ h t and d t+1
n = g'

yn ;
P t
r=1 ^ r ^ hr (xn)

4. Output: fT
The General Case These algorithms can be summarized in Alg. 1 (where case (5) is
slightly degenerated, cf. Sec. 3.2) for some appropriately defined functions G and g': plug­
in G[f ] =
P N
n=1 g(y n ; f(xn )) and choosing g as g(y; f(x)) = exp( yf(x)) for Ada­
Boost (cf. (1)), g(y; f(x)) = log(1 + exp( yf(x))) for Logistic Regression (cf. (2)) and
g(y; f(x)) = 1
2 (y f(x)) 2 for LS­Boost (cf. (4)).
It can easily be verified that the function g', used for computing the weights d, is the deriva­
tive of g with respect to the second argument [3, 12].
The Optimization Problem It has been argued in [3, 18, 11, 17] and finally shown in
[5] that AdaBoost and Logistic Regression under certain condition asymptotically con­
verge to a combined hypothesis f minimizing the respective loss G on the training sam­
ple, where f is a linear combination of hypotheses from H, i.e. f 2 lin(H) :=
n P J
j=1  j h j jh j 2 H; j 2 R
o
. Thus, they solve the optimization problem:
min f2lin(H) G[f ] = min 2R J G(H); (6)
where we defined a matrix H 2 R NJ with H ij = h j (x i ).
To avoid confusions, note that hypotheses and coefficients generated during the iterative
algorithm are marked by a hat. In the algorithms discussed so far, the optimization takes
place by employing the leveraging scheme outlined in Alg. 1. The output of such an algo­
rithm is a sequence of pairs (^ t ; ^ h t ) and a combined hypothesis f(x) =
P t
r=1 ^
 r ^ h r (x).
With  t
j =
P t
r=1 ^
 r I( ^ h r = h j ), j = 1; : : : ; J , it is easy to verify that
P t
r ^
 r ^ h r (x) =
P J
j=1  t
j h j (x), which is in lin(H) (note the missing hat).
Other Preliminaries Throughout the paper we assume the loss function G is of the form
G[f ] = G(H) =
P N
n=1 g(y n ; f (xn ));
Although, this assumption is not necessary, the presentation becomes easier. In [7, 5, 19]
a more general case of Legendre­type cost functions is considered. However, note that
additive loss functions are commonly used, if one considers i.i.d.­drawn examples.
We assume that each element H nj and yn is finite (j = 1; : : : ; J , n = 1; : : : ; N ) and H
does not contain a zero column. Furthermore, the function g(y; ) : R ! R is assumed to
be strictly convex for all y 2 Y .
For simplicity we assume for the rest of the paper that H is finite and complementation
closed, i.e. for every h 2 H there exists also h 2 H. The assumption on the finiteness is
not crucial for classification (cf. footnote 4). For regression problems the hypothesis space
might be infinite. This case has explicitly been analyzed in [20, 19] and goes beyond the
scope of this paper (see also [27]).
3 Main Result
We now state a result known from the field of numerical optimization. Then we show how
the reviewed leveraging algorithms fit into this optimization framework.

3.1 Coordinate Descent
The idea of coordinate descent is to iteratively select a coordinate, say the j­th, and find
 j such that some functional F([ 1 ; : : : ;  j ; : : : ;  T ] > ) is minimized with respect to  j .
There exist several different strategies for selecting the coordinates [e.g. 15]; however, we
are in particular interested in the Gauss­Southwell­type (GS) selection scheme: It selects
the coordinate that has the largest absolute value in the gradient vector  := r F(), i.e.
j = argmax j 0 =1;:::;J j j 0 j. Instead of doing steps in the direction of the negative gradient
as in standard gradient descent methods, one only changes the variable that has the largest
gradient component. This can be efficient, if there are many variables and most of them are
zero at the minimum.
We start with the following general convergence result, which seemed to be fallen into
oblivion even in the optimization community. It will be very useful in the analysis of
leveraging algorithms. Due to a lack of space we omit proofs (see [21, 19]).
Theorem 1 (Convergence of Coordinate Descent [16]). Suppose G : R N !R is twice
continuously differentiable and strictly convex on domG. Assume that domG is open, the
set of solutions S   R J to
min
2S
F() := G(H) +  >  (7)
is not empty, where H 2 R NJ is a fixed matrix having no zero column,  2 R J fixed
and S  R J is a (possibly unbounded) box­constrained set. Furthermore assume that
the Hessian r 2 G(H  ) is a positive matrix for all   2 S  . Let f t g be the sequence
generated by coordinate descent, where the coordinate selection j 1 ; j 2 ; : : : satisfies
j t+1;j t
j t
 t
j t
j   max
j=1;:::;J
j t+1;j
j  t
j j (8)
for some  2 (0; 1], where  t+1;j
j is the optimal value of  t+1
j if it would be selected, i.e.
 t+1;j
j := min
2S j
G H t +H j (  t
j )

+  j : (9)
Then f t g converges to an element in S  .
The coordinate selection in Thm. 1 is slightly different from the Gauss­Southwell selection
rule described before. We therefore need the following:
Proposition 2 (Convergence of GS on R J ). Assume the conditions on G and H as in
Thm. 1. Let S b be a convex subset of S := R J such that  t;j 2 S b . Assume
@ 2 G(H)
@ 2
j
  u and @ 2 G(H)
@ 2
j
  l 8 2 S (10)
holds for some fixed  l ;  u > 0. Then a coordinate selection j 1 ; j 2 ; : : : satisfies (8) of
Thm. 1, if there exists a fixed Æ 2 (0; 1] such that
    
@ F( t )
@ t
j t
      Æ max
j=1;:::;J
    
@ F( t )
@ t
j
     8t = 1; 2; : : : (11)
Thus the approximate Gauss­Southwell method on R J as described above converges. To
show the convergence of the second variant of LS­Boost (cf. (5)) we need the following
Proposition 3 (Convergence of the maximal improvement scheme on R J ). Let G; H;S
and S b as in Proposition 2 and assume (10) holds. Then a coordinate selection j 1 ; j 2 ; : : :
satisfies (8), if there exists a fixed Æ 2 (0; 1] with
F( t ) F( t+1;j t )  Æ max
j=1;:::;J
F( t ) F( t+1;j )

8t = 1; 2; : : : (12)

Thus the maximal improvement scheme on R J as above converges in the sense of Thm. 1.
Finally we can also state a rate of convergence, which is surprisingly not worse than the
rates for standard gradient descent methods:
Theorem 4 (Rate of Convergence of Coordinate Descent, [16]). Assume the conditions
of Thm. 1 hold. Let S b as in Prop. 2 and assume (10) holds for some  l > 0. Then we have
 t+1 := F( t+1 ) F(  ) 

1 1


(F( t ) F(  )); (13)
where  t is the estimate after the t­th coordinate descent step,   denotes a optimal solu­
tion, and 0 <  < 1. Especially at iteration t:  t  (1 1=) t  0 .
Following [16] one can show that the constant  is O(  2 LJ 4 N 2
Æ 2 ), where L is the Lipschitz
constant of rG and  is a constant that depends on H and therefore on the geometry of
the hypothesis set (cf. [16, 13] for details). While the upper bound on  can be rather large,
making the convergence slow, it is important to note (i) that this is only a rough estimate
of the true constant and (ii) still guarantees an exponential decrease in the error functional
with the number of iterations.
3.2 Leveraging and Coordinate Descent
We now return from the abstract convergence results in Sec. 3.1 to our examples of lever­
aging algorithms, i.e. we show how to retrieve the Gauss­Southwell algorithm on R J as a
part of Alg. 1. For now we set  = 0. The gradient of G with respect to  j is given by
@ G()
@ j
=
P N
n=1 g'(y n ; f (xn ))h j (xn ) =
P N
n=1 dn h j (xn ) (14)
where dn is given as in step 3c of Alg. 1. Thus, the coordinate with maximal absolute gra­
dient corresponds to the hypothesis with largest absolute edge (see definition). However,
according to Proposition 2 and 3 we need to assume less on the base learner. It either has
to return a hypothesis that (approximately) maximizes the edge, or alternatively (approxi­
mately) minimizes the loss function.
Definition 5 (Æ­Optimality). A base learning algorithm L is called Æ­optimal, if it always
returns hypotheses that either satisfy condition (11) or (12) for some fixed Æ > 0.
Since we have assumed H is closed under complementation, there always exist two hy­
potheses having the same absolute gradient (h and h). We therefore only need to consider
the hypothesis with maximum edge as opposed to the maximum absolute edge.
For classification it means: if the base learner returns the hypothesis with approximately
smallest weighted training error, this condition is satisfied. It is left to show that we can
apply the Thm. 1 for the loss functions reviewed in Sec. 2:
Lemma 6. The loss functions of AdaBoost, Logistic regression and LS­Boost are bounded,
strongly convex and fulfill the conditions in Thm. 1 on any bounded subset of R N .
We can finally state the convergence result for leveraging algorithms:
Theorem 7. Let G be a loss function satisfying the conditions in Thm. 1. Suppose Alg. 1
generates a sequence of hypotheses ^ h 1 ; ^
h 1 ; : : : and weights ^
 1 ; ^
 2 ; : : : using a Æ­optimal
base learner. Assume f t g with  t
j =
P t
r=1 ^
 r I( ^ h r = h j ) is bounded. Then any limit
point of f t g is a solution of (6) and converges linearly in the sense of Thm. 4.
Note that this result in particular applies to AdaBoost, Logistic regression and the second
version of LS­Boost. For the selection scheme of LS­Boost given by (3) and (4), both
conditions in Definition 5 cannot be satisfied in general, unless
P N
n=1 h j (xn ) 2 is constant
for all hypotheses. Since
P N
n=1 (d n h j (xn )) 2 =
P N
n=1 (h j (xn ) 2 2dnhn (xn )+const: ),
the base learner prefers hypotheses with small
P N
n=1 h j (xn ) 2 and could therefore stop
improving the objective while being not optimal (see [20, Section 4.3] and [19, Section 5]
for more details).

4 Regularized Leveraging approaches
We have not yet exploited all features of Thm. 1. It additionally allows for box constraints
and a linear function in terms of the hypothesis coefficients. Here, we are in particular
interested in ` 1 ­norm penalized loss functions of the type F() = G(H)+Ckk 1 , which
are frequently used in machine learning. The LASSO algorithm for regression [26] and the
PBVM algorithm for classification [25] are examples. Since we assumed complementation
closeness of H, we can assume without loss of generality that a solution   satisfies   
0. We can therefore implement the ` 1 ­norm regularization using the linear term  > ,
where  = C1 and C  0 is the regularization constant. Clearly, the regularization
defines a structure of nested subsets of H, where the hypothesis set is restricted to a smaller
set for larger values of C.
The constraint   0 causes some minor complications with the assumptions on the base
learning algorithm. However, these can easily be resolved (cf. [21]), while not assuming
more on the base learning algorithm. The first step in solving the problem is to add the
additional constraint  t  0 to the minimization with respect to  t in step 3b of Alg. 1.
Roughly speaking, this induces the problem that hypothesis coefficient chosen too large in
a previous iteration, cannot be reduced again. To solve this problem one can check for each
coefficient of a previously selected hypothesis whether not selecting it would violate the
Æ­optimality condition (11) or (12). If so, the
Algorithm 2 -- A Leveraging algorithm for ` 1 ­norm regularized loss G.
1. Input: Sample S, No. of Iterations T , Loss function G : R N ! R, Reg. const. C > 0
2. Initialize: f0  0, d 1
n = g'(yn ; f0 (xn)) for all n = 1 : : : N
3. Do for t = 1; : : : ; T ,
(a) Train classifier on fS; d t g and obtain hypothesis ^ h t : X ! Y
(b) Let ^ r =
P N
n=1 d t
n
^ hr (xn) and  r =
P t
s=1 ^
r I( ^ hs = ^ hr ) for r = 1; : : : ; t
(c) r  = argmin i2J ^  i , where J = fi j i 2 f1; : : : ; t 1g and  i > 0g.
(d) if ^  t C < C r  then ^ h t = ^ hr  and  t =  r  else  t = 0
(e) Set ^
 t = argmin  t
G[f t +  ^
h t ] + C
(f) Update f t+1 = f t + ^
 t ^ h t and d t+1
n = g'(yn ; f t+1 (xn)); n = 1; : : : ; N
4. Output: fT
algorithm selects such a coordinate for the next iteration instead of calling the base learning
algorithm. This idea leads to Alg. 2 (see [21] for a detailed discussion). For this algorithm
we can show the following:
Theorem 8 (Convergence of ` 1 ­norm penalized Leveraging). Assume G; H are as
Thm. 1, G is strictly convex, C > 0, and the base learner satisfies
@ F( t )
@ t
j t
 Æ max j=1;:::;J
@ F( t )
@ t
j
8t = 1; 2; : : : (15)
for Æ > 0. Then Alg. 2 converges linearly to a minimum of the regularized loss function.
This can also be shown for a maximum­improvement like condition on the base learner,
which we have to omit due to space limitation.
In [27] a similar algorithm has been suggested that solves a similar optimization problem
(keeping kk 1 fixed). For this algorithm one can show order one convergence (which is
weaker than linear convergence), which also holds if the hypothesis set is infinite.
5 Conclusion
We gave a unifying convergence analysis for a fairly general family of leveraging methods.
These convergence results were obtained under rather mild assumptions on the base learner
and, additionally, led to linear convergence rates. This was achieved by relating leveraging

algorithms to the Gauss­Southwell method known from numerical optimization.
While the main theorem used here was already proven in [16], its applications closes a
central gap between existing algorithms and their theoretical understanding in terms of
convergence. Future investigations include the generalization to infinite hypotheses spaces
and an improvement of the convergence rate . Furthermore, we conjecture that our results
can be extended to many other variants of boosting type algorithms proposed recently in
the literature (cf. http://www.boosting.org).
References
[1] H.H. Bauschke and J.M. Borwein. Legendre functions and the method of random bregman
projections. Journal of Convex Analysis, 4:27--67, 1997.
[2] K.P. Bennett, A. Demiriz, and J. Shawe­Taylor. A column generation algorithm for boosting.
In P. Langley, editor, Proceedings, 17th ICML, pages 65--72. Morgan Kaufmann, 2000.
[3] L. Breiman. Prediction games and arcing algorithms. Neural Comp., 11(7):1493--1518, 1999.
[4] N. Cesa­Bianchi, A. Krogh, and M. Warmuth. Bounds on approximate steepest descent for
likelihood maximization in exponential families. IEEE Trans. Inf. Th., 40(4):1215--1220, 1994.
[5] M. Collins, R.E. Schapire, and Y. Singer. Logistic Regression, Adaboost and Bregman dis­
tances. In Proc. COLT, pages 158--169, San Francisco, 2000. Morgan Kaufmann.
[6] J. Copas. Regression, prediction and shrinkage. J.R. Statist. Soc. B, 45:311--354, 1983.
[7] S. Della Pietra, V. Della Pietra, and J. Lafferty. Duality and auxiliary functions for bregman
distances. TR CMU­CS­01­109, Carnegie Mellon University, 2001.
[8] N. Duffy and D.P. Helmbold. A geometric approach to leveraging weak learners. In P. Fischer
and H. U. Simon, editors, Proc. EuroCOLT '99, pages 18--33, 1999.
[9] N. Duffy and D.P. Helmbold. Potential boosters? In S.A. Solla, T.K. Leen, and K.­R. M˜uller,
editors, NIPS, volume 12, pages 258--264. MIT Press, 2000.
[10] Y. Freund and R.E. Schapire. A decision­theoretic generalization of on­line learning and an
application to boosting. Journal of Computer and System Sciences, 55(1):119--139, 1997.
[11] J. Friedman, T. Hastie, and R.J. Tibshirani. Additive Logistic Regression: a statistical view of
boosting. Annals of Statistics, 2:337--374, 2000.
[12] J.H. Friedman. Greedy function approximation. Tech. rep., Stanford University, 1999.
[13] A.J. Hoffmann. On approximate solutions of systems of linear inequalities. Journal of Research
of the National Bureau of Standards, 49(4):263--265, October 1952.
[14] J. Kivinen and M. Warmuth. Boosting as entropy projection. In Proc. 12th Annu. Conference
on Comput. Learning Theory, pages 134--144. ACM Press, New York, NY, 1999.
[15] D.G. Luenberger. Linear and Nonlinear Programming. Addison­Wesley Publishing Co., Read­
ing, second edition, May 1984. Reprinted with corrections in May, 1989.
[16] Z.­Q. Luo and P. Tseng. On the convergence of coordinate descent method for convex differen­
tiable minimization. Journal of Optimization Theory and Applications, 72(1):7--35, 1992.
[17] L. Mason, J. Baxter, P.L. Bartlett, and M. Frean. Functional gradient techniques for combining
hypotheses. In Adv. Large Margin Class., pages 221--247. MIT Press, 2000.
[18] T. Onoda, G. R˜atsch, and K.­R. M˜uller. An asymptotic analysis of AdaBoost in the binary
classification case. In L. Niklasson, M. Bod’en, and T. Ziemke, editors, Proc. of the Int. Conf. on
Artificial Neural Networks (ICANN'98), pages 195--200, March 1998.
[19] G. R˜atsch. Robust Boosting via Convex Optimization. PhD thesis, University of Potsdam,
October 2001. http://mlg.anu.edu.au/”raetsch/thesis.ps.gz.
[20] G. R˜atsch, A. Demiriz, and K. Bennett. Sparse regression ensembles in infinite and finite
hypothesis spaces. Machine Learning, 48(1­3):193--221, 2002.
[21] G. R˜atsch, S. Mika, and M.K. Warmuth. On the convergence of leveraging. NeuroCOLT2
Technical Report 98, Royal Holloway College, London, 2001.
[22] G. R˜atsch, T. Onoda, and K.­R. M˜uller. Soft margins for AdaBoost. Machine Learning,
42(3):287--320, March 2001. also NeuroCOLT Technical Report NC­TR­1998­021.
[23] G. R˜atsch and M.K. Warmuth. Marginal boosting. NeuroCOLT2 Tech. Rep. 97, 2001.
[24] R.T. Rockafellar. Convex Analysis. Princeton University Press, 1970.
[25] Y. Singer. Leveraged vector machines. In S.A. Solla, T.K. Leen, and K.­R. M˜uller, editors,
NIPS, volume 12, pages 610--616. MIT Press, 2000.
[26] R.J. Tibshirani. Regression selection and shrinkage via the LASSO. Technical report, Depart­
ment of Statistics, University of Toronto, June 1994. ftp://utstat.toronto.edu/pub/tibs/lasso.ps.
[27] T. Zhang. A general greedy approximation algorithm with applications. In Advances in Neural
Information Processing Systems, volume 14. MIT Press, 2002. in press.

