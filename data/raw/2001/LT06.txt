Information­Geometrical Significance of
Sparsity in Gallager Codes
Toshiyuki Tanaka
Department of Electronics and Information Engineering
Tokyo Metropolitan University
Tokyo 192­0397, Japan
tanaka@eei.metro­u.ac.jp
Shiro Ikeda
Kyushu Institute of Technology & JST
Fukuoka 808­0196, Japan
shiro@brain.kyutech.ac.jp
Shun­ichi Amari
RIKEN, Brain Science Institute
Saitama 351­0198, Japan
amari@brain.riken.go.jp
Abstract
We report a result of perturbation analysis on decoding error of the belief
propagation decoder for Gallager codes. The analysis is based on infor­
mation geometry, and it shows that the principal term of decoding error
at equilibrium comes from the m­embedding curvature of the log­linear
submanifold spanned by the estimated pseudoposteriors, one for the full
marginal, and K for partial posteriors, each of which takes a single check
into account, where K is the number of checks in the Gallager code. It is
then shown that the principal error term vanishes when the parity­check
matrix of the code is so sparse that there are no two columns with overlap
greater than 1.
1 Introduction
Recent progress on error­correcting codes has attracted much attention because their de­
coders, exhibiting performance very close to Shannon's limit, can be implemented as neu­
ral networks. Important examples are turbo codes and Gallager codes [1]. It is now well
understood that application of belief propagation to the respective graphical representa­
tions of the decoding problems for both codes yields practically efficient decoding algo­
rithms which are the same as the existing ones (the turbo decoding [2] and the sum­product
decoding [3], respectively). They are, however, not exact but approximate, since the as­
sociated graphical representations have loops in both cases. An important problem posed
is to quantify the effect that comes from the existence of loops in the underlying graph.
The so­called TAP approach [4] in statistical physics is an alternative way to formulate the
same decoding algorithm [5]. Since this approach also assumes that the underlying graph
is locally loop­free, one is faced with the same problem as above.
In this paper, we analyze the properties of the belief propagation decoder to Gallager codes,
expecting that better theoretical understanding of the properties of the belief propagation

s
information
vector
G T
generator
matrix
t
codeword
r
received
vector
A
parity­check
matrix
z
syndrome
vector
BSC(# ) BSC(# # )
Figure 1: Gallager code
decoder will help understand the properties and efficiency of belief propagation in general,
applied to loopy graphs, as well as those of the TAP approach. We specifically make use of
the information geometry [6] and report a result of perturbation analysis on decoding error
of the belief propagation decoder.
2 Gallager codes
Gallager code is defined by its parity­check matrix A, which has the form
A = [C 1 | C 2 ], (1)
where C 1 and C 2 are K × M and K × K matrices, both of which are taken to be very
sparse. C 2 is assumed invertible. We define the generator matrix of the Gallager code to be
G T
= # I
C -1 2 C 1
# (2)
where I is the M × M identity matrix. AG T
= O mod 2 holds by definition.
The whole model of communication with the Gallager code is shown in Fig. 1. An informa­
tion vector s of length M is encoded into a codeword t = G T s mod 2 of length N # K+M .
The codeword t is then transmitted over a channel. We assume that the transmission chan­
nel is a binary symmetric channel (BSC) with bit­error probability # . The received vector
is then r = t +n mod 2, where n is the noise vector. Decoder tries to find the most probable
x satisfying the parity­check equation
Ax = z mod 2, (3)
where z # Ar mod 2 is the syndrome vector. Since At = AG T s = 0 mod 2, we have
z = An mod 2. Therefore, the solution x serves as an estimate of the noise vector n. If we
are successful in finding the true noise vector n, we can reconstruct, from r, the original
codeword t by t = r + n mod 2, and then the information vector s. Since Eq. (3) is
underdetermined, one has to take into account the prior knowledge of the noise in order to
solve it properly.
The decoding problem can be cast into the Bayes framework. In the sequel, we transform
expression of a bit from binary (1, 0) to bipolar (-1, 1). The prior for x is
p(x) = exp # #1 · x - N#(#) # , #(#) # log(e #
+ e -# ), (4)
where 1 is an N­dimensional vector whose elements are all 1, i.e., 1 # [1, . . . , 1]. # is a
parameter which is related with the bit­error probability # of the transmission channel by
# = 1
2 (1 - tanh #). (5)

For the sake of analytical tractability, we assume that the syndrome vector z is observed via
another BSC channel with bit­error probability # # (see Fig. 1). This leads
p(z|x) # exp # #
K
# r=1
z r #
i #L(r)
x i # , (6)
where L(r) is the set of all indices of nonzero elements in row r of the parity­check matrix
A, i.e., L(r) # {i | A r i = 1}, and # is defined by # # = (1/2)(1 - tanh #). If we take the
limit # # +#, then we recover the conventional situation of observing the syndrome in
a deterministic way. In what follows, we consider the case in which # is finite, or equiva­
lently, the case with soft parity­check constraints. Since experimental findings suggest that
it is usually the case for decoding results of Gallager codes to violate no parity­check con­
straints [3], we believe that making the parity­check constraints soft does not alter essential
properties of the problem.
3 Decoding
The posterior distribution of x for given observed syndrome z is derived from the prior p(x)
and the conditional p(z | x) by applying the Bayes formula, and the result is
p(x|z) # exp # c 0 (x) + #
K
# r=1
c r (x) # , (7)
where we let
c 0 (x) # #1 · x, c r (x) # z r #
i #L(r)
x i (r = 1, . . . , K ). (8)
The objective of decoder of Gallager codes is to obtain the marginal­posterior­mode
(MPM) estimate based on the posterior p(x|z):
“
x i = arg max x i # x\x i
p(x|z). (9)
The MPM estimation provides the Bayes­optimum decoder minimizing expected bit­error
probability of the decoding results. However, the marginalization is in general computa­
tionally hard, which renders the decoding problem intractable. An approximate decoding
algorithm, originally proposed by Gallager himself [1], is known to be efficient in practice.
It has been recently rediscovered by MacKay [3] by application of the belief propagation
to the underlying graphical model. Murayama et al. [5] also formulated the same algo­
rithm based on the so­called TAP approach [4]. The decoder implementing the algorithm
is called the belief propagation decoder, and is analyzed in the following.
We define a family of distributions with a set of parameters # = (# 1 , . . . , # N ) T and v =
(v 1 , . . . , v K ):
S = # p(x; # , v) # # # p(x; # , v) = exp # # · x + v · c(x) - #(# , v) # # , (10)
where c(x) # # c 1 (x), . . . , c K (x) # T . The family S includes the factorizable test distribu­
tion p 0 (x; # 0 ) (= p(x; # 0 , 0)), the true posterior p(x|z) (= p(x; #1, #1)), and K partial
posteriors p r (x; # r ) (= p(x; # r , #e r ); e r # (0, . . . , 0, 1
“ r
, 0, . . . , 0) T ).
We then define the expectation parameter #(# , v) by
#(# , v) # # x
x p(x; # , v). (11)

The marginalization in Eq. (9) corresponds to evaluating the expectation parameter of the
true posterior. We now introduce the equimarginal family
M(# 0 ) # # p(x; # , v) # # #(# , v) = #(# 0 , 0) # , (12)
and define the marginalization operator # as follows: For p # S, ## p # # 0 if p # M(# 0 ).
Knowing # 0 = ## p is regarded as being equivalent to knowing the expectation parameter
of p, since #(# 0 , 0) is easily evaluated from # 0 ; in other words, the marginalization is
tractable for distributions belonging to the factorizable model:
M 0 # # p 0 (x; # 0 ) # p(x; # 0 , 0) = exp(# 0 · x - # 0 (# 0 )) # (13)
The basic assumption of iterative decoding is that the marginalization is also tractable for
the models corresponding to constituent decoders with single checks, with factorizable
priors:
M r # # p r (x; #) # p(x; #, #e r ) = exp(# · x + #c r (x) - # r (#)) # (14)
The algorithm of the belief propagation decoder is described in the notation employed here
as follows:
Initialization: Let t = 0 and # 0
r = #1, r = 1, . . . , K .
Horizontal step: Evaluate the marginalization of p r (x; # t
r ) # M r to produce a
guess # t
r based on the current prior information # t
r and the check z r :
# t
r = # # p r (x; # t
r ), r = 1, . . . , K , (15)
and calculate a net contribution (the 'cavity field' [7]) from the check z r by sub­
tracting the prior information:
# t
r = # t
r - # t
r . (16)
(It should be noted that (# t
r ) i = 0 holds for i
# # L(r) as it should be, since the
constituent decoder with check z r cannot provide any contribution regarding in­
formation of x i , i
# # L(r).)
Vertical step: Compose the 'leave­one­out' estimates [7]
# t +1
r = #1 + #
r #
# =r
# t
r # , r = 1, . . . , K , (17)
and the pseudoposterior
# t +1 = #1 +
K
# r=1
# t
r . (18)
Iterate the above steps until convergence is achieved. The desired decoding result
#(#1, #1) is then approximated by #(# # , 0), where # # is the convergent value of
{# t
}.
4 Information­geometrical characterization of equilibrium
Assume that the convergence is achieved and let (# # , # # 1 , . . . , # # K ) be the equilibrium values
of (# t , # t
1 , . . . , # t
K ). Then, from Eqs. (15) and (16), we have
# # p r (x; # # - # # r ) = # # , r = 1, . . . , K . (19)
This means that p 0 (x; # # ) and p r (x; # # - # # r ), r = 1, . . . , K , are equimarginal, that is,
p r (x; # # - # # r ) # M(# # ), r = 1, . . . , K (20)

S
M 0
M K
M 1
M 2
p 0 (x; # # )
p 1 (x; # # - # # 1 )
p K (x; # # - # # K )
p 2 (x; # # - # # 2 )
p(x|z)
M(# # )
E(# # )
Figure 2: Geometric structure of belief propagation decoder
holds. Another property of the equilibrium is the log­linear relation
log p(x|z) - log p 0 (x; # # ) =
K
# r=1
# log p r (x; # # - # # r ) - log p 0 (x; # # ) # + const. (21)
or, in the (# , v) coordinate,
(#1, #1) - (# # , 0) =
K
# r=1
# (# # - # # r , #e r ) - (# # , 0) # . (22)
This means that the true posterior p(x|z) belongs to the 'log­linear submanifold' E(# # ),
the affine subspace in the (# , v)­coordinate rooted at (# # , 0) and spanned by (-# # r , #e r ),
r = 1, . . . , K .
These two properties do not imply p(x|z) # M(# # ). In fact, if we were to assume, instead
of the log­linear relation (21), the linear relation
p(x|z) - p 0 (x; # # ) =
K
# r=1
# p r (x; # # r ) - p 0 (x; # # ) # , (23)
then we would have p(x|z) # M(# # ) and thus #(#1, #1) = #(# # , 0). This is not the
case because of the difference between the linear and log­linear relations. To what degree
the log­linear relation deviates from the linear relation determines the decoding error of
belief propagation decoder. The structure is best described on the basis of information
geometry [6]. Figure 2 illustrates the geometric structure of the belief propagation decoder.
It should be noted that the geometrical structure shown here is essentially the same as that
for the turbo decoding [8, 9].
5 Main result
Based on the information geometry, we have evaluated decoding error, the difference be­
tween the true expectation #(#1, #1) and its estimate by the belief propagation decoder

#(# # , 0), via perturbation analysis. Taking into account the terms up to second order, we
have
#(#1, #1) - #(# # , 0) = # 2
2 #
r,s;r
# =s
B rs #(# # , 0) + O(# 3 ), (24)
where
B rs # # #
#v r
-
N
# k=1
g kk A k
r
#
## k
## #
#v s
-
N
# j =1
g j j A j
s
#
## j
# , (25)
and
A i
r # ## i (# # , 0)
#v r
= Cov # # ,0 # x i , c r (x) # . (26)
{B rs } are the elements of the m­embedding curvature tensor of the manifold E(# # ) in S.
g i i
# 1/(1-# i i (# # , 0) 2 ) are the diagonal elements of the inverse of the Fisher information
of p 0 (x; # # ). This is the generalization of the result obtained for the turbo decoding [8].
Explicit calculation gives the following theorem.
Theorem 1. The decoding error of belief propagation decoder is given, within the second­
order with respect to #, by
# i (#1, #1) - # i (# # , 0)
= # 2 (1 - # 2
i ) # -# i # r,s
r
# =s,i#L(r)#L(s)
z r z s #
j #(L(r)#L(s))\i
(1 - # 2
j )
× #
k#(L(r)#L(s))\i, j
# 2
k #
l#(L(r)-L(s))#(L(s)-L(r))
# l
+ # r,s
r
# =s,i#L(r)-L(s)
z r z s # 1 - #
j #L(r)#L(s)
# 2
j - #
j #L(r)#L(s)
(1 - # 2
j ) #
k#(L(r)#L(s))\ j
# 2
k #
× #
l#[(L(r)-L(s))\i]#[L(s)-L(r)]
# l #
+ O(# 3 ) (27)
where # i # # i (# # , 0).
From this theorem, it immediately follows that:
Corollary 2. If the parity­check matrix A has no two columns with overlap greater than
1, then the principal error term, given in Eq. (27), vanishes.
These are the main result of this paper.
6 Discussion
The general result given in Eq. (24) shows that the principal error term is not coordinate
invariant, since the summation with respect to r and s in the right­hand side of Eq. (24)
excludes terms with r = s. This corresponds to the empirical fact that the performance does
depend on the design of the code, that is, the choice of the parity­check matrix A. Explicit
evaluation of the principal error term, as in Theorem 1, makes it possible to improve the

performance of a code, just in the same way as the perturbational approach to improving
the naive mean­field approximation [10, 11, 12, 13, 14, 15, 16, 17].
It is believed [3] that Gallager codes have smaller average probability of decoding error
if we avoid any two columns of the parity­check matrix A to have overlap greater than 1.
An intuitive explanation to this belief is that such avoidance prevents loops with length 4
from appearing in the graphical representation. Since short loops are expected to do harm
in proper functioning of belief propagation, their existence may raise the possibility of
decoding errors. Our result supports this belief by showing analytically that the principal
term of decoding error vanishes when the parity­check matrix of the code is so sparse and
prepared with care so that there are no two columns with overlap greater than 1.
Loops with length longer than 4 do not contribute to the decoding error at least via the
principal term, but they may have effects via higher­order terms. Our analysis presented
here can be extended in a straightforward manner to higher­order perturbation analysis in
order to quantify these effects.
It should be noted that our approach taken in this paper is different from the common
approach to analyzing the properties of the belief propagation decoder in the literature,
in that we do not consider ensembles of codes. A typical reasoning found in the literature
(e.g., [18]) is first to consider an ensemble of random parity­check matrices, to state that the
probability (over the ensemble) of containing short loops in the associated graph decreases
down to zero as the size of the parity­check matrix tends to infinity, and to assume that the
behavior of the belief propagation decoder for codes with longer loops is the same as that of
belief propagation for loop­free case. The statistical­mechanical approach to performance
analysis of Gallager­type codes [5] also assumes random ensembles. Our analysis, on the
other hand, does not assume ensembles but allows, although asymptotically, performance
evaluation of the belief propagation decoder to Gallager codes with any single instance of
the parity­check matrix with finite size.
Acknowledgments
The authors would like to thank Dr. Yoshiyuki Kabashima for his helpful suggestions and
comments.
References
[1] R. G. Gallager, Low Density Parity Check Codes, Ph. D. Thesis, Mass. Inst. Tech., 1960.
[2] R. J. McEliece, D. J. C. MacKay, and J. Cheng, ``Turbo decoding as an instance of Pearl's ‘belief
propagation' algorithm,'' IEEE J. Select. A. Commun., vol. 16, no. 2, pp. 140--152, 1998.
[3] D. J. C. MacKay, ``Good error­correcting codes based on very sparse matrices,'' IEEE Trans.
Inform. Theory, vol. 45, no. 2, pp. 399--431, 1999.
[4] D. J. Thouless, P. W. Anderson, and R. G. Palmer, ``Solution of ‘Solvable model of a spin glass',''
Phil. Mag., vol. 35, no. 3, pp. 593--601, 1977.
[5] T. Murayama, Y. Kabashima, D. Saad, and R. Vicente, ``Statistical physics of regular low­density
parity­check error­correcting codes,'' Phys. Rev. E, vol. 62, no. 2, pp. 1577--1591, 2000.
[6] S. Amari and H. Nagaoka (Transl. by D. Harada), Methods of Information Geometry, Transla­
tions of Mathematical Monographs, vol. 191, American Math. Soc., 2000.
[7] Y. Kabashima and D. Saad, ``The TAP approach to intensive and extensive connectivity systems,''
in M. Opper and D. Saad (eds.), Advanced Mean Field Methods --- Theory and Practice, The
MIT Press, 2001, pp. 65--84.
[8] S. Ikeda, T. Tanaka, and S. Amari, ``Information geometrical framework for analyzing belief
propagation decoder,'' in T. G. Dietterich et al. (eds.), Advances in Neural Information Process­
ing Systems, vol. 14 (this volume), The MIT Press, 2002.

[9] S. Ikeda, T. Tanaka, and S. Amari, ``Information geometry of turbo codes and low­density parity­
check codes,'' submitted to IEEE Trans. Inform. Theory, 2001.
[10] H. J. Kappen and F. B. Rodriguez, ``Efficient learning in Boltzmann machines using linear
response theory,'' Neural Computation, vol. 10, no. 5, pp. 1137--1156, 1998.
[11] H. J. Kappen and F. B. Rodriguez, ``Boltzmann machine learning using mean field theory and
linear response correction,'' in M. I. Jordan et al. (eds.), Advances in Neural Information Pro­
cessing Systems, vol. 10, The MIT Press, 1998, pp. 280--286.
[12] T. Tanaka, ``A theory of mean field approximation,'' in M. S. Kearns et al. (eds.), Advances in
Neural Information Processing Systems, vol. 11, The MIT Press, 1999, pp. 351--357.
[13] T. Tanaka, ``Information geometry of mean­field approximation,'' Neural Computation, vol. 12,
no. 8, pp. 1951--1968, 2000.
[14] J. S. Yedidia, ``An idiosyncratic journey beyond mean field theory,'' in M. Opper and D. Saad
(eds.), Advanced Mean Field Methods --- Theory and Practice, The MIT Press, 2001, pp. 21--35.
[15] H. J. Kappen and W. J. Wiegerinck, ``Mean field theory for graphical models,'' in M. Opper and
D. Saad (eds.), Advanced Mean Field Methods --- Theory and Practice, The MIT Press, 2001,
pp. 37--49.
[16] S. Amari, S. Ikeda, and H. Shimokawa, ``Information geometry of #­projection in mean field
approximation,'' in M. Opper and D. Saad (eds.), Advanced Mean Field Methods --- Theory and
Practice, The MIT Press, 2001, pp. 241--257.
[17] T. Tanaka, ``Information geometry of mean­field approximation,'' in M. Opper and D. Saad
(eds.), Advanced Mean Field Methods --- Theory and Practice, The MIT Press, 2001, pp. 259--
273.
[18] T. J. Richardson and R. L. Urbanke, ``The capacity of low­density parity­check codes under
message­passing decodeing,'' IEEE Trans. Inform. Theory, vol. 47, no. 2, pp. 599--618, 2001.

