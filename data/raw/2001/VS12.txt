The g Factor: Relating Distributions on
Features to Distributions on Images
James M. Coughlan and A. L. Yuille
Smith-Kettlewell Eye Research Institute,
2318 Fillmore Street,
San Francisco, CA 94115, USA.
Tel. (415) 345-2146/2144. Fax. (415) 345-8455.
Email: coughlan@ski.org, yuille@ski.org
Abstract
We describe the g-factor, which relates probability distributions
on image features to distributions on the images themselves. The
g-factor depends only on our choice of features and lattice quanti-
zation and is independent of the training image data. We illustrate
the importance of the g-factor by analyzing how the parameters of
Markov Random Field (i.e. Gibbs or log-linear) probability models
of images are learned from data by maximum likelihood estimation.
In particular, we study homogeneous MRF models which learn im-
age distributions in terms of clique potentials corresponding to fea-
ture histogram statistics (cf. Minimax Entropy Learning (MEL)
by Zhu, Wu and Mumford 1997 [11]). We rst use our analysis
of the g-factor to determine when the clique potentials decouple
for dierent features. Second, we show that clique potentials can
be computed analytically by approximating the g-factor. Third,
we demonstrate a connection between this approximation and the
Generalized Iterative Scaling algorithm (GIS), due to Darroch and
Ratcli 1972 [2], for calculating potentials. This connection en-
ables us to use GIS to improve our multinomial approximation,
using Bethe-Kikuchi[8] approximations to simplify the GIS proce-
dure. We support our analysis by computer simulations.
1 Introduction
There has recently been a lot of interest in learning probability models for vision.
The most common approach is to learn histograms of lter responses or, equiva-
lently, to learn probability distributions on features (see right panel of gure (1)).
See, for example, [6], [5], [4]. (In this paper the features we are considering will be
extracted from the image by lters { hence we use the terms \features" and \lters"
synonymously.)

An alternative approach, however, is to learn probability distributions on the images
themselves. The Minimax Entropy Learning (MEL) theory [11] uses the maximum
entropy principle to learn MRF distributions in terms of clique potentials deter-
mined by the feature statistics (i.e. histograms of lter responses). (We note that
the maximum entropy principle is equivalent to performing maximum likelihood es-
timation on an MRF whose form is determined by the choice of feature statistics.)
When applied to texture modeling it gives a way to unify the lter based approaches
(which are often very eective) with the MRF distribution approaches (which are
theoretically attractive).
-150 -100 -50 50 100 150
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
Figure 1: Distributions on images vs. distributions on features. Left and center
panels show a natural image and its image gradient magnitude map, respectively.
Right panel shows the empirical histogram (i.e. a distribution on a feature) of
the image gradient across a dataset of natural images. This feature distribution
can be used to create a MRF distribution over images[10]. This paper introduces
the g-factor to examine connections between the distribution over images and the
distribution over features.
As we describe in this paper (see gure (1)), distributions on images and on fea-
tures can be related by a g-factor (such factors arise in statistical physics, see [3]).
Understanding the g-factor allows us to approximate it in a form that helps explain
why the clique potentials learned by MEL take the form that they do as functions
of the feature statistics. Moreover, the MEL clique potentials for dierent features
often seem to be decoupled and the g-factor can explain why, and when, this occurs.
(I.e. the two clique potentials corresponding to two features A and B are identical
whether we learn them jointly or independently).
The g-factor is determined only by the form of the features chosen and the spatial
lattice and quantization of the image gray-levels. It is completely independent of
the training image data. It should be stressed that the choice of image lattice,
gray-level quantization and histogram quantization can make a big dierence to the
g-factor and hence to the probability distributions which are the output of MEL.
In Section (2), we briey review Minimax Entropy Learning. Section (3) introduces
the g-factor and determines conditions for when clique potentials are decoupled.
In Section (4) we describe a simple approximation which enables us to learn the
clique potentials analytically, and in Section (5) we discuss connections between
this approximation and the Generalized Iterative Scaling (GIS) algorithm.
2 Minimax Entropy Learning
Suppose we have training image data which we assume has been generated by an
(unknown) probability distribution P T (~x) where ~x represents an image. Minimax
Entropy Learning (MEL) [11] approximates P T (~x) by selecting the distribution with

maximum entropy constrained by observed feature statistics ~ (~x) = ~
  obs . This gives
P (~xj ~ ) = e ~  ~ (~x)
Z[ ~ ]
; where ~  is a parameter chosen such that
P
x
P (~xj)(~x) = ~
  obs .
Or equivalently, so that @ log Z[ ~ ]
@ ~ 
= ~
  obs .
We will treat the special case where the statistics ~  are the histogram of a shift-
invariant lter ff i (~x) : i = 1; :::; Ng, where N is the total number of pixels in the im-
age. So   a =  a (~x) = 1
N
P N
i=1 ∆ a;f i (~x) where a = 1; :::; Q indicates the (quantized)
lter response values. The potentials become ~  ~
(~x) = 1
N
P Q
a=1
P N
i=1 (a)∆ a;f i (~x) =
1
N
P N
i=1 (f i (~x)): Hence P (~xj ~ ) becomes a MRF distribution with clique potentials
given by (f i (~x)). This determines a Markov random eld with the clique structure
given by the lters ff i g.
MEL also has a feature selection stage based on Minimum Entropy to determine
which features to use in the Maximum Entropy Principle. The features are evalu-
ated by computing the entropy
P
~x P (~xj ~ ) log P (~xj ~ ) for each choice of features
(with small entropies being preferred). A lter pursuit procedure was described to
determine which lters/features should be considered (our approximations work for
this also).
3 The g-Factor
This section denes the g-factor and starts investigating its properties in subsec-
tion (3.1). In particular, when, and why, do clique potentials decouple? More
precisely, when do the potentials for lters A and B learned simultaneously dier
from the potentials for the two lters when they are learned independently?
We address these issues by introducing the g-factor g( ~
 ) and the associated distri-
bution ^
P 0 ( ~
 ):
g( ~
 ) =
X
~x
∆ ~
(~x); ~
  ; ^
P 0 ( ~
 ) = 1
L N g( ~
 ): (1)
x
.
space space
= number of images
   with histogram
g(  ) x
Figure 2: The g-factor g( ~
 ) counts the number of images ~x that have statistics ~
 .
Note that the g-factor depends only on the choice of lters and is independent of
the training image data.
Here L is the number of grayscale levels of each pixel, so that L N is the total number
of possible images. The g-factor is essentially a combinational factor which counts
the number of ways that one can obtain statistics ~
 , see gure (2). Equivalently,
^
P 0 is the default distribution on ~
  if the images are generated by white noise (i.e.
completely random images).

We can use the g-factor to compute the induced distribution ^
P ( ~
 j ~ ) on the statistics
determined by MEL:
^
P ( ~
 j ~ ) =
X
~x
∆ ~
 ; ~ (~x) P (~xj ~ ) = g( ~
 )e ~  ~
 
Z[ ~ ]
; Z[ ~ ] =
X
~
 
g( ~
 )e ~  ~
  : (2)
Observe that both ^
P ( ~
 j ~ ) and log Z[ ~ ] are su∆cient for computing the parameters
~ . The ~  can be found by solving either of the following two (equivalent) equations:
P
~
 
^
P ( ~
 j ~ ) ~
  = ~
  obs ; or @ log Z[ ~ ]
@ ~ 
= ~
  obs , which shows that knowledge of the g-factor
and e ~  ~
  are all that is required to do MEL.
Observe from equation (2) that we have ^
P ( ~
 j ~  = 0) = P 0 ( ~
 ). In other words,
setting ~  = 0 corresponds to a uniform distribution on the images ~x.
3.1 Decoupling Filters
We now derive an important property of the minimax entropy approach. As men-
tioned earlier, it often seems that the potentials for lters A and B decouple.
In other words, if one applies MEL to two lters A; B simultaneously by letting
~
  = ( ~
  A ; ~
  B ), ~  = ( ~  A ; ~  B ), and ~
  obs = ( ~
  A
obs ; ~
  B
obs ), then the solutions ~  A ; ~  B to
the equations:
X
~x
P (~xj ~  A ; ~  B )( ~  A (~x); ~
 B (~x)) = ( ~
  A
obs ; ~
  B
obs ); (3)
are the same (approximately) as the solutions to the equations
P
~x P (~xj ~  A ) ~
 A (~x) =
~
  A
obs and
P
~x P (~xj ~  B ) ~
 B (~x) = ~
  B
obs , see gure (3) for an example.
-15 -10 -5 10 15
-2
-1.5
-1
-0.5
0.5
1.5
-15 -10 -5 10 15
-1.5
-1
-0.5
0.5
1.5
Figure 3: Evidence for decoupling of features. The left and right panels show the
clique potentials learned for the features @=@x and @=@y respectively. The solid lines
give the potentials when they are learned individually. The dashed lines show the
potentials when they are learned simultaneously. Figure courtesy of Prof. Xiuwen
Liu, Florida State University.
We now show how this decoupling property arises naturally if the g-factor for the
two lters factorizes. This factorization, of course, is a property only of the form
of the statistics and is completely independent of whether the statistics of the two
lters are dependent for the training data.
Property I: Suppose we have two su∆cient statistics ~  A (~x); ~
 B (~x) which are in-
dependent on the lattice in the sense that g( ~
  A ; ~
  B ) = g A ( ~
  A )g B ( ~
  B ), then
log Z[ ~  A ; ~  B ] = log Z A [ ~  A ] + log Z B [ ~  B ] and ^
P ( ~
  A ; ~
  B ) = ^
P A ( ~
  A ) ^
P B ( ~
  B ).

This implies that the parameters ~  A ; ~  B can be solved from the independent
equations @ log Z A [ ~  A ]
@ ~  A = ~
  A
obs , @ log Z B [ ~  B ]
@ ~  B = ~
  B
obs or
P
~
  A
^
P A ( ~
  A ) ~
  A = ~
  A
obs ,
P
~
  B
^
P B ( ~
  B ) ~
  B = ~
  B
obs .
Moreover, the resulting distribution P (~x) can be obtained by multiplying the distri-
butions (1=Z A )e ~  A  ~
  A (~x) and (1=Z B )e ~  B  ~
  B (~x) together.
The point here is that the potential terms for the two statistics ~
  A ; ~
  B decouple if
the phase factor g( ~
  A ; ~
  B ) can be factorized. We conjecture that this is eectively
the case for many linear lters used in vision processing. For example, it is plausible
that the g-factor for features @=@x and @=@y factorizes { and gure (3) shows
that their clique potentials do decouple (approximately). Clearly, if factorization
between lters occurs then it gives great simplication to the system.
4 Approximating the g-factor for a Single Histogram
We now consider the case where the statistic is a single histogram. Our aim is to
understand why features whose histograms are of stereotypical shape give rise to
potentials of the form given by gure (3). Our results, of course, can be directly
extended to multiple histograms if the lters decouple, see subsection (3.1). We
rst describe the approximation and then discuss its relevance for lter pursuit.
We rescale the ~  variables by N so that we have:
P (~xj) = e N ~  ~ (~x)
Z[ ~ ]
; ^
P ( ~
 j) = g( ~
 )
e N ~  ~
 
Z[ ~ ]
; (4)
We now consider the approximation that the lter responses ff i g are independent
of each other when the images are uniformly distributed. This is the multinomial
approximation. (We attempted a related approximation [1] which was less success-
ful.) It implies that we can express the phase factor as being proportional to a
multinomial distribution:
g( ~
 ) = L N N !
(N  1 )!:::(N  Q )!  N 1
1 ::: N Q
Q ; ^
P 0 ( ~
 ) = N !
(N  1 )!:::(N  Q )!  N 1
1 ::: N Q
Q ;
(5)
where
P Q
a=1   a = 1 (by denition) and the f a g are the means of the components
f  a g with respect to the distribution ^
P 0 ( ~
 ). As we will describe later, the f a g
will be determined by the lters ff i g. See Coughlan and Yuille, in preparation, for
details of how to compute the f a g.
This approximation enables us to calculate MEL analytically.
Theorem With the multinomial approximation the log partition function is:
log Z[ ~ ] = N log L +N logf
Q
X
a=1
e a+log a g; (6)
and the \potentials" f a g can be solved in terms of the observed data f  obs;a g to
be:
 a = log   obs;a
 a
; a = 1; :::; Q: (7)

-60 -40 -20 0 20 40 60
-4
-2
potential
5 10 15 20 25 30
10
15
20
25
30
-150 -100 -50 50 100 150
0
0.02
0.04
0.06
0.08
0.1
0.12
mcmc in bold, target in dashed line
-60 -40 -20 20 40 60
-4
-2
potential
5 10 15 20 25 30
10
15
20
25
30
-150 -100 -50 50 100 150
0
0.02
0.04
0.06
0.08
0.1
0.12
mcmc in bold, target in dashed line
Figure 4: Top row: the multinomial approximation. Bottom row: full implemen-
tation of MEL (see text). (Left panels) the potentials, (center panels) synthesized
images, and (right panels) the dierence between the observed histogram (dashed
line) and the histogram of the synthesized images (bold line). Filters were d=dx
and d=dy.
We note that there is an ambiguity  a 7!  a +K where K is an arbitrary number
(recall that
P Q
a=1  (a) = 1). We x this ambiguity by setting ~  = 0 if ~  = ~
  obs .
Proof. Direct calculation.
Our simulation results show that this simple approximation gives the typical po-
tential forms generated by Markov Chain Monte Carlo (MCMC) algorithms for
Minimax Entropy Learning. Compare the multinomial approximation results with
those obtained from a full implementation of MEL by the algorithm used in [11],
see gure (4).
Filter pursuit is required to determine which lters carry most information.
MEL [11] prefers lters (statistics) which give rise to low entropy distribu-
tions (this is the \Min" part of Minimax). The entropy is given by H(P ) =
P
~x P (~xj ~ ) log P (~xj ~ ) = log Z[ ~ ]
P Q
a=1  a   a . For the multinomial approxi-
mation this can be computed to be N log L N
P Q
a=1   a log  a
a . This gives an
intuitive interpretation of feature pursuit: we should prefer lters whose statistical
response to the image training data is as large as possible from their responses to
uniformly distributed images. This is measured by the Kullback-Leibler divergence
P Q
a=1   a log  a
a . Recall that if the multinomial approximation is used for multiple
lters then we should simply add together the entropies of dierent lters.

5 Connections to Generalized Iterative Scaling
In this section we demonstrate a connection between the multinomial approxima-
tion and Generalized Iterative Scaling (GIS)[2]. GIS is an iterative procedure for
calculating clique potentials that is guaranteed to converge to the maximum likeli-
hood values of the potentials given the desired empirical lter marginals (e.g. lter
histograms). We show that estimating the potentials by the multinomial approx-
imation is equivalent to the estimate obtained after performing the rst iteration
of GIS. We also outline an e∆cient procedure that allows us to continue additional
GIS iterations to improve upon the multinomial approximation.
The GIS procedure calculates a sequence of distributions on the entire image
(and is guaranteed to converge to the correct maximum likelihood distribu-
tion), with an update rule given by P (t+1) (~x) / P (0) (~x)
Q Q
a=1 f   obs
a
  (t)
a
g a (~x) , where
  (t)
a =<  a (~x) > P (t) (~x) is the expected histogram for the distribution at time t.
This implies that the corresponding clique potential update equation is given by:
 (t+1)
a =  (t)
a + log   obs
a log   (t)
a .
If we initialize GIS so that the initial distribution is the uniform distribution,
i.e. P (0) (~x) = L N , then the distribution after one iteration is P (1) (~x) /
e
P
a a (~x) log(  obs
a =a ) . In other words, the distribution after one iteration is the
MEL distribution with clique potential given by the multinomial approximation.
(The result can be adapted to the case of multiple lters, as explained in Coughlan
and Yuille, in preparation.)
We can iterate GIS to improve the estimate of the clique potentials beyond the
accuracy of the multinomial approximation. The main di∆culty lies in estimating
  (t)
a for t > 0 (at t = 0 this expectation is just the mean histogram with respect
to the uniform distribution,  a , which may be calculated e∆ciently as described in
Coughlan and Yuille, in preparation). One way to approximate these expectations is
to apply a Bethe-Kikuchi approximation technique [8], used for estimating marginals
on Markov Random Fields, to our MEL distribution. Our technique, which was
inspired by the Unied Propagation and Scaling Algorithm [7], consists of writing
the Bethe free energy [8] for our 2-d image lattice, simplifying it using the shift
invariance of the lattice (which enables the algorithm to run swiftly), and using the
Convex-Concave Procedure (CCCP) [9] procedure to obtain an iterative update
equation to estimate the histogram expectations. The GIS algorithm is then run
using these histogram expectations (the results were accurate and did not improve
appreciably by using the higher-order Kikuchi free energy approximation). See
Coughlan and Yuille, in preparation, for details of this procedure.
6 Discussion
This paper describes the g-factor, which depends on the lattice and quantization
and is independent of the training image data. Alternatively it can be thought of as
being proportional to the distribution of feature responses when the input images
are uniformly distributed.
We showed that the g-factor can be used to relate probability distributions on
features to distributions on images. In particular, we described approximations

which, when valid, enable MEL to be computed analytically. In addition, we can
determine when the clique potentials for features decouple, and evaluate how infor-
mative each feature is. Finally, we establish a connection between the multinomial
approximation and GIS, and outline an e∆cient procedure based on Bethe-Kikuchi
approximations that allows us to continue additional GIS iterations to improve upon
the multinomial approximation.
Acknowledgements
We would like to thank Michael Jordan and Yair Weiss for introducing us to Gener-
alized Iterative Scaling and related algorithms. We also thank Anand Rangarajan,
Xiuwen Liu, and Song Chun Zhu for helpful conversations. Sabino Ferreira gave use-
ful feedback on the manuscript. This work was supported by the National Institute
of Health (NEI) with grant number RO1-EY 12691-01.
References
[1] J.M. Coughlan and A.L. Yuille. \A Phase Space Approach to Minimax Entropy
Learning and The Minutemax approximation". In Proceedings NIPS'98. 1998.
[2] J. N. Darroch and D. Ratcli. \Generalized Iterative Scaling for Log-Linear
Models". The Annals of Mathematical Statistics. 1972. Vol. 43, No. 5, 1470-
1480.
[3] C. Domb and M.S. Green (Eds). Phase Transitions and Critical Phenom-
ena. Vol. 2. Academic Press. London. 1972.
[4] S. M. Konishi, A.L. Yuille, J.M. Coughlan and Song Chun Zhu. \Fundamen-
tal Bounds on Edge Detection: An Information Theoretic Evaluation of Dif-
ferent Edge Cues." In Proceedings Computer Vision and Pattern Recognition
CVPR'99. Fort Collins, Colorado. June 1999.
[5] A.B. Lee, D.B. Mumford, and J. Huang. \Occlusion Models of Natural Im-
ages: A Statistical Study of a Scale-Invariant Dead Leaf Model". International
Journal of Computer Vision. Vol. 41, No.'s 1/2. January/February 2001.
[6] J. Portilla and E. P. Simoncelli. \Parametric Texture Model based on Joint
Statistics of Complex Wavelet Coe∆cients". International Journal of Computer
Vision. October 2000.
[7] Y. W. Teh and M. Welling. \The Unied Propagation and Scaling Algorithm."
In Proceedings NIPS'01. 2001.
[8] J.S. Yedidia, W.T. Freeman, Y. Weiss, \Generalized Belief Propagation." In
Proceedings NIPS'00. 2000.
[9] A.L. Yuille. \CCCP Algorithms to Minimize the Bethe and Kikuchi Free En-
ergies," Neural Computation. In press. 2002.
[10] S.C. Zhu and D. Mumford. \Prior Learning and Gibbs Reaction-Diusion."
PAMI vol.19, no.11, pp1236-1250, Nov. 1997.
[11] S.C. Zhu, Y. Wu, and D. Mumford. \Minimax Entropy Principle and Its Ap-
plication to Texture Modeling". Neural Computation. Vol. 9. no. 8. Nov. 1997.

