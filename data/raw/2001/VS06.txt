Learning Body Pose via Specialized Maps
R'omer Rosales
Department of Computer Science
Boston University, Boston, MA 02215
rrosales@cs.bu.edu
Stan Sclaroff
Department of Computer Science
Boston University, Boston, MA 02215
sclaroff@cs.bu.edu
Abstract
A nonlinear supervised learning model, the Specialized Mappings
Architecture (SMA), is described and applied to the estimation of
human body pose from monocular images. The SMA consists of
several specialized forward mapping functions and an inverse map­
ping function. Each specialized function maps certain domains
of the input space (image features) onto the output space (body
pose parameters). The key algorithmic problems faced are those of
learning the specialized domains and mapping functions in an op­
timal way, as well as performing inference given inputs and knowl­
edge of the inverse function. Solutions to these problems employ
the EM algorithm and alternating choices of conditional indepen­
dence assumptions. Performance of the approach is evaluated with
synthetic and real video sequences of human motion.
1 Introduction
In everyday life, humans can easily estimate body part locations (body pose) from
relatively low­resolution images of the projected 3D world (e.g., when viewing a
photograph or a video). However, body pose estimation is a very difficult computer
vision problem. It is believed that humans employ extensive prior knowledge about
human body structure and motion in this task [10]. Assuming this, we consider
how a computer might learn the underlying structure and thereby infer body pose.
In computer vision, this task is usually posed as a tracking problem. Typically,
models comprised of 2D or 3D geometric primitives are designed for tracking a
specific articulated body [13, 5, 2, 15]. At each frame, these models are fitted to the
image to optimize some cost function. Careful manual placement of the model on
the first frame is required, and tracking in subsequent frames tends to be sensitive to
errors in initialization and numerical drift. Generally, these systems cannot recover
from tracking errors in the middle of a sequence. To address these weaknesses,
more complex dynamic models have been proposed [14, 13, 9]; these methods learn
a prior over some specific motion (such as walking). This strong prior however,
substantially limits the generality of the motions that can be tracked.

Departing from the aforementioned tracking paradigm, in [8] a Gaussian probability
model was learned for short human motion sequences. In [17] dynamic program­
ming was used to calculate the best global labeling according to the learned joint
probability density function of the position and velocity of body features. Still,
in these approaches, the joint locations, correspondences, or model initialization
must be provided by hand. In [1], the manifold of human body dynamics was mod­
eled via a hidden Markov model and learned via entropic minimization. In all of
these approaches models were learned. Although the approach presented here can
be used to model dynamics, we argue that when general human motion dynamics
are intended to be learned, the amount of training data, model complexity, and
computational resources required are impractical. As a consequence, models with
large priors towards specific motions (e.g., walking) are generated. In this paper we
describe a non­linear supervised learning algorithm, the Specialized Maps Archi­
tecture (SMA), for recovering articulated body pose from single monocular images.
This approach avoids the need for initialization and tracking per se, and reduces
the above mentioned disadvantages.
2 Specialized Maps
There at least two key characteristics of the problem we are trying to solve which
make it different from other supervised learning problems. First, we have access to
the inverse map. We are trying to learn unknown probabilistic maps from inputs to
outputs space, but we have access to the map (in general probabilistic) from outputs
to inputs. In our pose estimation problem, it is easy to see how we can artificially,
using computer graphics (CG), produce some visual features (e.g., body silhouettes)
given joint positions 1 . Second, it is one­to­many: one input can be associated with
more than one output. Features obtained from silhouettes (and many other visual
features) are ambiguous. Consider an occluded arm, or the reflective ambiguity
generated by symmetric poses. This last observation precludes the use of standard
algorithms for supervised learning that fit a single mapping function to the data.
Given input and output spaces ! c and ! t , and the inverse function i : ! t ! ! c , we
describe a solution for these supervised learning problems. Our approach consists
in generating a series of m functions OE k : ! c ! ! t . Each of these functions is
specialized to map only certain inputs (for a specialized sub­domain) better than
others. For example, each sub­domain can be a region of the input space. However,
the specialized sub­domain of OE k can be more general than just a connected region
in the input space.
Several other learning models use a similar concept of fitting surfaces to the observed
data by splitting the input space into several regions and approximating simpler
functions in these regions (e.g., [11, 7, 6]). However, in these approaches, the inverse
map is not incorporated in the estimation algorithm because it is not considered
in the problem definition and the forward model is usually more complex, making
inference and learning more difficult.
The key algorithmic problems are that of estimating the specialized domains and
functions in an optimal way (taking into account the form of the specialized func­
tions), and using the knowledge of the inverse function to formulate efficient infer­
1 Thus, i is a computer graphics rendering, in general called forward kinematics

ence and learning algorithms. We propose to determine the specialized domains
and functions using an approximate EM algorithm and to perform inference using,
in an alternating fashion, the conditional independence assumptions specified by
the forward and inverse models. Fig. 1(a) illustrates a learned forward model.
(a) (b)
Figure 1: SMA diagram illustrating (a) an already learned SMA model with m specialized
functions mapping subsets of the training data, each subset is drawn with a different color
(at initializations, coloring is random) and (b) the mean­output inference process in which a
given observation is mapped by all the specialized functions, and then a feedback matching
step, using i, is performed to choose the best of the m estimates.
3 Probabilistic Model
Let the training sets of output­input observations be \Psi = f/ 1 ; :::; /N g, and \Upsilon =
fAE 1 ; :::; AE N g respectively. We will use z i = (/ i ; AE i ) to define the given output­input
training pair, and Z = fz 1 ; :::; zN g as our observed training set.
We introduce the unobserved random variable y = (y 1 ; :::; yn ). In our model any y i
has domain the discrete set C = f1; :::; Mg of labels for the specialized functions, and
can be thought as the function number used to map data point i; thus M is the num­
ber of specialized mapping functions. Our model uses parameters ` = (` 1 ; :::; ` M ; –),
` k represents the parameters of the mapping function k; – = (– 1 ; :::; –M ), where
– k represents P (y i = kj`): the prior probability that mapping function with label
i will be used to map an unknown point. As an example, P (y i jz i ; `) represents the
probability that function number y i generated data point number i.
Using Bayes' rule and assuming independence of observations given `, we have the
log­probability of our data given the model log p(Z j`), which we want to maximize:
arg max
`
X
i
log
X
k
p(/ i jAE i ; y i = k; `)P (y i = kj`)p(AE i ); (1)
where we used the independence assumption p(AEj`) = p(AE). This is also equivalent
to maximizing the conditional likelihood of the model.
Because of the log­sum encountered, this problem is intractable in general. How­
ever, there exist practical approximate optimization procedures, one of them is
Expectation Maximization (EM) [3, 4, 12].
3.1 Learning
The EM algorithm is well known, therefore here we only provide the derivations
specific to SMA's. The E­step consists of finding P (y = kjz; `) = ~
P (y). Note that
the variables y i are assumed independent (given z i ). Thus, factorizing ~
P (y):

~
P (y) =
Y
i
~
P (t) (y i ) =
Y
i
[(– y i
p(/ i jAE i ; y i ; `))=(
X
k2C
– k p(/ i jAE i ; y i = k; `))] (2)
However, p(/ i jAE i ; y i = k; `) is still undefined. For the implementation described in
this paper we use N (/ i ; OE k (AE i ; ` k ); \Sigma k ), where ` k are the parameters of the k­th
specialized function, and \Sigma k the error covariance of the specialized function k. One
way to interpret this choice is to think that the error cost in estimating / once
we know the specialized function to use, is a Gaussian distribution with mean the
output of the specialized function and some covariance which is map dependent.
This also led to tractable further derivations. Other choices were given in [16].
The M­step consists of finding ` (t) = arg max ` E ~
P (t)
[log p(Z ; yj`)]. In our case we
can show that this is equivalent to finding:
arg min
`
X
i
X
k
~
P (t) (y i = k)(/ i \Gamma OE k (AE i ; ` k )) ? \Sigma \Gamma1
k (z i \Gamma OE k (z i ; ` k )): (3)
This gives the following update rules for – k and \Sigma k (where Lagrange multipliers
were used to incorporate the constraint that the sum of the – k 's is 1.
– k = 1
n
X
i
P (y i = kjz i ; `) (4)
\Sigma k =
X
i
~
P (t) (y i = k)(/ i \Gamma OE k (AE i ; ` k ))(/ i \Gamma OE k (AE i ; ` k )) ? =
X
i
~
P (t) (y i = k) (5)
In keeping the formulation general, we have not defined the form of the specialized
functions OE k . Whether or not we can find a closed form solution for the update of
` k depends on the form of OE k . For example if OE k is a non­linear function, we may
have to use iterative optimization to find ` (t)
k . In case OE k yield a quadratic form,
then a closed form update exists. However, in general we have:
@E
@` k
=
X
i
~
P i
(t)
(y i = k)[(
@
@` k
OE k (AE i ; ` k )) ? \Sigma \Gamma1
k (/ i \Gamma OE k (AE i ; ` k ))]; (6)
In our experiments, OE k is a 1­hidden layer perceptron. Thus, the M­step is an
approximate, iterative optimization procedure.
4 Inference
Once learning is accomplished, each specialized function maps (with different levels
of accuracy) the input space. We can formally state the inference process as that
of maximum­a­posteriori (MAP) estimation where we are interested in finding the
most likely output h given an input configuration x:
h \Lambda = arg max
h
p(hjx) = arg max
h
X
y
p(hjy; x)P (y); (7)
Any further treatment depends on the properties of the probability distributions
involved. If p(hjx; y) = N (h; OE y (x); \Sigma y ), the MAP estimate involves finding the
maximum in a mixture of Gaussians. However, no closed form solution exists and
moreover, we have not incorporated the potentially useful knowledge of the inverse
function i.

4.1 MAP by Using the Inverse Function i
The access to a forward kinematics function i (called here the inverse function)
allows to formulate a different inference algorithm. We are again interested in
finding an optimal h \Lambda given an input x (e.g., an optimal body pose given features
taken from an image). This can be formulated as:
h \Lambda = arg max
h
p(hjx) = arg max
h
p(xjh)
X
y
p(hjy; x)P (y); (8)
simply by Bayes' rule, and marginalizing over all variables except h. Note that we
have made the distribution p(xjh) appear in the solution. This is important because
we can know use our knowledge of i to define this distribution. This solution is
completely general within our architecture, we did not make any assumptions on
the form of the distributions or algorithms used.
5 Approximate Inference using i
Let us assume that we can approximate
P
y p(hjy; x)P (y) by a set of samples gen­
erated according to p(hjy; x)P (y) and a kernel function K(h; h s ). Denote the set
of samples HSpl = fh s g s=1:::S . An approximate to
P
y p(hjy; x)P (y) is formally
built by 1
S
P S
s=1 K(h; h s ), with the normalizing condition
R K(h; h s )dh = 1 for
any given h s .
We will consider two simple forms of K. If K(h; h s ) = ffi(h \Gamma h s ), we have: “ h =
arg maxh p(xjh)
P S
s=1 ffi(h \Gamma h s ):
After some simple manipulations, this can be reduced to the following equivalent
discrete optimization problem whose goal is to find the most likely sample s \Lambda :
s \Lambda = arg max
s
p(xjh s ) = arg min
s
(x \Gamma i(h s )) ? \Sigma i (x \Gamma i(h s )); (9)
where the last equivalence used the assumption p(xjh) = N (x; i(h); \Sigma i ).
If K(h; h s ) = N (h; h s ; \Sigma Spl ), we have: “
h = arg maxh p(xjh)
P S
s=1 N (h; h s ; \Sigma Spl ).
This case is hard to use in practice, because contrary to the case above (Eq. 9), in
general, there is no guarantee that the optimal h is among the samples.
5.1 A Deterministic Approximation based on the Functions Mean
Output
The structure of the inference in SMA, and the choice of probabilities p(hjx; y)
allows us to construct a newer approximation that is considerably less expensive to
compute, and it is deterministic. Intuitively they idea consists of asking each of the
specialized functions OE k what their most likely estimate for h is, given the observed
input x. The opinions of each of these specialized functions are then evaluated
using our distribution p(xjh) similar to the above sampling method.
This can be justified by the observation that the probability of the mean is maximal
in a Gaussian distribution. Thus by considering the means OE k (x), we would be
considering the most likely output of each specialized function. Of course, in many
cases this approximation could be very far from the best solution, for example when

the uncertainty in the function estimate is relatively high relative to the difference
between means.
We use Fig. 1(b) to illustrate the mean­output (MO) approximate inference process.
When generating an estimate of body pose, denoted “
h, given an input x (the gray
point with a dark contour in the lower plane), the SMA generates a series of output
hypotheses H OE = fh OE
k g k obtained using h k = OE k (x), with k 2 C (illustrated by each
of the points pointed to by the arrows).
Given the set H OE , the most accurate hypothesis under the mean­output criteria is
the one that minimizes the function:
k \Lambda = arg min
k
(x \Gamma i(h OE
k )) ? \Sigma i (x \Gamma i(h OE
k )); (10)
where in the last equation we have assumed p(xjh) is Gaussian.
5.2 Bayesian Inference
Note that in many cases, there may not be any need to simply provide a point
estimate, in terms of a most likely output h. In fact we could instead use the whole
distribution found in the inference process. We can show that using the above
choices for K we can respectively obtain.
p(hjx) = 1
S
S
X
s=1
N (x; i(h s ); \Sigma i ); (11)
p(hjx) = N (h; h s ; \Sigma Spl )
S
X
s=1
N (x; i(h); \Sigma i ): (12)
6 Experiments
The described architecture was tested using a computer graphics rendering as our
i inverse function. The training data set consisted of approx. 7,000 frames of
human body poses obtained through motion capture. The output consisted of 20
2D marker positions (i.e., 3D markers projected to the image plane using a per­
spective model) but linearly encoded by 8 real values using Principal Component
Analysis (PCA). The input (visual features) consisted of 7 real­valued Hu moments
computed on synthetically generated silhouettes of the articulated figure. For train­
ing/testing we generated 120,000 data points: our 3D poses from motion capture
were projected to 16 views along the view­sphere equator. We took 8,000 for train­
ing and the rest for testing. The only free parameter in this test, related to the
given SMA, was the number of specialized functions used; this was set to 15. For
this, several model selection approaches could be used instead. Due to space limita­
tions, in this paper we show results using the mean­output inference algorithm only,
readers are referred to http://cs­people.bu.edu/rrosales/SMABodyInference where
inference using multiple samples is shown.
Fig. 2(left) shows the reconstruction obtained in several single images coming from
three different artificial sequences. The agreement between reconstruction and ob­
servation is easy to perceive for all sequences. Note that for self­occluding configu­
rations, reconstruction is harder, but still the estimate is close to ground­truth. No

human intervention nor pose initialization was required. For quantitative results,
Fig. 2(right) shows the average marker error and variance per body orientation in
percentage of body height. Note that the error is bigger for orientations closer
to 0 and ß radians. This intuitively agrees with the notion that at those angles
(side­views), there is less visibility of the body parts. We consider this performance
promising, given the complexity of the task and the simplicity of the approach. By
choosing poses at random from training set, the RMSE was 17% of body height. In
related work, quantitative performance have been usually ignored, in part due to
the lack of ground­truth and standard evaluation data sets.
40 100
20
40
60
80
20
80
100
120
60
40 100
20
100
120
60
50
100 200 300
100
150
200
250
300
350
250 350
50 150 100 200 300
100
150
200
250
300
350
350
40 100
20
40
60
80
20
80
100
120
60
40 100
20
100
120
60
50
100 200 300
100
150
200
250
300
350
250 350
50 150 100 200 300
100
150
200
250
300
350
350
40 100
20
40
60
80
20
80
100
120
60
40 100
20
100
120
60
50
100 200 300
100
150
200
250
300
350
250 350
50 150 100 200 300
100
150
200
250
300
350
350
0 2 4 6 8 10 12 14 16
2
2.15
2.3
2.45
2.6
2.75
2.9
Performance regarding camera viewpoint (16 total)
Viewpoint x 2p/32 (=11.25 o
%
RMSE
(per
marker)
Figure 2: Left: Example reconstruction of several test sequences with CG­generated
silhouettes. Each set consists of input images and reconstruction (every 5th frame). Right:
Marker root­mean­square­error and variance per camera viewpoint (every 2ß=32 rads.).
Units are percentage of body height. Approx. 110,000 test poses were used.
6.1 Experiments using Real Visual Cues
Fig. 3 shows examples of system performance with real segmented visual data,
obtained from observing a human subject. Reconstruction for several relatively
complex sequences are shown. Note that even though the characteristics of the
segmented body differ from the ones used for training, good performance is still
achieved. Most reconstructions are visually close to what can be thought as the
right pose reconstruction. Body orientation is also generally accurate.
7 Conclusion
In this paper, we have proposed the Specialized Mappings Architecture (SMA). A
learning algorithm was developed for this architecture using ideas from ML estima­
tion and latent variable models. Inference was based on the possibility of alterna­
tively use different sets of conditional independence assumptions specified by the
forward and inverse models. The incorporation of the inverse function in the model
allows for simpler forward models. For example the inverse function is an architec­
tural alternative to the gating networks of Mixture of Experts [11]. SMA advantages
for body pose estimation include: no iterative methods for inference are used, the

40 180 200
100
120
140
160 120 140
80 200
20
40
60
80
20 160 180
100
120
140
160 120 140
80
20
40 160 180
100
120
140
160 120 140
60 200
20
40
60
80
20 160 180
100
120
140
160
200 300
100
150
200
250
300
350
350
50
100 200 300
100
150
200
250
300
350
250 350
50 150 100 200 300
100
150
200
250
300
350
350
50
100 200
100
150
200
250
300
350
40 180 200
100
120
140
160 120 140
80 200
20
40
60
80
20 160 180
100
120
140
160 120 140
80
20
40 160 180
100
120
140
160 120 140
60 200
20
40
60
80
20 160 180
100
120
140
160
200 300
100
150
200
250
300
350
350
50
100 200 300
100
150
200
250
300
350
250 350
50 150 100 200 300
100
150
200
250
300
350
350
50
100 200
100
150
200
250
300 350
Figure 3: Reconstruction obtained from observing a human subject (every 10th frame).
algorithm for inference runs in constant time and scales only linearly O(M) with
respect to the number of specialized functions M ; manual initialization is not re­
quired; compared to approaches that learn dynamical models, the requirements for
data are much smaller, and also large priors to specific motions are prevented thus
improving generalization capabilities.
References
[1] M. Brand. Shadow puppetry. In ICCV, 1999.
[2] C. Bregler. Tracking people with twists and exponential maps. In CVPR, 1998.
[3] I. Csiszar and G. Tusnady. Information geometry and alternating minimization pro­
cedures. Statistics and Decisions, 1:205--237, 1984.
[4] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood estimation from incom­
plete data. Journal of the Royal Statistical Society (B), 39(1), 1977.
[5] J. Deutscher, A. Blake, and I. Reid. Articulated body motion capture by annealed
particle filtering. In CVPR, 2000.
[6] J.H. Friedman. Multivatiate adaptive regression splines. The Annals of Statistics,
19,1­141, 1991.
[7] G. Hinton, B. Sallans, and Z. Ghahramani. A hierarchical community of experts.
Learning in Graphical Models, M. Jordan (editor), 1998.
[8] N. Howe, M. Leventon, and B. Freeman. Bayesian reconstruction of 3d human motion
from single­camera video. In NIPS­12, 2000.
[9] M. Isard and A. Blake. Contour tracking by stochastic propagation of conditional
density. In ECCV, 1996.
[10] G. Johansson. Visual perception of biological motion and a model for its analysis.
Perception and Psychophysics, 14(2): 210­211, 1973.
[11] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm.
Neural Computation, 6, 181­214, 1994.
[12] R. Neal and G. Hinton. A view of the em algorithm that justifies incremental, sparse,
and other variants. Learning in Graphical Models, M. Jordan (editor), 1998.
[13] Dirk Ormoneit, Hedvig Sidenbladh, Michael J. Black, and Trevor Hastie. Learning
and tracking cyclic human motion. In NIPS­13, 2001.
[14] Vladimir Pavlovi'c, James M. Rehg, and John MacCormick. Learning switching linear
models of human motion. In NIPS­13, 2001.
[15] J. M. Regh and T. Kanade. Model­based tracking of self­occluding articulated objects.
In ICCV, 1995.
[16] R. Rosales and S. Sclaroff. Specialized mappings and the estimation of body pose
from a single image. In IEEE Human Motion Workshop, 2000.
[17] Y. Song, Xiaoling Feng, and P. Perona. Towards detection of human motion. In
CVPR, 2000.

