Gaussian Process Regression with
Mismatched Models
Peter Sollich
Department of Mathematics, King's College London
Strand, London WC2R 2LS, U.K. Email peter.sollich@kcl.ac.uk
Abstract
Learning curves for Gaussian process regression are well understood
when the `student' model happens to match the `teacher' (true data
generation process). I derive approximations to the learning curves
for the more generic case of mismatched models, and find very rich
behaviour: For large input space dimensionality, where the results
become exact, there are universal (student­independent) plateaux
in the learning curve, with transitions in between that can exhibit
arbitrarily many over­fitting maxima; over­fitting can occur even
if the student estimates the teacher noise level correctly. In lower
dimensions, plateaux also appear, and the learning curve remains
dependent on the mismatch between student and teacher even in
the asymptotic limit of a large number of training examples. Learn­
ing with excessively strong smoothness assumptions can be partic­
ularly dangerous: For example, a student with a standard radial
basis function covariance function will learn a rougher teacher func­
tion only logarithmically slowly. All predictions are confirmed by
simulations.
1 Introduction
There has in the last few years been a good deal of excitement about the use
of Gaussian processes (GPs) as an alternative to feedforward networks [1]. GPs
make prior assumptions about the problem to be learned very transparent, and
even though they are non­parametric models, inference---at least in the case of
regression considered below---is relatively straightforward. One crucial question
for applications is then how `fast' GPs learn, i.e. how many training examples are
needed to achieve a certain level of generalization performance. The typical (as
opposed to worst case) behaviour is captured in the learning curve, which gives the
average generalization error ffl as a function of the number of training examples n.
Good bounds and approximations for ffl(n) are now available [1, 2, 3, 4, 5], but these
are mostly restricted to the case where the `student' model exactly matches the true
`teacher' generating the data 1 . In practice, such a match is unlikely, and so it is
1 The exception is the elegant work of Malzahn and Opper [2], which uses a statistical
physics framework to derive approximate learning curves that also apply for any fixed
target function. However, this framework has not yet to my knowledge been exploited to

important to understand how GPs learn if there is some model mismatch. This is
the aim of this paper.
In its simplest form, the regression problem is this: We are trying to learn a function
` \Lambda which maps inputs x (real­valued vectors) to (real­valued scalar) outputs ` \Lambda (x).
We are given a set of training data D, consisting of n input­output pairs (x l ; y l );
the training outputs y l may differ from the `clean' teacher outputs ` \Lambda (x l ) due to
corruption by noise. Given a test input x, we are then asked to come up with a
prediction “
`(x), plus error bar, for the corresponding output `(x). In a Bayesian
setting, we do this by specifying a prior P (`) over hypothesis functions, and a like­
lihood P (Dj`) with which each ` could have generated the training data; from this
we deduce the posterior distribution P (`jD) / P (Dj`)P (`). For a GP, the prior is
defined directly over input­output functions `; this is simpler than for a Bayesian
feedforward net since no weights are involved which would have to be integrated
out. Any ` is uniquely determined by its output values `(x) for all x from the in­
put domain, and for a GP, these are assumed to have a joint Gaussian distribution
(hence the name). If we set the means to zero as is commonly done, this distri­
bution is fully specified by the covariance function h`(x)`(x 0 )i ` = C(x; x 0 ). The
latter transparently encodes prior assumptions about the function to be learned.
Smoothness, for example, is controlled by the behaviour of C(x; x 0 ) for x 0 ! x: The
Ornstein­Uhlenbeck (OU) covariance function C(x; x 0 ) = exp(\Gammajx \Gamma x 0 j=l) produces
very rough (non­differentiable) functions, while functions sampled from the radial
basis function (RBF) prior with C(x; x 0 ) = exp[\Gammajx \Gamma x 0 j 2 =(2l 2 )] are infinitely differ­
entiable. Here l is a lengthscale parameter, corresponding directly to the distance
in input space over which we expect significant variation in the function values.
There are good reviews on how inference with GPs works [1, 6], so I only give
a brief summary here. The student assumes that outputs y are generated from
the `clean' values of a hypothesis function `(x) by adding Gaussian noise of x­
independent variance oe 2 . The joint distribution of a set of training outputs fy l g
and the function values `(x) is then also Gaussian, with covariances given (under
the student model) by
hy l y m i = C(x l ; x m ) + oe 2 ffi lm = (K)lm ; hy l `(x)i = C(x l ; x) = (k(x)) l
Here I have defined an n \Theta n matrix K and an x­dependent n­component vector
k(x). The posterior distribution P (`jD) is then obtained by conditioning on the
fy l g; it is again Gaussian and has mean and variance
h`(x)i `jD j “
`(xjD) = k(x) T K \Gamma1 y (1)
h(`(x) \Gamma “
`(x)) 2 i `jD = C(x; x) \Gamma k(x) T K \Gamma1 k(x) (2)
From the student's point of view, this solves the inference problem: The best pre­
diction for `(x) on the basis of the data D is “ `(xjD), with a (squared) error bar
given by (2). The squared deviation between the prediction and the teacher is
[ “
`(xjD) \Gamma ` \Lambda (x)] 2 ; the average generalization error (which, as a function of n, de­
fines the learning curve) is obtained by averaging this over the posterior distribution
of teachers, all datasets, and the test input x:
ffl = hhh[ “ `(xjD) \Gamma ` \Lambda (x)] 2 i ` \Lambda jD i D i x (3)
Now of course the student does not know the true posterior of the teacher; to
estimate ffl, she must assume that it is identical to the student posterior, giving
from (2)
“ ffl = hhh[ “ `(xjD) \Gamma `(x)] 2 i `jD i D i x = hhC(x; x) \Gamma k(x) T K \Gamma1 k(x)i fx l g i x (4)
consider systematically the effects of having a mismatch between the teacher prior over
target functions and the prior assumed by the student.

where in the last expression I have replaced the average over D by one over the
training inputs since the outputs no longer appear. If the student model matches
the true teacher model, ffl and “ ffl coincide and give the Bayes error, i.e. the best
achievable (average) generalization performance for the given teacher.
I assume in what follows that the teacher is also a GP, but with a possibly different
covariance function C \Lambda (x; x 0 ) and noise level oe 2
\Lambda . This allows eq. (3) for ffl to be
simplified, since by exact analogy with the argument for the student posterior
h` \Lambda (x)i ` \Lambda jD =k \Lambda (x) T K \Gamma1
\Lambda y; h` 2
\Lambda (x)i ` \Lambda jD =h` \Lambda (x)i 2
` \Lambda jD +C \Lambda (x; x)\Gammak \Lambda (x) T K \Gamma1
\Lambda k \Lambda (x)
and thus, abbreviating a(x) = K \Gamma1 k(x) \Gamma K \Gamma1
\Lambda k \Lambda (x),
ffl = hha(x) T yy T a(x) + C \Lambda (x; x) \Gamma k \Lambda (x) T K \Gamma1
\Lambda k \Lambda (x)i D i x
Conditional on the training inputs, the training outputs have a Gaussian distribu­
tion given by the true (teacher) model; hence hyy T i fy l gjfx l g = K \Lambda , giving
ffl = hhC \Lambda (x; x) \Gamma 2k \Lambda (x) T K \Gamma1 k(x) + k(x) T K \Gamma1 K \Lambda K \Gamma1 k(x)i fx l g i x (5)
2 Calculating the learning curves
An exact calculation of the learning curve ffl(n) is difficult because of the joint av­
erage in (5) over the training inputs X and the test input x. A more convenient
starting point is obtained if (using Mercer's theorem) we decompose the covariance
function into its eigenfunctions OE i (x) and eigenvalues \Lambda i , defined w.r.t. the input
distribution so that hC(x; x 0 )OE i (x 0 )i x 0 = \Lambda i OE i (x) with the corresponding normaliza­
tion hOE i (x)OE j (x)i x = ffi ij . Then
C(x; x 0 ) =
1
X
i=1
\Lambda i OE i (x)OE i (x 0 ); and similarly C \Lambda (x; x 0 ) =
1
X
i=1
\Lambda \Lambda
i OE i (x)OE i (x 0 ) (6)
For simplicity I assume here that the student and teacher covariance functions have
the same eigenfunctions (but different eigenvalues). This is not as restrictive as it
may seem; several examples are given below. The averages over the test input x
in (5) are now easily carried out: E.g. for the last term we need
h(k(x)k(x) T ) lm i x =
X
ij
\Lambda i \Lambda j OE i (x l )hOE i (x)OE j (x)i x OE j (x m ) =
X
i
\Lambda 2
i OE i (x l )OE i (x m )
Introducing the diagonal eigenvalue matrix (\Lambda) ij = \Lambda i ffi ij and the `design matrix'
(\Phi) li = OE i (x l ), this reads hk(x)k(x) T i x = \Phi\Lambda 2 \Phi T . Similarly, for the second term
in (5), hk(x)k T
\Lambda (x)i x = \Phi\Lambda\Lambda \Lambda \Phi T , and hC \Lambda (x; x)i x = tr \Lambda \Lambda . This gives, dropping
the training inputs subscript from the remaining average,
ffl = htr \Lambda \Lambda \Gamma 2 tr \Phi\Lambda\Lambda \Lambda \Phi T K \Gamma1 + tr \Phi\Lambda 2 \Phi T K \Gamma1 K \Lambda K \Gamma1 i
In this new representation we also have K = oe 2 I + \Phi\Lambda\Phi T and similarly for K \Lambda ;
for the inverse of K we can use the Woodbury formula to write K \Gamma1 = oe \Gamma2 [I \Gamma
oe \Gamma2 \PhiG \Phi T ], where G = (\Lambda \Gamma1 + oe \Gamma2 \Phi T \Phi) \Gamma1 . Inserting these results, one finds after
some algebra that
ffl = oe 2
\Lambda oe \Gamma2 \Theta htr Gi \Gamma htr G \Lambda \Gamma1 Gi
\Lambda + htr G \Lambda \Lambda \Lambda \Gamma2 Gi (7)
which for the matched case reduces to the known result for the Bayes error [4]
“ ffl = htr Gi (8)

Eqs. (7,8) are still exact. We now need to tackle the remaining averages over training
inputs. Two of these are of the form htr GMGi; if we generalize the definition of
G to G = (\Lambda \Gamma1 + vI + wM + oe \Gamma2 \Phi T \Phi) \Gamma1 and define g = htr Gi, then they reduce
to htr GMGi = \Gamma@ g=@w. (The derivative is taken at v = w = 0; the idea behind
introducing v will become clear shortly.) So it is sufficient to calculate g. To do
this, consider how G changes when a new example is added to the training set. One
has
G(n + 1) \Gamma G(n) =
\Theta
G \Gamma1 (n) + oe \Gamma2 // T
\Lambda \Gamma1
\Gamma G(n) = \Gamma G(n)// T G(n)
oe 2 + / T G(n)/ (9)
in terms of the vector / with elements (/) i = OE i (x n+1 ), using again the Woodbury
formula. To obtain the change in g we need the average of (9) over both the
new training input xn+1 and all previous ones. This cannot be done exactly, but
we can approximate by averaging numerator and denominator separately; taking
the trace then gives g(n + 1) \Gamma g(n) = \Gammahtr G 2 (n)i=[oe 2 + g(n)]. Now, using our
auxiliary parameter v, \Gammahtr G 2 i = @g=@v; if we also approximate n as continuous,
we get the simple partial differential equation @g=@n = (@g=@v)=(oe 2 + g) with the
initial condition gj n=0 = tr (\Lambda \Gamma1 + vI + wM) \Gamma1 . Solving this using the method of
characteristics [7] gives a self­consistent equation for g,
g = tr
Ÿ
\Lambda \Gamma1 +
`
v + n
oe 2 + g
'
I + wM
– \Gamma1
(10)
The Bayes error (8) is “ ffl = gj v=w=0 and therefore obeys
“ ffl = tr G; G \Gamma1 = \Lambda \Gamma1 + n
oe 2 + “ ffl
I (11)
within our approximation (called `LC' in [4]). To obtain ffl, we differentiate both
sides of (10) w.r.t. w, set v = w = 0 and rearrange to give
htr GMGi = \Gamma@ g=@w = (tr MG 2 )=[1 \Gamma (tr G 2 )n=(oe 2 + “ffl) 2 ]
Using this result in (7), with M = \Lambda \Gamma1 and M = \Lambda \Gamma1 \Lambda \Lambda \Lambda \Gamma1 , we find after some
further simplifications the final (approximate) result for the learning curve:
ffl = “ffl oe 2
\Lambda tr G 2 + n \Gamma1 (oe 2 + “ffl) 2 tr \Lambda \Lambda \Lambda \Gamma2 G 2
oe 2 tr G 2 + n \Gamma1 (oe 2 + “
ffl) 2 tr \Lambda \Gamma1 G 2
(12)
which transparently shows how in the matched case ffl and “ffl become identical.
3 Examples
I now apply the result for the learning curve (11,12) to some exemplary learning
scenarios. First, consider inputs x which are binary vectors 2 with d components
x a 2 f\Gamma1; 1g, and assume that the input distribution is uniform. We consider
covariance functions for student and teacher which depend on the product x \Delta x 0
only; this includes the standard choices (e.g. OU and RBF) which depend on the
Euclidean distance jx \Gamma x 0 j, since jx \Gamma x 0 j 2 = 2d \Gamma 2x \Delta x 0 . All these have the same
eigenfunctions [9], so our above assumption is satisfied. The eigenfunctions are
indexed by subsets ae of f1; 2 : : : dg and given explicitly by OE ae (x) =
Q
a2ae x a . The
2 This scenario may seem strange, but simplifies the determination of the eigenfunctions
and eigenvalues. For large d, one expects other distributions with continuously varying
x and the same first­ and second­order statistics (hxai = 0, hxax b i = ffi ab ) to give similar
results [8].

corresponding eigenvalues depend only on the size s = jaej of the subsets and are
therefore ( d
s )­fold degenerate; letting e = (1; 1 : : : 1) be the `all ones' input vector,
they have the values \Lambda s = hC(x; e)OE ae (x)i x (which can easily be evaluated as an
average over two binomially distributed variables, counting the number of +1's in
x overall and among the x a with a 2 ae). With the \Lambda s and \Lambda \Lambda
s determined, it is then
a simple matter to evaluate the predicted learning curve (11,12) numerically. First,
though, focus on the limit of large d, where much more can be said. If we write
C(x; x 0 ) = f(x \Delta x 0 =d), the eigenvalues become, for d ! 1, \Lambda s = d \Gammas f (s) (0) and
the contribution to C(x; x) = f(1) from the s­th eigenvalue block is – s j ( d
s )\Lambda s !
f (s) (0)=s!, consistent with f(1) =
P 1
s=0 f (s) (0)=s! The \Lambda s , because of their scaling
with d, become infinitely separated for d !1. For training sets of size n = O(d L ),
we then see from (11) that eigenvalues with s ? L contribute as if n = 0, since
\Lambda s AE n=(oe 2 + “ffl); they have effectively not yet been learned. On the other hand,
eigenvalues with s ! L are completely suppressed and have been learnt perfectly.
We thus have a hierarchical learning scenario, where different scalings of n with
d---as defined by L---correspond to different `learning stages'. Formally, we can
analyse the stages separately by letting d !1 at a constant ratio ff = n=( d
L ) of the
number of examples to the number of parameters to be learned at stage L (note
( d
L ) = O(d L ) for large d). An independent (replica) calculation along the lines of
Ref. [8] shows that our approximation for the learning curve actually becomes exact
in this limit. The resulting ff­dependence of ffl can be determined explicitly: Set
fL =
P
s–L – s (so that f 0 = f(1)) and similarly for f \Lambda
L . Then for large ff,
ffl = f \Lambda
L+1 + (f \Lambda
L+1 + oe 2
\Lambda )ff \Gamma1 +O(ff \Gamma2 ) (13)
This implies that, during successive learning stages, (teacher) eigenvalues are learnt
one by one and their contribution eliminated from the generalization error, giving
plateaux in the learning curve at ffl = f \Lambda
1 , f \Lambda
2 , . . . . These plateaux, as well as the
asymptotic decay (13) towards them, are universal [8], i.e. student­independent.
The (non­universal) behaviour for smaller ff can also be fully characterized: Con­
sider first the simple case of linear perceptron learning (see e.g. [7]), which corre­
sponds to both student and teacher having simple dot­product covariance functions
C(x; x 0 ) = C \Lambda (x; x 0 ) = x \Delta x 0 =d. In this case there is only a single learning stage (only
– 1 = – \Lambda
1 = 1 are nonzero), and ffl = r(ff) decays from r(0) = 1 to r(1) = 0, with
an over­fitting maximum around ff = 1 if oe 2 is sufficiently small compared to oe 2
\Lambda .
In terms of this function r(ff), the learning curve at stage L for general covariance
functions is then exactly given by ffl = f \Lambda
L+1 + – \Lambda
L r(ff) if in the evaluation of r(ff)
the effective noise levels ~ oe 2 = (f L+1 + oe 2 )=–L and ~ oe 2
\Lambda = (f \Lambda
L+1 + oe 2
\Lambda )=– \Lambda
L are used.
Note how in ~
oe 2
\Lambda , the contribution f \Lambda
L+1 from the not­yet­learned eigenvalues acts as
effective noise, and is normalized by the amount of `signal' – \Lambda
L = f \Lambda
L \Gamma f \Lambda
L+1 available
at learning stage L. The analogous definition of ~
oe 2 implies that, for small oe 2 and
depending on the choice of student covariance function, there can be arbitrarily
many learning stages L where ~
oe 2 ø ~ oe 2
\Lambda , and therefore arbitrarily many over­fitting
maxima in the resulting learning curves. From the definitions of ~
oe 2 and ~
oe 2
\Lambda it is
clear that this situation can occur even if the student knows the exact teacher noise
level, i.e. even if oe 2 = oe 2
\Lambda .
Fig. 1(left) demonstrates that the above conclusions hold not just for d !1; even
for the cases shown, with d = 10, up to three over­fitting maxima are apparent.
Our theory provides a very good description of the numerically simulated learning
curves even though, at such small d, the predictions are still significantly different
from those for d !1 (see Fig. 1(right)) and therefore not guaranteed to be exact.
In the second example scenario, I consider continuous­valued input vectors, uni­

1 2 3 4
a
L=1
1 2 3 4
a
L=2
1 2 3 4
a
L=3
10 100
n
0
1
2
e
Figure 1: Left: Learning curves for RBF student and teacher, with uniformly dis­
tributed, binary input vectors with d = 10 components. Noise levels: Teacher
oe 2
\Lambda = 1, student oe 2 = 10 \Gamma4 ; 10 \Gamma3 ; : : : ; 1 (top to bottom). Length scales: Teacher
l \Lambda = d 1=2 , student l = 2d 1=2 . Dashed: numerical simulations, solid: theoretical
prediction. Right: Learning curves for oe 2 = 10 \Gamma4 and increasing d (top to bottom:
10, 20, 30, 40, 60, 80, [bold] 1). The x­axis shows ff = n=( d
L ), for learning stages
L = 1; 2; 3; the dashed lines are the universal asymptotes (13) for d !1.
formly distributed over the unit interval [0; 1]; generalization to d dimensions
(x 2 [0; 1] d ) is straightforward. For covariance functions which are stationary, i.e.
dependent on x and x 0 only through x \Gamma x 0 , and assuming periodic boundary condi­
tions (see [4] for details), one then again has covariance function­independent eigen­
functions. They are indexed by integers 3 q, with OE q (x) = e 2ßiqx ; the corresponding
eigenvalues are \Lambda q =
R
dx C(0; x)e \Gamma2ßiqx . For the (`periodified') RBF covariance
function C(x; x 0 ) = exp[\Gamma(x \Gamma x 0 ) 2 =(2l 2 )], for example, one has \Lambda q / exp(\Gamma~q 2 =2),
where ~
q = 2ßlq. The OU case C(x; x 0 ) = exp(\Gammajx \Gamma x 0 j=l), on the other hand,
gives \Lambda q / (1 + ~ q 2 ) \Gamma1 , thus \Lambda q / q \Gamma2 for large q. I also consider below covariance
functions which interpolate in smoothness between the OU and RBF limits: E.g.
the MB2 (modified Bessel) covariance C(x; x 0 ) = e \Gammaa (1 + a), with a = jx \Gamma x 0 j=l,
yields functions which are once differentiable [5]; its eigenvalues \Lambda q / (1 + ~ q 2 ) \Gamma2
show a faster asymptotic power law decay, \Lambda q / q \Gamma4 , than those of the OU covari­
ance function. To subsume all these cases I assume in the following analysis of the
general shape of the learning curves that \Lambda q / q \Gammar (and similarly \Lambda \Lambda
q / q \Gammar \Lambda ). Here
r = 2 for OU, r = 4 for MB2, and (due to the faster­than­power law decay of its
eigenvalues) effectively r = 1 for RBF.
From (11,12), it is clear that the n­dependence of the Bayes error “ ffl has a strong
effect on the true generalization error ffl. From previous work [4], we know that “ ffl(n)
has two regimes: For small n, where “ ffl AE oe 2 , “ ffl is dominated by regions in input
space which are too far from the training examples to have significant correlation
with them, and one finds “ffl / n \Gamma(r\Gamma1) . For much larger n, learning is essentially
against noise, and one has a slower decay “ffl / (n=oe 2 ) \Gamma(r\Gamma1)=r . These power laws can
be derived from (11) by approximating factors such as [\Lambda \Gamma1
q +n=(oe 2 +“ffl)] \Gamma1 as equal
to either \Lambda q or to 0, depending on whether n=(oe 2 + “ffl) ! or ? \Lambda \Gamma1
q . With the same
technique, one can estimate the behaviour of ffl from (12). In the small n­regime, one
finds ffl ß c 1 oe 2
\Lambda + c 2 n \Gamma(r \Lambda \Gamma1) , with prefactors c 1 , c 2 depending on the student. Note
3 Since \Lambda q = \Lambda \Gammaq , one can assume q – 0 if all \Lambda q for q ? 0 are taken as doubly
degenerate.

0.1
1
e s 2 =10 -1
10 100
n
0.1
1
e s 2
=10 -3
1 10 100 1000
n
10 -3
10 -2
10 -1
10 0
10 1
e
OU
RBF
MB2
1 10 100
0.01
0.1
1
0.1
1
e s 2
=10 -2
Figure 2: Learning curves for inputs x uniformly distributed over [0; 1]. Teacher:
MB2 covariance function, lengthscale l \Lambda = 0:1, noise level oe 2
\Lambda = 0:1; student length­
scale l = 0:1 throughout. Dashed: simulations, solid: theory. Left: OU student
with oe 2 as shown. The predicted plateau appears as oe 2 decreases. Right: Stu­
dents with oe 2 = 0:1 and covariance function as shown; for clarity, the RBF and
OU results have been multiplied by
p
10 and 10, respectively. Dash­dotted lines
show the predicted asymptotic power laws for MB2 and OU; the RBF data have a
persistent upward curvature consistent with the predicted logarithmic decay. Inset:
RBF student with oe 2 = 10 \Gamma3 , showing the occurrence of over­fitting maxima.
that the contribution proportional to oe 2
\Lambda is automatically negligible in the matched
case (since then ffl = “ ffl AE oe 2 = oe 2
\Lambda for small n); if there is a model mismatch, however,
and if the small­n regime extends far enough, it will become significant. This is the
case for small oe 2 ; indeed, for oe 2 ! 0, the `small n' criterion “ ffl AE oe 2 is satisfied for
any n. Our theory thus predicts the appearance of plateaux in the learning curves,
becoming more pronounced as oe 2 decreases; Fig. 2(left) confirms this 4 . Numerical
evaluation also shows that for small oe 2 , over­fitting maxima may occur before the
plateau is reached, consistent with simulations; see inset in Fig. 2(right). In the
large n­regime (“ffl ø oe 2 ), our theory predicts that the generalization error decays as
a power law. If the student assumes a rougher function than the teacher provides
(r ! r \Lambda ), the asymptotic power law exponent ffl / n \Gamma(r\Gamma1)=r is determined by the
student alone. In the converse case, the asymptotic decay is ffl / n \Gamma(r \Lambda \Gamma1)=r and
can be very slow, actually becoming logarithmic for an RBF student (r !1). For
r = r \Lambda , the fastest decay for given r \Lambda is obtained, as expected from the properties of
the Bayes error. The simulation data in Fig. 2 are compatible with these predictions
(though the simulations cover too small a range of n to allow exponents to be
determined precisely). It should be stressed that the above results imply that there
is no asymptotic regime of large training sets in which the learning curve assumes a
universal form, in contrast to the case of parametric models where the generalization
error decays as ffl / 1=n for sufficiently large n independently of model mismatch
(as long as the problem is learnable at all). This conclusion may seem counter­
intuitive, but becomes clear if one remembers that a GP covariance function with
an infinite number of nonzero eigenvalues \Lambda i always has arbitrarily many eigenvalues
4 If oe 2 = 0 exactly, the plateau will extend to n ! 1. With hindsight, this is clear:
a GP with an infinite number of nonzero eigenvalues has no limit on the number of its
`degrees of freedom' and can fit perfectly any amount of noisy training data, without ever
learning the true teacher function.

that are arbitrarily close to zero (since the \Lambda i are positive and
P
i \Lambda i = hC(x; x)i is
finite). Whatever n, there are therefore many eigenvalues for which \Lambda \Gamma1
i AE n=oe 2 ,
corresponding to degrees of freedom which are still mainly determined by the prior
rather than the data (compare (11)). In other words, a regime where the data
completely overwhelms the mismatched prior---and where the learning curve could
therefore become independent of model mismatch---can never be reached.
In summary, the above approximate theory makes a number of non­trivial predic­
tions for GP learning with mismatched models, all borne out by simulations: for
large input space dimensions, the occurrence of multiple over­fitting maxima; in
lower dimensions, the generic presence of plateaux in the learning curve if the stu­
dent assumes too small a noise level oe 2 , and strong effects of model mismatch on the
asymptotic learning curve decay. The behaviour is much richer than for the matched
case, and could guide the choice of (student) priors in real­world applications of GP
regression; RBF students, for example, run the risk of very slow logarithmic decay
of the learning curve if the target (teacher) is less smooth than assumed.
An important issue for future work---some of which is in progress---is to analyse to
which extent hyperparameter tuning (e.g. via evidence maximization) can make GP
learning robust against some forms of model mismatch, e.g. a misspecified functional
form of the covariance function. One would like to know, for example, whether a
data­dependent adjustment of the lengthscale of an RBF covariance function would
be sufficient to avoid the logarithmically slow learning of rough target functions.
References
[1] See e.g. D J C MacKay, Gaussian Processes, Tutorial at NIPS 10; recent papers
by Csat'o et al. (NIPS 12), Goldberg/Williams/Bishop (NIPS 10), Williams
and Barber/Williams (NIPS 9), Williams/Rasmussen (NIPS 8); and references
below.
[2] D Malzahn and M Opper. In NIPS 13, pages 273--279; also in NIPS 14.
[3] C A Michelli and G Wahba. In Z Ziegler, editor, Approximation theory and
applications, pages 329--348. Academic Press, 1981; M Opper. In I K Kwok­
Yee et al., editors, Theoretical Aspects of Neural Computation, pages 17--23.
Springer, 1997.
[4] P Sollich. In NIPS 11, pages 344--350.
[5] C K I Williams and F Vivarelli. Mach. Learn., 40:77--102, 2000.
[6] C K I Williams. In M I Jordan, editor, Learning and Inference in Graphical
Models, pages 599--621. Kluwer Academic, 1998.
[7] P Sollich. J. Phys. A, 27:7771--7784, 1994.
[8] M Opper and R Urbanczik. Phys. Rev. Lett., 86:4410--4413, 2001.
[9] R Dietrich, M Opper, and H Sompolinsky. Phys. Rev. Lett., 82:2975--2978,
1999.

