Convergence of Optimistic and
Incremental Q-Learning
Eyal Even-Dar  Yishay Mansour y
Abstract
We show the convergence of two deterministic variants of Q-
learning. The rst is the widely used optimistic Q-learning, which
initializes the Q-values to large initial values and then follows a
greedy policy with respect to the Q-values. We show that setting
the initial value su∆ciently large guarantees the converges to an -
optimal policy. The second is a new and novel algorithm incremen-
tal Q-learning, which gradually promotes the values of actions that
are not taken. We show that incremental Q-learning converges, in
the limit, to the optimal policy. Our incremental Q-learning algo-
rithm can be viewed as derandomization of the -greedy Q-learning.
1 Introduction
One of the challenges of Reinforcement Learning is learning in an unknown envi-
ronment. The environment is modeled by an MDP and we can only observe the
trajectory of states, actions and rewards generated by the agent wandering in the
MDP. There are two basic conceptual approaches to the learning problem. The rst
is model base, where we rst reconstruct a model of the MDP, and then nd an
optimal policy for the approximate model. Recently polynomial time algorithms
have been developed for this approach, initially in [7] and latter extended in [3].
The second are direct methods that update their estimated policy after each step.
The most popular of the direct methods is Q-learning [13].
Q-learning uses the information observed to approximate the optimal value function,
from which one can construct an optimal policy. There are various proofs that Q-
learning converges, in the limit, to the optimal value function, under very mild
conditions [1, 11, 12, 8, 6, 2]. In a recent result the convergence rates of Q-learning
are computed and an interesting dependence on the learning rates is exhibited [4].
Q-learning is an o-policy that can be run on top of any strategy. Although, it is
an o policy algorithm, in many cases its estimated value function is used to guide
the selection of actions. Being always greedy with respect to the value function may
result in poor performance, due to the lack of exploration, and often randomization
is used guarantee proper exploration.
We show the convergence of two deterministic strategies. The rst is optimistic
 School of Computer Science, Tel-Aviv University, Tel-Aviv, Israel. evend@cs.tau.ac.il
y School of Computer Science, Tel-Aviv University, Israel. mansour@cs.tau.ac.il

Q-learning, that initializes the estimates to large values and then follows a greedy
policy. Optimistic Q-learning is widely used in applications and has been recognized
as having good convergence in practice [10].
We prove that optimistic Q-learning, with the right setting of initial values, converge
to a near optimal policy. This is not the rst theoretical result showing that opti-
mism helps in reinforcement learning, however previous results where concern with
model based methods [7, 3]. We show the convergence of the widely used optimistic
Q-learning, thus explaining and supporting the results observed in practice.
Our second result is a new and novel deterministic algorithm incremental Q-
learning, which gradually promotes the values of actions that are not taken. We
show that the frequency of sub-optimal actions vanishes, in the limit, and that
the strategy dened by incremental Q-learning converges, in the limit, to the op-
timal policy (rather than a near optimal policy). Another view of incremental
Q-learning is as a derandomization of the -greedy Q-learning. The -greedy Q-
learning performs the sub optimal action every 1= times in expectation, while the
incremental Q-learning performs sub optimal action every (Q(s; a(s)) Q(s; b))=
times. Furthermore, by taking the appropriate values it can be similar to the Boltz-
man machine.
2 The Model
We dene a Markov Decision process (MDP) as follows
Denition 2.1 A Markov Decision process (MDP) M is a 4-tuple (S; A; P; R),
where S is a set of the states, A is a set of actions, P M
i;j (a) is the transition proba-
bility from state i to state j when performing action a 2 A in state i, and RM (s; a)
is the reward received when performing action a in state s.
A strategy for an MDP assigns, at each time t, for each state s a probability for
performing action a 2 A, given a history F t 1 = fs 1 ; a 1 ; r 1 ; :::; s t 1 ; a t 1 ; r t 1 g
which includes the states, actions and rewards observed until time t 1. While
executing a strategy  we perform at time t action a t in state s t and observe a
reward r t (distributed according to RM (s; a)), and the next state s t+1 distributed
according to P M
s t ;s t+1 (a t ). We combine the sequence of rewards to a single value
called return, and our goal is to maximize the return. In this work we focus on
discounted return, which has a parameter  2 (0; 1), and the discounted return of
policy  is V 
M =
P 1
t=0  t r t , where r t is the reward observed at time t.
We assume that RM (s; a) is non-negative and bounded by Rmax , i.e, 8s; a : 0 
RM (s; a)  Rmax . This implies that the discounted return is bounded by Vmax =
Rmax
1  .
We dene a value function for each state s, under policy , as V 
M (s) = E[
P 1
i=0 r i  i ],
where the expectation is over a run of policy  starting at state s, and a state-action
value function Q 
M (s; a) = E[RM (s; a)] + 
P
s 0 P M
s;s 0 (a)V 
M (s 0 ).
Let   be an optimal policy which maximizes the return from any start state.
This implies that for any policy  and any state s we have V  
M (s)  V 
M (s), and
  (s) = argmax a (E[RM (s; a)] + (
P
s 0 P M
s;s 0 (a)V  (s 0 )).
We use V 
M and Q 
M for V  
M and Q  
M , respectively. We say that a policy  is an
-optimal if kV 
M V 
M k1  .

Given a trajectory let T s;a be the set of times in which we perform action a in state
s, T s = [ a T s;a be the times when state s is visited, T s;not(a) = T s n T s;a be the set
of times where in state s an action a 0 6= a is performed, and T not(s) = [ s 0 6=s T s 0
be
the set of times in which a state s 0 6= s is visited. Also, [#(s; a; t)] is the number of
times action a is performed in state s up to time t, i.e., jT s;a \ [1; t]j.
Finally, throughout the paper we assume that the MDP is a uni-chain (see [9]),
namely that from every state we can reach any other state.
3 Q-Learning
The Q-Learning algorithm [13] estimates the state-action value function (for dis-
counted return) as follows:
Q t+1 (s; a) = (1  t (s; a))Q t (s; a) +  t (s; a)(r t (s; a) + V t (s 0 ))
where s 0 is the state reached from state s when performing action a at time t, and
V t (s) = max a Q t (s; a). We assume that  t (s 0 ; a 0 ) = 0 for t =
2 T s 0 ;a 0
.
A learning rate  t is well-behaved if for every state action pair (s; a): (1)
P 1
t=1  t (s; a) = 1 and (2)
P 1
t=1  2
t (s; a) < 1. If the learning rate is well-behaved
and every state action pair is performed innitely often then Q-Learning converges
to Q  with probability 1 (see [1, 11, 12, 8, 6]).
The convergence of Q-learning holds using any exploration policy, and only requires
that each state action pair is executed innitely often. The greedy policy with
respect to the Q-values tries to exploit continuously, however, since it does not
explore properly, it might result in poor return. At the other extreme random
policy continuously explores, but its actual return may be very poor. An interesting
compromise between the two extremes is the -greedy policy, which is widely used
in practice [10]. This policy executes the greedy policy with probability 1 
and the random policy with probability . This balance between exploration and
exploitation both guarantees convergence and often good performance.
Common to many of the exploration techniques, is the use of randomization, which
is also a very natural choice. In this work we explore strategies which perform
exploration but avoids randomization and uses deterministic strategies.
4 Optimistic Q-Learning
Optimistic Q-learning is a simple greedy algorithm with respect to the Q-values,
where the initial Q-values are set to large values, larger than their optimal values.
We show that optimistic Q-learning converges to an -optimal policy if the initial
Q-values are set su∆ciently large.
Let   =
Q 
i=1 (1  i ). We set the initial conditions of the Q-values as follows:
8s; a : Q 0 (s; a) = 1
 T
Vmax ;
where T = T (; ∆; S; A; ~) will be specied later. Let  i; =  i
Q 
j=i+1 (1  j ) =
 i   = i . Note that
Q t+1 (s; a) = (1  t )Q t (s; a)+ t (r t +V t (s 0 )) =   Q 0 (s; a)+

X
i=1
 i; r i (s; a)+

X
i=1
 i; V t i (s i );

where  = [#(s; a; t)] and s i is the next state arrived at time t i when action a is
performed for the ith time in state s.
First we show that as long as  = [#(s; a; t)]  T actions a are performed in state s,
we have Q t (s; a)  Vmax . Latter we will use this to show that action a is performed
at least T times in state s.
Lemma 4.1 In optimistic Q-learning for any state s, action a and time t, such
that  = [#(s; a; t)]  T we have Q t (s; a)  Vmax  Q  (s; a).
Lemma 4.1 follows from the following observation:
Q t (s; a) =   Q 0 (s; a) +

X
i=1
 i; r i (s; a) + 

X
i=1
 i; V t i (s i )   
 T
Vmax  V  (s):
Now we bound T as a function of the algorithm parameters (i.e., ; ∆; jSj; jAj) and
the learning rate. We need to set T large enough to guarantee that with probability
1 ∆, for any t > T updates, using the given learning rate, the deviation from the
true value is at most . Formally, given a sequence X t of i.i.d. random variables
with zero mean and bounded by Vmax , and a learning rate  t = (1=[#(s; a; t)]) ! let
Z t+1 = (1  t )Z t +  t X t . A time T (; ∆) is an initialization time if P r[8t  T :
Z t  ]  1 ∆. The following lemma bounds the initialization time as a function
of the parameter ! of the learning rate.
Lemma 4.2 The initialization time for ~
X and ~  is at most T (; ∆) =
c
  V 2
max
 2 (ln(1=∆) + ln(V max =))
 1
!

, for some constant c.
We dene a modied process, in which we update using the optimal value function,
rather than our current estimate. For t  1 we have,
^
Q t+1 (s; a) = (1  t (s; a)) ^
Q t (s; a) +  t (s; a)(r t (s; a) + V  (s 0 ));
where s 0 is the next state. The following lemma bounds the dierence between Q 
and ^
Q t .
Lemma 4.3 Consider optimistic Q-learning and let T = T (; ∆) be the initialization
time. Then with probability 1 ∆, for any t > T , we have Q  (s; a) ^
Q(s; a)  .
Proof: Let  = [#(s; a; t)]. By denition we have
^
Q t (s; a) =   Q 0 (s; a) +

X
i=1
 i; r i + 

X
i=1
 i; V  (s i ):
This implies that,
Q  (s; a) ^
Q(s; a) =   Q 0 (s; a) + error r[s; a; t] + error v[s; a; t]
where error r[s; a; t] = E[R(s; a)]
P 
i=1  i; r i , and error v[s; a; t] =
E[V  (s 0 )js; a]
P 
i=1  i; V  (s i ). We bound both error r[s; a; t] and error v[s; a; t]
using Lemma 4.2. Therefore, with probability 1 ∆, we have Q  (s; a) ^
Q(s; a)  ,
for any t  T . Q.E.D.
Next we bound the dierence between our estimate V t (s) and V  (s).

Lemma 4.4 Consider optimistic Q-learning and let T = T ((1 ); ∆=jSjjAj) be
the initialization time. With probability at least 1 ∆ for any state s and time t, we
have V  (s) V t (s)  .
Proof: By Lemma 4.3 we have that with probability 1 ∆ for every state s, action
a and time t we have Q  (s; a) ^
Q t (s; a)  (1 ). We show by induction on t that
V  (s) V t (s)  , for every state s. For t = 0 we have V 0 (s) > Vmax and hence
the claim holds. For the inductive step assume it holds up to time t and show that
it hold for time t + 1. Let (s; a) be the state action pair executed in time t + 1. If
[#(s; a; t + 1)]  T then by Lemma 4.1, V t (s)  Vmax  V  (s), and the induction
claim holds. Otherwise, let a  be the optimal action at state s, then,
V  (s) V t+1 (s)  Q  (s; a  ) Q t+1 (s; a  )
= Q  (s; a  ) ^
Q t+1 (s; a  ) + ^
Q t+1 (s; a  ) Q t+1 (s; a  )
 (1 ) + 

X
i=1
 i; (V  (s i ) V t i (s i ));
where  = [#(s; a; t)], t i is the time when the i-th time the action a is performed
in state s, and state s i is the next state. Since t i  t, by the inductive hypothesis
we have that V  (s i ) V t i (s i )  , and therefore,
V  (s) V t+1 (s)  (1 ) +  = :
Q.E.D.
Lemma 4.5 Consider optimistic Q-learning and let T = T ((1 ); ∆=jSjjAj) be
the initialization time. With probability at least 1 ∆ any state action pair (s; a)
that is executed innitely often is -optimal, i.e., V  (s) Q  (s; a)  .
Proof: Given a trajectory let U 0 be the set of state action pairs that are executed
innitely often, and let M 0 be the original MDP M restricted to U 0 . For M 0 we
can use the classical convergence proofs, and claim that V t (s) converges to V 
M 0 (s)
and Q t (s; a), for (s; a) 2 U 0 , converges to Q 
M 0 (s; a), both with probability 1. Since
(s; a) 2 U 0 is performed innitely often it implies that Q t (s; a) converges to V t (s) =
V 
M 0 (s) and therefore Q 
M 0 (s; a) = V 
M 0 (s). By Lemma 4.4 with probability 1 ∆ we
have that V 
M (s) V t (s)  , therefore V 
M (s) Q 
M (s; a)  V 
M (s) Q 
M 0 (s; a)  .
Q.E.D.
A simple corollary is that if we set  small enough, e.g.,  < min (s;a) fV  (s)
Q  (s; a)jV  (s) 6= Q  (s; a)g, then optimistic Q-learning converges to the optimal
policy. Another simple corollary is the following theorem.
Theorem 4.6 Consider optimistic Q-learning and let T = T ((1 ); ∆=jSjjAj) be
the initialization time. For any constant , with probability at least 1 ∆ there is
a time T  > T such that at any time t > T  the strategy dened by the optimistic
Q-learning is ( + )=(1 )-optimal.
5 Incremental Q-learning
In this section we describe a new algorithm that we call incremental Q-learning. The
main idea of the algorithm is to achieve a deterministic tradeo between exploration
and exploitation.
Incremental Q-learning is a greedy policy with respect to the estimated Q-values
plus a promotion term. The promotion term of a state-action pair (s; a) is promoted

each time the action a is not executed in state s, and zeroed each time action a is
executed. We show that in incremental Q-learning every state-action pair is taken
innitely often, which implies standard convergence of the estimates. We show that
the fraction of time in which sub-optimal actions are executed vanishes in the limit.
This implies that the strategy dened by incremental Q-learning converges, in the
limit, to the optimal policy. Incremental Q-learning estimates the Q-function as in
Q-learning:
Q t+1 (s; a) = (1  t (s; a))Q t (s; a) +  t (s; a)(r t (s; a) + V t (s 0 ))
where s 0 is the next state reached when performing action a in state s at time t.
The promotion term A t is dene as follows:
A t+1 (s; a) = 0 : t 2 T s;a
A t+1 (s; a) = A t (s; a) +  ([#(s; a; t)]) : t 2 T s;not(a)
A t+1 (s; a) = A t (s; a) : t 2 T not(s) ;
where  (i) is a promotion function which in our case depends only on the number
of times we performed (s; a 0 ), a 0 6= a, since the last time we performed (s; a). We
say that a promotion function   is well-behaved if: (1) The function   converges to
zero, i.e., lim i!1  (i) = 0, and (2)  (1) = 1 and  (k) >  (k+1) > 0. For example
 (i) = 1
i is well behaved promotion function.
Incremental Q-learning is a greedy policy with respect to S t (s; a) = Q t (s; a) +
A t (s; a). First we show that Q t , in incremental Q-learning, converges to Q  .
Lemma 5.1 Consider incremental Q-learning using a well-behaved learning rate
and a well-behaved promotion function. Then Q t converges to Q  with probability
1.
Proof: Since the learning rate is well-behaved, we need only to show that each state
action pair is performed innitely often. We show that each state that is visited
innitely often, all of its actions are performed innitely often. Since the MDP is
uni-chain this will imply that with probability 1 we reach all states innitely often,
which completes the proof.
Assume that state s is visited innitely often. Since s is visited innitely often,
there has to be a non-empty subset of the actions A 0 which are performed innitely
often in s. The proof is by contradiction, namely assume that A 0 6= A. Let t 1 be the
last time that an action not in A 0 is performed in state s. Since   is well behaved
we have that  (t 1 ) is constant for a xed t 1 , it implies that A t (s; a) diverges for
a 62 A 0 . Therefore, eventually we reach a time t 2 > t 1 such that A t 2 (s; a) > Vmax ,
for every a 62 A 0 . Since the actions in A 0 are performed innitely often there is a
time t 3 > t 2 such that each action a 0 2 A 0 is performed at least once in [t 2 ; t 3 ]. This
implies that A t 3 (s; a) > Vmax + A t 3 (s; a 0 ) for any a 0 2 A 0 and a 62 A 0 . Therefore,
some action in a 2 A n A 0 will be performed after t 1 , contradicting our assumption.
Q.E.D.
The following lemma shows that the frequency of sub-optimal actions vanishes.
Lemma 5.2 Consider incremental Q-learning using a well behaved learning rate
and a well behaved promotion function. Let f t (s; a) = jT s;a j=jT s j and (s; a) be any
sub-optimal state-action pair. Then lim t!1 f t (s; a) = 0, with probability 1.
The intuition behind Lemma 5.2 is the following. Let a  be an optimal action in
state s and a be a sub-optimal action. By Lemma 5.1, with probability 1 both

100 200 300 400 500 600 700 800 900 1000
0
0.2
0.4
0.6
0.8
1
1.2
Number of steps 10 3
Precision,
distance
from
the
optimal
value
Figure 1: Example of 50 states MDP, where the discount factor, , is 0.9. The
leaning rate of both Incremantal and epsilon greedy Q-learning is set to 0.8. The
dashed line represents the epsilon greedy Q-learning.
Q t (s; a  ) converges to Q  (s; a  ) = V  (s) and Q t (s; a) converges to Q  (s; a). This
implies, intuitively, that A t (s; a) has to be at least V  (s) Q  (s; a) = h > 0 for
(s; a) to be executed. Since the promotion function is well behaved, the number
of time steps required until A t (s; a) changes from 0 to h increases after each time
we perform (s; a). Since the inter-time between executions of (s; a) diverges, the
frequency f t (s; a) vanishes.
The following corollary gives a quantitative bound.
Corollary 5.3 Consider incremental Q-learning with learning rate  t (s; a) =
1=[#(s; a; t)] and  (k) = 1=e k . Let (s; a) be a sub-optimal state-action pair. The
number of times (s; a) is performed in the rst n visits to state s is ( ln(n)
V  (s) Q  (s;a) ),
for su∆ciently large n.
Furthermore, the return obtained by incremental Q-learning converges to the opti-
mal return.
Corollary 5.4 Consider incremental Q-learning using a well behaved learning rate
and a well behaved promotion function. For every  there exists a time T  such that
for any t > T  we have that the strategy  dened by incremental Q-learning is
-optimal with probability 1.
6 Experiments
In this section we show some experimental results, comparing Incremental Q-
Learning and epsilon-greedy Q-Learning. One can consider incremental Q-learning
as a derandomization of  t -greedy Q-Learning, where the promotion function satis-
es   t =  t .

The experiment was made on MDP, which includes 50 states and two actions per
state. Each state action pair immediate reward is randomly chosen in the interval
[0; 10]. For each state and action (s; a) the next state transition is random, i.e., for
every state s 0 we have a random variable X s;a
s 0 2 [0; 1] and P a
s;s 0 = X s;a
s 0
P
^ s X s;a
^ s
. For
the  t -greedy Q-learning, we have  t = 10000=t at time t, while for the incremental
we have   t = 10000=t. Each result in the experiment is an average of ten dierent
runs. In Figure 1, we observe similar behavior of the two algorithms. This experi-
ment demonstrates the strong experimental connection between these methods. We
plan to further investigate the theoretical connection between -greedy, Boltzman
machine and incremental Q-Learning.
7 Acknowledgements
This research was supported in part by a grant from the Israel Science Foundation.
References
[1] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena
Scientic, Belmont, MA, 1996.
[2] V.S. Borkar and S.P. Meyn. The O.D.E. method for convergence of stochastic
approximation and reinforcement learning. Siam J. control, 38 (2):447{69,
2000.
[3] R. I. Brafman and M. Tennenholtz. R-max - a general polynomial time algo-
rithm for near-optimal reinforcement learning. In IJCAI, 2001.
[4] E. Even-Dar and Y. Mansour. Learning rates for Q-learning. In COLT, 2001.
[5] J. C. Gittins and D. M. Jones. A dynamic allocation index for the sequential
design of experiments. Progress in Statistics, pages 241 {266, 1974.
[6] T. Jaakkola, M. I. Jordan, and S. P. Singh. On the convergence of stochastic
iterative dynamic programming algorithms. Neural Computation, 6, 1994.
[7] M. Kearns and S. Singh. E∆cient reinforcement learning: theoretical frame-
work and algorithms. In ICML, 1998.
[8] M. Littman and Cs. Szepesvari. A generalized reinforcement learning model:
convergence and applications. In ICML, 1996.
[9] M.L Puterman. Markov Decision Processes - Discrete Stochastic Dynamic
Programming. John Wiley & Sons. Inc., New York, NY, 1994.
[10] R. S. Sutton and A. G. Bato. Reinforcement Learning. MIT press, 1998.
[11] J. N. Tsitsiklis. Asynchronous stochastic approximation and Q-learning. Ma-
chine Learning, 16:185{202, 1994.
[12] C. Watkins and P. Dyan. Q-learning. Machine Learning, 8(3/4):279 {292,
1992.
[13] C. Watkins. Learning from Delayed Rewards. PhD thesis, Cambridge Univer-
sity, 1989.

