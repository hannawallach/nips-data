Computing Time Lower Bounds for
Recurrent Sigmoidal Neural Networks
Michael Schmitt
Lehrstuhl Mathematik und Informatik, Fakult¨at f¨ur Mathematik
Ruhr­Universit¨at Bochum, D--44780 Bochum, Germany
mschmitt@lmi.ruhr­uni­bochum.de
Abstract
Recurrent neural networks of analog units are computers for real­
valued functions. We study the time complexity of real computa­
tion in general recurrent neural networks. These have sigmoidal,
linear, and product units of unlimited order as nodes and no re­
strictions on the weights. For networks operating in discrete time,
we exhibit a family of functions with arbitrarily high complexity,
and we derive almost tight bounds on the time required to compute
these functions. Thus, evidence is given of the computational lim­
itations that time­bounded analog recurrent neural networks are
subject to.
1 Introduction
Analog recurrent neural networks are known to have computational capabilities that
exceed those of classical Turing machines (see, e.g., Siegelmann and Sontag, 1995;
Kilian and Siegelmann, 1996; Siegelmann, 1999). Very little, however, is known
about their limitations. Among the rare results in this direction, for instance,
is the one of Ÿ S'ima and Orponen (2001) showing that continuous­time Hopfield
networks may require exponential time before converging to a stable state. This
bound, however, is expressed in terms of the size of the network and, hence, does
not apply to fixed­size networks with a given number of nodes. Other bounds
on the computational power of analog recurrent networks have been established by
Maass and Orponen (1998) and Maass and Sontag (1999). They show that discrete­
time recurrent neural networks recognize only a subset of the regular languages in
the presence of noise. This model of computation in recurrent networks, however,
receives its inputs as sequences. Therefore, computing time is not an issue since
the network halts when the input sequence terminates. Analog recurrent neural
networks, however, can also be run as ``real'' computers that get as input a vector
of real numbers and, after computing for a while, yield a real output value. No
results are available thus far regarding the time complexity of analog recurrent
neural networks with given size.
We investigate here the time complexity of discrete­time recurrent neural networks
that compute functions over the reals. As network nodes we allow sigmoidal units,
linear units, and product units---that is, monomials where the exponents are ad­

justable weights (Durbin and Rumelhart, 1989). We study the complexity of real
computation in the sense of Blum et al. (1998). That means, we consider real num­
bers as entities that are represented exactly and processed without restricting their
precision. Moreover, we do not assume that the information content of the network
weights is bounded (as done, e.g., in the works of Balc'azar et al., 1997; Gavald`a and
Siegelmann, 1999). With such a general type of network, the question arises which
functions can be computed with a given number of nodes and a limited amount of
time. In the following, we exhibit a family of real­valued functions f l ; l – 1, in one
variable that is computed by some fixed size network in time O(l). Our main result
is, then, showing that every recurrent neural network computing the functions f l
requires at least
time\Omega\Gamma l 1=4 ). Thus, we obtain almost tight time bounds for real
computation in recurrent neural networks.
2 Analog Computation in Recurrent Neural Networks
We study a very comprehensive type of discrete­time recurrent neural network that
we call general recurrent neural network (see Figure 1). For every k; n 2 N there is
a recurrent neural architecture consisting of k computation nodes y 1 ; : : : ; y k and n
input nodes x 1 ; : : : ; xn . The size of a network is defined to be the number of its com­
putation nodes. The computation nodes form a fully connected recurrent network.
Every computation node also receives connections from every input node. The input
nodes play the role of the input variables of the system. All connections are param­
eterized by real­valued adjustable weights. There are three types of computation
nodes: product units, sigmoidal units, and linear units. Assume that computation
node i has connections from computation nodes weighted by w i1 ; : : : ; w ik and from
input nodes weighted by v i1 ; : : : ; v in . Let y 1 (t); : : : ; y k (t) and x 1 (t); : : : ; xn (t) be the
values of the computation nodes and input nodes at time t, respectively. If node i
is a product unit, it computes at time t + 1 the value
y i (t + 1) = y w i1
1 (t) \Delta \Delta \Delta y w ik
k (t)x v i1
1 (t) \Delta \Delta \Delta x vin
n (t); (1)
that is, after weighting them exponentially, the incoming values are multiplied.
Sigmoidal and linear units have an additional parameter associated with them, the
threshold or bias ` i . A sigmoidal unit computes the value
y i (t + 1) = oe(w i1 y 1 (t) + \Delta \Delta \Delta + w ik y k (t) + v i1 x 1 (t) + \Delta \Delta \Delta + v in xn (t) \Gamma ` i ); (2)
where oe is the standard sigmoid oe(z) = 1=(1 + e \Gammaz ). If node i is a linear unit, it
simply outputs the weighted sum
y i (t + 1) = w i1 y 1 (t) + \Delta \Delta \Delta + w ik y k (t) + v i1 x 1 (t) + \Delta \Delta \Delta + v in xn (t) \Gamma ` i : (3)
We allow the networks to be heterogeneous, that is, they may contain all three types
of computation nodes simultaneously. Thus, this model encompasses a wide class of
network types considered in research and applications. For instance, architectures
have been proposed that include a second layer of linear computation nodes which
have no recurrent connections to computation nodes but serve as output nodes (see,
e.g., Koiran and Sontag, 1998; Haykin, 1999; Siegelmann, 1999). It is clear that in
the definition given here, the linear units can function as these output nodes if the
weights of the outgoing connections are set to 0. Also very common is the use
of sigmoidal units with higher­order as computation nodes in recurrent networks
(see, e.g., Omlin and Giles, 1996; Gavald`a and Siegelmann, 1999; Carrasco et al.,
2000). Obviously, the model here includes these higher­order networks as a special
case since the computation of a higher­order sigmoidal unit can be simulated by
first computing the higher­order terms using product units and then passing their

sigmoidal, product, and linear units
computation
nodes
input nodes
PSfrag replacements
x 1 x n
y 1 y k
Figure 1: A general recurrent neural network of size k. Any computation node may
serve as output node.
outputs to a sigmoidal unit. Product units, however, are even more powerful than
higher­order terms since they allow to perform division operations using negative
weights. Moreover, if a negative input value is weighted by a non­integer weight,
the output of a product unit may be a complex number. We shall ensure here that
all computations are real­valued. Since we are mainly interested in lower bounds,
however, these bounds obviously remain valid if the computations of the networks
are extended to the complex domain.
We now define what it means that a recurrent neural network N computes a function
f : R n ! R. Assume that N has n input nodes and let x 2 R n . Given t 2 N,
we say that N computes f(x) in t steps if after initializing at time 0 the input
nodes with x and the computation nodes with some fixed values, and performing t
computation steps as defined in Equations (1), (2), and (3), one of the computation
nodes yields the value f(x). We assume that the input nodes remain unchanged
during the computation. We further say that N computes f in time t if for every
x 2 R n , network N computes f in at most t steps. Note that t may depend
on f but must be independent of the input vector. We emphasize that this is
a very general definition of analog computation in recurrent neural networks. In
particular, we do not specify any definite output node but allow the output to occur
at any node. Moreover, it is not even required that the network reaches a stable
state, as with attractor or Hopfield networks. It is sufficient that the output value
appears at some point of the trajectory the network performs. A similar view of
computation in recurrent networks is captured in a model proposed by Maass et al.
(2001). Clearly, the lower bounds remain valid for more restrictive definitions of
analog computation that require output nodes or stable states. Moreover, they
hold for architectures that have no input nodes but receive their inputs as initial
values of the computation nodes. Thus, the bounds serve as lower bounds also for
the transition times between real­valued states of discrete­time dynamical systems
comprising the networks considered here.
Our main tool of investigation is the Vapnik­Chervonenkis dimension of neural
networks. It is defined as follows (see also Anthony and Bartlett, 1999): A dichotomy
of a set S ` R n is a partition of S into two disjoint subsets (S 0 ; S 1 ) satisfying
S 0 [ S 1 = S. A class F of functions mapping R n to f0; 1g is said to shatter S if
for every dichotomy (S 0 ; S 1 ) of S there is some f 2 F that satisfies f(S 0 ) ` f0g
and f(S 1 ) ` f1g. The Vapnik­Chervonenkis (VC) dimension of F is defined as

output
input
PSfrag replacements
x
0 1
y 1
x \Delta y 1
y 2
y 3
y 4
4y 2 + 4y 5
1 \Gamma y 2 \Gamma y 5
y 3 \Delta y 4
y 5
Figure 2: A recurrent neural network computing the functions f l in time 2l + 1.
the largest number m such that there is a set of m elements shattered by F . A
neural network given in terms of an architecture represents a class of functions
obtained by assigning real numbers to all its adjustable parameters, that is, weights
and thresholds or a subset thereof. The output of the network is assumed to be
thresholded at some fixed constant so that the output values are binary. The VC
dimension of a neural network is then defined as the VC dimension of the class of
functions computed by this network.
In deriving lower bounds in the next section, we make use of the following result
on networks with product and sigmoidal units that has been previously established
(Schmitt, 2002). We emphasize that the only constraint on the parameters of the
product units is that they yield real­valued, that is, not complex­valued, functions.
This means further that the statement holds for networks of arbitrary order, that is,
it does not impose any restrictions on the magnitude of the weights of the product
units.
Proposition 1. (Schmitt, 2002, Theorem 2) Suppose N is a feedforward neural
network consisting of sigmoidal, product, and linear units. Let k be its size and W
the number of adjustable weights. The VC dimension of N restricted to real­valued
functions is at most 4(Wk) 2 + 20Wk log(36Wk).
3 Bounds on Computing Time
We establish bounds on the time required by recurrent neural networks for comput­
ing a family of functions f l : R ! R; l – 1, where l can be considered as a measure
of the complexity of f l . Specifically, f l is defined in terms of a dynamical system as
the lth iterate of the logistic map OE(x) = 4x(1 \Gamma x), that is,
f l (x) =
(
OE(x) l = 1;
OE(f l\Gamma1 (x)) l – 2:
We observe that there is a single recurrent network capable of computing every f l
in time O(l).
Lemma 2. There is a general recurrent neural network that computes f l in time
2l + 1 for every l.
Proof. The network is shown in Figure 2. It consists of linear and second­order
units. All computation nodes are initialized with 0, except y 1 , which starts with 1
and outputs 0 during all following steps. The purpose of y 1 is to let the input x

output
PSfrag replacements
w
x (3) x (2) x (1)
N (3)
l
N (2)
l
N (1)
l
Figure 3: Network N l .
enter node y 2 at time 1 and keep it away at later times. Clearly, the value f l (x)
results at node y 5 after 2l + 1 steps.
The network used for computing f l requires only linear and second­order units. The
following result shows that the established upper bound is asymptotically almost
tight, with a gap only of order four. Moreover, the lower bound holds for networks
of unrestricted order and with sigmoidal units.
Theorem 3. Every general recurrent neural network of size k requires at least time
cl 1=4 =k to compute function f l , where c ? 0 is some constant.
Proof. The idea is to construct higher­order networks N l of small size that have
comparatively large VC dimension. Such a network will consist of linear and product
units and hypothetical units that compute functions f j for certain values of j. We
shall derive a lower bound on the VC dimension of these networks. Assuming that
the hypothetical units can be replaced by time­bounded general recurrent networks,
we determine an upper bound on the VC dimension of the resulting networks in
terms of size and computing time using an idea from Koiran and Sontag (1998) and
Proposition 1. The comparison of the lower and upper VC dimension bounds will
give an estimate of the time required for computing f l .
Network N l , shown in Figure 3, is a feedforward network composed of three networks
N (1)
l ; N (2)
l ; N (3)
l . Each network N (¯)
l ; ¯ = 1; 2; 3, has l input nodes x (¯)
1 ; : : : ; x (¯)
l
and 2l + 2 computation nodes y (¯)
0 ; : : : ; y (¯)
2l+1 (see Figure 4). There is only one
adjustable parameter in N l , denoted w, all other weights are fixed. The computation
nodes are defined as follows (omitting time parameter t):
y (¯)
0 =
(
w for ¯ = 3;
y (¯+1)
2l+1 for ¯ = 1; 2;
y (¯)
i = f l ¯\Gamma1 (y (¯)
i\Gamma1 ) for i = 1; : : : ; l and ¯ = 1; 2; 3;
y (¯)
l+i = y (¯)
i \Delta x (¯)
i ; for i = 1; : : : ; l and ¯ = 1; 2; 3;
y (¯)
2l+1 = y (¯)
l+1 + \Delta \Delta \Delta + y (¯)
2l for ¯ = 1; 2; 3:
The nodes y (¯)
0 can be considered as additional input nodes for N (¯)
l , where N (3)
l
gets this input from w, and N (¯)
l from N (¯+1)
l for ¯ = 1; 2. Node y (¯)
2l+1 is the
output node of N (¯)
l , and node y (1)
2l+1 is also the output node of N l . Thus, the entire
network has 3l +6 nodes that are linear or product units and 3l nodes that compute
functions f 1 ; f l ; or f l 2 .

output
...
output of
input: or
PSfrag replacements
w
N (¯+1)
l
f l ¯\Gamma1
f l ¯\Gamma1
x (¯)
1 x (¯)
l
x \Delta y x \Delta y
+
Figure 4: Network N (¯)
l .
We show that N l shatters some set of cardinality l 3 , in particular, the set S = (fe i :
i = 1; : : : ; lg) 3 , where e i 2 f0; 1g l is the unit vector with a 1 in position i and 0
elsewhere. Every dichotomy of S can be programmed into the network parameter
w using the following fact about the logistic function OE (see Koiran and Sontag,
1998, Lemma 2): For every binary vector b 2 f0; 1g m ; b = b 1 : : : b m , there is some
real number w 2 [0; 1] such that for i = 1; : : : ; m
OE i (w) 2
(
[0; 1=2) if b i = 0;
(1=2; 1] if b i = 1:
Hence, for every dichotomy (S 0 ; S 1 ) of S the parameter w can be chosen such that
every (e i 1
; e i 2
; e i 3
) 2 S satisfies
OE i 1 (OE i 2 \Deltal (OE i 3 \Deltal 2
(w)))
(
! 1=2 if (e i 1
; e i 2
; e i 3
) 2 S 0 ;
? 1=2 if (e i 1
; e i 2
; e i 3
) 2 S 1 :
Since f i 1 +i2 \Deltal+i 3 \Deltal 2(w) = OE i 1 (OE i 2 \Deltal (OE i 3 \Deltal 2
(w))), this is the value computed by N l on
input (e i 1
; e i 2
; e i 3
), where e i ¯
is the input given to network N (¯)
l . (Input e i ¯
selects
the function f i ¯ \Deltal ¯\Gamma1 in N (¯)
l .) Hence, S is shattered by N l , implying that N l has
VC dimension at least l 3 .

Assume now that f j can be computed by a general recurrent neural network of size
at most k j in time t j . Using an idea of Koiran and Sontag (1998), we unfold the
network to obtain a feedforward network of size at most k j t j computing f j . Thus we
can replace the nodes computing f 1 ; f l ; f l 2 in N l by networks of size k 1 t 1 ; k l t l ; k l 2 t l 2 ,
respectively, such that we have a feedforward network N 0
l consisting of sigmoidal,
product, and linear units. Since there are 3l units in N l computing f 1 ; f l ; or f l 2
and at most 3l + 6 product and linear units, the size of N 0
l is at most c 1 lk l 2t l 2
for some constant c 1 ? 0. Using that N 0
l has one adjustable weight, we get from
Proposition 1 that its VC dimension is at most c 2 l 2 k 2
l 2 t 2
l 2 for some constant c 2 ? 0.
On the other hand, since N l and N 0
l both shatter S, the VC dimension of N 0
l is at
least l 3 . Hence, l 3 Ÿ c 2 l 2 k 2
l 2 t 2
l 2 holds, which implies that t l 2 – cl 1=2 =k l 2 for some
c ? 0, and hence t l – cl 1=4 =k l .
Lemma 2 shows that a single recurrent network is capable of computing every
function f l in time O(l). The following consequence of Theorem 3 establishes that
this bound cannot be much improved.
Corollary 4. Every general recurrent neural network requires at least
time\Omega\Gamma l 1=4 )
to compute the functions f l .
4 Conclusions and Perspectives
We have established bounds on the computing time of analog recurrent neural
networks. The result shows that for every network of given size there are functions
of arbitrarily high time complexity. This fact does not rely on a bound on the
magnitude of weights. We have derived upper and lower bounds that are rather
tight---with a polynomial gap of order four---and hold for the computation of a
specific family of real­valued functions in one variable. Interestingly, the upper
bound is shown using second­order networks without sigmoidal units, whereas the
lower bound is valid even for networks with sigmoidal units and arbitrary product
units. This indicates that adding these units might decrease the computing time
only marginally. The derivation made use of an upper bound on the VC dimension
of higher­order sigmoidal networks. This bound is not known to be optimal. Any
future improvement will therefore lead to a better lower bound on the computing
time.
We have focussed on product and sigmoidal units as nonlinear computing elements.
However, the construction presented here is generic. Thus, it is possible to derive
similar results for radial basis function units, models of spiking neurons, and other
unit types that are known to yield networks with bounded VC dimension. The
questions whether such results can be obtained for continuous­time networks and for
networks operating in the domain of complex numbers, are challenging. A further
assumption made here is that the networks compute the functions exactly. By a
more detailed analysis and using the fact that the shattering of sets requires the
outputs only to lie below or above some threshold, similar results can be obtained
for networks that approximate the functions more or less closely and for networks
that are subject to noise.
Acknowledgment
The author gratefully acknowledges funding from the Deutsche Forschungsgemein­
schaft (DFG). This work was also supported in part by the ESPRIT Working Group
in Neural and Computational Learning II, NeuroCOLT2, No. 27150.

References
Anthony, M. and Bartlett, P. L. (1999). Neural Network Learning: Theoretical
Foundations. Cambridge University Press, Cambridge.
Balc'azar, J., Gavald`a, R., and Siegelmann, H. T. (1997). Computational power of
neural networks: A characterization in terms of Kolmogorov complexity. IEEE
Transcations on Information Theory, 43:1175--1183.
Blum, L., Cucker, F., Shub, M., and Smale, S. (1998). Complexity and Real Com­
putation. Springer­Verlag, New York.
Carrasco, R. C., Forcada, M. L., Vald'es­Mu~noz, M. A., and ~
Neco, R. P. (2000).
Stable encoding of finite state machines in discrete­time recurrent neural nets
with sigmoid units. Neural Computation, 12:2129--2174.
Durbin, R. and Rumelhart, D. (1989). Product units: A computationally pow­
erful and biologically plausible extension to backpropagation networks. Neural
Computation, 1:133--142.
Gavald`a, R. and Siegelmann, H. T. (1999). Discontinuities in recurrent neural
networks. Neural Computation, 11:715--745.
Haykin, S. (1999). Neural Networks: A Comprehensive Foundation. Prentice Hall,
Upper Saddle River, NJ, second edition.
Kilian, J. and Siegelmann, H. T. (1996). The dynamic universality of sigmoidal
neural networks. Information and Computation, 128:48--56.
Koiran, P. and Sontag, E. D. (1998). Vapnik­Chervonenkis dimension of recurrent
neural networks. Discrete Applied Mathematics, 86:63--79.
Maass, W., Natschl¨ager, T., and Markram, H. (2001). Real­time computing without
stable states: A new framework for neural computation based on perturbations.
Preprint.
Maass, W. and Orponen, P. (1998). On the effect of analog noise in discrete­time
analog computations. Neural Computation, 10:1071--1095.
Maass, W. and Sontag, E. D. (1999). Analog neural nets with Gaussian or other
common noise distributions cannot recognize arbitrary regular languages. Neural
Computation, 11:771--782.
Omlin, C. W. and Giles, C. L. (1996). Constructing deterministic finite­state au­
tomata in recurrent neural networks. Journal of the Association for Computing
Machinery, 43:937--972.
Schmitt, M. (2002). On the complexity of computing and learning with multiplica­
tive neural networks. Neural Computation, 14. In press.
Siegelmann, H. T. (1999). Neural Networks and Analog Computation: Beyond the
Turing Limit. Progress in Theoretical Computer Science. Birkh¨auser, Boston.
Siegelmann, H. T. and Sontag, E. D. (1995). On the computational power of neural
nets. Journal of Computer and System Sciences, 50:132--150.
Ÿ
S'ima, J. and Orponen, P. (2001). Exponential transients in continuous­time
symmetric Hopfield nets. In Dorffner, G., Bischof, H., and Hornik, K., edi­
tors, Proceedings of the International Conference on Artificial Neural Networks
ICANN 2001, volume 2130 of Lecture Notes in Computer Science, pages 806--813,
Springer, Berlin.

