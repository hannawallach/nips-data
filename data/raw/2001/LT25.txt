A Variational Approach to Learning Curves
D˜ orthe Malzahn Manfred Opper
Neural Computing Research Group
School of Engineering and Applied Science
Aston University, Birmingham B4 7ET, United Kingdom.
[malzahnd,opperm]@aston.ac.uk
Abstract
We combine the replica approach from statistical physics with a varia­
tional approach to analyze learning curves analytically. We apply the
method to Gaussian process regression. As a main result we derive ap­
proximative relations between empirical error measures, the generaliza­
tion error and the posterior variance.
1 Introduction
Approximate expressions for generalization errors for finite dimensional statistical data
models can be often obtained in the large data limit using asymptotic expansions. Such
methods can yield approximate relations for empirical and true errors which can be used
to assess the quality of the trained model see e.g. [1]. Unfortunately, such an approxi­
mation scheme does not seem to be easily applicable to popular non­parametric models
like Gaussian process (GP) models and Support Vector Machines (SVMs). We apply the
replica approach of statistical physics to asses the average case learning performance of
these kernel machines. So far, the tools of statistical physics have been successfully ap­
plied to a variety of learning problems [2]. However, this elegant method suffers from the
drawback that data averages can be performed exactly only under very idealistic assump­
tions on the data distribution in the ''thermodynamic'' limit of infinite data space dimension.
We try to overcome these limitations by combining the replica method with a variational
approximation. For Bayesian models, our method allows us to express useful data aver­
aged a­posteriori expectations by means of an approximate measure. The derivation of this
measure requires no assumptions about the data density and no assumptions about the input
dimension.
The main focus of this article is Gaussian process regression where we demonstrate the
various strengths of the presented method. It solves some of the problems stated at the end
of our previous NIPS paper [3] which was based on a simpler somewhat unmotivated trun­
cation of a cumulant expansion. For Gaussian process models we show that our method
does not only give explicit approximations for generalization errors but also of their sample
fluctuations. Furthermore, we show how to compute corrections to our theory and demon­
strate the possibility of deriving approximate universal relations between average empirical
and true errors which might be of practical interest.
An earlier version of our approach, which was still restricted to the assumption of idealized
data distributions appeared in [4].

2 Setup and Notation
We assume that a class of elementary predictors (neural networks, regressors etc.) is given
by functions f(x). In a Bayesian formulation, we have a prior distribution over this class of
functions f . Assuming that a set of observations y 1 ; : : : ; ym is conditionally independent
given inputs x 1 ; : : : ; xm , we assign a likelihood term of the form exp ( h(f(x i ); y i )) to
each observation. Posterior expectations (denoted by angular brackets) of any functional
Fffg are expressed in the form
hFffgi =
1
Zm
E
 
Fffg exp
 

m
X
i=1
h(f(x i ); y i )
!!
(1)
where the partition function Zm normalizes the posterior and E denotes the expectation
with respect to the prior. We are interested in computing averages [hF ffgi] D of posterior
expectations over different drawings of training data sets D = f(x 1 ; y 1 ); : : : ; (x m ; ym )g
were all data examples are independently generated from the same distribution. In the next
section we will show how to derive a measure which enables us to compute analytically
approximate combined data and posterior averages.
3 A Grand­Canonical Approach
We utilize the statistical mechanics approach to the analysis of learning. Our aim is to
compute the so­called averaged ''free energy'' [ ln Zm ] D which serves as a generating
function for suitable data averages of posterior expectations. The partition function Zm is
Zm = E exp
 

m
X
i=1
h(f(x i ); y i )
!
: (2)
To perform the average [ln Zm ] D we use the replica trick [ln Zm ] D = lim n!0
@ ln[Z n
m ] D
@n ,
where [Z n
m ] D is computed for integer n and the continuation is performed at the end [5].
We obtain
Zn (m) :
= [Z n
m ] D = En
0
@
"
exp
 

n
X
a=1
h(f a (x); y)
!#
(x;y)
1
A
m
; (3)
where En denotes the expectation over the replicated prior measure.
Eq.(3) can be transformed into a simpler form by introducing the ''grand canonical'' parti­
tion function n ()
n ()
:
=
1
X
m=0
e m
m! Zn (m) = En exp( Hn ) (4)
with the Hamiltonian
Hn = e 
"
exp
 

n
X
a=1
h(f a (x); y)
!#
(x;y)
: (5)
The density e Hn evaluates all n replicas f 1 ; : : : ; fn of the predictor at the same data point
(x; y) and the expectation [  ] (x;y) is taken with respect to the true data density p(x; y).
The ''grand canonical'' partition function n () represents a ''poissonized'' version of the
original model with fluctuating number of examples. The ''chemical potential''  deter­
mines the expected value of m = @ ln n ()
@ which yields simply  = ln m for n ! 0. For
sufficiently large m, we can replace the sum in Eq. (4) by its dominating term
ln Zn (m)  ln n () +m(lnm 1) m: (6)

thereby neglecting relative fluctuations. We recover the original (canonical) free energy as
@ ln Zn (m)
@ n  @ ln n (ln m)
@ n .
4 Variational Approximation
For most interesting cases, the partition function n () can not be computed in closed
form for given n. Hence, we use a variational approach to approximate Hn by a different
tractable Hamiltonian H 0
n . It is easy to write down the first terms in an expansion of the
''grand canonical'' free energy with respect to the difference Hn H 0
n
ln n () = ln En e H 0
n +hHn H 0
n i 0
1
2

h(Hn H 0
n ) 2 i 0 hHn H 0
n i 2
0

: : : : (7)
The brackets h: : :i 0 denote averages with respect to the effective measure which is induced
by the prior and e H 0
n and acts in the space of replicated variables. As is well known,
the first two leading terms in Eq.(7) present an upper bound [6] to ln n (). Although
differentiating the bound with respect to n will usually not preserve the inequality, we still
expect 1 that an optimization with respect to H 0
n is a sensible thing to do [7].
4.1 Variational Equations
The grand­canonical ensemble was chosen such that Eq.(5) can be rewritten as an integral
over a local quantity in the input variable x, i.e. in the form Hn = m
R
dx H(x; ff a (x)g)
with
H(x; ff a (x)g) =
Z
dy p(y; x) exp 
n
X
a=1
h(f a (x); y)
!
: (8)
We will now specialize to Gaussian priors over f , for which a local quadratic expression
H 0
n =
Z
dx
X
ab
1
2
 ab (x)f a (x)f b (x) +
X
a
^
r a (x)f a (x) (9)
is a suitable trial Hamiltonian, leading to Gaussian averages h: : :i 0 . The functions  ab (x)
and ^
r a (x) are variational parameters to be optimized. It is important to have an explicit de­
pendence on the input variable x in order to take a non uniform input density into account.
To perform the variation of the first two terms in Eq.(7) we note that the locality of Eq.(8)
makes the ''variational free energy'' ln En exp( H 0
n ) + hHn H 0
n i 0 an explicit function
of the first two local moments
K ab (x) :
= hf a (x)f b (x)i 0 r a (x) :
= hf a (x)i 0 : (10)
Hence, a straightforward variation yields
m
dhH(x; f(x))i 0
dK ab (x)
=  ab (x) m
dhH(x; f(x))i 0
d r a (x)
= ^
r a (x) : (11)
To extend the variational solutions to non­integer values of n, we assume that for all a the
optimal parameters are replica symmetric, ie. ^ r a (x) = ^
r(x) as well as  ab (x) = (x) for
a 6= b and  aa (x) =  0 (x). We also use a corresponding notation for K ab (x) and r a (x).
1 Guided by the success of the method in physical applications, for instance in polymer physics.

4.2 Interpretation of H 0
n
Note, that our approach is not equivalent to a variational approximation of the original
posterior. In contrast, H 0 contains the full information of the statistics of the training data.
We can use the distribution induced by the prior and e H 0
n in order to compute approximate
combined data and posterior averages. As an example, we first consider the expected local
posterior variance  2 (x) :
= [hf 2 (x)i hf(x)i 2 ] D . Following the algebra of the replica
method (see [5]) this is approximated within the variational replica approach as
 2 (x) = lim
n!0
hf 2
a (x)i 0 hf a (x)f b (x)i 0

= K 0 (x) K(x) : (12)
Second, we consider the noisy local mean square prediction error of the posterior mean
predictor ^
f(x) = hf(x)i which is given by "(x; y) = [( ^
f(x) y) 2 ] D . In this case
"(x; y) = lim
n!0
hf a (x)f b (x)i 0 + y 2 2yhf a (x)i 0

= K(x) r 2 (x) + (r(x) y) 2 : (13)
We can also calculate fluctuations with respect to the data average, for example
[( ^
f(x) y) 2 ( ^
f(x 0 ) y 0 ) 2 ] D = lim
n!0
* Y
a=1;2
(f a (x) y)
Y
b=3;4
(f b (x 0 ) y 0 )
+
0
: (14)
5 Regression with Gaussian Processes
This statistical model assumes that data are generated as y i = f(x i ) +  i , where  is
Gaussian white noise with variance  1 . The prior over functions has zero mean and
covariance C(x; x 0 ) = E[f(x)f(x 0 )]. Hence, we have h(f; y) = 1
2 (y f(x)) 2 . Using the
definitions Eqs.(12,13), we get
@hH(x; ff a (x)g)i 0
@n =
p(x)
2

ln(1 +  2 (x)) +
R
dy p(yjx)"(x; y)
 1 +  2 (x)

(15)
which yields the set of variational equations (11). They become particularly easy when the
regression model uses a translationally invariant kernel C(x x 0 ) and the input distribution
is homogeneous in a finite interval. The variational equations (11) can then be solved in
terms of the eigenvalues of the Gaussian process kernel.
[8, 9] studied learning curves for Gaussian process regression which are not only averaged
over the data but also over the data generating process f  using a Gaussian process prior on
f  . Applying these averages to our theory and adapting the notation of [9] simply replaces
in Eq.(15) the term
R
dy p(yjx)"(x; y) by (x) +  2
 while  2 (x)  ^(x).
5.1 Learning Curves and Fluctuations
Practical situations differ from this ''typical case'' analysis. The data generating process
is unknown but assumed to be fixed. The resulting learning curve is then conditioned on
this particular ''teacher'' f  . The left panel of Fig.1 shows an example. Displayed are the
mean square prediction error " (circle and solid line) and its sample fluctuations (error bars)
" with respect to the data average (cross and broken line). The target f  was a random
but fixed realization from a Gaussian process prior with a periodic Radial Basis Function
kernel C(x; x 0 ) =
P
k exp( (x x 0 k) 2 =2l 2 ), l = 0:1. We keep the example simple, e.g
the Gaussian process regression model used the same kernel and noise  1 =  2
 = 0:01.
The inputs are one dimensional, independent and uniformly distributed x 2 [0; 1]. Symbols
represent simulation data. A typical property of our theory (lines) is that it becomes very
accurate for sufficiently large number of example data.

0 50 100 150 200
Number m of Example Data
10 -4
10 -3
10 -2
10 -1
10 0
Generalization
Error
e,
Fluctuation
De
e
De
Theory: Lines
Simulation: Symbols
0 20 40 60 80 100
Number m of Example Data
-4
-3
-2
-1
0
Correction
of
Free
Energy b -1
=0.25
b -1
=0.01
b -1
=0.0001
Figure 1: Gaussian process regression using a periodic Radial Basis Function kernel, input
dimension d=1, x 2 [0; 1], and homogeneous input density. Left: Generalization error "
and fluctuations " for data noise  2
 =  1 = 0:01. Right: Correction of the free energy.
Symbols: We subtracted the first two contributions to Eq.(7) from the true value of the free
energy. The latter was obtained by simulations. Lines show the third contribution of Eq.(7).
The value of the noise variance  1 decreases from top to bottom. All y­data was set equal
to zero.
5.2 Corrections to the Variational Approximation
It is a strength of our method that the quality of the variational approximation Eq.(7) can
be characterized and systematically improved. In this paper, we restrict ourself to a char­
acterization and consider the case where all y­data is set equal to zero. Since the posterior
variance  2 (x) is independent of the data this is still an interesting model from which the
posterior variance can be estimated. We consider the third term in the expansion to the free
energy Eq.(7). It is a correction to the variational free energy and evaluates to
lim
n!0
@
@n
1
2

h(Hn H 0
n ) 2 i 0 hHn H 0
n i 2
0

=
1
4
[ 0 (x) 0 (x 0 ) ^
C 2 (x; x 0 )] x 0 ;x
+
m 2
4
"
ln
 
1
^
C 2 (x; x 0 )
( 2 (x) +  1 ) 2
!#
x 0 ;x
+
m
2
"
 0 (x 0 ) ^
C 2 (x; x 0 )
( 2 (x) +  1 )
#
x 0 ;x
(16)
with ^
C(x; x 0 ) = lim n!0 hf a (x)f a (x 0 ) f a (x)f b (x 0 )i 0 . Eq.(16) is shown by lines in the
right panel of Fig.1 for different values of the model noise  1 . We considered a homo­
geneous input density, the input dimension is one and the regression model uses a periodic
RBF kernel. The symbols in Fig.1 show the difference between the true value of the free
energy which is obtained by simulations and the first two terms of Eq.(7). The correction
term is found to be qualitatively accurate and emphasizes a discrepancy between free en­
ergy and the first two terms of the expansion Eq.(7) for a medium amount of example data.
The calculated learning curves inherit this behaviour.
5.3 Universal Relations
We can relate the training error " T and the empirical posterior variance  2
T
" T
:
=
1
m
" m
X
i=1

^
f(x i ) y i
 2
#
D
;  2
T
:
=
1
m
" m
X
i=1
 2 (x i )
#
D
(17)

0 0.2 0.4 0.6 0.8 1
bs T
2
0
0.2
0.4
0.6
0.8
1
[bs
2
(x)/(bs
2
(x)+1)]
x
Theory
d=1, periodic
d=2, periodic
d=3, periodic
d=2+2, non­periodic
0 0.2 0.4 0.6 0.8 1
be T
0
0.2
0.4
0.6
0.8
1
[be(x,y)/(bs
2
(x)+1)
2 ]
(x,y)
Theory
1d, periodic
2d, periodic
3d, periodic
Figure 2: Illustration of relation Eq.(19) (left) and Eq.(20) (right). All error measures are
scaled with . Symbols show simulation results for Radial Basis Function (RBF) regression
and a homogeneous input distribution in d = 1; 2; 3 dimensions (square, circle, diamond).
The RBF kernel was periodic. Additionally, the left figure shows an example were the
inputs lie on a quasi two­dimensional manifold which is embedded in d = 4 dimensions
(cross). In this case the RBF kernel was non­periodic.
to the free energy d
d [ ln Zm ] D = m
2 (" T +  2
T ). Using Eqs.(6,7) and the stationarity of
the grand­canonical free energy with respect to the variational parameters we obtain the
following relation
d
d [ ln Zm ] D  m
@
@
Z
dx
@hH(x; ff a (x)g)i 0
@n : (18)
We use the fact that the posterior variance is independent of the y­data and simply estimate
it from the model where all y­data is set equal to zero. In this case, Eq.(18) yields
 2
T =
Z
dx p(x)
 2 (x)
1 +  2 (x)
(19)
which relates the empirical posterior variance  2
T to the local posterior variance  2 (x) at
test inputs x. Similarly, we can derive an expression for the training error " T by using
Eqs.(15,18) in combination with Eq.(19)
" T =
Z
dx dy p(x; y)
"(x; y)
(1 +  2 (x)) 2 : (20)
It is interesting to note, that the relations (19,20) contain no assumptions about the data
generating process. They hold in general for Gaussian process models with a Gaussian
likelihood. An illustration of Eqs.(19,20) is given by Fig.2 for the example of Gaussian
process regression with a Radial Basis Function kernel. In the left panel of Fig.2, learning
starts in the upper right corner as the rescaled empirical posterior variance  2
T is initially
one and decreases with increasing number of example data. For the right panel of Fig.2,
learning starts in the lower left corner. The rescaled training error " T on the noisy data set
is initially zero and increases to one with increasing number of example data. The theory
(line) holds for a sufficiently large number of example data and its accuracy increases with
the input dimension. Eqs.(19,20) can also be tested on real data. For common benchmark
sets such as Abalone and Boston Housing data we find that Eqs.(19,20) hold well even for
small and medium sizes of the training data set.

6 Outlook
One may question if our approximate universal relations are of any practical use as, for
example, the relation between training error and generalization error involves also the un­
known posterior variance  2 (x). Nevertheless, this relation could be useful for cases, where
a large number of data inputs without output labels are available. Since for regression, the
posterior variance is independent of the output labels, we could use these extra input points
to estimate  2 (x).
The application of our technique to more complicated models is possible and technically
more involved. For example, replacing e h by (yf(x) 1) in Eq.(1) and further rescal­
ing the kernel C(x; x 0 ) = K(x; x 0 )= of the Gaussian process prior gives a model for hard
margin Support Vector Machine Classification with SVM kernel K(x; x 0 ). The condition
of maximum margin classification will be ensured by the limes  !1.
Of particular interest is the computation of empirical estimators that can be used in practice
for model selection as well as the calculation of fluctuations (error bars) for such estimators.
A prominent example is an efficient approximate leave­one­out estimator for SVMs.
Work on these issues is in progress.
Acknowledgement
We would like to thank Peter Sollich for may inspiring discussions. The work was sup­
ported by EPSRC grant GR/M81601.
References
[1] N. Murata, S. Yoshizawa, S. Amari, IEEE Transactions on Neural Networks 5, p.
865­872, (1994).
[2] A. Engel, C. Van den Broeck, Statistical Mechanics of Learning, Cambridge Univer­
sity Press (2001).
[3] D. Malzahn, M. Opper, Neural Information Processing Systems 13, p. 273, T. K.
Leen, T. G. Dietterich and V. Tresp, eds., MIT Press, Cambridge MA (2001).
[4] D. Malzahn, M. Opper, Lecture Notes in Computer Science 2130, p. 271, G. Dorffner,
H. Bischof and K. Hornik, eds., Springer, Berlin (2001).
[5] M. M’ezard, G. Parisi, M. Virasoro, Spin Glass Theory and Beyond, World Scientific,
Singapore, (1987).
[6] R. P. Feynman and A. R. Hibbs, Quantum mechanics and path integrals, Mc Graw­
Hill Inc., (1965).
[7] T. Garel, H. Orland, Europhys. Lett. 6, p. 307 (1988).
[8] P. Sollich, Neural Information Processing Systems 11, p. 344, M. S. Kearns, S. A.
Solla and D. A. Cohn, eds., MIT Press, Cambridge MA (1999).
[9] P. Sollich, Neural Information Processing Systems 14, T. G. Dietterich, S. Becker, Z.
Ghahramani, eds., MIT Press (2002).

