Geometrical Singularities in the
Neuromanifold of Multilayer Perceptrons
Shun­ichi Amari, Hyeyoung Park, and Tomoko Ozeki
Brain Science Institute, RIKEN
Hirosawa 2­1, Wako, Saitama, 351­0198, Japan
famari, hypark, tomokog@brain.riken.go.jp
Abstract
Singularities are ubiquitous in the parameter space of hierarchical
models such as multilayer perceptrons. At singularities, the Fisher
information matrix degenerates, and the Cram'er­Rao paradigm
does no more hold, implying that the classical model selection the­
ory such as AIC and MDL cannot be applied. It is important to
study the relation between the generalization error and the training
error at singularities. The present paper demonstrates a method
of analyzing these errors both for the maximum likelihood estima­
tor and the Bayesian predictive distribution in terms of Gaussian
random fields, by using simple models.
1 Introduction
A neural network is specified by a number of parameters which are synaptic weights
and biases. Learning takes place by modifying these parameters from observed
input­output examples. Let us denote these parameters by a vector ` = (` 1 ; \Delta \Delta \Delta ; ` n ).
Then, a network is represented by a point in the parameter space S, where ` plays
the role of a coordinate system. The parameter space S is called a neuromanifold.
A learning process is represented by a trajectory in the neuromanifold. The dy­
namical behavior of learning is known to be very slow, because of the plateau
phenomenon. The statistical physical method [1] has made it clear that plateaus
are ubiquitous in a large­scale perceptron. In order to improve the dynamics of
learning, the natural gradient learning method has been introduced by taking the
Riemannian geometrical structure of the neuromanifold into account [2, 3]. Its
adaptive version, where the inverse of the Fisher information matrix is estimated
adaptively, is shown to have excellent behaviors by computer simulations [4, 5].
Because of the symmetry in the architecture of the multilayer perceptrons, the
parameter space of the MLP admits an equivalence relation [6, 7]. The residue class
divided by the equivalence relation gives rise to singularities in the neuromanifold,
and plateaus exist at such singularities [8]. The Fisher information matrix becomes
singular at singularities, so that the neuromanifold is strongly curved like the space­
time including black holes.
In the neighborhood of singularities, the Fisher­Cram'er­Rao paradigm does not

hold, and the estimator is no more subject to the Gaussian distribution even asymp­
totically. This is essential in neural learning and model selection. The AIC and MDL
criteria of model selection use the Gaussian paradigm, so that it is not appropriate.
The problem was first pointed out by Hagiwara et al. [9]. Watanabe [10] applied
algebraic geometry to elucidate the behavior of the Bayesian predictive estimator in
MLP, showing sharp difference in regular cases and singular cases. Fukumizu [11]
gives a general analysis of the maximum likelihood estimators in singular statistical
models including the multilayer perceptrons.
The present paper is a first step to elucidate effects of singularities in the neuro­
manifold of multilayer perceptrons. We use a simple cone model to elucidate how
different the behaviors of the maximum likelihood estimator and the Bayes predic­
tive distribution are from the regular case. To this end, we introduce the Gaussian
random field [11, 12, 13], and analyze the generalization error and training error for
both the mle (maximum likelihood estimator) and the Bayes estimator.
2 Topology of neuromanifold
Let us consider MLP with h hidden units and one output unit,
y =
h
X
i=1
v i ' (w i \Delta x) + n: (1)
where y is output, x is input and n is Gaussian noise. Let us summarize all the
parameters in a single parameter vector ` = (w 1 ; \Delta \Delta \Delta ; w h ; v 1 ; \Delta \Delta \Delta ; v h ) and write
f(x; `) =
h
X
i=1
v i ' (w i \Delta x) : (2)
Then, ` is a coordinate system of the neuromanifold. Because of the noise, the
input­output relation is stochastic, given by the conditional probability distribution
p(yjx; `) = 1
p
2
exp
ae
\Gamma
1
2 (y \Gamma f(x; `)) 2
oe
; (3)
where we normalized the scale of noise equal to 1. Each point in the neuromanifold
represents a neural network or its probability distribution.
It is known that the behavior of MLP is invariant under 1) permutations of hidden
units, and 2) sign change of both w i and v i at the same time. Two networks
are equivalent when they are mapped by any of the above operations which form
a group. Hence, it is natural to treat the residual space S= ß, where ß is the
equivalence relation. There are some points which are invariant under a some non­
trivial isotropy subgroup, on which singularities occurs.
When v i = 0; v i ' (w i \Delta x) = 0 so that all the points on the submanifold v i = 0 are
equivalent whatever w i is. We do not need this hidden unit. Hence, in M = S= ß,
all of these points are reduced to one and the same point. When w i = w j hold,
these two units may be merged into one, and when v i +v j is the same, the two points
are equivalent even when they differ in v i \Gamma v j . Hence, the dimension reduction takes
place in the subspace satisfying w i = w j . Such singularities occur on the critical
submanifolds of the two types
1) v i w i = 0; 2) w i = w j : (4)

3 Simple toy models
Given training data, the parameters of the neural network are estimated or trained
by learning. It is important to elucidate the effects of singularities on learning or
estimation. We use simple toy models to attack this problem. One is a very simple
multilayer perceptron having only one hidden unit. The other is a simple cone
model: Let x be Gaussian random variable x 2 R d+2 , with mean ¯ and identity
covariance matrix I,
p(xj¯) = 1
( p
2ß) d+2
exp
ae
\Gamma
1
2 jjx \Gamma ¯jj 2
oe
(5)
and let S = f¯j¯ 2 R d+2 g be the parameter space. The cone model M is a subset
of S, embedded as
M : ¯ = ¸
p
1 + c 2
`
1
c!
'
= ¸a(!) (6)
where c is a constant, jja 2 jj = 1, ! 2 S d and S d is a d­dimensional unit sphere.
When d = 1, S 1 is a circle so that ! is replaced by angle `, and we have
¯ = ¸
p
1 + c 2
/ 1
c cos `
c sin `
!
: (7)
See Figure 1. The M is a cone, having (¸; !) as coordinates, where the apex ¸ = 0
is the singular point.
2
x 3
S 1
x q
( )
x
a
1
x
Figure 1: One­dimensional cone model
The input­output relation of a simple multilayer perceptron is given by
y = v'(w \Delta x) + n (8)
When v = 0, the behavior is the same whatever w is. Let us put w = fi!, where
fi = jwj and ! 2 S d , and ¸ = vjwj, /(x; fi; !) = ' ffi(! \Delta x)g =fi. We then have
y = ¸/(x; fi; !) + n (9)
which shows the cone structure with apex at ¸ = 0. In this paper, we assume that
fi is knwon and does not need to be estimateed.

4 Asymptotic statistical inference: generalization error and
training error
Let D = fx 1 ; \Delta \Delta \Delta ; x T g be T independent observations from the true distribution
p 0 (x) which is specified by ¸ = 0, that is, at the singular point. In the case
of neural networks, the training set D is T input­output pairs (x t ; y t ), from the
conditional probability distributions p(yjx; ¸; !) and the true one is at ¸ = 0. The
discussions go in parallel, so that we show here only the cone model. We study the
characteristics of both the mle and the Bayesian predictive estimator.
Let “
p(x) be the estimated distribution from data D. In the case of mle, it is given
by “
p(x; “
`) where “ ` is the mle given by the maximizer of the log likelihood. For the
Bayes estimator, it is given by the Bayes predictive distribution p(xjD).
We evaluate the estimator by the generalization error defined by the KL­divergence
from p 0 (x) to “
p(x),
E gen = ED [K[p o : “ p]] ; K[p o : “
p] = E po
Ÿ
log p o (x)
“
p(x)
–
: (10)
Similarly, the training error is defined by using the empirical expectation,
E train = ED
''
1
T
T
X
t=1
log p o (x t )
“
p(x t )
#
: (11)
In order to evaluate the estimator “
p, one uses E gen , but it is not computable.
Instead, one uses the E train which is computable. Hence, it is important to see the
difference between E gen and E train . This is used as a principle of model selection.
When the statistical model M is regular, or the true distribution p o (x) is at a regular
point, the mle­based p(x; “ `) and the Bayes predictive distribution are asymptoti­
cally equivalent, and are Fisher efficient under reasonable regularity conditions,
E gen ß
d
2T ; E gen ß E train + d
T ; (12)
where d is the dimension number of parameter vector `.
All of these good relations do not hold in the singular case. The mle is no more
asymptotically Gaussian, the mle and the Bayes estimators have different asymp­
totic characteristics, although 1=T consistency is guaranteed. The relation between
the generalization and training error is different, so that we need a different model
selection criterion to determine the number of hidden units.
5 Gaussian random fields and mle
Here, we introduce the Gaussian random field [11, 12, 13] in the case of the cone
model. The log likelihood of data D is written as
L(D; ¸; !) = \Gamma
1
2
T
X
t=1
jjx t \Gamma ¸a(!)jj 2 : (13)
Following Hartigan [13] (see also [11] for details), we first fix ! and search for the ¸
that maximizes L. This is easy since L is a quadratic function of ¸. The maximum

“
¸ is given by
“
¸(!) = argmax ¸ L(D; ¸; !) = 1
p
T
Y (!); (14)
Y (!) = a(!) \Delta ~ x; ~ x = 1
p
T
T
X
t=1
x t : (15)
By the central limit theorem, Y (!) = a(!) \Delta ~
x is a Gaussian random field defined
on S d = f!g. By substituting “
¸(w) in (14) the log likelihood function becomes
“
L(!) = \Gamma
1
2
T
X
t=1
jjx t jj 2 + 1
2 Y 2 (!): (16)
Therefore, the mle “
w is given by the maximizer of “
L(w), “
! = argmax ! Y 2 (!).
Theorem 1. In the case of the cone model, the mle satisfies
E gen = 1
2T ED
Ÿ
sup
!
Y 2 (!)
–
; (17)
E train = \Gamma
1
2T ED
Ÿ
sup
!
Y 2 (!)
–
: (18)
Corollary 1. When d is large, the mle satisfies
E gen ß
c 2 d
2T (1 + c 2 ) ; (19)
E train ß \Gamma
c 2 d
2T (1 + c 2 ) : (20)
It should be remarked that the generalization and training errors depend on the
shape parameter c as well as the dimension number d.
6 Bayesian predictive distribution
The Bayes paradigm uses the posterior probability of the parameters based on the
set of observations D. The posterior probability density is written as,
p(¸; !jD) = c(D)ß(¸; !)
T
Y
t=1
p (x t j¸; !) ; (21)
where c(D) is the normalization factor depending only on data D, ß(¸; !) is a prior
distribution on the parameter space. The Bayesian predictive distribution p(xjD) is
obtained by averaging p(xj¸; !) with respect to the posterior distribution p(¸; !jD),
and can be written as
p(xjD) =
Z
p(xj¸; !)p(¸; !jD)d¸d!: (22)
The Bayes predictive distribution depends on the prior distribution ß(¸; !). As long
as the prior is a smooth function, the first order asymptotic properties are the same
for the mle and Bayes estimators in the regular case. However, at singularities, the
situation is different. Here, we assume a uniform prior for !. For ¸, we assume two
different priors, the uniform prior and the Jeffreys prior.

We show here a sketch of calculations in the case of Jeffreys prior, ß(¸; !) / j¸j d .
By introducing
I d (u) = 1
p
2ß
Z
jz + uj d exp
ae
\Gamma
1
2 z 2
oe
dz; (23)
after lengthy calculations, we obtain
p(xjD) = 1
p
2ß d+2
r
T
T + 1
d+1
exp
(
\Gamma jjxjj
2
2
)
P d (~ x T+1 )
P d (~ x)
; (24)
where
~
x T+1 = 1
p
T + 1 (x +
p
T ~ x); P d (~ x) =
Z
I d (Y (!)) exp
ae 1
2 Y 2 (!)
oe
d!: (25)
Here Y (!) has the same form defined in (15), and P d (~ x) is the function of the
sufficient statistics ~
x. By using the Edgeworth expansion, we have
p(xjD) ¸ = 1
p
2ß d+2 exp
(
\Gamma jjxjj
2
2
)
ae
1 + 1
p
T
r log P d (~ x) \Delta x + 1
2T tr
` rrP d
P d
H 2 (x)
'oe
; (26)
where r is the gradient and H 2 (x) is the Hermite polynomial. We thus have the
following theorem.
Theorem 2. Under the Jeffreys prior for ¸, the generalization error and the
training error of the predictive distribution are given by
E gen = 1
2T ED
h
kr log P d (~ x)k 2
i
; (27)
E train = E gen \Gamma
1
T ED
h
r log P d (~ x) \Delta ~
x
i
: (28)
Under the uniform prior, the above results hold by replacing I d (Y ) in the definition
of P d (~ x) by 1. In addition, From (24), we can easily obtain E gen = (d + 1)=2T for
the Jeffreys prior, and E gen = 1=2T for the uniform prior.
The theorem shows rather surprising results : Under the uniform prior, the general­
ization error is constant and does not depend on d. This is completely different from
the regular case. However, this striking result is given rise to by the uniform prior
on ¸. The uniform prior puts strong emphasis on the singularity, showing that one
should be very careful for choosing a prior when the model includes singularities. In
the case of Jeffreys prior, the generalization error increases in proportion to d, which
is the same result as the regular case. In addition, the symmetric duality between
E gen and E train does not hold for both of the uniform prior and the Jeffreys prior.
7 Gaussian random field of MLP
In the case of MLP with one hidden unit, the log likelihood is written as
L(D; ¸; !) = \Gamma
1
2
T
X
t=1
fy t \Gamma ¸' fi (! \Delta x t )g 2 : (29)

Let us define a Gaussian random field depending on D and !,
Y (!) = 1
p
T
T
X
t=1
y t ' fi (! \Delta x t ) ¸ N(0; A(!; ! 0 )) (30)
where A(!; ! 0 ) = Ex [' fi (! \Delta x)' fi (! 0
\Delta x)].
Theorem 3. For the mle, we have
“
! mle = argmax ! Y 2 (!); (31)
E gen = 1
2T ED
Ÿ
sup
!
Y (!) 2
A(!)
–
; (32)
E train = \Gamma
1
2T ED
Ÿ
sup
!
Y (!) 2
A(!)
–
; (33)
where A(!) = A(!; !).
In order to analyze the Bayes predictive distribution, we define
S d (D; !) = 1
p
A(!) d+1 I d
/
Y (!)
p
A(!)
!
exp
ae 1
2
Y 2 (!)
A(!)
oe
: (34)
We then have the Edgeworth expansion of the predictive distribution of the form,
p(yjx; D) ¸ = 1
p
2ß
exp
ae
\Gamma
y 2
2
oe ae
1 + y
p
T
E! [rS d (D; !)' fi (! \Delta x)]
E! [S d (D; !)]
(35)
+ 1
2T
E! [rrS d (D; !)A(!)]
E! [S d (D; !)]
H 2 (y)
oe
;
where r is the gradient with respect to Y (!). We thus have the following theorem.
Theorem 4. Under the Jeffreys prior for ¸, the generalization error and the
training error of the predictive distribution are given by
E gen = 1
2T ED
Ÿ E !! 0 [rS d (D; !)rS d (D; ! 0 )A(!; ! 0 )]
E! [S d (D; !)] 2
–
;
E train = E gen \Gamma
1
T ED
Ÿ E! [rS d (D; !)Y (!)]
E! [S d (D; !)]
–
: (36)
Under the uniform prior, the above results hold by redefining
S d (D; !) = 1
p
A
exp
ae 1
2
Y 2 (!)
A(!)
oe
: (37)
We can also obtain E gen = (d + 1)=2T for the Jeffreys prior, and E gen = 1=2T for
the uniform prior.
There is a nice correspondence between the cone model and MLP. However, there
is no sufficient statistics in the MLP case, while all the data are summarized in the
sufficient statistics ~
x in the cone model.

8 Conclusions and discussions
We have analyzed the asymptotic behaviors of the MLE and Bayes estimators in
terms of the generalization error and the training error by using simple statistical
models (cone model and simple MLP), when the true parameter is at singularity.
Since the classic paradigm of statistical inference based on the Cram'er­Rao theorem
does not hold in such a singular case, we need a new theory. The Gaussian random
field has played a fundamental role. We can compare the estimation accuracy of
the maximum likelihood estimator and the Bayesian predictive distribution from the
results of analysis. Under the proposed framework, the various estimation methods
can be studied and compared to each other.
References
[1] Saad, D. and Solla, S. A. (1995). Physical Review E, 52, 4225­4243.
[2] Amari, S. (1998). Neural Computation, 10, 251­276.
[3] Amari S. and Nagaoka, H. (2000). Methods of Information Geometry, AMS.
[4] Amari, S., Park, H., and Fukumizu, F. (2000). Neural Computation, 12, 1399­
1409.
[5] Park, H., Amari, S. and Fukumizu, F. (2000). Neural Networks, 13, 755­764.
[6] Chen, A. M., Lu, H., and Hecht­Nielsen, R. (1993). Neural Computations, 5,
910­927.
[7] R¨ugger, S. M. and Ossen, A. (1997). Neural Processing Letters, 5, 63­72.
[8] Fukumizu, K. and Amari, S. (2000) Neural Networks, 13 317­327.
[9] Hagiwara, K., Hayasaka, K., Toda, N., Usui, S., and Kuno, K. (2001). Neural
Networks, 14 1419­1430.
[10] Watanabe, S. (2001). Neural Computation, 13, 899­933.
[11] Fukumizu, K. (2001). Research Memorandum, 780, Inst. of Statistical Mathe­
matics.
[12] Dacunha­Castelle, D. and Gassiat, E. (1997). Probability and Statistics, 1, 285­
317.
[13] Hartigan, J. A. (1985). Proceedings of Berkeley Conference in Honor of J.
Neyman and J. Kiefer, 2, 807­810.

