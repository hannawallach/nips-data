Kernel Machines and Boolean Functions
Adam Kowalczyk
Telstra Research Laboratories
Telstra, Clayton, VIC 3168
a.kowalczyk@trl.oz.au
Alex J. Smola, Robert C. Williamson
RSISE, MLG and TelEng
ANU, Canberra, ACT, 0200
fAlex.Smola, Bob.Williamsong@anu.edu.au
Abstract
We give results about the learnability and required complexity of logical
formulae to solve classification problems. These results are obtained by
linking propositional logic with kernel machines. In particular we show
that decision trees and disjunctive normal forms (DNF) can be repre­
sented by the help of a special kernel, linking regularized risk to separa­
tion margin. Subsequently we derive a number of lower bounds on the
required complexity of logic formulae using properties of algorithms for
generation of linear estimators, such as perceptron and maximal percep­
tron learning.
1 Introduction
The question of how many Boolean primitives are needed to learn a logical formula is
typically an NP­hard problem, especially when learning from noisy data. Likewise, when
dealing with decision trees, the question what depth and complexity of a tree is required to
learn a certain mapping has proven to be a difficult task.
We address this issue in the present paper and give lower bounds on the number of Boolean
functions required to learn a mapping. This is achieved by a constructive algorithm which
can be carried out in polynomial time. Our tools for this purpose are a Support Vector
learning algorithm and a special polynomial kernel.
In Section 2 we define the classes of functions to be studied. We show that we can treat
propositional logic and decision trees within the same framework. Furthermore we will
argue that in the limit boosted decision trees correspond to polynomial classifiers built
directly on the data. Section 3 contains our main result linking the margin of separation
to a simple complexity measure on the class of logical formulae (number of terms and
depth). Subsequently we apply this connection to devise test procedures concerning the
complexity of logical formulae capable of learning a certain dataset. More specifically,
this will involve the training of a perceptron to minimize the regularized risk functional.
Experimental results and a discussion conclude the paper. Some proofs have been omitted
due to space constraints. They can be found in an extended version of this paper (available
at http://www.kernel­machines.org).

2 Polynomial Representation of Boolean Formulae
We use the standard assumptions of supervised learning: we have a training set
f(x 1 ; y 1 ); : : : ; (xm ; ym )g ae X \Theta Y . Based on these observations we attempt to find a
function f : X ! Y which incorporates the information given by the training set. Here
goodness of fit is measured relative to some predefined loss function or a probabilistic
model.
What makes the situation in this paper special is that we assume that X ` B n where
B = f0; 1g and moreover Y = f\Sigma1g. In other words, we attempt to learn a binary
function on Boolean variables. A few definitions will be useful.
The set of all polynomials f of degree Ÿ d 2 N on B n will be denoted 1 by Pol d . They are
given by the expansion
f(x) =
X
i2I
c i x i where I ae fiji 2 B n and jij Ÿ dg; (1)
for every x = (x 1 ; ::; xn ) 2 X , where c i 2 R and we use a compact notation x i :=
x i 1
1 \Delta \Delta \Delta x i n
n for every (i 1 ; :::; i n ) 2 B n for monomials on B n , with the usual convention of
x 0 := 1 for every x – 0. In order to avoid further notation we always assume in such
expansions that c i 6= 0 for all i 2 I.
The subset DNF d ae Pol d of all polynomials of the form
f(x) = \Gamma1 + 2
X
i2I
x i where I ae fiji 2 B n and jij Ÿ dg (2)
will be called disjunctive normal forms. It is linked by the following lemma to the set of
disjunctive normal forms DNF d
logic commonly used in the propositional logic. The latter
consist of all clauses OE : B n ! fFALSE; TRUEg which can be expressed by disjunctions
of terms, each being a conjunction of up to d logical primitives [x i = 1] and [x i = 0].
Lemma 1 Assume for each i 2 f1; : : : ; ng there exists an i 0 2 f1; : : : ; ng such that
x i = 1 \Gamma x i 0 for all x = (x 1 ; :::; xn ) 2 X: (3)
Then for every f 2 DNF d there exists OE 2 DNF d
logic such that for every x 2 X
f(x) – 1 if and only if OE(x) j TRUE:
And vice versa, for every such OE there exists f satisfying the above relation.
Standing Assumption: Unless stated otherwise, we will in the rest of the paper assume
that (3) of the above lemma holds. This is not a major restriction, since we can always
satisfy (3) by artificially enlarging X ae B n into f(x i ; 1 \Gamma x i ) ; (x i ) 2 Xg ae B 2n .
Now we consider another special subclass of polynomials, DT d (X) ae Pol d , called deci­
sion trees . These are polynomials which have expansions of type (1) where (i) all coeffi­
cients c i 2 f\Gamma1; 1g and (ii) for every x 2 X exactly one monomial x i , i 2 I, `fires', i.e.
exactly one of the numbers fx i g i2I equals 1 and all the others are 0.
Eq. (1) shows that each decision tree can be expressed as half of a difference between two
disjunctive normal forms such that for any given input, one and only one of the conjunctions
comprising them will be true. There exists also an obvious link to popular decision trees (on
Boolean variables) used for classification in machine learning, cf. [4, 12]. Here the depth
of a leaf equals the degree of the corresponding monomial, and the coefficient c i 2 f\Sigma1g
corresponds to the class associated with the leaf.
1 Such binary polynomials are widely used under the name of score tables, e.g. typically loan
applications are assessed by financial institutions by an evaluation of such score tables.

3 Reproducing Kernel Hilbert Space and Risk
Kernel The next step is to map the complexity measure applied to decision trees, such as
depth or number of leaves, to a Reproducing Kernel Hilbert Space (RKHS), as used in Sup­
port Vector machines. This is defined as H = Pol d with the scalar product corresponding
to the norm defined via the quadratic form on f
kfk 2
H :=
X
i2I
K jij c 2
i : (4)
Here K i ? 0 with i 2 f0; : : : ; dg are complexity weights for each degree of the polynomi­
als and the coefficients c i are the coefficients of expansion (1).
Lemma 2 (Existence of Kernel) The RKHS kernel k realizing the dot product corre­
sponding to the quadratic form (4) with k(x; \Delta) 2 Pol d has the following efficient functional
form:
k(x; x 0 ) =
d
X
j=0
K \Gamma1
j
` hx; x 0 i
j
'
: (5)
Proof The norm kfkH is well defined by (4) for all f 2 Pol d and the space Pol d is
complete. Furthermore it is easy to check that (4) defines a homogeneous quadratic form
on Pol d . Via the polarization identity we can reconstruct a bilinear form (dot product) from
(4). This gives us the desired Hilbert space. From [1] we obtain that there exists a unique
kernel k(x; x 0 ) corresponding to k \Delta k 2
H . The key observation for derivation of its form (5)
is that given x; x 0 2 X and j ? 0 there are exactly
\Gamma hx;x 0 i
j
\Delta
non­vanishing monomials of
the form x i x 0 i
= x ff 1
x 0
ff 1
\Delta \Delta \Delta x ff j x 0
ff j , where 1 Ÿ ff 1 ! ff 2 ! \Delta \Delta \Delta ! ff j Ÿ n are positions
of 1's in the sequence i.
Note that for the special case where K j = ffl \Gammaj with ffl ? 0 and d – hx; x 0 i, (5) simply leads
to a binomial expansion and we obtain
k(x; x 0 ) =
d
X
j=0
ffl j
` hx; x 0 i
j
'
= (1 + ffl) hx;x 0 i : (6)
The larger ffl, the less severely we will penalize higher order polynomials, which provides
us with an effective means of controlling the complexity of the estimates. Note that this is
applicable to the case when d – jxj, and always holds for d = n.
Due to the choice of the c i in DNF d and DT d (X) we obtain
jjf jj 2
H = 1 + 4
X
i2I
K jij for f 2 DNF d and jjf jj 2
H =
X
i2I
K jij for f 2 DT d (X):
Next we introduce regularized risk functionals. They follow the standard assumptions made
in soft­margin SVM and regularization networks.
For our training set (x i ; y i ) of size m and a regularization constant – ? 0 we define
R[f; –] := jjf jj 2
H + – \Gamma1
m
X
i=1
(1 \Gamma y i f(x i )) 2 ;
R+ [f; –] := jjf jj 2
H + – \Gamma1
m
X
i=1
[1 \Gamma y i f(x i )] 2
+ ;

for every f 2 Pol d , where [¸] + := max(0; ¸) for every ¸ 2 R.
The first risk is typically used by regularization networks [8], the other by support vector
machines [5]. Note that for all f 2 Pol d we have R[f; –] – R+ [f; –]. Furthermore, if
f 2 DNF d [ DT d (X), then jf(x i )j – 1 and hence
R[f; –] – R+ [f; –] –
I
X
i2I
K jij + 4– \Gamma1 err(f) (7)
where err(f) := #fi j y i 6= f(x i )g (8)
denotes the number of classification errors (on the training set).
Note that in (7) equalities hold throughout for f 2 DT d (X) and in such a case the risks
are fully determined by the depths of the leaves of the decision tree and the number of
classification errors. Furthermore, in the particular case of decision trees and all coef­
ficients K jij = 1, i.e. when jjf jj 2
K equals to the number of leaves of the decision tree
f 2 DT d (X), the regularized risks R[f; –] = R+ [f; –] are exactly equal to the ``cost com­
plexity'' employed to prune decision trees by CART algorithm [4]. In other words, the basis
of the pruning algorithm in CART is the minimisation of the regularised risk in the class
of subtrees of the maximal tree, with the regularisation constant – selected by a heuristic
applied to a validation set.
Our reasoning in the following relies on the idea that if we can find some function f 2 Pol d
which minimizes R[f; –] or R+ [f; –], then the minimizer of the risk functionals, when cho­
sen from the more restrictive set f 2 DT d (X) or f 2 DNF d , must have a risk functional
at least as large as the one found by optimizing over Pol d . This can then be translated into
a lower bound on the complexity of f since DT d (X); DNF d ae Pol d .
4 Complexity Bounds
The last part missing to establish a polynomial­time device to lower­bound the required
complexity of a logical formula is to present actual algorithms for minimizing R[f; –] or
R+ [f; –]. In this section we will study two such methods: the kernel perceptron and the
maximum margin perceptron and establish bounds on execution time and regularized risk.
Kernel Perceptron Test The k; –­perceptron learning algorithm is a direct modification
of ordinary linear perceptron learning rule. In the particular case of – = 0 it becomes
the ordinary perceptron learning rule in the feature space R N . For – ? 0 it implements
perceptron learning rule in the extended feature space R N \Theta R m ; cf. [7, 6] for details.
Algorithm 1 Regularized kernel perceptron (k; –­perceptron)
Given: a Mercer kernel k and a constant – – 0.
Initialize: t = 0 and ff i = 0 for i = 1; :::; m.
while an update is possible do
find j such that y j
P m
i=1 y i ff i k(x j ; x i ) + –ff j Ÿ 0, then update:
ff j ff j + 1 and t t + 1:
end while
We introduce the special notation: R = max i
p
k(x i ; x i ), r = min i
p
k(x i ; x i ) and
f ~
ff;k (x) :=
P
i y i ff i k(x i ; x) for every x 2 X and ~ ff 2 R m . Note that f ~
ff;k 2 Pol d and
jjf ~ ff;k jj 2
H =
P
ij y i y j ff i ff j k(x i ; x j ).

A modification of the standard proof of convergence of linear perceptron [11] combined
with the extended feature space trick [13] gives the following result.
Theorem 3 Assume that the coefficients ~ ff = (ff i ) 2 R m were generated after t­th update
of the k; –­perceptron and
ae \Lambda :=
Ÿ
max
~ ff2R m \Gamma0
min
i
y i f ~ ff;k (x i )=jjf ~ ff;k jj H
–
+
:
Then t Ÿ (R 2 + –)=(ae \Lambda
2 + –=m) and
R[f; –] – R+ [f; –] – t 2
jjf ~ ff;k jj 2
H + –jj~ffjj 2
– t
R 2 + –
for every f 2 Pol d . (9)
Note that ae \Lambda defined above is the maximal margin of separation of the training data by
polynomials from Pol d (treated as elements of the RKHS).
Maximum Margin Perceptron Test Below we state formally the soft margin version of
maximal margin perceptron algorithm. This is a simplified (homogeneous) version of the
algorithm introduced in [9].
Algorithm 2 Greedy Maximal Margin Perceptron (k; –­MMP)
Given: ffl ? 0, – – 0 a and a Mercer kernel k.
Initialize: k \Lambda
j = k(x j ; x j ) + – for j = 1; ::; m;
i = arg min j k \Lambda
j , kfk 2 = k \Lambda
i , ø i = 0;
f j = y j k(x i ; x j ) + ffi ij – and ff j = ffi ij for j = 1; :::; m;
while 9 i 0 y i 0 w \Delta x i 0 Ÿ (1 \Gamma ffl)kf k 2 do
for for every j = 1; ::; m do
f j / (1 \Gamma ø i )f j + ø i y i y j k(x i ; x j ) + –ø i ffi ij ;
g j / kfk 2 \Gamma f j ; d j = kfk 2 + k \Lambda
j \Gamma 2f j ;
ff j / ø i ffi ij + (1 \Gamma ø i )ff j ;
ø j / g j
d j
if g j ? 0, else / max( \Gammaff j
1\Gammaff j
; g j
d j
) if g j ! 0 & 0 ! ff j ! 1, else / 0.
end for
find i = arg max j (ø j g j ), then set
kfk 2 / (1 \Gamma ø i ) 2 kfk 2 + ø i k \Lambda
i + 2ø i (1 \Gamma ø i )f i ;
end while
The proof of the following theorem uses the extended feature space [13].
Theorem 4 Given 0 ! ffl and – – 0. Assume that the vector ~ ff = (ff i ) 2 R m was
generated after t­th iteration of the ``while loop'' of the k; –­MMP learning rule. Then
t Ÿ R 2 + r 2 + 2–
ffl 2
` 1
ae \Lambda 2 + –=m
\Gamma 1
r 2 + –
'
(10)
R[f; –] – R+ [f; –] – 1
jjf ~ ff;k jj 2
H + –jj~ffjj 2 – 1
r 2 + –
+
tffl 2
R 2 + r 2 + 2–
(11)
for every f 2 Pol d . If the algorithm halts after t­th update, then
1
jjf ~ ff;k jj 2
H + –jj~ffjj 2 – min
f2Pol d R+ [f; –](1 \Gamma ffl) 2 : (12)

Note that condition (10) ensures the convergence of the algorithm in a finite time. The
above theorem for – = 0 ensures that solution generated by Algorithm 2 converges to the
(hard) maximum margin classifier. Further, it can be shown that the bound (11) holds for
every ~ ff = (ff i ) such that each ff i – 0 and
P ff i = 1.
Bounds on classification error The task of finding a linear perceptron minimizing the
number of classification errors on the training set is known to be NP­hard. On this basis
it is reasonable to expect that finding a decision tree or disjunctive normal form of upper
bounded complexity and minimizing the number of errors is also hard. In this section we
provide a lower bound on the number of errors for such classifiers.
The following estimates on err(f), i.e. the number of classification errors (8), can be
derived from Theorems 3 and 4:
Theorem 5 Let –; ffl ? 0 and f 2 DT d . If the vector ~ ff = (ff i ) 2 R m has been generated
after t­th iteration of the ``while loop'' of the k; –­perceptron learning rule, then
err(f) – –
4
` t 2
jjf ~ ff;k jj 2
H + –jj~ffjj 2 \Gamma jjf jj 2
H
'
– –
4
` tffl 2
R 2 + –
\Gamma jjf jj 2
H
'
: (13)
On the other hand, if ~
ff = (ff i ) 2 R m has been generated after t­th iteration of the ``while
loop'' of the k; –­MMP learning rule, then
err(f) – –
4
`
1
jjf ~ ff;k jj 2
H + –jj~ffjj 2 \Gamma jjf jj 2
H
'
(14)
– –
4
` 1
r 2 + –
+
tffl 2
R 2 + r 2 + 2–
\Gamma jjf jj 2
H
'
: (15)
Additionally, the estimate (14) holds for every ~ ff = (ff i ) 2 R m such that
P ff i = 1 and
each ff i – 0.
Note that
P
ff i equals t in (13), while it is 1 in (14). The following result is derived form
some recent results of Ben David and Simon [2] on efficient learning of perceptrons.
Theorem 6 Given ¯ ? 0 and integer d ? 0. There exists an algorithm A ¯ which runs in
time polynomial in both the input dimension n and the number of training samples m, that
given the labelled training sample (x i ; y i ), i = 1; :::; m, it outputs a polynomial h 2 Pol d
such that err(h) Ÿ err(f); for every in f 2 DT d [ DNF d .
Following [2] we give an explicit formulation of the algorithm A ¯ : for each subset of Ÿ
d4=¯ 4 e elements of the training set f(z i ; y i )g i=1;:::;m find the maximal margin hyperplane,
if one exists. Using the standard quadratic programming approach this can be done in time
polynomial in both N and m [3]. Next, define w h 2 R N as the vector of the hyperplane
with the lowest error rate on the whole training set. Finally, set h(\Delta) := w h \Delta \Phi(\Delta) 2 Pol d .
5 Experimental Results and Discussion
We have used a standard machine learning benchmark of noisy 7 bit LED display for 10
digits, 0 though 9, originally introduced in [4]. We generated 500 examples for training and
5000 for independent test, under assumption of 10% probability of a bit being reversed.
The task set was to discriminate between two classes, digits 0­4 and digits 5­9. Each
``noisy digit'' data vector (x 1 ; ::::; x 7 ) was complemented by an additional 7 bits vector
(1 \Gamma x 1 ; :::; 1 \Gamma x 7 ) to ensure that our Standing Assumption of Section 2 holds true.

For a sake of simplicity we used fixed complexity weights, K i = 1, i = 0; :::; d, and – = 4,
which for a decision tree f 2 DT d gives a simple formula for the risk
R+ [f; –] = R[f; –] = [number of leaves] + [number of errors]:
Four different algorithms have been applied to this data: (i) Decision Trees, version C4.5
[12] (available from www.cse.unsw.edu.au/¸quinlan/), (ii) regularized kernel perceptron
(Algorithm 1) with the generated coefficients scaled ~ ff ! ~ ff=(t \Lambda (jjf ~ ff;k jj H + –jj~ffjj 2 ),
where t is the number of updates to the convergence, (iii) greedy maximal margin classifier
(Algorithm 2) and (iv) mask perceptron [10] which for this data generates a polynomial
f 2 Pol d using some greedy search heuristics. Table 1 gives the experimental results.
Table 1: Results for recognition of two groups of digits on faulty LED­display.
Algorithm Risk (no. of leaves /SV/terms ) Error rate %: train/test
d = 3 d = 7 d = 3 d = 7
Decision tree 110 (4 leaves) 80 (17 leaves) 21.3 / 22.9 12.0 / 15.8
Kernel SVM 44.4(413 SV) 40.8 (382 SV) 12.2 / 15.1 11.2 / 14.8
Kernel percep. 53.1 (294 SV) 54.9 (286 SV) 11.8 / 16.3 13.8 / 17.1
Mask percep. 53.2(10 terms) 49.1 (26 terms) 12.8 / 15.7 11.8 / 15.6
The lower bound on risk from maximal margin criterion (Eq. 11) are 44.3 and 40.7 for
d = 3 and d = 7, respectively. Similarly, the lower bound on risk from kernel perceptron
criterion (Eq. 9) were 39.7 and 36.2, respectively. Risks for SVM solutions approach this
bound and for kernel perceptron they are reasonably close. Comparison with the risks ob­
tained for decision trees show that our lower bounds are meaningful (for the ``un­pruned''
decision trees risks were only slightly worse). The mask perceptron results show that sim­
ple (low number of terms) polynomial solutions with risks approaching our lower bounds
can be practically found.
The Bayes­optimal classifier can be evaluated on this data set, since we know explicitly the
distribution from which data is drawn. Its error rates are 11.2% and 13.8% on the training
and test sets, respectively. SVM solutions have error rates closest to the Bayesian classifier
(the test error rate for d = 7 exceeds the one of the Bayes­optimal classifier by only 7%).
Boosted Decision Trees An obvious question to ask is what happens if we take a large
enough linear combination of decision trees. This is the case, for instance, in boosting. We
can show that Pol d is spanned by DT d (X). In a nutshell, the proof relies on the partition
of the identity into
1 =
X
a+b=i
x a ¯
x b where ¯
x = (1 \Gamma x 1 ; 1 \Gamma x 2 ; : : : ; 1 \Gamma xn )
and solving this expansion for x i , where the remainder turns out to be a decision tree. This
means that in the limit, boosting decision trees finds a maximum margin solution in Pol d ,
a goal more directly achievable via a maximum margin perceptron on Pol d .
Conclusion We have shown that kernel methods with their analytical tools are applicable
well outside of their traditional domain, namely in the area of propositional logic, which
traditionally has been an area of discrete, combinatorial rather then continuous analytical
methods. The constructive lower bounds we proved offer a fresh approach to some seem­
ingly intractable problems. For instance, such bounds can be used as points of reference
for practical applications of inductive techniques like as decision trees.

The use of Boolean kernels introduced here allows a more insightful comparison of perfor­
mance of logic based and analytical, linear machine learning algorithms.
This contributes to the research in the theory of learning systems as illustrated by the result
on existence of polynomial time algorithm for estimation of minimal number of training
errors for decision trees and disjunctive normal forms.
A potentially more practical link, to boosted decision trees, and their convergence to the
maximum margin solutions has to be investigated further. The current paper sets founda­
tions for such research.
Boolean kernels can potentially stimulate more accurate (kernel) support vector machines
by providing more intuitive construction of kernels. This is the subject of ongoing research.
Acknowledgments A.K. acknowledges permission of the Chief Technology Officer, Tel­
stra to publish this paper. A.S. was supported by a grant of the DFG Sm 62/1­1. Parts of this
work were supported by the ARC and an R& D grant from Telstra. Thanks to P. Sember
and H. Ferra for help in preparation of this paper.
References
[1] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathe­
matical Society, 68:337 -- 404, 1950.
[2] S. Ben­David and H. U. Simon. Efficient learning of linear perceptron. In T.K. Leen,
T.G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing
Systems 13, pages 189--195, Cambridge, MA, 2001. MIT Press.
[3] D. P. Bertsekas. Nonlinear Programming. Athena Scientific, Belmont, MA, 1995.
[4] L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. Classification and Regres­
sion Trees. Wadsworth Int., Belmont, Ca., 1984.
[5] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273 -- 297,
1995.
[6] N. Cristianini and J. Shawe­Taylor. An Introduction to Support Vector Machines and
other kernel­based learning methods. Cambridge University Press, Cambridge, 2000.
[7] Y. Freund and R. E. Schapire. Large margin classification using the perceptron algo­
rithm. In J. Shavlik, editor, Machine Learning: Proceedings of the Fifteenth Interna­
tional Conference, San Francisco, CA, 1998. Morgan Kaufmann.
[8] F. Girosi, M. Jones, and T. Poggio. Regularization theory and neural networks archi­
tectures. Neural Computation, 7(2):219--269, 1995.
[9] A. Kowalczyk. Maximal margin perceptron. In A. Smola, P.Bartlett, B. Sch˜olkopf,
and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 61--100,
Cambridge, MA, 2000. MIT Press.
[10] A. Kowalczyk and H. Ferr‘a. Developing higher­order networks with empirically se­
lected units. IEEE Transactions on Neural Networks, 5:698--711, 1994.
[11] A. B. Novikoff. On convergence proofs on perceptrons. Symposium on the Mathe­
matical Theory of Automata, 12:615--622, 1962.
[12] J.R. Quinlan. Simplifying decision trees. Int. J. Man­Machine Studies, 27:221--234,
(1987).
[13] J. Shawe­Taylor and N. Christianini. Margin distribution and soft margin. In A. J.
Smola, P. L. Bartlett, B. Sch˜olkopf, and D. Schuurmans, editors, Advances in Large
Margin Classifiers, pages 349--358, Cambridge, MA, 2000. MIT Press.

