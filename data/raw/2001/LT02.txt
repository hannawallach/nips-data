Efficiency versus Convergence of Boolean
Kernels for On­Line Learning Algorithms
Roni Khardon
Tufts University
Medford, MA 02155
roni@eecs.tufts.edu
Dan Roth
University of Illinois
Urbana, IL 61801
danr@cs.uiuc.edu
Rocco Servedio
Harvard University
Cambridge, MA 02138
rocco@deas.harvard.edu
Abstract
We study online learning in Boolean domains using kernels which cap­
ture feature expansions equivalent to using conjunctions over basic fea­
tures. We demonstrate a tradeoff between the computational efficiency
with which these kernels can be computed and the generalization abil­
ity of the resulting classifier. We first describe several kernel functions
which capture either limited forms of conjunctions or all conjunctions.
We show that these kernels can be used to efficiently run the Percep­
tron algorithm over an exponential number of conjunctions; however we
also prove that using such kernels the Perceptron algorithm can make
an exponential number of mistakes even when learning simple func­
tions. We also consider an analogous use of kernel functions to run the
multiplicative­update Winnow algorithm over an expanded feature space
of exponentially many conjunctions. While known upper bounds imply
that Winnow can learn DNF formulae with a polynomial mistake bound
in this setting, we prove that it is computationally hard to simulate Win­
now's behavior for learning DNF over such a feature set, and thus that
such kernel functions for Winnow are not efficiently computable.
1 Introduction
The Perceptron and Winnow algorithms are well known learning algorithms that make pre­
dictions using a linear function in their feature space. Despite their limited expressiveness,
they have been applied successfully in recent years to several large scale real world classifi­
cation problems. The SNoW system [7, 2], for example, has successfully applied variations
of Perceptron [6] and Winnow [4] to problems in natural language processing. The system
first extracts Boolean features from examples (given as text) and then runs learning algo­
rithms over restricted conjunctions of these basic features.
There are several ways to enhance the set of features after the initial extraction. One idea is
to expand the set of basic features x 1 ; : : : ; x n using conjunctions such as x 1 x 3 x 4 and use
these expanded higher­dimensional examples, in which each conjunction plays the role of
a basic feature, for learning. This approach clearly leads to an increase in expressiveness
and thus may improve performance. However, it also dramatically increases the number of
features (from n to 3 n if all conjunctions are used) and thus may adversely affect both the
computation time and convergence rate of learning.

This paper studies the computational efficiency and convergence of the Perceptron and
Winnow algorithms over such expanded feature spaces of conjunctions. Specifically, we
study the use of kernel functions to expand the feature space and thus enhance the learn­
ing abilities of Perceptron and Winnow; we refer to these enhanced algorithms as kernel
Perceptron and kernel Winnow.
1.1 Background: Perceptron and Winnow
Throughout its execution Perceptron maintains a weight vector w 2 < N which is initially
(0; : : : ; 0): Upon receiving an example x 2 < N the algorithm predicts according to the
linear threshold function w  x  0: If the prediction is 1 and the label is 1 (false positive
prediction) then the vector w is set to w x, while if the prediction is 1 and the label is
1 (false negative) then w is set to w + x: No change is made if the prediction is correct.
The famous Perceptron Convergence Theorem [6] bounds the number of mistakes which
the Perceptron algorithm can make:
Theorem 1 Let hx 1 ; y 1 i; : : : ; hx t ; y t i be a sequence of labeled examples with x i 2 < N ;
kx i k  R and y i 2 f1; 1g for all i. Let u 2 < N ;  > 0 be such that y i u  x i   for all
i: Then Perceptron makes at most R 2 kuk 2
 2 mistakes on this example sequence.
The Winnow algorithm [4] has a very similar structure. Winnow maintains a hypothesis
vector w 2 < N which is initially w = (1; : : : ; 1): Winnow is parameterized by a promotion
factor   1 and a threshold  > 0; upon receiving an example x 2 f0; 1g N Winnow
predicts according to the threshold function w  x  : If the prediction is 1 and the label is
1 then for all i such that x i = 1 the value of w i is set to w i =; this is a demotion step. If
the prediction is 1 and the label is 1 then for all i such that x i = 1 the value of w i is set
to w i ; this is a promotion step. No change is made if the prediction is correct.
For our purposes the following mistake bound, implicit in [4], is of interest:
Theorem 2 Let the target function be a k­literal monotone disjunction f(x 1 ; : : : ; xN ) =
x i 1 _    _x i k : For any sequence of examples in f0; 1g N labeled according to f the number
of prediction mistakes made by Winnow(; ) is at most 
 1  N
 + k( + 1)(1 + log  ):
1.2 Our Results
Our first result in Section 2 shows that it is possible to efficiently run the kernel Perceptron
algorithm over an exponential number of conjunctive features:
Theorem 3 There is an algorithm that simulates Perceptron over the 3 n ­dimensional fea­
ture space of all conjunctions of n basic features. Given a sequence of t labeled examples
in f0; 1g n the prediction and update for each example take poly(n; t) time steps.
This result is closely related to one of the main open problems in learning theory: efficient
learnability of disjunctions of conjunctions, or DNF (Disjunctive Normal Form) expres­
sions. 1 Since linear threshold elements can represent disjunctions (e.g. x 1 _x 2 _x 3 is true
iff x 1 + x 2 + x 3  1), Theorems 1 and 3 imply that kernel Perceptron can be used to learn
DNF. However, in this framework the values of N and R in Theorem 1 can be exponen­
tially large, and hence the mistake bound given by Theorem 1 is exponential rather than
polynomial in n: The question thus arises whether, for kernel Perceptron, the exponential
1 Angluin [1] proved that DNF expressions cannot be learned efficiently using hypotheses which
are themselves DNF expressions from equivalence queries and thus also in the mistake bound model
which we are considering here. However this result does not preclude the efficient learnability of DNF
using a different class of hypotheses such as those generated by the kernel Perceptron algorithm.

upper bound implied by Theorem 1 is essentially tight. We give an affirmative answer, thus
showing that kernel Perceptron cannot efficiently learn DNF:
Theorem 4 There is a monotone DNF f over x 1 ; : : : ; x n and a sequence of examples la­
beled according to f which causes the kernel Perceptron algorithm to make 2

 n) mistakes.
Turning to Winnow, an attractive feature of Theorem 2 is that for suitable ;  the bound
is logarithmic in the total number of features N (e.g.  = 2 and  = N ). Therefore, as
noted by several researchers [5], if a Winnow analogue of Theorem 3 could be obtained
this would imply efficient learnability of DNF. We show that no such analogue can exist:
Theorem 5 There is no polynomial time algorithm which simulates Winnow over expo­
nentially many monotone conjunctive features for learning monotone DNF, unless every
problem in #P can be solved in polynomial time.
We observe that, in contrast to Theorem 5, Maass and Warmuth have shown that the Win­
now algorithm can be simulated efficiently over exponentially many conjunctive features
for learning some simple geometric concept classes [5].
While several of our results are negative, in practice one can achieve good performance
by using kernel Perceptron (if n is small) or the limited­conjunction kernel described in
Section 2 (if n is large). This is similar to common practice with polynomial kernels 2 where
typically a small degree is used to aid convergence. These observations are supported by
our preliminary experiments in an NLP domain which are not reported here.
2 Theorem 3: Kernel Perceptron with Exponentially Many Features
It is easily observed, and well known, that the hypothesis w of the Perceptron algorithm
is a  sum of the previous examples on which prediction mistakes were made. If we let
L(x) 2 f1; 1g denote the label of example x, then w =
P
v2M L(v)v where M is the
set of examples on which the algorithm made a mistake. Thus the prediction of Perceptron
on x is 1 iff w  x = (
P
v2M L(v)v)  x =
P
v2M L(v)(v  x)  0.
For an example x 2 f0; 1g n let (x) denote its transformation into an enhanced feature
space such as the space of all conjunctions. To run the Perceptron algorithm over the
enhanced space we must predict 1 iff w   (x)  0 where w  is the weight vector in the
enhanced space; from the above discussion this holds iff
P
v2M L(v)((v)  (x))  0.
Denoting K(v; x) = (v)  (x) this holds iff
P
v2M L(v)K(v; x)  0.
Thus we never need to construct the enhanced feature space explicitly; we need only be
able to compute the kernel function K(v; x) efficiently. This is the idea behind all so­called
kernel methods, which can be applied to any algorithm (such as support vector machines)
whose prediction is a function of inner products of examples; see e.g. [3] for a discussion.
The result in Theorem 3 is simply obtained by presenting a kernel function capturing all
conjunctions. We also describe kernels for all monotone conjunctions which allow no
negative literals, and kernels capturing all (monotone) conjunctions of up to k literals.
The general case: When () includes all 3 n conjunctions (with positive and negative
literals) K(x; y) must compute the number of conjunctions which are true in both x and
y. Clearly, any literal in such a conjunction must satisfy both x and y and thus the cor­
responding bit in x; y must have the same value. Counting all such conjunctions gives
K(x; y) = 2 same(x;y) where same(x; y) is the number of original features that have the
same value in x and y. This kernel has been obtained independently by [8].
2 Our Boolean kernels are different than standard polynomial kernels in that all the conjunctions
are weighted equally. While expressive power does not change, convergence and behavior, do.

Monotone Monomials: In some applications the total number n of basic features may
be very large but in any one example only a small number of features take value 1. In
other applications the number of features n may not be known in advance (e.g. due to
unseen words in text domains). In these cases it may be useful to consider only monotone
monomials. To express all monotone monomials we take K(x; y) = 2 samepos(x;y) where
samepos(x; y) is the number of active features common to both x and y.
A parameterized kernel: In general, one may want to trade off expressivity against
number of examples and convergence time. Thus we consider a parameterized kernel
which captures all conjunctions of size at most k for some k < n: The number of
such conjunctions that satisfy both x and y is K(x; y) =
P k
l=0
same(x;y)
l

. This ker­
nel is reported also in [10]. For monotone conjunctions of size at most k we have
K(x; y) =
P k
l=0
samepos(x;y)
l

.
3 Theorem 4: Kernel Perceptron with Exponentially Many Mistakes
We describe a monotone DNF target function and a sequence of labeled examples which
cause the monotone kernel Perceptron algorithm to make exponentially many mistakes.
For x; y 2 f0; 1g n we write jxj to denote the number of 1's in x and jx \ yj to denote
samepos(x; y): We use the following lemma (constants have not been optimized):
Lemma 6 There is a set S of n­bit strings S = fx 1 ; : : : ; x t g  f0; 1g n with t = e n=9600
such that jx i j = n=20 for 1  i  t and jx i \ x j j  n=80 for 1  i < j  t:
Proof: The proof uses the probabilistic method. For each i = 1; : : : ; t let x i 2 f0; 1g n be
chosen by independently setting each bit to 1 with probability 1/10. For any i it is clear
that E[jx i j] = n=10; a Chernoff bound implies that Pr[jx i j < n=20]  e n=80 ; and thus
the probability that any x i satisfies jx i j < n=20 is at most te n=80 : Similarly, for any i 6= j
we have E[jx i \ x j j] = n=100; a Chernoff bound implies that Pr[jx i \ x j j > n=80] 
e n=4800 ; and thus the probability that any x i ; x j with i 6= j satisfies jx i \x j j > n=80 is at
most t
2

e n=4800 : For t = e n=9600 the value of t
2

e n=4800 + te n=80 is less than 1. Thus
for some choice of x 1 ; : : : ; x t we have each jx i j  n=20 and jx i \ x j j  n=80: For any x i
which has jx i j > n=20 we can set jx i j n=20 of the 1s to 0s, and the lemma is proved.
The target DNF is very simple: it is the single conjunction x 1 x 2 : : : x n : While the original
Perceptron algorithm over the n features x 1 ; : : : ; x n makes at most poly(n) mistakes for
this target function, we now show that the monotone kernel Perceptron algorithm which
runs over all 2 n monotone monomials can make 2 + e n=9600 mistakes.
Recall that at the beginning of the Perceptron algorithm's execution all 2 n coordinates of
w  are 0. The first example is the negative example 0 n ; since w   (x) = 0 Perceptron
incorrectly predicts 1 on this example. The resulting update causes the coefficient w 
;
corresponding to the empty monomial (satisfied by any example x) to become 1 but all
2 n 1 other coordinates of w  remain 0. The next example is the positive example 1 n :
For this example we have w   (x) = 1 so Perceptron incorrectly predicts 1: Since
all 2 n monotone conjunctions are satisfied by this example the resulting update causes
w 
;
to become 0 and all 2 n 1 other coordinates of w  to become 1. The next e n=9600
examples are the vectors x 1 ; : : : ; x t described in Lemma 6. Since each such example has
jx i j = n=20 each example is negative; however as we now show the Perceptron algorithm
will predict 1 on each of these examples.
Fix any value 1  i  e n=9600 and consider the hypothesis vector w  just before example
x i is received. Since jx i j = n=20 the value of w   (x i ) is a sum of the 2 n=20 different

coordinates w 
T which correspond to the monomials satisfied by x i : More precisely we
have w   (x i ) =
P
T2A i
w 
T +
P
T2B i
w 
T where A i contains the monomials which are
satisfied by x i and x j for some j 6= i and B i contains the monomials which are satisfied
by x i but no x j with j 6= i: We lower bound the two sums separately.
Let T be any monomial in A i : By Lemma 6 any T 2 A i contains at most n=80 variables
and thus there can be at most
P n=80
r=0
n=20
r

monomials in A i : Using the well known bound
P `
j=0
`
j

 2 H()` where   1=2 and H() is the binary entropy function there can be
at most 2 0:041n terms in A i : Moreover the value of each w 
T must be at least e n=9600 since
w 
T decreases by at most 1 for each example, and hence
P
T2A i
w 
T  e n=9600 2 0:041n >
2 0:042n : On the other hand, for any T 2 B i we clearly have w 
T = 1: By Lemma 6 for
any r > n=80 every r­variable monomial satisfied by x i must belong to B i ; and hence
P
T2B i
w 
T 
P n=20
r=n=80+1
n=20
r

> 2 0:049n : Combining these inequalities we have w 
x i  2 0:042n + 2 0:049n > 0 and hence the Perceptron prediction on x i is 1.
4 Theorem 5: Learning DNF with Kernel Winnow is Hard
In this section, for x 2 f0; 1g n (x) denotes the (2 n 1)­element vector whose coordinates
are all nonempty monomials (monotone conjunctions) over x 1 ; : : : ; x n : A sequence of la­
beled examples hx 1 ; b 1 i; : : : ; hx t ; b t i is monotone consistent if it is consistent with some
monotone function, i.e. x i
k  x j
k for all k = 1; : : : ; n implies b i  b j : If S is monotone
consistent and has t labeled examples then clearly there is a monotone DNF formula con­
sistent with S which contains at most t conjunctions. We consider the following problem:
KERNEL WINNOW PREDICTION(; ) (KWP)
Instance: Monotone consistent sequence S = hx 1 ; b 1 i; : : : ; hx t ; b t i of labeled examples
with each x i 2 f0; 1g m and each b i 2 f1; 1g; unlabeled example z 2 f0; 1g m :
Question: Is w   (z)  ; where w  is the N = (2 m 1)­dimensional hypothesis vector
generated by running Winnow(; ) on the example sequence h(x 1 ); b 1 i; : : : h(x t ); b t i?
In order to run Winnow over all 2 m 1 nonempty monomials to learn monotone DNF, one
must be able to solve KWP efficiently. The main result of this section is proved by showing
that KWP is computationally hard for any parameter settings which yield a polynomial
mistake bound for Winnow via Theorem 2.
Theorem 7 Let N = 2 m 1 and  > 1;   1 be such that max( 
 1  N
 ; ( + 1)(1 +
log  )) = poly(m): Then KWP(; ) is #P­hard.
Proof of Theorem 7: For N; and  as described above it can easily be verified that
1 + 1
poly(m) <  < poly(m) and 2 m
poly(m) <  < 2 poly(m) : The proof of the theorem is a
reduction from the following #P­hard problem [9]: (See [9] also for details on #P.)
MONOTONE 2­SAT (M2SAT)
Instance: Monotone 2­CNF Boolean formula F = c 1 ^ c 2 ^ : : : ^ c r with c i = (y i 1 _ y i 2 )
and each y i j 2 fy 1 ; : : : ; y n g; integer K such that 1  K  2 n :
Question: Is jF 1 (1)j  K; i.e. does F have at least K satisfying assignments in f0; 1g n ?
4.1 High­Level Idea of the Proof
The high level idea of the proof is simple: let (F; K) be an instance of M2SAT where
F is defined over variables y 1 ; : : : ; y n : The Winnow algorithm maintains a weight w 
T
for each monomial T over variables x 1 ; : : : ; x n : We define a 1­1 correspondence between
these monomials T and truth assignments y T 2 f0; 1g n for F; and we give a sequence of
examples for Winnow which causes w 
T  0 if F (y T ) = 0 and w 
T = 1 if F (y T ) = 1:

The value of w   (z) is thus related to jF 1 (1)j; some additional work ensures that
w   (z)   if and only if jF 1 (1)j  K:
In more detail, let U = n+ 1 + d(dlog  4e + 1) log e; V = d n+1
log  e + 1; W = d U+2
log  e + 1
and m = n + U + 6V n 2 + 6UW + 3: We describe a polynomial time transformation
which maps an n­variable instance (F; K) of M2SAT to an m­variable instance (S; z) of
KWP(; ) where S = hx 1 ; b 1 i; : : : ; hx t ; b t i is monotone consistent, each x i and z belong
to f0; 1g m and w   (z)   if and only if jF 1 (1)j  K:
The Winnow variables x 1 ; : : : ; xm are divided into three sets A; B and C where A =
fx 1 ; : : : ; x n g; B = fx n+1 ; : : : ; x n+U g and C = fx n+U+1 ; : : : ; xm g: The unlabeled ex­
ample z is 1 n+U 0 m n U ; i.e. all variables in A and B are set to 1 and all variables in C
are set to 0. We thus have w   (z) = MA + MB + MAB where MA =
P
;6=TA w 
T ;
MB =
P
;6=TB w 
T and MAB =
P
TA[B;T\A6=;;T\B 6=; w 
T : We refer to monomials
; 6= T  A as type­A monomials, monomials ; 6= T  B as type­B monomials, and
monomials T  A [ B; T \ A 6= ;; T \ B 6= ; as type­AB monomials.
The example sequence S is divided into four stages. Stage 1 results in MA  jF 1 (1)j; as
described below the n variables in A correspond to the n variables in the CNF formula F:
Stage 2 results in MA   q jF 1 (1)j for some positive integer q: Stages 3 and 4 together
result in MB + MAB    q K: Thus the final value of w   (z) is approximately
 +  q (jF 1 (1)j K); so we have w   (z)   if and only if jF 1 (1)j  K:
Since all variables in C are 0 in z; if T includes a variable in C then the value of w 
T
does not affect w   (z): The variables in C are ``slack variables'' which (i) make Winnow
perform the correct promotions/demotions and (ii) ensure that S is monotone consistent.
4.2 Details of the Proof
Stage 1: Setting MA  jF 1 (1)j. We define the following correspondence between
truth assignments y T 2 f0; 1g n and monomials T  A : y T
i = 0 if and only if x i is not
present in T : For each clause y i 1 _ y i 2 in F; Stage 1 contains V negative examples such
that x i 1 = x i 2 = 0 and x i = 1 for all other x i 2 A: Assuming that (1) Winnow makes a
false positive prediction on each of these examples and (2) in Stage 1 Winnow never does a
promotion on any example which has any variable in A set to 1, then after Stage 1 we will
have that w 
T = 1 if F (y T ) = 1 and 0 < w 
T   V if F (y T ) = 0: Thus we will have
MA = jF 1 (1)j +  1 for some 0 <  1 < 2 n  V < 1
2 :
We now show how the Stage 1 examples cause Winnow to make a false positive predic­
tion on negative examples which have x i 1 = x i 2 = 0 and x i = 1 for all other i in A
as described above. For each such negative example in Stage 1 six new slack variables
x +1 ; : : : ; x +6 2 C are used as follows: Stage 1 has dlog  (=3)e repeated instances of
the positive example which has x +1 = x +2 = 1 and all other bits 0. These examples
cause promotions which result in   w 
x+1 + w 
x+2 + w 
x+1x+2 <  and hence
w 
x+1  =3: Two other groups of similar examples (the first with x +3 = x +4 = 1;
the second with x +5 = x +6 = 1) cause w 
x+3  =3 and w 
x+5  =3: The
next example in S is the negative example which has x i 1 = x i 2 = 0; x i = 1 for
all other x i in A; x +1 = x +3 = x +5 = 1 and all other bits 0. For this example
w   (x) > w 
x+1 + w 
x+3 + w 
x+5   so Winnow makes a false positive prediction.
Since F has at most n 2 clauses and there are V negative examples per clause, this con­
struction can be carried out using 6V n 2 slack variables x n+U+1 ; : : : ; x n+U+6V n 2 :
Stage 2: Setting MA   q jF 1 (1)j. The first Stage 2 example is a positive example
with x i = 1 for all x i 2 A, x n+U+6V n 2 +1 = 1 and all other bits 0. Since each of the 2 n

monomials which contain x n+U+6V n 2 +1 and are satisfied by this example have w 
T = 1;
we have w   (x) = 2 n + jF 1 (1)j +  1 < 2 n+1 : Since  > 2 m =poly(m) > 2 n+1 after
the resulting promotion we have w   (x) = (2 n + jF 1 (1)j +  1 ) < 2 n+1 :
Let q = dlog  (=2 n+1 )e 1; so  q 2 n+1 <    q+1 2 n+1 : Stage 2 consists of q repeated
instances of the positive example described above. After these promotions we have w  
(x) =  q (2 n + jF 1 (1)j +  1 ) <  q 2 n+1 < : Since 1 < jF 1 (1)j +  1 < 2 n we also
have  q < MA =  q (jF 1 (1)j +  1 ) <  q 2 n < =2:
Stage 3: Setting MB  p. At the start of Stage 3 each type­B and type­AB monomial
T has w 
T = 1: There are n variables in A and U variables in B so at the start of Stage 2
we have MB = 2 U 1 and MAB = (2 n 1)(2 U 1): Since no example in Stages 3 or 4
satisfies any x i in A; at the end of Stage 4 MA will still be  q (jF 1 (1)j +  1 ) and MAB
will still be (2 n 1)(2 U 1): Thus ideally at the end of Stage 4 the value of MB would be
 (2 n 1)(2 U 1)  q (K+ 1 ); since this would imply that w  (z) = + q (jF 1 (1)j
K) which is at least  if and only if jF 1 (1)j  K: However it is not necessary for MB
to assume this exact value; since jF 1 (1)j must be an integer and 0 <  1 < 1
2 ; as long as
 (2 n 1)(2 U 1)  q K  MB <  (2 n 1)(2 U 1)  q (K 1
2 ) we will have
that MA +MB +MAB   if and only if jF 1 (1)j  K:
For ease of notation let D denote  (2 n 1)(2 U 1)  q K: We now describe the examples
in Stages 3 and 4 and show that they will cause MB to satisfy D  MB < D + 1
2  q :
Let c = dlog  4e; so  q c  1
4  q and hence there is a unique smallest integer p such that
D  p q c < D+ 1
4  q : The Stage 3 examples will result in p < MB < p + 1
4 : Using the
definition of D and the fact that 1  K < 2 n it can be verified that  q c < D  p q c <
D + 1
4  q   3
4  q   q+1 2 n+1 3 q c =  q c  ( c+1 2 n+1 3): Hence we have
1 < p   c+1 2 n+1 3  2 n+1+d(c+1) log e 3 = 2 U 3: We use the following lemma:
Lemma 8 For all `  1; for all 1  p  2 ` 1; there is a monotone CNF F `;p over
` Boolean variables which has at most ` clauses, has exactly p satisfying assignments in
f0; 1g ` ; and can be constructed from ` and p in poly(`) time.
Proof: The proof is by induction on `. For the base case ` = 1 we have p = 1 and
F `;p = x 1 : Assuming the lemma is true for ` = 1; : : : ; k we now prove it for ` = k + 1 :
If 1  p  2 k 1 then the desired CNF is F k+1;p = x k+1 ^F k;p : Since F k;p has at most k
clauses F k+1;p has at most k + 1 clauses. If 2 k + 1  p  2 k+1 1 then the desired CNF
is F k+1;p = x k+1 _ F k;p 2 k : By distributing x k over each clause of F k;p 2 k we can write
F k+1;p as a CNF with at most k clauses. If p = 2 k then F k;p = x 1 :
Let FU;p be an r­clause monotone CNF formula over the U variables in B which has p
satisfying assignments. Similar to Stage 1, for each clause of FU;p , Stage 3 has W negative
examples corresponding to that clause, and as in Stage 1 slack variables in C are used to
ensure that Winnow makes a false positive prediction on each such negative example. Thus
the examples in Stage 3 cause MB = p +  2 where 0 <  2 < 2 U  W < 1
4 : Since six
slack variables in C are used for each negative example and there are rW  UW negative
examples, the slack variables x n+U+6V n 2 +2 ; : : : ; xm 2 are sufficient for Stage 3.
Stage 4: Setting MB + MAB    q K. All that remains is to perform q c
promotions on examples which have each x i in B set to 1. This will cause D  p q c <
(p +  2 ) q c = MB < D + 1
4  q +  2  q c < D + 1
2  q which is as desired.
It can be verified from the definitions of U and c that U n
log   c: The first q d U n
log  e
examples in S are all the same positive example which has each x i in B set to 1 and
xm 1 = 1: The first time this example is received w   (x) = 2 U +p+ 2 < 2 U+1 : It can

be verified that 2 U+1 < ; so Winnow performs a promotion; after q d U n
log  e occurrences
of this example w   (x) =  qd U n
log  e (2 U + p +  2 ) <  qd U n
log  e 2 U+1   q 2 n+1 < 
and MB =  qd U n
log  e (p +  2 ):
The remaining examples in Stage 4 are d U n
log  e c repetitions of the positive example x
which has each x i in B set to 1 and xm = 1: If promotions occurred on each repetition of
this example then we would have w   (x) =  d U n
log  ec (2 U +  qd U n
log  e (p +  2 )); so
we need only show that this is less than : We reexpress this quantity as  d U n
log  ec 2 U +
 q c (p+ 2 ): We have  q c (p+ 2 ) < p q c + 1
4  q c   3
4  q + 1
16  q <  1
2  q : Some
easy manipulations show that  d U n
log  ec 2 U  
2 2 2U n < 1
2  q ; so indeed w   (x) < :
Finally, we observe that by construction the example sequence S is monotone consistent.
Since m = poly(n) and S contains poly(n) examples the transformation from M2SAT to
KWP(; ) is polynomial­time computable and the theorem is proved. (Theorem 7)
5 Conclusion
It is necessary to expand the feature space if linear learning algorithms are to learn ex­
pressive functions. This work explores the tradeoff between computational efficiency and
convergence (i.e. generalization ability) when using expanded feature spaces. We have
shown that additive and multiplicative update algorithms differ significantly in this respect;
we believe that this fact could have significant practical implications. Future directions
include the utilization of the kernels developed here and studying convergence issues of
Boolean­kernel Perceptron and Support Vector Machines in the PAC model.
Acknowledgements: R. Khardon was supported by NSF grant IIS­0099446. D. Roth
was supported by NSF grants ITR­IIS­00­85836 and IIS­9984168 and by EPSRC grant
GR/N03167 while visiting University of Edinburgh. R. Servedio was supported by NSF
grant CCR­98­77049 and by a NSF Mathematical Sciences Postdoctoral Fellowship.
References
[1] D. Angluin. Negative results for equivalence queries. Machine Learning, 2:121--150, 1990.
[2] A. Carlson, C. Cumby, J. Rosen, and D. Roth. The SNoW learning architecture. Technical
Report UIUCDCS­R­99­2101, UIUC Computer Science Department, May 1999.
[3] N. Cristianini and J. Shaw­Taylor. An Introduction to Support Vector Machines. Cambridge
Press, 2000.
[4] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear­threshold
algorithm. Machine Learning, 2:285--318, 1988.
[5] W. Maass and M. K. Warmuth. Efficient learning with virtual threshold gates. Information and
Computation, 141(1):378--386, 1998.
[6] A. Novikoff. On convergence proofs for perceptrons. In Proceeding of the Symposium on the
Mathematical Theory of Automata, volume 12, pages 615--622, 1963.
[7] D. Roth. Learning to resolve natural language ambiguities: A unified approach. In Proc. of the
American Association of Artificial Intelligence, pages 806--813, 1998.
[8] K. Sadohara. Learning of boolean functions using support vector machines. In Proc. of the
Conference on Algorithmic Learning Theory, pages 106--118. Springer, 2001. LNAI 2225.
[9] L. G. Valiant. The complexity of enumeration and reliability problems. SIAM Journal of Com­
puting, 8:410--421, 1979.
[10] C. Watkins. Kernels from matching operations. Technical Report CSD­TR­98­07, Computer
Science Department, Royal Holloway, University of London, 1999.

