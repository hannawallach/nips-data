Generalization Performance of Some Learning
Problems in Hilbert Functional Spaces
Tong Zhang
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
tzhang@watson.ibm.com
Abstract
We investigate the generalization performance of some learning prob­
lems in Hilbert functional Spaces. We introduce a notion of convergence
of the estimated functional predictor to the best underlying predictor, and
obtain an estimate on the rate of the convergence. This estimate allows
us to derive generalization bounds on some learning formulations.
1 Introduction
In machine learning, our goal is often to predict an unobserved output value y based on an
observed input vector x. This requires us to estimate a functional relationship y  p(x)
from a set of example pairs of (x; y). Usually the quality of the predictor p(x) can be
measured by a loss function L(p(x); y) that is problem dependent. In machine learning,
we assume that the data (x; y) are drawn from an underlying distribution D which is not
known. Our goal is to find p(x) so that the expected true loss of p given below is as small
as possible:
R(p()) = E (x;y)D L(p(x); y):
In order to estimate a good predictor p(x) from a set of training data (x; y) randomly drawn
from D, it is necessary to start with a model of the functional relationship. In this paper,
we consider models that are subsets in some Hilbert functional spaces H . Denote by k  kH
the norm in H , we consider models in the
set
 = fp(x) 2 H : kp(x)k H  Ag, where
A is a parameter that can be used to control the size of the underlying model family. We
would like to find the best model
in
 which is given by:
^
p() = arg min
p2

E (x;y) L(p(x); y): (1)
By introducing a non­negative Lagrangian multiplier   0, we may rewrite the above
problem as:
^
p() = arg min
p2H
[E (x;y) L(p(x); y) +

2
kp()k 2
H ]: (2)
We shall only consider this equivalent formulation in this paper. In addition, for technical
reasons, we also assume that L(a; b) is a convex function of a.

Given n training examples (x 1 ; y 1 ); : : : ; (x n ; yn ), we consider the following estimation
method to approximate the optimal predictor ^
p(x)
2
 :
^
pn () = arg min
p2H
[
1
n
n
X
i=1
L(p(x i ); y i ) +

2
kp()k 2
H ]: (3)
The goal of this paper is to show that as n ! 1, ^
pn ! ^
p in probability under appropri­
ate regularity conditions. Furthermore, we obtain an estimate on the rate of convergence.
Consequences of this result in some specific learning formulations are examined.
2 Convergence of the estimated predictor
Assume that input x belongs to a set X . We make the reasonable assumption that p is point­
wise continuous under the k  kH topology: 8x 2 X , lim p!p0 p(x) = p 0 (x) where p ! p 0
is in the sense that kp p 0 kH ! 0. This assumption is equivalent to sup kpkH1 p(x) <
+1 8x 2 X . The condition implies that each data point x can be regarded as a bounded
linear functional q x on H such that 8p 2 H : q x (p) = p(x). Since a Hilbert space H is
self­dual, we can represent q x by an element in H . For notational simplicity, 8x we shall
let q x 2 H defined as q x  p = p(x) for all p 2 H , where  denotes the inner product of H .
It is clear that q x can be regarded as a representing feature vector of x in H . This represen­
tation can be computed as follows. Let ~ p = arg max kpkH =1 p(x), then it is not difficult to
see that ~
p = q x =kq x kH and ~
p(x) = kq x kH . It follows that q x () = ~
p(x)~p(). Note that this
method of computing q x is not important for the purpose of this paper.
Since p(x) = p  q x can now be considered as a linear functional using the feature space
representation q x of x, we can use the idea from [6] to analyze the convergence behavior
of ^
pn in H . Following [6], using the linear representation of p(x), we differentiate (2) at
the optimal solution ^
p(), which leads to the following first order condition:
E x;y L 0
1 (^p  q x ; y)q x + ^p = 0; (4)
where L 0
1 (a; b) is the derivative of L(a; b) with respect to a if L is smooth; it denotes a
subgradient (see [4]) otherwise. Since we have assumed that L(a; b) is a convex function
of a, we know that L(a 1 ; b) + (a 2 a 1 )L 0
1 (a 1 ; b)  L(a 2 ; b). This implies the following
inequality:
1
n
n
X
i=1
L(^p(x i ); y i ) + 1
n
n
X
i=1
L 0
1 (^p(x i ); y i )(^p n (x i ) ^
p(x i ))  1
n
n
X
i=1
L(^p n (x i ); y i );
which is equivalent to:
1
n
n
X
i=1
L(^p(x i ); y i ) +

2
^
p 2 +
[
1
n
n
X
i=1
L 0
1 (^p(x i ); y i )(^p n (x i ) ^
p(x i )) + ^p  (^p n ^
p)] +

2
(^p ^
pn ) 2
 1
n
n
X
i=1
L(^p n (x i ); y i ) +

2
^
p 2
n :
Note that we have used p 2 to denote p  p = kpk 2
H . Also note that by the definition of ^ pn ,
we have
1
n
n
X
i=1
L(^p(x i ); y i ) +

2
^
p 2  1
n
n
X
i=1
L(^p n (x i ); y i ) +

2
^
p 2
n :

Therefore by comparing the above two inequalities, we obtain:

2
(^p ^ pn ) 2  [
1
n
n
X
i=1
L 0
1 (^p(x i ); y i )q x i  (^p n ^ p) + ^p  (^p n ^
p)]
k 1
n
n
X
i=1
L 0
1 (^p(x i ); y i )q x i + ^pk H k^p ^ pn kH :
This implies that
k^p ^
pn kH  2
 k 1
n
n
X
i=1
L 0
1 (^p(x i ); y i )q x i + ^pk H
=
2
 k 1
n
n
X
i=1
L 0
1 (^p(x i ); y i )q x i E x;y L 0
1 (^p(x); y)q x kH : (5)
Note that the last equality follows from the first order condition (4). This is the only place
the condition is used. In (5), we have already bounded the convergence of ^
pn to ^ p in terms
of the convergence of the empirical expectation of a random vector L 0
1 (^p(x); y)q x to its
mean. The latter is often easier to estimate. For example, if its variance can be bounded,
then we may use the Chebyshev inequality to obtain a probability bound. In this paper,
we are interested in obtaining an exponential probability bound. In order to do so, similar
to the analysis in [6], we use the following form of concentration inequality which can be
found in [5], page 95:
Theorem 2.1 ([5]) Let  i be zero­mean independent random vectors in a Hilbert space H .
If there exists M > 0 such that for all natural numbers l  2:
P n
i=1 Ek i k l
H  nb
2 l!M l .
Then for all Æ > 0: P (k 1
n
P
i  i kH  Æ)  2 exp( n
2 Æ 2 =(bM 2 + ÆM)).
We may now use the following form of Jensen's inequality to bound the moments of the
zero­mean random vector L 0
1 (^p(x i ); y i )q x i E x;y L 0
1 (^p(x); y)q x :
Ej Ej l  2 l 1 (Ejj l + jEj l )  2 l 1 (Ejj l +Ejj l ) = 2 l Ejj l :
From inequality (5) and Theorem 2.1, we immediately obtain the following bound:
Theorem 2.2 If there exists M > 0 such that for all natural numbers l  2:
E x;y jL 0
1 (^p(x); y)kq x kH j l  b
2 l!M l . Then for all Æ > 0:
P (k^p n ^
pkH  Æ)  2 exp(
n
8
 2 Æ 2 =(4bM 2 + ÆM)):
Although Theorem 2.2 is quite general, the quantity b and M on the right hand side of
the bound depend on the optimal predictor ^
p which requires to be estimated. In order to
obtain a bound that does not require any knowledge of the true distribution D, we may
impose the following assumptions: both L 0
1 (a; b) and kq x kH are bounded. Observe that
kq x kH = sup p2H:kpkH=1 p(x), we obtain the following result:
Corollary 2.1 Assume that 8x 2 X , sup p2H:kpkH=1 p(x)  . Also assume that the loss
function L(a; b) satisfies the condition that jL 0
1 (a; b)j  M , then 8Æ > 0:
P (k^p n ^ pkH  Æ)  2 exp(
n
8
 2 Æ 2 =(4 2 M 2 +MÆ)):

3 Generalization performance
We study some consequences of Corollary 2.1, which bounds the convergence rate of the
estimated predictor to the best predictor.
3.1 Regression
We consider the following type of Huber's robust loss function:
L(p; y) =
( 1
2 (p y) 2 if jp yj  M
M jp yj M 2
2 if jp yj  M :
(6)
It is clear that L is continuous differentiable and jL 0
1 (p; y)j  M for all p and y. It is also
not hard to check that 8p 1 ; p 2 and y:
L(p 2 ; y) L(p 1 ; y) (p 2 p 1 )L 0
1 (p 1 ; y)  1
2
(p 2 p 1 ) 2 :
Using this inequality and (4), we obtain:
[E x;y L(^p n (x); y) +

2
k^p n k 2
H ] [E x;y L(^p(x); y) +

2
k^pk 2
H ]
=E x;y (L(^p n (x); y) L(^p(x); y) L 0
1 (^p(x); y)(^p n (x) ^
p(x))) +

2
k^p n ^
pk 2
H
E x;y
1
2 (^p n (x) ^
p(x)) 2 +

2
k^p n ^
pk 2
H :
If we assume that k^p n ^ pkH  Æ and sup p2H:kpkH =1 p(x)  , then
E x;y L(^p n (x); y)  E x;y L(^p(x); y) +

2
(k^pk 2
H k^p n k 2
H ) +
Æ 2
2
( 2 + ): (7)
This gives the following inequality:
E x;y L(^p n (x); y) E x;y L(^p(x); y)  Æk^p n kH + Æ 2 (
 2
2
+ ):
It is clear that the right­hand side of the above inequality does not depend on the unobserved
function ^
p. Using Corollary 2.1, we obtain the following bound:
Theorem 3.1 Using loss function (6) in (3). Assume that sup p2H:kpkH =1 p(x)  , then
80 < Æ  M=, with probability of at least 1 2 exp( n 2 Æ 2 =40 2 M 2 ), we have
E x;y L(^p n (x); y)  E x;y L(^p(x); y) + Æk^p n kH + Æ 2 (
 2
2
+ ):
Theorem 3.1 compares the performance of the computed function with that of the optimal
predictor ^
p
2
  H in (1). This style of analysis has been extensively used in the
literature. For example, see [3] and references therein. In order to compare with their
results, we can rewrite Theorem 3.1 in another form as: with probability of at least 1 ,
E x;y L(^p n (x); y)  E x;y L(^p(x); y) +O(
r
1 ln 
n ):
In [3], the authors employed a covering number analysis which led to a bound of the form
(for squared loss)
E x;y L(^p n (x); y)  E x;y L(^p(x); y) +O(
ln n ln 
n )

for finite dimensional problems. Note that the constant in their O() depends on the pseudo­
dimension, which can be infinity for problems considered in this paper. It is possible to em­
ploy their analysis using some covering number bounds for general Hilbert spaces. How­
ever, such an analysis would have led to a result of the following form for our problems:
E x;y L(^p n (x); y)  E x;y L(^p(x); y) +O((
ln n ln 
n ) 1=3 ):
It is also interesting to compare Theorem 3.1 with the leave­one­out analysis in [7]. The
generalization error averaged over all training examples for squared loss can be bounded
as
E x;y L(^p n (x); y)  (1 +O(
1
n ))(E x;y L(^p(x); y) + k^pk 2
H ):
This result is not directly comparable with Theorem 3.1 since the right hand side includes
an extra term of k^pk 2
H . Using the analysis in this paper, we may obtain a similar result
from (7) which leads to an average bound of the form:
E x;y L(^p n (x); y)  (E x;y L(^p(x); y) + k^pk 2
H ) +O(
1
 2 n ):
It is clear that the term O( 1
 2 n ) resulted in our paper is not as good as O( 1
n ) from [7].
However analysis in this paper leads to probability bounds while the leave­one­out analysis
in [7] only gives average bounds. It is also worth mentioning that it is possible to refine
the analysis presented in this section to obtain a probability bound which when averaged,
gives a bound with the correct term of O( 1
n ), rather than O( 1
n ) in the current analysis.
However due to the space limitation, we shall skip this more elaborated derivation.
In addition to the above style bounds, it is also interesting to compare the generalization
performance of the computed function to the empirical error of the computed function.
Such results have occurred, for example, in [1]. In order to obtain a comparable result, we
may use a derivation similar to that of (7), together with the first order condition of (3) as
follows:
1
n
n
X
i=1
L 0
1 (^p n  q x i ; y i )q x i + ^p n = 0:
This leads to a bound of the form:
1
n
n
X
i=1
L(^p(x i ); y i )  1
n
n
X
i=1
L(^p n (x i ); y i ) +

2
(k^p n k 2
H k^pk 2
H ) +
Æ 2
2
( 2 + ):
Combining the above inequality and (7), we obtain the following theorem:
Theorem 3.2 Using loss function (6) in (3). Assume that sup p2H:kpkH =1 p(x)  , then
80 < Æ  M=, with probability of at least 1 2 exp( n 2 Æ 2 =40 2 M 2 ), we have
E x;y L(^p n (x); y) 1
n
n
X
i=1
L(^p n (x i ); y i )
Æ 2 ( 2 + ) + [E x;y L(^p(x); y) 1
n
n
X
i=1
L(^p(x i ); y i )]:
Unlike Theorem 3.1, the bound given in Theorem 3.2 contains a term [E x;y L(^p(x); y)
1
n
P n
i=1 L(^p(x i ); y i )] which relies on the unknown optimal predictor ^
p. From Theorem 3.1,
we know that this term does not affect the performance of the estimated function ^
pn when

compared with the performance of ^
p. In order for us to compare with the bound in [1]
obtained from an algorithmic stability point of view, we make the additional assumption
that j ^
p(x) yj  M for all (x; y). Note that this assumption is also required in [1]. Using
Hoeffding's inequality, we obtain that with probability of at most exp( n 2 Æ 2 =40 2 M 2 ),
E x;y L(^p(x); y) 1
n
n
X
i=1
L(^p(x i ); y i ) > ÆM=(8
p
5):
Together with Theorem 3.2, we have with probability of at least 1 ,
E x;y L(^p n (x); y) 1
n
n
X
i=1
L(^p n (x i ); y i )  ( 2 + )
40 2 M 2 ln 3

 2 n +M 2
s
ln 3

8n
:
This compares very favorably to the following bound in [1]: 1
E x;y L(^p n (x); y) 1
n
n
X
i=1
L(^p n (x i ); y i )  2 2 M 2
n + 2M 2
s
(
2 4
 2 +
4 2
 + 2)
ln 2

n :
3.2 Binary classification
In binary classification, the output value y 2 f1g is a discrete variable. Given a continu­
ous model p(x), we consider the following prediction rule: predict y = 1 if p(x)  0, and
predict y = 1 otherwise. The classification error (we shall ignore the point p(x) = 0,
which is assumed to occur rarely) is
I(p(x); y) =

1 if p(x)y  0;
0 if p(x)y > 0:
Unfortunately, this classification error function is not convex, which cannot be handled in
our formulation. In fact, even in many other popular methods, such as logistic regression
and support vector machines, some kind of convex formulations have to be employed. We
shall thus consider the following soft­margin SVM style loss as an illustration:
L(p; y) = max(1 py; 0): (8)
Note that the separable case of this loss was investigated in [6]. In this case, L 0
1 (p; y)
denotes a subgradient rather than gradient since L(p; y) is non­smooth: at py = 1,
L 0
1 (p; y)y 2 [ 1; 0]; L 0
1 (p; y)y = 1 when py < 1 and L 0
1 (p; y) = 0 when py > 1.
Since 8jp 1 p 2 j  Æ and y = 1, I(p 2 ; y)  I(p 1 Æy; y), we know that if k^p n ^
pkH  Æ,
then
E x;y I(^p n (x); y)  E x;y I(^p(x) Æy; y);
1
n
n
X
i=1
I(^p(x i ) Æy i ; y i )  1
n
n
X
i=1
I(^p n (x i ) 2Æy i ; y i ):
Using the standard Hoeffding's inequality, we have with probability of at most
exp( n 2 Æ 2 =40 2 ),
E x;y I(^p(x) Æy; y) > 1
n
n
X
i=1
I(^p(x i ) Æy i ; y i ) + Æ=(4
p
5):
1 In [1], there was a small error after equation (11). As a result, the original bound in their paper
was in a form equivalent to the one we cite here with  replaced by  1=2 .

When 1
n
P n
i=1 I(^p(x i ) Æy i ; y i )  0, it is usually better to use a different (multi­
plicative) form of Hoeffding's inequality, which implies that with probability of at most
exp( n 2 Æ 2 =40 2 ),
E x;y I(^p(x) Æy; y) > max(
2
n
n
X
i=1
I(^p(x i ) Æy i ; y i );  2 Æ 2 =(5 2 )):
Together with Corollary 2.1, we obtain the following margin­percentile result:
Theorem 3.3 Using loss function (8) in (3). Assume that sup p2H:kpkH =1 p(x)  , then
80 < Æ  =, with probability of at least 1 3 exp( n 2 Æ 2 =40 2 ), we have
E x;y I(^p n (x); y)  1
n
n
X
i=1
I(^p n (x i ) 2Æy i ; y i ) + Æ=(4
p
5):
We also have with probability of at least 1 3 exp( n 2 Æ 2 =40 2 ),
E x;y I(^p n (x); y)  max(
2
n
n
X
i=1
I(^p n (x i ) 2Æy i ; y i );  2 Æ 2 =(5 2 )):
We may obtain from Theorem 3.3 the following result: with probability of at least 1 ,
E x;y I(^p n (x); y)  1
n
n
X
i=1
I(^p n (x i ) 4
s
10 ln 3

 2 n  2 y i ; y i ) +
s
ln 3

2n
:
It is interesting to compare this result with margin percentile style bounds from VC analy­
sis. For example, Theorem 4.19 in [2] implies that there exists a constant C such that with
probability of at least 1 : for all  we have
E x;y I(^p n (x); y)  1
n
n
X
i=1
I(^p n (x i ) y i ; y i ) + C
s
 2 ln 2 n
 2 n +
ln 1

n :
We can see that if we assume that  is small and the margin  = 4
q
10 ln 3

 2 n  2 is also small,
then the above bound with this choice of  is inferior to the bound in Theorem 3.3. Clearly,
this implies that our analysis has some advantages over VC analysis due to the fact that we
directly analyze the numerical formulation of support vector classification.
4 Conclusion
In this paper, we have introduced a notion of the convergence of the estimated predictor
to the best underlying predictor for some learning problems in Hilbert spaces. This gen­
eralizes an earlier study in [6]. We derived generalization bounds for some regression and
classification problems. We have shown that results from our analysis compare favorably
with a number of earlier studies. This indicates that the concept introduced in this paper
can lead to valuable insights into certain numerical formulations of learning problems.
References
[1] Olivier Bousquet and Andr’e Elisseeff. Algorithmic stability and generalization perfor­
mance. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances
in Neural Information Processing Systems 13, pages 196--202. MIT Press, 2001.

[2] Nello Cristianini and John Shawe­Taylor. An Introduction to Support Vector Machines
and other Kernel­based Learning Methods. Cambridge University Press, 2000.
[3] Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. The importance of con­
vexity in learning with squared loss. IEEE Trans. Inform. Theory, 44(5):1974--1980,
1998.
[4] R. Tyrrell Rockafellar. Convex analysis. Princeton University Press, Princeton, NJ,
1970.
[5] Vadim Yurinsky. Sums and Gaussian vectors. Springer­Verlag, Berlin, 1995.
[6] Tong Zhang. Convergence of large margin separable linear classification. In Advances
in Neural Information Processing Systems 13, pages 357--363, 2001.
[7] Tong Zhang. A leave­one­out cross validation bound for kernel methods with appli­
cations in learning. In 14th Annual Conference on Computational Learning Theory,
pages 427--443, 2001.

