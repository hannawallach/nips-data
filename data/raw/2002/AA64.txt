Discriminative Densities from Maximum Contrast Estimation

Peter Meinicke Neuroinformatics Group University of Bielefeld Bielefeld, Germany pmeinick@techfak.uni-bielefeld.de Thorsten Twellmann Neuroinformatics Group University of Bielefeld Bielefeld, Germany ttwellma@techfak.uni-bielefeld.de

Helge Ritter Neuroinformatics Group University of Bielefeld Bielefeld, Germany helge@techfak.uni-bielefeld.de

Abstract We propose a framework for classifier design based on discriminative densities for representation of the differences of the class-conditional distributions in a way that is optimal for classification. The densities are selected from a parametrized set by constrained maximization of some objective function which measures the average (bounded) difference, i.e. the contrast between discriminative densities. We show that maximization of the contrast is equivalent to minimization of an approximation of the Bayes risk. Therefore using suitable classes of probability density functions, the resulting maximum contrast classifiers (MCCs) can approximate the Bayes rule for the general multiclass case. In particular for a certain parametrization of the density functions we obtain MCCs which have the same functional form as the well-known Support Vector Machines (SVMs). We show that MCC-training in general requires some nonlinear optimization but under certain conditions the problem is concave and can be tackled by a single linear program. We indicate the close relation between SVM- and MCC-training and in particular we show that Linear Programming Machines can be viewed as an approximate realization of MCCs. In the experiments on benchmark data sets, the MCC shows a competitive classification performance.

1 Introduction In the Bayesian framework of classification the ultimate goal of a classifier

!"!#%$&

 ¢¡¤£¦¥¨§©

is to minimize the expected risk of misclassification measured by ' )

¡¤( 0) ¥

which

denotes the loss for assigning a given feature vector to class , while it actually belongs to ability density functions (PDFs) and 576 denoting the corresponding apriori probabilities of

class , with being the number of classes. With 1 being the class-conditional prob-

( $ ¡¤£320(4¥


class-membership we have the risk

 ¢¡¤£¦¥'¨§¡ ©¥ ¡6

  ¢¡ £¦¥

With the standard "zero-one" loss function '

5 6

¡ (  ) ¥

'

¡¤(   ¢¡ £¦¥ ¥ ¡¤£320(4¥ £

1

¡ #"%$'&6



! (1)

, where 6 denotes the Kro-

$(&

necker delta, it is easy to show (see e.g. [3]) that the expected risk is minimized, if one

chooses the classifier

 ¢¡ £¦¥

¡¤)103254¦)766

5 61 ¡¤£320(4¥

The resulting lower bound on

mance of the classifier

 ¢¡¤£¦¥

 

(2)

is known as the Bayes risk which limits the average perfor-

. Because the class-conditional densities are usually unknown,

one way to realize the above classifier is to use estimates of these densities instead. This leads to the so-called plug-in classifiers, which are Bayes-consistent if the density estimators are consistent (e.g. [9]). Due to the notoriously slow convergence of density estimates the plug-in scheme usually isn't the best recipe for classifier design and as an alternative many discriminant functions including Neural Networks (see [1, 9] for an overview) and Support Vector Machines (SVMs) [2, 12] have been proposed which are trained directly to minimize the empirical classification error. We recently proposed a method for the design of density-based classifiers without resorting to the usual density estimation schemes of the plug-in approach [6]. Instead we utilized discriminative densities with parameters optimized to solve the classification problem. The approach requires maximization of the average bounded difference between class (discrim-

8&

FI9A@5BDC¡¤£§HG-bounded¥ ¡ 4VUXW

£E¥F §HG ©¥ inative) densities 1 tributions. The

¡ £ 

6 2 ¥

9 @5BPC

, which we refer to as the contrast of the underlying "true" discontrast is the expectation with

&RQTS



9 @5BDC S1 S &1 &

 5

¡ £

YGH`

¥ 5 ¡ £ (3)

YGH` " YGH`9@5BDC¥&

¡¤£ 

$

distributions with "true" densities 1

discriminative densities 1 ¡ £320(4¥

¡¤£ The idea is to find

6 , which represent the underlying

¥

in a way, that is optimal for classification. When

maximizing the contrast with respect to the parameters 6 of the discriminative densities increasing the differences between discriminative densities where the differences between the true densities are already large. In this paper we show that with some slight modification the contrast can be viewed as an approximation of the negative Bayes risk (up to some constant shift and scaling) which is valid for the binary as well as for the general multiclass case. Therefore for certain parametrizations of the discriminative densities MCCs allow to find an optimal trade-off between the classical plug-in Bayes-consistency and the consistency which arises from direct minimization of the approximate Bayes risk. Furthermore, for a particular parametrization of the PDFs, we obtain certain kinds of Linear Programming Machines (LPMs) [4] as (in general) approximate solutions of maximum contrast estimation. In that way MCCs provide a Bayes-consistent approach to realize multiclass LPMs / SVMs and they suggest an interpretation of the magnitude of the LPM / SVM classification function in terms of density differences which provide a probabilistic measure of confidence. For the case of LPMs we propose an extended optimization procedure for maximization of the contrast via iteration of linear optimizations. Inspired by the MCC-framework, for the resulting Sequential Linear Programming Machines (SLPM) we propose a new regularizer which allows to find an optimal trade-off between the above mentioned two approaches to Bayes consistency. In the experiments we analyse the performance of the SLPM on simulated and real world data.

the upper bound

9@5BDC

plays a central role because it prevents the learning algorithm from

`


2 Maximum Contrast Estimation For the design of MCCs the first step, which is the same as for the plug-in concept, requires to replace the unknown class-conditional densities of the Bayes classifier (2) by suitably parametrized PDFs. Then, instead of choosing the parameters for an approximation of the original (true) densities (e.g. by maximum likelihood estimation) as with the plug-in scheme, the density parameters are choosen to maximize the so-called contrast which is

the expected value of the

9 @5BDC-bounded

density differences as defined in (3).

¡ 

For the case of an unbounded contrast, i.e.

9!@5BDC

, the general maximum contrast

solution can be found analytically and for notational simplicity we derive it for the binary case with equal apriori probabilities, where the contrast can be written as

¨§

¡

  £¢1

¡1 ¡¤£¦¥ ¡1 ¡¤£32  ¥

"

¡ £¦¥ ¥ ¡¤£32  ¥ £

! ¥¤

 ¦¢1

¡ ¡¤£¦¥ ¡1 ¡¤£32 ¥

¡¤£¦¥

£

 1

¡¤£¦¥ ¥ ¡ £32 ¥ £ ¡¤£32  ¥ ¥ ¡¤£¦¥ £ 1

1

¢

!

 " ©§1 ! ¥¤

1

¡ £ 2 ¥ ¥ ¡¤£¦¥ £ 1 

Thus the unbounded contrast is maximized for 1

with the peaks of the Delta (Dirac) functions located at

and 1

£

criminative densities we may think of and therefore we require an appropriate bound

For finite

¢

¡ ) 0D254¦)16  ©§ "

¡ ¡¤£32 ¥ 1

9A@5BDC,

¡¤£32  ¥ ¥

¨§" "1 ¡ $¡ "



¡ £ £ ¥  ¡¤£¦¥¡¤£32

) 0D2 4¦)76 1

¡¢1

¡¥ $" "

¡ £ £ ¥

1

¡ £ 2 ¥ ¥

©§¢

, respectively. Obviously, these are not the best dis-

9 @5BPC.

maximization of the contrast enforces a redistribution of the estimated

probability mass and gives rise to a constrained linear optimization problem in the space of discriminative densities which may be solved by variational methods in some cases. The relation between contrast and Bayes risk becomes more convenient when we slightly modify the above definition (3) by a unit upper bound and by adding a lower bound on the

 ¥

&RQTS



-scaled density differences:

F §HG ¡ 4¦)16 "

¡¤£ 







$ "  4VUXW ¥5 YG ` " YGH` ©&&    S%1 S !&5 ¡ £ ¥

1

¡¤£



&¥

with scale factor

contrast scaling:

F ¡ £¦¥F § G9A@5BDC©¥ ¡ ¥







 ¡¡¤£

(4)

. Therefore, for an infinite scale factor the (expected)

approaches the negative Bayes risk up to constant shift and

  XUX4 £¦¥ " F §HG © ¡ " XU4 F

¡

"!$#

§ §



¡ £  ¥ § §

ary and which becomes increasingly focused in their vicinity as

region is defined by the bounds

( ¡'£ 0)§§3§72F §HG¥"221



"!%#

¡ ¥

 (5)

Thus the scale factor defines a subset of the input-space, which includes the decision bound-

. The extent of the

on the difference between discriminative densities.

& 

In terms of the contrast function it can be defined as

¡¤£   &

(6)

Since for MCC-training we maximize the empirical contrast, i.e. the corresponding sample average of , the scale factor then defines a subset of the training data which has impact on learning of the decision boundary. Thus for increasing scale factor the relative size of that subset is shrinking. However for increasing size of the training set the scale factor can be gradually increased and then, for suitable classes of PDFs, MCCs can approach the Bayes rule. In other words, acts as a regularization parameter such that, for particular choices of the PDF class convergence to the Bayes classifier can be achieved if the quality of the approximation of the loss function is gradually increased for increasing sample sizes. In the following section we shall consider such a class of PDFs which is flexible enough and which turns out to include a certain kind of SVMs.



F §HG ¡ £  ¥


3 MCC-Realizations In the following we shall first consider a particularly useful parametrization of the discriminative densities which gives rise to classifiers which in the binary case have the same functional form as SVMs up to a "missing" bias term in the MCC-case. For training of these MCCs we derive a suitable objective function which can be maximized by sequential linear programming where we show the close relation to training of Linear Programming Machines. 3.1 Density Parametrization We first have to choose a set of candidate functions from which we select the required PDF. Because this set should provide some flexibility with respect to contrast maximization the



6¢¡¤£¢¥

usual kernel density estimator (KDE)[11]

1

¡¤£32)¥ ¡  2  2 & ¦ ¡¤£  £ ¥ 6 (7)

 

&

functions according to §

P ¡

)

with index set containing indices of examples from class

¦

¡¤£  ¥ £ 

and with normalized kernel

isn't a quite good choice, since the only free

parameter is the kernel bandwidth which doesn't allow for any local adaptation. On the other hand if we allow for local variation of the bandwidth we get a complicated contrast which is difficult to maximize due to nonlinear dependencies on the parameters. The same is true if we treat the kernel centers as free parameters. However, if we modify the kernel

YG ` ¡ ¡ ¡

density estimator to have flexible mixing weights according to

1

¡¤£ &¥



(&6

¦

 

& & ¡¤£¦¥



with

6¨¡¤£©¥

¡¤£  £ ¥ 6





&    

we get an objective function, which is linear in the mixing parameters conditions. Thus we have class-specific densities with mixing weights the contribution of a single training example to the PDF. 



& (&6'&6

! (8)

under certain which control

With that choice we achieve plug-in Bayes-consistency for the case of equal mixing weights, since then we have the usual kernel density estimator (KDE), which, besides some mild assumptions about the distributions, requires a vanishing kernel bandwidth for

"

& 

.

3.2 Objective Function For notational simplicity in the following we shall incorporate the scale factor

mixing weigths

and $# 

&  ¡



&

into a common parameter vector

$#%& ('!)10

4



)32

(&6 ¡ £

#

¡ ¡ !!"!  #

YG. ¡

Further we define the scaled density difference

¥



¡¤£¦¥

¥

#



¡¤£¦¥

#

5 6 6 6¡

#

F  65

5

& & &

# 



 #

" 

¥ with

#

& and the



¡  &

so that we can write the empirical contrast examples, as:

F ¡   75 ¡ ¥ # "

&D



6¢¡¤£

"

, i.e. the sample average over

(9) training

8@9

61A 

¥

F 9

¤  4VUXW G " 

$  " &

)

"   4 & ¡¤£ ¥ &CB ED % 6 #

%

Q &

where the assignment variables 6GF 9

fixed assignment variables 6 , 75

(10)

realize the maximum function in (4). With

is concave and maximization with respect to # gives rise


to a linear optimization problem. On the other hand, for fixed 9 9

to the 6 is achieved by setting 6

)

¡

# maximization with respect

for negative terms. This suggests a sequential linear

optimization strategy for overall maximization of the contrast which shall be introduced in

detail in the following section. Since we have already incorporated

now identified with the norm $#

& .



 

as a scaling factor into the parameter vector # , is

Therefore the scale factor can be adjusted implicitly

by a regularization term which penalizes some suitable norm of the objective function can be defined by

F

75

¡  ¡ ¥

#

¡ F "

75

¡ ¥ ¡£¢ ¡ ¥  ¡

# # ' )

¡

#

&.

Thus a suitable (11)

with determining the weight of the penalty, i.e. the degree of regularization. We now

consider several instances of the case where the penalty corresponds to some 1 -norm of 

¡ 

With the -norm, for

 

# .

the probability mass of the discriminative densities is con-

centrated on those two kernel-functions which yield the highest average density difference. Although that property forces the sparsest solution for large enough , clearly, that solution isn't Bayes-consistent in general because as pointed out in Sec.2, for all probability mass of the discriminative densities is concentrated at the two points with maximum ¡ ¡ 

¢

average density difference.

¢ ¡ ¥

# Conversely taking

¡ ¢,

$#G which resembles the standard SVM regularizer [10],

¡ 

 

 

yields the KDE with equal mixing weights for

1 -norm penalties with 1

'



. Indeed, it is easy to see that all

share this convenient property, which guarantees "plug-in"

Bayes consistency in the case where the solution is totally determined by the regularizer. In that case kernel density estimators are achieved as the "default" solution. Therefore we chose a combination of the -norm with the maximum-norm 

&D ¤

¢ ¡ ¥

#

¡ 

¡ ¥

$#

&  & #

 E#  (12)

which is easily incorporated into a linear program, as to be shown in the following. For that kind of penalty in the limiting case we achieve an equal distribution of the weights which corresponds to the kernel density estimator (KDE) solution. In that way we have a nice trade-off between two kinds of Bayes consistency: for increasing the class-specific densities converge to the KDE with equal mixing weights, whereas for decreasing the probability mass of the discriminative densities is more and more concentrated near the Bayes-optimal decision boundary. By a suitable choice of the kernel width and the scale of the weights, e.g. via cross-validation, the solution with fastest convergence to the Bayes rule may be selected. With an 1-norm penalty on the weights and on the vector ¤ of soft margin slack variables we get the Linear Programming Machine which requires to minimize

¡

¡

¤

¦¥



§  §

5

&D 6 ¦

¡¤£  £ ¥  6



"66

E# 

§ "

F

"

 §¤  subject to &©¨T& &  ) (13)

¥

¡ 

 

" &

with

¡

subtracting , setting

!"

and with the above constraints on



9

9

# . Dividing the objective by ,

objective shows that LPM training corresponds to a special case of MCC training with fixed

6 ¡

¥

and -norm regularizer with .

6 6 andturningminimizationtomaximizationofthenegative

 

¡ 

3.3 Sequential Linear Programming Estimation of mixing weights is now achieved9 by maximizing the sample contrast with iterative optimization scheme:

respect to the ¨

(&6 and the assignment variables 6 . This can be achieved by the following


9

1. Initialization: 6

¡  (

0

#

for fixed   :

2. Maximization w.r.t.

& 



&D9

(&6

 H& " 

9

6  %

¨¡¤£ ¥ % % 

Q

 ¨

¡

4

¨



 )32

0

9 '&6 &

¡ ¡ £  ¥

6

©¡¨

%

¡

maximize subject to 6 ¦¥



%

@5BDC&¤£ '&

#

&D"

¢¡¤£

96(&

¨ 6

¨

@5BDC& ¤ 

6¨¡¤£

) 0

¨

% !)



¥

(&6

¥

¨§ )  ( 0) 2

¡

 

3. Maximization w.r.t.   for fixed : #

¡ &RQTS &

  



4

) otherwise.

9

6 %

¡¤£  ¥  $ 6

# '

"

4. If convergence in contrast then stop else proceed with step 2.

¡

Where

96(&

% are slack variables, measuring the part of the density difference 4

$#

#

& 

 

&

%

¡ £  ¥

6

#

which can be charged to the objective function. The constraint

¡

program was chosen in order to prevent the trivial solution

¡



in the linear

appear for larger values of . Since we used unnormalized Gaussian kernel functions with exclude any useful solutions for the weights. 4 Experiments In the following section we consider the task of solving binary classification problems within the MCC-framework, using the above SLPM with Gaussian kernel function. The first experiment illustrates the behaviour of the MCC for different values for the regularization by means of a simple two-dimensional toy dataset. The second experiment compares the classification performance of the MCC with those of the SVM and KernelDensity-Classifier (KDC) which is a special case of the MCC with equal weighting of each kernel function. To this end, we selected four frequently used benchmark datasets from the UCI Machine Learning Repository. The two-dimensional toy dataset consists of 300 data points, sampled from two overlapping isotropic normal distributions with a mutual distance of and standard deviation  .

¦

, i.e. we excluded all multiplicative density constants, that constraint doesn't

¡

¡ § 

¡

Figure 1 shows the solution of the MCC for two different values of

with non-zero weights according the criterion ¨ 6

F

'

 (only data points

!#" are marked by symbols). In

)

both figures, data points with large mixing weights are located near the decision border. In

¡

particular for small there are regions of high contrast

¡

F

(illustrated by isolines). For increasing

alongside the decision function

the number of data points with non-zero ¨ 6 in-

2 2

¡ £  £¦¥ 

which may otherwise

creases. At the same time, one can note a decrease of the difference between the weights. are nearer to the decision border than for large values. This illustrates that for increasing data points are misclassified with a contrast . The MCC identified those data points as outliers and deactivated them during the training (encircled symbols). The second experiment demonstrates the performance of the MCC in comparison with those of a Support Vector Machine, as one of the state-of-the-art binary classifiers, and with the KDC. For this experiment we selected the Pima Indian Diabetes, Breast-Cancer, Heart and Thyroid dataset from the UCI Machine Learning repository. The Support Vector Machine was trained using the Sequential Minimal Optimization algorithm by J. Platt[7] adjusted according to the modification proposed by S.S. Keerthi [5].

Regions with contrast 

¡ are highlighted gray. For small values of , these regions

F ¡

the quality of the approximation of the loss function decreases. In both figures, several

2 2 



2 2 


300 datapoints /  = 0.2

-0.5

300 datapoints /  = 4.2

-0.5

-1

0

-1 -1.5 -2 -2.5

-1.5 0

-2 0.5

2.5 2 1.5

1.5 -0.5

0

0 1

1

0.5 0.5

(left:

¡ §

) , right:

¡¡  §).

¡ Figure 1: Two MCC solutions for the two-dimensional toy dataset for different values of

¡ ¡

The symbols and

¢ £

depict the positions of data points

responding ¨ 6 . Encircled symbols have been deactivated during the training (symbols for with with non-zero ¨ 6 . The size of each symbol is scaled according the value of the cordeactivated data points are not scaled according to ¨ 6 , since in most cases ¨ 6 is zero). The absolute value of the contrast is illustrated by the isolines while the sign of the contrast de-

F 1 2 2 

picts the binary classification of the classifier. The region with

to

(

as defined in (6) is colored white and the complement colored gray. The percentage

 ¥¤§¦

§ ¦

)

which corresponds (right figure) of the

of data points that define the solution is dataset. (left figure) and

The experimental setup was comparable with that in [8]: After normalization to zero mean and unit standard deviation, each dataset was divided 100 times in different http://ida.first.gmd.de/ raetsch/data/benchmarks.htm). Since we used for all classifiers the Gaussian kernel function, all three algorithms are parametrized by the bandwidth . Addiparametrization was chosen by estimating the generalization performance for different values of bandwidth and regularization by means of the average test error on the first five dataset partitions. More precisely, a first coarse scan was performed, followed by a fine scan in the interval near the optimal values of the first one. Each scan considered 1600 error, the pair constructing the sparsest solution was kept. Finally, the reported values in Tab.1 and Tab.2 are averaged over all 100 dataset partitions. classification rate and sparseness of the solution (measured as percentage non-zero ¨ 6 ). Additionally, the corresponding values after the first MCC iteration are given in brackets. The last two columns show the absolute number of iterations and the final number of deactivated examples. For all four datasets the MCC is able to find a sparse solution. In particular for the Heart, Breast-Cancer and Diabetes dataset the solution of the MCC is significantly sparser than those of the SVM (see Tab.2). Nevertheless, Tab.2 indicates that the classification rates of the MCC are competitive with those of the SVM. 5 Conclusion The MCC-approach provides an understanding of SVMs / LPMs in terms of generative modelling using discriminative densities. While usual unsupervised density estimation schemes try to minimize some distance criterion (e.g. Kullback-Leibler divergence) be-

¦   ¦ :

) pairs of disjoint train- and testsets with a ratio of

© ¨ ) (provided by G. R¨atsch at

¡



had to be chosen. The optimal tionally, for the SVM and MCC the regularization value

¥ different combinations of  ¡ and , resp. 

and . For parameter pairs with identical test

¡  ¡ ¥

Table 1 shows the optimal parametrization

 of the MCC in combination with the


¡  ¡ ¥

Table 1: Optimal parametrization

number of iterations of the MCC and number of 6



, classification rate, percentage of non-zero ¨ 6 , 9

)

. The results are averaged over all

¡

100 dataset partitions. For the classification rate and percentage of non-zero ¨ -coefficients the corresponding value after the first MCC iteration is given in brackets.

¡

12.17 2.066

§!© "

2.624

Dataset Breast-Cancer Heart Thyroid Diabetes

 

1.38 2.69 0.49 4.52

Classif. rate

74.3 84.3 95.5 76.6 (74.4 ) (84.1 ) (95.5 ) (76.5 )

¢¤£¦¥¨§© ¢

13.6 20.4 46.1 5.3 (13.8 ) (21.2 ) (46.1 ) (5.5 )

Iter. 2.23 3.10 1.00 5.86

¢©

2.6 6.4 0.0 40.7

Table 2: Summary of the performance of the KDC, SVM and MCC for the four benchmark datasets. Given are the classification rates with percentage of non-zero ¨ 6 (in brackets). Note that our results for the SVM are slightly better to those reported in [8]. One reason could be the coarse parameter selection for the SVM as already mentioned by the author.

Dataset Breast-Cancer Heart Thyroid Diabetes

73.1 84.1 95.6 74.2

KDC (100 (100 (100 (100

) ) ) ) 74.5 84.4 95.7 76.7

SVM (58.5 (60.9 (15.8 (53.6

) ) ) ) 74.3 84.3 95.5 76.6

MCC (13.6 (20.4 (46.1 ( 5.3

) ) ) )

tween the models and the true densities, MC-estimation aims at learning of densities which represent the differences of the underlying distributions in an optimal way for classification. Future work will address the investigation of the general multiclass performance and the capability to cope with misslabeled data. References [1] C. M. Bishop. Neural Networks for Pattern Recognition. Clarendon Press, Oxford, 1995. [2] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273­297, 1995. [3] R. O. Duda and P. E. Hart. Pattern Classification and Scene Analysis. Wiley, New York, 1973. [4] T. Graepel, R. Herbrich, B. Scholkopf, A. Smola, P. Bartlett, K. Robert-Muller, K. Obermayer, and B. Williamson. Classification on proximity data with lp­machines, 1999. [5] S.S. Keerthi, S.K. Shevade, C. Bhattacharyya, and K.R.K. Murthy. Improvements to platt's SMO algorithm for SVM classifier design. Technical report, Dept of CSA, IISc, Bangalore, India, 1999. [6] P. Meinicke, T. Twellmann, and H. Ritter. Maximum contrast classifiers. In Proc. of the Int. Conf. on Artificial Neural Networks, Berlin, 2002. Springer. in press. [7] J. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨olkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods -- Support Vector Learning, pages 185­208, Cambridge, MA, 1999. MIT Press. [8] G. R¨atsch, T. Onoda, and K.-R. M¨uller. Soft margins for AdaBoost. Technical Report NC-TR1998-021, Department of Computer Science, Royal Holloway, University of London, Egham, UK, August 1998. Submitted to Machine Learning. [9] B. D. Ripley. Pattern Recognition and Neural Networks. Cambridge University Press, Cambridge, 1996. [10] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT Press, 2002. [11] D. W. Scott. Multivariate Density Estimation. Wiley, 1992. [12] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1995.


