Hyperkernels

Cheng Soon Ong, Alexander J. Smola, Robert C. Williamson Research School of Information Sciences and Engineering The Australian National University Canberra, 0200 ACT, Australia Cheng.Ong, Alex.Smola, Bob.Williamson @anu.edu.au

 

¡

Abstract We consider the problem of choosing a kernel suitable for estimation using a Gaussian Process estimator or a Support Vector Machine. A novel solution is presented which involves defining a Reproducing Kernel Hilbert Space on the space of kernels itself. By utilizing an analog of the classical representer theorem, the problem of choosing a kernel from a parameterized family of kernels (e.g. of varying width) is reduced to a statistical estimation problem akin to the problem of minimizing a regularized risk functional. Various classical settings for model or kernel selection are special cases of our framework.

1 Introduction Choosing suitable kernel functions for estimation using Gaussian Processes and Support Vector Machines is an important step in the inference process. To date, there are few if any systematic techniques to assist in this choice. Even the restricted problem of choosing the "width" of a parameterized family of kernels (e.g. Gaussian) has not had a simple and elegant solution. A recent development [1] which solves the above problem in a restricted sense involves the use of semidefinite programming to learn an arbitrary positive semidefinite matrix , subject to minimization of criteria such as the kernel target alignment [1], the maximum of the posterior probability [2], the minimization of a learning-theoreticalbound [3], or subject to cross-validation settings [4]. The restriction mentioned is that the methods work with the kernel matrix, rather than the kernel itself. Furthermore, whilst demonstrably improving the performance of estimators to some degree, they require clever parameterization and design to make the method work in the particular situations. There are still no general principles to guide the choice of a) which family of kernels to choose, b) efficient parameterizations over this space, and c) suitable penalty terms to combat overfitting. (The last point is particularly an issue when we have a very large set of semidefinite matrices at our disposal). Whilst not yet providing a complete solution to these problems, this paper presents a framework that allows the optimization within a parameterized family relatively simply, and crucially, intrinsically captures the tradeoff between the size of the family of kernels and the sample size available. Furthermore, the solution presented is for optimizing kernels themselves, rather than the kernel matrix as in [1]. Other approaches on learning the kernel include using boosting [5] and by bounding the Rademacher complexity [6].

¢


Outline of the Paper We show (Section 2) that for most kernel-based learning methods there exists a functional, the quality functional1, which plays a similar role to the empirical risk functional, and that subsequently (Section 3) the introduction of a kernel on kernels, a so-called hyperkernel, in conjunction with regularization on the Reproducing Kernel Hilbert Space formed on kernels leads to a systematic way of parameterizing function classes whilst managing overfitting. We give several examples of hyperkernels (Section 4) and show (Section 5) how they can be used practically. Due to space constraints we only consider Support Vector classification.

2 Quality Functionals

!!!"$#

 

Let

 ¢¡¤£¦¥¨§©

denote the set of training data and ¡

%$¡¤£¤¥"§©¢&

'(!!)"'0#  

set of corresponding labels, jointly drawn iid from some probability distribution

on

8@9BA G"'H6

and

132.

 IP   ¡¤£¦¥¨§©RQ   ¡¤CEDF¡denote%ST% ¡U£¦¥"§©VQ % ¡UCEDF¡

¡¤CED¦¡ % ¡¤CEDF¡

13245'76

the ¡

Furthermore, let

). Let the same

the corresponding test sets (drawn from on data which we call quality functionals. Their

. and

 6

We introduce a new class of functionals

purpose is to indicate, given a kernel

X

W

and the training data

2¦ Y¡¤£¦¥¨§© %7¡U£¦¥"§©

, how suitable



the kernel is for explaining the training data.

Definition 1 (Empirical Quality Functional) Given a kernel , and data

W`CEacb7dX  5 %`e

where where stdXf2  e

g 5; h 6 g¦uh

to be an empirical quality functional if it depends on

i.e. if there exists a function

is the kernel matrix.

p

such that

g hYi

X

W CEacb dX   %`eqrpH2   %

¢

  %Xf2,fg"5Hh6

define

 6

WwCEacb

is

via  only

X

¢

The basic idea is that

WvCEacb

could be used to adapt

minimized, based on this single dataset

nels

X

   %

it is in general possible to find a kernel

values of

 W CEacb dXy   ¡¤£¤¥"§© % ¡¤£¦¥¨§© e 

  Xy x

iX

in a manner such that

x

. Given a sufficiently rich class of ker-

that attains arbitrarily small

W`CEacb7dXy  ¢¡¤CEDF¡ %¡¤CEDF¡Ee

for any training set. However, it is very unlikely that

would be similarly small in general. Analogously to the standard

methods of statistical learning theory, we aim to minimize the expected quality functional:

Definition 2 (Expected Quality Functional) Suppose

tional. Then

W CEacb is an empirical quality func-

(1)

W3dXeG&P dWvCEacbdX   %`ee u  

1

#

is the expected quality functional, where the expectation is taken with respect to

 

Note the similarity between

CEacb7d   %`ev  

 # g4  #

2$g"5'0g" q2$g656

W CEacb dX   %`e

.

and the empirical risk of an estimator

(where



is a suitable loss function): in both

cases we compute the value of a functional which depends on some sample

from and a function, and in both cases we have



W3dXeP dW`CEacb7dX   %`ee u   

and

d$eT d CEacb7d   %`ee u   

132G5'76

d$e

   %

drawn (2)

Here is known as the expected risk. We now present some examples of quality func-

tionals, and derive their exact minimizers whenever possible. Example 1 (Kernel Target Alignment) This quality functional was introduced in [7] to assess the "alignment" of a kernel with training labels. It is defined by

W CEacb ¥¨§¨©!acCE©¡

 dX   ¡¤£¦¥¨§© % ¡¤£¦¥¨§© eG&tdfe hi'jh)k'7gk h h  

¢

¢

'

denotes the where

'

is the Frobenius norm:

h h kk

¢

&nm"o

¢ ¢

% ¡¤£¤¥"§©

h)'jhg&hkk

¢

k

(3)

h h

¢

k

denotes the vector of elements of

g g¤uh,

l

k norm of , and '

. Note that the definition in [7] looks

somewhat different, yet it is algebraically identical to (3).

1 We actually mean badness, since we are minimizing this functional.


By decomposing which case

¢

It is clear that one cannot expect that

the set chosen to determine

Xy

.

dX y  ¡U£¦¥"§© %7¡¤£¤¥"§©erdfe h)'jh)k' h)'(' g h rdRe hi'jh)khi'jh¢k h)'jh)kk 

into its eigensystem, one can see that (3) is minimized if

  ¡ k

¥¨§"©acCE©¡ W CEacb dXyk   ¡¤£¦¥¨§© % ¡¤£¦¥¨§© e  k

¢

g '' g '

W CEacb ¥"§¨©!acCE©¡

 '(' g

¢ , in (4)

for data other than

Example 2 (Regularized Risk Functional) If £ is the Reproducing Kernel Hilbert Space (RKHS) associated with the kernel , the regularized risk functionals have the form #

 X £¦CE d   ¡¤£¤¥"§© % ¡¤£¦¥¨§© eG  

¤ g4

d ¥   g "' g   g 656§¦©¨fh h 2 q2 



k 

(5)

where

h h k



 is the RKHS norm of . By virtue of the representer theorem (see e.g., [4, 8])

 £



we know that the minimizer over

For a given loss this leads to the quality functional

W CEacb dX   ¡¤£¦¥¨§© % ¡U£¦¥"§© e &

£¦CE¨£¦§D  

'

"!$#&% ¤ g4

d ¥   g 5' g  2



i

of (5) can be written as a kernel expansion.

#

d)(Geg

¢

60¦1¨ g ( 2(&3

¢

(6)

The minimizer of (6) is more difficult to find, since we have to carry out a double mini( )( C4ED . Thus W CEacb dX   ¡¤£¤¥"§© % ¡¤£¦¥¨§© e` kG6 . For sufficiently large 4 , we can

mization over 54  687@9A7@B 2(T , and

g

make



and ( . First, note that for

  £¦CE"£¤§DF and (

¢ ¢ ¢

W CEacb dX   ¡¤£¦¥¨§© % ¡¤£¦¥¨§© eto

£¦CE¨£¦§D ¢

¢

 

arbitrarily close to .

 7IH$7 BQPRP

¢

zero, by setting

, where

6§¦©¨ g

( )( 

¢

P

g iTSm¨o#

# g4

¢

, and (,

Even if we disallow setting

of (6) as follows. Set

¤ g4

g

Choosing each

proof that

P

d ¥

#

¢

 d

we can determine the minimum

 P . Then )(

¢

 P k

P k

and so

'' g

 ' '

  g "' g  2 d2(jeg

¢

¥   g "' g  g 6U¦1¨Vh h

2 P

WV0oYXR`"a 2

 $g¨"'0g cb06U¦ b

Gk yieldstheminimumwithrespectto . The P

k

¢

is the global minimizer of this quality functional is omitted for brevity.

Example 3 (Negative Log-Posterior) In Gaussian processes, this functional is similar to loss term (the negative log-likelihood). In addition, it also includes the log-determinant of £¦CE(d  ¢¡¤£¤¥"§© %¡¤£¦¥¨§©e

since it includes a regularization term (the negative log prior) and a

which measures the size of the space spanned by

'

W CEacb dX  ¢¡¤£¤¥"§© %¡¤£¦¥¨§©eGef e g !$# %

Note that any

"¨bd $d"DF¡   ¥

X0p42

. The quality functional is

' gYq$g¨ g 60¦ d g ¢



 ¢

# g Uhi

 D  ¢

esr

¦ d

  

hi X q q3

¢

¢

which does not have full rank will send (7) to

q q

(7) , and thus such cases

need to be excluded. When we fix

q q

t4 D

¢

minimum of

W CEacb dX  ¢¡¤£¦¥¨§©at %¡¤£¦¥¨§© e. ¢

¨"bd $dis¨DF¡attained

g  d 

h)'fh k '' g ¦

¢

td

, to exclude the above case, we can set

4 Dvu%xw 2ywe u

D

hi'jh k '' g 6

  g ' g,

we can see that 4r still leads to the overall

e X&p42' h`i

(8)

 g " g  g 6 which leads to with respect to

. Under the assumption that the minimum of

Other examples, such as cross-validation, leave-one-out estimators, the Luckiness framework, the Radius-Margin bound also have empirical quality functionals which can be arbitrarily minimized. The above examples illustrate how many existing methods for assessing the quality of a kernel fit within the quality functional framework. We also saw that given a rich enough useless for prediction purposes. This is yet another example of the danger of optimizing too much -- there is (still) no free lunch.

class of kernels x , optimization of W CEacb over x would result in a kernel that would be


3 A Hyper Reproducing Kernel Hilbert Space We now introduce a method for optimizing quality functionals in an effective way. The method we propose involves the introduction of a Reproducing Kernel Hilbert Space on (see Def 2.9 and Thm 4.2 in [8] and citations for more details).

the kernel X itself -- a "Hyper"-RKHS. We begin with the basic properties of an RKHS

Definition 3 (Reproducing Kernel Hilbert Space) Let 8 be a nonempty set (often called

the index set) and denote by £ a Hilbert space of functions   8  S

norm  §¦  

¡

1. 2.

X¡

) if there exists a function

X

Xf2has Xf2reproducingXf2G5¨

G¤¢the ¨¢&6£¥ 6)

spans £ , i.e. £

property

VA Xf2.

 



F6

 4X¢&6£¥  Xf2 8 9 856  q2

where

 

satisfying,

G"©¨ i

¡£¢ ¤¢ ¥

. Then £ is

(and the

8

:

called a reproducing¥ kernel Hilbert space endowed with the dot product S

¡

for all

i  £

G¢&6 q i

h h 

; in particular,



8

¡

is the completion of

 

.

The advantage of optimization in an RKHS is that under certain conditions the optimal solutions can be found as the linear combination of a finite number of basis functions, regardless of the dimensionality of the space £ , as can be seen in the theorem below.

 6 S

Theorem 4 (Representer Theorem) Denote by 

increasing function, by

8

a set, and by

i R2 8 9

function. Then each minimizer

2"2 q2

 S k 6 S

a strictly monotonic

¡

 £

¢d#¢ r 

Q r

 

 f 5'  656!!of $#5'0# $#`656¨60¦ h h 6

the regularized risk

q26 

2 # q2g

 g4 ( Xf2 $g"56



2  

admits a representation of the form . The above definition allows us to define an RKHS on kernels

introducing

8 T8 9 8

and by treating

X

as functions

X  8 

8 9 8  S

an arbitrary loss (9)

S , simply by

Definition 5 (Hyper Reproducing Kernel Hilbert Space) Let

let

X  8 

8 & 8 9 8

S , endowed with a dot product (and the norm

h8 h

: be a nonempty set and

(the compounded index¡¢set). Then the Hilbert space¡ £ of¥ functions ¢¥

X ¦ X X



X $8 9) 8  is called S

a Hyper Reproducing Kernel Hilbert Space if there exists a hyperkernel

with the following properties:

1. 2.

X¡

has the reproducing property

¨ ¨

.

spans £ , i.e. £

 i

8

VA X 2

 

¤¢6)  ¢&6£¥

¡

" 6

  ¢&6£¥ X X 2  Xf2 6

for all

X

X 2 X 2

8 Xthe

2

3. For any fixed

any fixed

 i

 ¢&6 q i

, the function

Xf2G5 & X 2 2

¡

6X 8

.

hyperkernel¨ is a kernel in¨ its second argument, i.e. for ¨

with 8 is a kernel.  G" 656

i X £ 45 i

, in particular,

What distinguishes £ from a normal RKHS is the particular form of its index set (

and the additional condition on

X

8  8

k

)

to be a kernel in its second argument for any fixed first

argument. This condition somewhat limits the choice of possible kernels. On the other are in the convex cone of . Analogously to the definition of the regularized risk functional

hand, it allows for simple optimization algorithms which consider kernels

X W £¤CE dX   %`eGTW CEacb dXh  h %ve

(5), we define the regularized quality functional:

 

a regularization constant and

  ¦ ¨ h h k  X

k

X

, which

i X £

h h kW`£¤CEisis

(10)

where ¨"! mization of

term G$# k X

¢

denotes the RKHS norm in £ . Mini-

less prone to overfitting than minimizing

WwCEacb , since the regularization

effectively controls the complexity of the class of kernels under consideration.

Regularizers other than G #

k X

h h k

are also possible. The question arising immediately from

(10) is how to minimize the regularized quality functional efficiently. In the following we show that the minimum can be found as a linear combination of hyperkernels.


Corollary 6 (Representer Theorem for Hyper-RKHS) Let £ be a hyper-RKHS and de-

note by  a strictly monotonic increasing function, by

an arbitrary quality functional. Then each minimizer

functional

W3dX   %`e

6

# g¤uh"4

¥

X

4 X 252

  ¦1¨ h h kX

Xf2G5  ¨ g&h g¨5Hh 6) G5 656 2 ¨

i 8 a set, and by W

£ of the regularized quality (11)

.

 6 S  d¢ r 

admits a representation of the form

Proof All we need to do is rewrite (11) so that it satisfies the conditions of Theorem 4. Let

. Then

h h k

on

X

via its values at

 ghWYdX.

has the properties of a loss function, as it only depends

Furthermore, G #

 gh & 2g¨5Hh 6     %e

X

k X

follows.

is an RKHS regularizer, so the representer

theorem applies and the expansion of

This result shows that even though we are optimizing over an entire (potentially infinite dimensional) Hilbert space of kernels, we are able to find the optimal solution by choosing nificantly larger than the number of kernels required in a kernel function expansion which makes a direct approach possible only for small problems. However, sparse expansion techniques, such as [9, 8], can be used to make the problem tractable in practice. 4 Examples of Hyperkernels Having introduced the theoretical basis of the Hyper-RKHS, we need to answer the quesaddress this question by giving a set of general recipes for building such kernels.

among a finite dimensional subspace. The dimension required ( ¤ k

) is, not surprisingly, sig-

tion whether practically useful X exist which satisfy the conditions of Definition 5. We

Example 4 (Power Series Construction) Denote by

by  

 

S S X

a positive semidefinite kernel, and

 g G ¤£ ¦¥ g g

a function with positive Taylor expansion coefficients

¨§

convergence radius



. Then for

X 2 5 6 ¨

X 2

k G"¨¦6  6 f2 

  ¢¡

g

¡ and

we have that

 f2UXj2 Xf2     45 656 ¨

© 

¨

6  656 ¥ £ g  g  6  656 2 Xf2 Xf2 ¨

¥ (12)

is a hyperkernel: for any fixed ,

a kernel itself (since ¨ ¡ 

X 2 " 6  2 2

¨ is a kernel if

, where

6i X 6£¥2G"¨ g

F6

X 26 2 2 t2

is a sum of kernel functions, hence it is

is). To show that



X 2



X 2

    6i  kX is6i!!&6

k



a kernel, note that

. 

X

¥

Example 5 (Harmonic Hyperkernel) A special case of (12) is the harmonic hyperkernel:

Denote by

set

 g r2 dX ea¨ ¨

kernel with



X 2

 for some

 2 dfe ¨ 5 6 ¨

6 g

¢



  (e.g., RBF kernels satisfy this property), and . Then we have

X  8@9Y85¨ d¢d d)e

6 ¥ £ g

¦¥



2 ¨ Xf2 Xf2  6  6"6 ¨

g

 dfe ¨ Xf2 Xj2 



dfe ¨

6  6 ¨ (13)

Xf245  G25e

¨ !#" 

Example 6 (Gaussian Harmonic Hyperkernel) For

X 252G5 2 ¨ 6i  5 6"6 ¨ ¨ ¨ ¨ ¨ 

6

%$

k h) ¨Eh k 6 e



norm of

, on d X to &(' ' )

u

dfe ¨ v2"e 2 e  #"  %$

; that is, the expression

k h)dfe  h k ¦ hi  h k 656

¨¨

h h k X

¨ ¨

e ¨ ¨ ¨

X  I9converges   .

, (14)

For ¨  converges to the Frobenius


f2

  ¢¡

6 Power series expansion

d¤¡

¢

¡

¡

¦

¢ ¡

¤©¨ ¤¦¥¡ ¦

P!$¦ ! ¦

¦

¦

k u

B¡

¢

¢

¦

B

 

"  ¡

Y` i £ £  ¡

¡

¡



¡

d¤ k¤!¥§

¡

¦

¤  ¦ ¤ kB

B§

P!!$¦ T! ¦



P!! ! T!

P! ¦ ¡ ¦

¦



r 1 1 r

¦ ¦

¦ ¤

0oV m VA

£

¦ ¤©¨

¤

¡

¤

k u

kB"#

r

We can find further hyperkernels, simply by consulting tables on power series of functions. Table 1 contains a list of suitable expansions. Recall that expansions such as (12) were mainly chosen for computational convenience, in particular whenever it is not clear which particular class of kernels would be useful for the expansion.

¢



e G2 dfe

h

6 ! ¦ P! ¦

¢

Table 1: Examples of Hyperkernels

Example 7 (Explicit Construction) If we know or have a reasonable guess as to which kernels could be potentially relevant (e.g., a range of scales of kernel width, polynomial



degrees, etc.), we may begin with a set of candidate kernels, say

X 2 5 6 ¨  ¥

¢

g4

X

, ...,

X ¢

and define

 g g  6 g  6) g  6%$ ¢ '& X 2 X 2

X 2 " 6¨

.

¨

X 2

¡ 

(15)

Clearly

2 X 2



   X 6i   6)!!)   6"6

is a hyperkernel, since



¢ ¢ X 2



k k X 2  2 2  6)  F6¨ £¥  , where 6 2 

5 An Application: Minimization of the Regularized Risk Recall that in the case of the Regularized Risk functional, the regularized quality optimiza-

tion problem takes on the form

! f!)(

g 

( Xf2 

g  g 56

u0

#

¤ g4

d ¥  g""'0g  g 6"60¦ 2 ¨ h h k ¦ ¨  h h k   X 

For

 

 g

loss function , the regularized quality functional (16) is convex in . The corresponding

regularized quality functional is:

W £¦CE dX   %`e W CEacb dX   %`e £¦CE¨£¦§D

  £¦CE¨£¦§D   ¦©¨ h h k X 

, the second term

q2 h h k 

(16)

 is a linear function of . Given a convex

X X

(17)

For fixed , the problem can be formulated as a constrained minimization problem in , and subsequently expressed in terms of the Lagrange multipliers ( . However, this minimum depends on , and for efficient minimization we would like to compute the derivatives with respect to . The following lemma tells us how (it is an extension of a result in [3] and we omit the proof for brevity):

X X

 i S

1

#

6i  g S

#

S

Lemma 7 Let and1 denote by

parameterized by . Let

denote by

h

 (61 2

 6 2 q2G¦1  

convex functions, where



is

X 

be the minimum of the following optimization problem (and

its minimizer):

minimize

!$# %

q241

2 98 k q2 2

the second argument of . Since the minimizer of (17) can be written as a kernel expansion (by the representer theorem for Hyper-RKHS), the optimal regularized quality functional can be written as (using

h '

6 subject to

iBA

 g 6 ¢ 2

and 8

§

k

d 32 54 § § for all c (18)

Then 6 7

 (61

1 1

6) 6

, where @ denotes the derivative with respect to


the soft margin loss and

g&h¡ 

¢

¢

 X 252$g"5Hh6i 2£ 

#

¤ g4

#

¨

d ¥ ¤ V

¢ 

"

75 6"6 ¢

dfe

 

:

¢

#

W £¤CE d ( 4   %we 

¦

£¤CE"£¦§D     ' g ¥

g¤uh"u¥u  4

¢

( ( 4 g h

¢

h¨u  u¢ 4 g&h¡  ¦1¨

¢

( 4

¨

h   ¢ gh¡ 

¢

¢¦¥ (19)

#

¢

g¤uh"u¥u  4

¢

4 4 gh  

¢

g&h¡ 

¢

¢

Minimization of (19) is achieved by alternating between minimization over ( for fixed 4 (this is a quadratic optimization problem), and subsequently minimization over 4 (with Low Rank Approximation While being finite in the number of parameters (despite the optimization over two possibly infinite dimensional Hilbert spaces £ and £ ), (19) still For an explicit expansion of type (15) we can optimize in the expansion coefficients of penalty on the expansion coefficients. Such an approach is recommended if there are few

4 gh¨§ ¢ to ensure positivity of the kernel matrix) for fixed ( .

presents a formidable optimization problem in practice (we have ¤ k

coefficients for 4 ).

X 2 X 2

g  6 g  6

¨

directly, which means that we simply have a quality functional with an

l

k

4©

terms in (15). In the general case (or if

¤

), we resort to a low-rank§ approximation, as §

6)¢&6 2

described in [9, 8]. This means that we pick from

fraction of terms which approximate

X   9  

on

X 2"2fg¨5Hh

with

d @ ¤ a small

sufficiently well.

6 Experimental Results and Summary Experimental Setup To test our claims of kernel adaptation via regularized quality functionals we performed preliminary tests on datasets from the UCI repository (Pima, Ionosphere, Wisconsin diagnostic breast cancer) and the USPS database of handwritten digits ('6' vs. '9'). The datasets were split into  training data and  test data, except for the USPS data, where the provided split was used. The experiments were repeated over 200 random 60/40 splits. We deliberately did not attempt to tune parameters and instead made the following choices uniformly for all four sets: data. We deliberately chose a too large value in comparison with the usual rules

¢ ¢





¢ ¢  The kernel width

$

was set to D $

Sd , where is the dimensionality of the

of thumb [8] to avoid good default kernels.

¨ wasadjustedsothat 

#

¢ ¢ ¢R¢

G

 d rd

(that is  in the Vapnik-style parameter-

ization of SVMs). This has commonly been reported to yield good results. ing adequate coverage over various kernel widths in (13) (small ¨ focus almost

¨ fortheGaussianHarmonicHyperkernelwaschosentobe

exclusively on wide kernels, ¨ close to





 throughout, giv

d

will treat all widths equally).

The hyperkernel regularization was set to ¨©  d D

¢ ¡

.



¢



We compared the results with the performance of a generic Support Vector Machine with

the same values chosen for validation. $

and ¨ and one for which ¨

$ had been hand-tuned using cross



Results Despite the fact that we did not try to tune the parameters we were able to achieve highly competitive results as shown in Table 2. It is also worth noticing that the number of hyperkernels required after a low-rank decomposition of the hyperkernel matrix contained typically less than 10 hyperkernels, thus rendering the optimization problem not much

more costly than a standard Support Vector Machine (even with a very high quality

approximation of

d D!

¢

) and that after the optimization of (19), typically less than " were being ¢

used. This dramatically reduced the computational burden. Using the same non-optimized parameters for different data sets we achieved results comparable to other recent work on classification such as boosting, optimized SVMs, and kernel target alignment [10, 11, 7] (note that we use a much smaller part of the data for training:




Train 25.2 2.0 13.4 2.0 5.7 0.8 2.1

 

 

 

£¦CE W £¦CE

Train 22.2 1.4 10.9 1.5 2.1 0.6 1.5

 

 

 

Data(size) pima(768) ionosph(351) wdbc(569) usps(1424) Test 26.2 3.3 16.5 3.4 5.7 1.3 3.4

 

 

 

Test 23.2 2.0 13.4 2.4 2.7 1.0 2.8

 

 

 

Best in [10, 11] 23.5 6.2 3.2 NA Tuned SVM 22.9 2.0 6.1 1.9 2.5 0.9 2.5

 

 

 

Table 2: Training and test error in percent

¢ ¢ only  rather than

¡

). Results based on

Ww£¦CE

are comparable to hand tuned SVMs

(right most column), except for the ionosphere data. We suspect that this is due to the small training sample. Summary and Outlook The regularized quality functional allows the systematic solution of problems associated with the choice of a kernel. Quality criteria that can be used include target alignment, regularized risk and the log posterior. The regularization implicit in our approach allows the control of overfitting that occurs if one optimizes over a too large a choice of kernels. A very promising aspect of the current work is that it opens the way to theoretical analyses research is devoted to working through this analysis and subsequently developing methods for the design of good hyperkernels. Acknowledgements This work was supported by a grant of the Australian Research Council. The authors thank Grace Wahba for helpful comments and suggestions. References [1] G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. Jordan. Learning the kernel matrix with semidefinite programming. In ICML. Morgan Kaufmann, 2002. [2] C. K. I. Williams. Prediction with Gaussian processes: From linear regression to linear prediction and beyond. In M. I. Jordan, editor, Learning and Inference in Graphical Models. Kluwer Academic, 1998. [3] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing kernel parameters for support vector machines. Machine Learning, 2002. Forthcoming. [4] G. Wahba. Spline Models for Observational Data, volume 59 of CBMS-NSF Regional Conference Series in Applied Mathematics. SIAM, Philadelphia, 1990. [5] K. Crammer, J. Keshet, and Y. Singer. Kernel design using boosting. In Advances in Neural Information Processing Systems 15, 2002. In press. [6] O. Bousquet and D. Herrmann. On the complexity of learning the kernel matrix. In Advances in Neural Information Processing Systems 15, 2002. In press. [7] N. Cristianini, A. Elisseeff, and J. Shawe-Taylor. On optimizing kernel alignment. Technical Report NC2-TR-2001-087, NeuroCOLT, http://www.neurocolt.com, 2001. [8] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT Press, 2002. [9] S. Fine and K. Scheinberg. Efficient SVM training using low-rank kernel representation. Technical report, IBM Watson Research Center, New York, 2000. [10] Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. In ICML, pages 148­146. Morgan Kaufmann Publishers, 1996. [11] G. R¨atsch, T. Onoda, and K. R. M¨uller. Soft margins for adaboost. Machine Learning, 42(3):287­320, 2001.

of the price one pays by optimizing over a larger set x of kernels. Current and future


