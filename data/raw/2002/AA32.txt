Adaptive Classification by Variational Kalman Filtering

Peter Sykacek Department of Engineering Science University of Oxford Oxford, OX1 3PJ, UK psyk@robots.ox.ac.uk Stephen Roberts Department of Engineering Science University of Oxford Oxford, OX1 3PJ, UK sjrob@robots.ox.ac.uk

Abstract We propose in this paper a probabilistic approach for adaptive inference of generalized nonlinear classification that combines the computational advantage of a parametric solution with the flexibility of sequential sampling techniques. We regard the parameters of the classifier as latent states in a first order Markov process and propose an algorithm which can be regarded as variational generalization of standard Kalman filtering. The variational Kalman filter is based on two novel lower bounds that enable us to use a non-degenerate distribution over the adaptation rate. An extensive empirical evaluation demonstrates that the proposed method is capable of infering competitive classifiers both in stationary and non-stationary environments. Although we focus on classification, the algorithm is easily extended to other generalized nonlinear models. 1 Introduction The demand for adaptive learning methods, e.g. for use in brain computer interfaces (BCIs) [15] has recently triggered a considerable interest in such algorithms. We may approach adaptive learning with algorithms that were designed for stationary environments and use learning rates to make these methods adaptive. These approaches can be traced back to early work on learning algorithms (e.g. [1]). A more recent account to this approach is [17], who combines the probabilistic method of sequential variational inference ([9]) and a forgetting factor to obtain an adaptive learning method. Probabilistic or Bayesian methods allow also for a completely different interpretation of adaptive learning. We may regard the model coefficients as latent (i.e. unobserved) states of a first order Markov process.

 ¢¡¤£¦¥¨§©¥¨§©  ¢¡¤£¦¥£¥§©!#"%$  ¢¡¤& ¥ ' ¥ (£ ¥ 

 ¢¡¤£ ¥¨§©  ¥¨§© 

(1)

The posterior distribution,

, at state )1032 summarizes all information ob-

tained about the model. This posterior and the conditional distribution,

 ¢¡¤£1¥£¦¥¨§©45"6$7 ,

represent the prior for the following state. The conditional distribution can be thought of as additive process or state noise with precision . Predictions are obtained by a probabilistic "

observation model

 8¡¤& ¥ ' ¥ 9£ ¥ 

. Using this model, we obtain an appropriate adaptation


rate by hierarchical Bayesian inference of the process noise precision . Equation (1) " suggests that we may interpret adaptive Bayesian inference as generalization of the well known Kalman filter ([12]). This view of adaptive learning has been used by [6], who use maximum likelihood II ([3]) for inference of the adaptation rate. Another generalization of Kalman filtering are the recently quite popular particle filters (e.g. [7]). Being Monte Carlo methods, particle filters have over Laplace approximations the advantage of much greater flexibility. This comes however at the expense of a higher representational and computational complexity. To combine the flexibility of particle filtering with the computational advantage of parametric methods, we propose a variational approximation (e.g. [11] , [2] and [8]) for inference of the Markov process in Equation (1). Unlike maximum likelihood II, the variational Kalman filter allows us to have a non degenerate distribution over the process noise precision. We derive in this paper a variational Kalman filter classifier and show with an extensive empirical evaluation that the resulting classifiers obtain excellent generalization accuracies both in stationary and non-stationary domains. 2 Methods 2.1 A generalized nonlinear classifier

extended Kalman filtering to obtain a Laplace approximation of the posterior over £ ¥ and

Classification is a prediction problem, where some regressor,

of a response variable

& ¥

' ¥

, predicts the expectation

. Since a   categorical polytomous solution is easily recovered

from 0 2 dichotomous solutions ([16], pages 44-45), we restrict all further discussions to dichotomos classification using ¡£¢¨2 responses. We thus have only one degree of free-

 

dom and predict the binary probability, ¤

parameters

£

¡¤& ¥¦¥ £ 5' ¥  ¡ , which depends on the model

. To obtain a flexible discriminant, we use a generalized nonlinear model, i.e.

a radial basis function (RBF) network ([14] and [5]), with logistic output transformation (Equation (3)).

§

¥ ¨ © 9' ¥  ¡ ' ¥ £¢ 2 ,

¡ &¥£ (£"! 9' ¥  ¨ ¡ 2$#&%('0)

§

¡9¡214& ¥ 0 2  ¥ (

§ ¥§©¨ ¥ £

¤

(2) (3)

The classifier has a nonlinear feature space

on

£ 

¥

which for reasons of adaptivity depends

¥

. We allow for Gaussian basis func-

or thin plate splines, i.e. 3H4

¡ ' ¥  ¨

and a linear mapping into latent space

¡ ' ¥  ¨

¡ ' ¥ED GF  4

tions, i.e. 4

' ¥ D IQPBR ¡9'354¥ D 

%('0) 06¡87@9BAC4 ¡

4 . Both basis functions are parameterized by their center locations 4 .

D

Since we want to have a simple unimodal posterior over model parameters, we update the

coefficients of the basis set

£ !

randomly according to a Metropolis Hastings kernel ([13])

and solve for the conditional posterior 2.2 The variational Kalman filter

 ¢¡¤£ £ !  ¥ 

analytically.

In order to ease discussion of adaptive inference, we illustrate the dependencies implied by Equation (1) in figure 1 as a directed acyclic graph (DAG). In accordance with Kalman

filtering, we assume a Gaussian posterior at time ) 0 2 with mean S £ ¥¨§©

and zero mean Gaussian state noise with isotropic precision

"%$

and precision T ¥¨§©

. Inference of

"

is based on

a "flat" proper Gamma prior specified by parameters U and V . In order to obtain reasonable posteriors over , we follow [10] and assume constant adaptation within a window of size and is thus based on maximizing a lower bound on the logarithmic model evidence of a windowed Kalman filter. Following these assumptions, we obtain the expression for the log evidence in Equation (4) by substituting the generalized nonlinear model (Equations (2) to (3)) into the formulation of adaptive Bayesian learning (1). We have then to make all

W

" . The proposed variational Bayesian approach ignores the anti-causal information flow


 

 I

 n-1

w w n-1 n

wn-1

 ¡ ¡ ¡ ¡ ¡  ¢¡¢¡¢¡¢¡¢¡¢  ¡ ¡ ¡ ¡ ¡  ¢¡¢¡¢¡¢¡¢¡¢  ¡ ¡ ¡ ¡ ¡  ¢¡¢¡¢¡¢¡¢¡¢  ¡ ¡ ¡ ¡ ¡  ¢¡¢¡¢¡¢¡¢¡¢  ¡ ¡ ¡ ¡ ¡  ¢¡¢¡¢¡¢¡¢¡¢  ¡ ¡ ¡ ¡ ¡  ¢¡¢¡¢¡¢¡¢¡¢

y n

observation n

Figure 1: This figure illustrates adaptive inference as a directed acyclic graph. The coef-

ficients of the classifier,

£ ¥

, are assumed to be Gaussian, following a first order Markov

process. The hyper parameter

" is given a Gamma prior specified by parameters U and V .

distributions explicit and integrate over all model coefficients, which is done analytically

over all prior states

£ ¥¨§©

.

£

¥8© £

IQPBR7¡ ¢¡ ¤£ 9 ¨ IQPBR

#

¥§¦©¨ 

¦

¡21  T ¥§© " $  #

§ §© §© §"!

(4)

% '0) 06¡ 7 9 ¡

¡

2 #&%('0)

V)( ¡  U "21

¡¤£¥ £¦¥¨§©   ¡ ¥¨§© " $ ¡ £ ¥ £¦¥¨§© (

0 S

¡(¡ 4&1 ¥ §©43

0 S 0 2 T  ¥ £ ¥ ( §

§©%$

# £ ¥'&

§©

#

# 0

§© §©

( %('0) 0 V ¡ "6 "65 $ 7

The structure of Equation (4) suggests that the approximate posterior 7

to be Gaussian and the approximate posterior 7

¡ "6

¡ £ ¥  can be chosen

can be chosen to be a Gamma distribu-

tion. These functional forms do however not simply result from a mean field approximation £

of the posterior as 7

¡ "668

¥¢©

7

¡¤£ ¥%

. In order to obtain the required conjugacy, we have

to use lower bounds for the probability of the target label, 2 # %('0)

for both T  ¥¨§© " $  # §© §© §9! §©

and %('0) 06¡87@9 ¡ 0 S  T

¡¤£ ¥ £ ¥¨§©  ¡ ¥¨§© " $02 ¡¤£ ¥ £ ¥¨§©and.

§© §© # 0 S (

¡ ¡9¡214&¨¥  ¥ £¥ ( §

§©

2.3 Variational lower bounds In order to achieve conjugacy with a Gaussian distribution, we use the lower bound for the logistic sigmoid proposed in [9] I P 6¡ ¡ & ¥  ¥ (£ ¥ 9A@

R ¤ §

0

¡ 4&1 ¥

G XW

¥

I

0 2 1 §  ¥ £ ¥ 0 0

¡   F §

SUTV Y `acb

¥ £ ¥ 1

IQPBR7¡21 IQPBRBDC PFE¡GHBPI 0 F

0

¥©efF

I

¥ 1RQ9Q

(5)

d


in which

2

 ¥ £ ¥  FI § ¥

are the variational parameters of a locally linear expansion in

¡9¡21 & ¥ 0

of every prediction contained in the window. In order to get expressions that

are conjugate with a Gamma distribution over the process noise precision , we derive two $

novel lower bounds. Assuming a -dimensional$ parameter vector

06¡87@9

IQPBR T ¥¨§© " $ 

#

§© §©

@

2

1

I P R "

¡ " 0

0 1

£ ¥

IQPBR  ¥

¡ T

$ 

" , we get

# §©

0 1 2 ¡

 !¡¢ $ £¡

S

#¦T

§© ¥ 

(6)

and

¡¤£ ¥ £ ¥¨§©   ¡ ¥§© "§© $# ¡§©£ ¥ £ ¥¨§©0 

0 S

06¡ 79

¡¤£ ¥ £¥¨§©   ¡#T ¥¨§©

0 S

¡

T

06¡ 79 0

which are expressions in

"

¡ "

§© 06¡ 79 §© §©

 ¡¤£ ¥ £¥¨§©  ¡£¡ ¥¨§© $ F ¡ £ ¥S £¦¥¨§© 

0 S

IQPBR ¡ " 

T # 0 S

¡

S $ ¡¤£¦¥§ §© §©0

@

£¥§©

(7)

and and thus conjugate with a Gamma distribution. Both

bounds are expanded in the identical parameter

expansions in

¡ " %¡ 0

¡

which is justified since both are linear

and maximization must thus lead to identical values. Using these £

lower bounds together with a mean field assumption, 7

¡ "  8 ¥ 8©

7

¡ £ ¥ 

, and the usual

Jensens inequalities, we immediately obtain a negative free energy as lower bound of the log evidence in Equation (4). For reasons of brevity we do not include this expression here. 2.4 Parameter updates In order to distinguish between the parameters of the prior and posterior distributions, we

¤

henceforth denote the latter with superscript . Inference requires to maximize the negative

W

free energy with respect to all variational parameters. These are the coefficients of the

Gaussian distributions, 7

¥

I

¡ £ ¥% W

, the parameters in the bounds of the logistic sigmoid,

, the coefficients of the Gamma posterior over the noise process precision, 7

the parameter in the Gamma conjugacy bounds, . Maximization with respect to 7

results in a Gaussian distribution with precision T

T ¥¤ ¨ ¡ ¥¨§© T

§©

# ¡

§© §© $

¡ ¥¨§© T §© #

# SUTV

¥¤ and mean S . £ ¥¤

8¡G 1 ¥

%W



F



§ § ¥ ¥

¡

¡ " ¡¤£

and ¥ 

(8)

£ ¥ ¨ ¡ ¥¤  S T Maximization with respect¤ to 7 U and scale parameter V$ . ¤

¤

W ¤

U ¨ 1

£

§© ¥ ¡

§© §©I

$ £ ¥¨§© S

4&1 ¥

# 1 0 2 § ¥ 5

¡ " 

results in a Gamma distribution with location parameter

V ¤ ¨

U # V # (9)

1

2

¥

¢ ¦¡ ¡ $ ¥¨§©  §© ¡ £ ¥ £ ¥¨§©   ¡£¡ ¥¨§© $ F ¤

S 0 S T #

§© §© § ¢ ¡9¡ ¥¤  ¡£¡ ¥¨§© $ F  T T # &

S

§© §

¥8©

#&T #

# ¡ £ ¥ £ ¥§©  S 0 S ¤ #

I

¥ S

According to [9], maximization with respect to

¥ ¨¨§ ¡ ¥ £ ¥  F ¤ S § I # §

leads to ¥ ¥¤ ¥

T § 7 (10)

Maximization with respect to the variational parameter

¤

¡ ¨ U

V ¤ 7

¡ leads for both bounds to

(11)


In order to allow the basis mapping in Equation (2) to track modifications in the input data from a Gaussian and accept the proposal according to probability W

distributions, we propose the perturbation ! ¨ £"! # , where is drawn

03ef ¥



£  §

 ¥

3   §

§©

§

 

© ¥8© ¥8©

£

0

"11  ¥ ¡£S

"11 3

§

¥¤ £ ¥¤  S

#&T #&T ¥¤  ¥ ¥¤  ¥

§© §

5

¡ £ ¢ ¢¤£ ¦¥¡ ¨§ 

¤  ¨

V

`a 2 % '0) % '0) ¥ 

"!F (!F

F



§© §©

§ F ¥

$#&%('0)2 $#&%('0)2

0

W

W

¥ ¡ £ ¥¤ £ ¥¤  S S

(12) 5

If we assume that the negative free energy describes the log evidence exactly, this is a We could thus represent the marginal posterior with random samples. For computational

Metropolis Hastings kernel ([13]) that leaves the marginal posterior  ¢¡¤£ !  ¥  invariant.

reasons however, we use the scheme only for random updates of

rameter inference will first propose a random update of

£ !

£ !

. An algorithm for pa-

and then iterate maximizations

according to Equation (8) to Equation (11) until we observe convergence of the negative free energy. Alternatively we can use a fixed number of iterations, for which our experiments suggest that 2 9 iterations suffice. 2.5 Model predictions Since we do not know the response when predicting, we have to sum the negative free

energy over

& ¥ ¤

. This results in a new expression for S which we obtain from Equation £ ¥

. Due to the dependency on (8) by dropping the term that depends on

with respect to 7

¡¤£ ¥ 

& ¥ ¥

I

, maximization

has to alternate with maximization with respect to

I

¥

, the latter

again being done according to Equation (10). Having reached convergence, we obtain an

approximate log probability for in Equation (5) with respect to 7

I P 6¡ ¡ &¥  ¥ ( ¨ R 54¤ §

& ¥

¡ £ ¥ 

by taking the expectation of the bound of the sigmoid

¥

I

0 2 1

and maximizing with respect to §  ¥ £ ¥

0 IQPBR ¡21  IQPBR B (PC 'E4G B I 0

. ¥ 1 Q9Q

¡214& ¥

0 7 (13)

Exponentiating the approximate log probabilities results in a sub probability measure over

&¥

with

!

 ¤



additional uncertainty about 3 Experiments

& ¥

2 , with the difference 2 0

!

 ¤ representing an

, introduced by the approximation of the logistic sigmoid.

 4 ¡¤& ¥§ ¥ 76  4 ¡&¥§ ¥ 

All experiments reported in this section use a model with 2 ¡ Gaussian basis functions with

precision A 4 ¨

§¡871

with precision

¨9 . For updating the basis, we use zero mean Gaussian random variates $

with isotropic precision

2 ¡B¡B¡ .¨ The$initial prior over parameters is a zero mean Gaussian iterations. The first experiment aims at obtaining a parametrization for U , V and the window actual "drift" of the problem. We use for that purpose the test set from the synthetic problem in [16]1. The samples of this balanced problem are reshuffled such that consecutive class labels differ. In order to get a non-stationarity, we swap the class labels in the second half compromise between fast tracking and high stationary accuracy. We are now ready to compare the algorithm with an equivalent static classifier using several public data sets and classification of single trial EEG which, due to learning effects in humans, is known to be non-stationary. In order to avoid that the model has an influence on

98T

¡87 2 . For maximizing the negative free energy we use 2 9

W

length, , that allows us to make inferences of the process noise

" that are insensitive to the

2

of the data. The results shown in figure 2 are obtained with U

W

We propose these settings together with a window size

¨

¨ ¡87 ¡ 2 and V ¨ 2 ¡ .

§

2 ¡ , because this is a good

1 This data set can be obtained at http://www.stats.ox.ac.uk/pub/PRNN/.


Simulations using =1e+003

350 300 250 200 150 100 50 0

window sz. 1 window sz. 5 window sz. 10 window sz. 15 window sz. 20

0 200 400 600 800 1000

Simulations using =1e+003

1

0.8

0.6

0.4

window sz. 1 window sz. 5 window sz. 10 window sz. 15 window sz. 20

0.2

0 0 200 400 600 800 1000

Figure 2: Results obtained on Ripleys' synthetic data set with swapped class labels after sample 500. The top graph shows the expected value of the precision of the noise process, for infering the adaptation rate). The bottom graph shows the instantaneous generalization   ¢¡" ¨

¤ 1 3 ¨

¤ ££

( for different window sizes (i.e. for different numbers of samples used

accuracy estimated in a window of size expectation 2 ¡ ¡ and variance 2 . §¦¡

¥B¡

. The prior over

"

is a Gamma distribution with

the results, we compare the generalization accuracy of the variational Kalman filter classifier (vkf) with an identical non-adaptive model. Inference of the static model is based on sequential variational learning ([9]). We obtain sequential variational inference (svi) from significance using McNemar's test, a method for analyzing paired results that is suggested in [16]. The comparison uses vehicle data2, satellite image data, Johns Hopkins University ionosphere data, balance scale weight and distance data and the wine recognition database, all taken from the StatLog database which is available at the UCI repository ([4]). The satellite image data set is used as is provided with 4435 samples in the training and 2000 samples in the test set. Vehicle data are merged such that we have 500 samples in the training and 252 in the test set. The other data were split into two equal sized data sets, which were both used as training and independent test sets respectively. We also use the pima diabetes data set from [16]3. Table 1 compares the generalization accuracies (in fractions) obtained with the variational Kalman filter with generalization accuracies obtained with classifiers are equal suggests that only the differences for the Balance scale and the Pima Indian data sets are significant, with either method being better in one case. Since the generalization accuracies of both methods are almost identical, we conclude that if applied to

our approach by setting " in Equation (1) to infinity. The comparisons are evaluated for

sequential variational inference. The probability of the null hypothesis, ¤

©¨, ¥ that both

2 3 Vehicle data was donated to StatLog by the Turing Institute Glasgow, Scotland. This data set can be obtained at http://www.stats.ox.ac.uk/pub/PRNN/.


Generalization results

vkf 0.87 0.81 0.89 0.76 0.77 0.97 svi 0.88 0.81 0.87 0.80 0.77 0.95

¤ ¥

0.41 0.29 0.03 0.03 0.42 0.25

Data sets J.H.U. ionosphere Satellite image Balance scale Pima diabetes Vehicle Wine

¨ 

Table 1: Generalization accuracies obtained with the variational Kalman filter (vkf) and sequential variational inference (svi).

Generalization results

vkf 0.69 0.71 0.69 0.64 svi 0.61 0.70 0.62 0.60

¤ ¥

0.00 0.39 0.00 0.00

Cognitive task rest/move, no feedback rest/move, feedback move/math, no feedback move/math, feedback

©¨

Table 2: Generalization accuracies obtained for classification of single trial EEG show that the variational Kalman filter significantly improves the results in three out of four cases.

stationary problems, we may expect the variational Kalman filter to obtain generalization accuracies that are similar to those of static methods. In order to assess the variational Kalman filter on a non-stationary problem, we apply it to classification of single trial EEG, a problem which is part of BCIs. The data for this experiment has been obtained from eight untrained subjects that perform two different task combinations (rest EEG vs. imagined movements and imagined movements vs. a mathematical task), once without and once with visual feedback. For one cognitive experiment each pair of tasks is repeated ten times. We classify on a one second basis an thus have per subject and task combination ¡B¡ samples. The regressors in this experiment are three reflection 1 coefficients (a parametrization of autoregressive models, see e.g. [18]). The comparison in table 2 reports within subject results obtained by two fold cross testing. Using half of the data, we allow for convergence of the methods before estimating the generalization accuracy on the other half of the data. The generalization accuracies in table 2 are averaged across subjects. We obtain in three out of four experiments a significant improvement with the variational Kalman filter. 4 Discussion We propose in this paper a parametric approach for adaptive inference of nonlinear classification. Our algorithm can be regarded as variational generalization of Kalman filtering which we obtain by using two novel lower bounds that allow us to have a non-degenerate distribution over the adaptation rate. Inference is done by iteratively maximizing a lower bound of the log evidence. As a result we obtain an approximate posterior that is a product of a multivariate Gaussian and a Gamma distribution. Our simulations have shown that the approach is capable of infering classifiers that have good generalization performance both in stationary and non-stationary domains. In situations with moderate sized latent spaces, e.g. in the BCI experiments reported above, prediction and parameter updates can be done in real time on conventional PCs. Although we focus on classification, the algorithm is


based on general ideas and thus easily applicable to other generalized nonlinear models. Acknowledgements We would like to express gratitude to the anonymous reviewers of this paper for their valuable suggestions for improving the paper. Peter Sykacek is currently supported by grant Nr. F46/399 kindly provided by the BUPA foundation. References [1] S.-I. Amari. A theory of adaptive pattern classifiers. IEEE Transactions on Electronic Computers, 16:299­307, 1967. [2] H. Attias. Inferring parameters and structure of latent variable models by variational Bayes. In Proc. 15th Conf. on Uncertainty in AI, 1999, 1999. [3] J. O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer, New York, 1985. [4] C.L. Blake and C.J. Merz. UCI repository of machine learning databases. http://www.ics.uci.edu/ mlearn/MLRepository.html, 1998. University of California, Irvine, Dept. of Information and Computer Sciences. [5] D. S. Broomhead and D. Lowe. Multivariable functional interpolation and adaptive networks. Complex Systems, 2:321­355, 1988. [6] J.F.G. de Freitas, M. Niranjan, and A.H. Gee. Regularisation in Sequential Learning Algorithms. In M. Jordan, M. Kearns, and S. Solla, editors, Advances in Neural Information Processing Systems (NIPS 10), pages 458­464, 1998. [7] A. Doucet, J. F. G. de Freitas, and N. Gordon, editors. Sequential Monte Carlo Methods in Practice. Springer-Verlag, 2001. [8] Z. Ghahramani and M. J. Beal. Variational inference for Bayesian mixture of factor analysers. In Advances in Neural Information Processing Systems 12, pages 449­455, 2000. [9] T. S. Jaakkola and M. I. Jordan. Bayesian parameter estimation via variational methods. Statistics and Computing, 10:25­37, 2000. [10] A.H. Jazwinski. Adaptive filtering. Automatica, pages 475­485, 1969. [11] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. In M. I. Jordan, editor, Learning in Graphical Models. MIT Press, Cambridge, MA, 1999. [12] R. E. Kalman. A new approach to linear filtering and prediction problems. Trans. ASME, J. Basic Eng., 82:35­45, 1960. [13] N. Metropolis, A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller. Equations of state calculations by fast computing machines. Journal of Chemical Physics, 21:1087­1091, 1953. [14] J. Moody and C. J. Darken. Fast learning in networks of locally-tuned processing units. Neural Computation, 1:281­294, 1989. [15] W. Penny, S. Roberts, E. Curran, and M. Stokes. EEG-based communication: a pattern recognition approach. IEEE Trans. Rehab. Eng., pages 214­216, 2000. [16] B. D. Ripley. Pattern Recognition and Neural Networks. Cambridge University Press, Cambridge, 1996. [17] Masa-aki Sato. Online model selection based on the variational Bayes. Neural Computation, pages 1649­1681, 2001. [18] P. Sykacek and S. Roberts. Bayesian time series classification. In T.G. Dietterich, S. Becker, and Z. Gharamani, editors, Advances in Neural Processing Systems 14, pages 937­944. MIT Press, 2002.

 


