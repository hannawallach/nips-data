Expected and Unexpected Uncertainty: ACh and NE in the Neocortex

Angela Yu Peter Dayan Gatsby Computational Neuroscience Unit 17 Queen Square, London WC1N 3AR, United Kingdom. feraina@gatsby.ucl.ac.uk dayan@gatsby.ucl.ac.uk Abstract Inference and adaptation in noisy and changing, rich sensory environments are rife with a variety of specific sorts of variability. Experimental and theoretical studies suggest that these different forms of variability play different behavioral, neural and computational roles, and may be reported by different (notably neuromodulatory) systems. Here, we refine our previous theory of acetylcholine's role in cortical inference in the (oxymoronic) terms of expected uncertainty, and advocate a theory for norepinephrine in terms of unexpected uncertainty. We suggest that norepinephrine reports the radical divergence of bottom-up inputs from prevailing top-down interpretations, to influence inference and plasticity. We illustrate this proposal using an adaptive factor analysis model.

1 Introduction Animals negotiating rich environments are faced with a set of hugely complex inference and learning problems, involving many forms of variability. They can be unsure which context presently pertains, cues can be systematically more or less reliable, and relationships amongst cues can change smoothly or abruptly. Computationally, such different forms of variability need to be represented, manipulated, and wielded in different ways. There is ample behavioral evidence that can be interpreted as suggesting that animals do make and respect these distinctions,5 and there is even some anatomical, physiological and pharmacological evidence as to which neural systems are engaged.29 Perhaps best delineated is the involvement of neocortical acetylcholine (ACh) in uncertainty. Following seminal earlier work,11,14 we suggested6,35 that ACh reports on the uncertainty associated with a top-down model, and thus controls the integration of bottom-up and top-down information during inference. A corollary is that ACh should also control the way that bottom-up information influences the learning of top-down models. Intuitively, this cholinergic signal reports on expected uncertainty, such that ACh levels are high when top-down information is not expected to support good predictions about bottom-up data and should be modified according to the incoming data. We6,35 formally demonstrated the inference aspects of this idea using a hidden Markov model (HMM), in which top-down uncertainty derives from slow contextual changes. In extending this quantitative model to learning, we found, surprisingly, that it violated our qualitative theory of ACh. That is, in the HMM model, greater uncertainty in the topdown model (ie a lower posterior responsibility for the predominant context), reported by higher ACh levels, leads to comparatively slower learning about that context. By contrast, we had expected that higher ACh should lead to faster learning, since it would indicate


that the top-down model is potentially inadequate. In resolving this conflict, we realized that, at least in this particular HMM framework, we had incorrectly fused different sorts of uncertainty. As a further consequence, by thinking more generally about contextual change, we also realized the formal need for a signal reporting on unexpected uncertainty, that is, on strong violation of top-down predictions that are expected to be correct. There is suggestive empirical evidence that one of many roles for neocortical norepinephrine (NE) is reporting this;29 it is also consonant with various existing theories associated with NE. In sum, we suggest that expected and unexpected uncertainty play complementary but distinct roles in representational inference and learning. Both forms of uncertainties are postulated to decrease the influence of top-down information on representational inference and increase the rate of learning. However, unexpected uncertainty rises whenever there is a global change in the world, such as a context change, while expected uncertainty is a more subtle quantity dependent on internal representations of properties of the world. Here, we start by outlining some of the evidence for the individual and joint roles of ACh and NE in uncertainty. In section 3, we describe a simple, adaptive, factor analysis model that clarifies the uncertainty notions. Differential effects induced by disrupting ACh and NE are discussed in Section 4, accompanied by a comparison to impairments found in animals. 2 ACh and NE ACh and NE are delivered to the cortex from a small number of subcortical nuclei: NE originates solely in the locus coeruleus, while the primary sources of ACh are nuclei in the basal forebrain (nucleus basalis magnocellularis, mainly targeting the neocortex, and medial septum, mainly targeting the hippocampus). Cortical innervations of these modulators are extensive, targeting all cortical regions and layers.9,30 As is typical for neuromodulators, physiological studies indicate that the effects of direct application of ACh or NE are confusingly diverse. Within a small cortical area, iontophoresis or perfusion of ACh or NE (or their agonists) may cause synatic facilitation or suppression, depending on the cell and depending on whether the firing is spontaneous or stimulusevoked; it may also induce direct hyperpolarization or depolarization.9,10,17 Direct application of either neuromodulator or its agonist, paired with sensory stimulation, results in a general enhancement of stimulus-evoked responses, as well as an increased propensity for experience-dependent reorganization of cortical maps (in contrast, depletion of either substance attenuates cortical plasticity).9 More interestingly, ACh and NE both seem to selectively suppress intracortical and feedback synaptic transmission while enhancing thalamocortical processing.8,12,13,15,17,18,20 Based on these roughly similar anatomical and physiological properties, cholinergic and noradrenergic systems have been attributed correspondingly similar general computational roles, such as modulating the signal-to-noise ratio in sensory processing.9,10 However, the effects of ACh and NE depletion in animal behavioral studies, as well as microdialysis of the neuromodulators during different conditions, point to more specific and distinct computational roles for ACh and NE. In our previous work on ACh,6,35 we suggested that it reports on expected uncertainty, ie uncertainty associated with estimated parameters in an internal model of the external world. This is consistent with results from animal conditioning experiments, in which animals learn faster about stimuli with variable predictive consequences.24 A series of lesion studies indicates cortical ACh innervation is essential for this sort of faster learning.14 In contrast to ACh, a large body of experimental data associates NE with the specific ability to learn new underlying relationships in the world, especially those contradicting existent knowledge. Locus coeruleus (LC) neurons fire phasically and robustly to novel objects encountered during free exploration,34 novel sensory stimuli,25,28 unpredicted changes in stimulus properties such as presentation time,2 introduction of association of a stimulus


with reinforcement,19,28,32 and extinction or reversal of that association.19,28 Moreover, this activation of NE neurons habituates rapidly when there is no predictive value or contingent response associated with the stimuli, and also disappears when conditioning is expressed at a behavioral level.28 There are few sophisticated behavioral studies into the interactions between ACh and NE. However, it is known that NE and ACh both rise when contingencies in an operant conditioning task are changed, but while NE level rapidly habituates, ACh level is elevated in a more sustained fashion.3,28 In a task designed to tax sustained attention, lesions of the basal forebrain cholinergic neurons induced persistent impairments,22 while deafferentation of cortical adrenergic inputs did not result in significant impairment compared to controls.21 One of the best worked-out computational theories of the drive and function of NE is that of Aston-Jones, Cohen and their colleagues.1,33 They studied NE in the context of vigilance and attention in well-learned tasks, showing how NE neurons are driven by selective task-relevant stimuli, and that, influenced by increased electrotonic coupling in the locus coeruleus, a transition from a high tonic, low phasic activity mode to a low tonic, high phasic activity mode is associated with increased behavioral performance through NE's suggested effect of increasing the signal to noise ratio of target cortical cells. This is a very impressive theory, with neural and computational support. However, its focus on well-learned tasks, means that other drives of NE activity (particularly novelty) and effects (particularly plasticity) are downplayed, and a link to ACh is only a secondary concern. We focus on these latter aspects, proposing that NE reports unexpected uncertainty, ie uncertainty induced by a mismatch between prediction and observation, such as when there is a dramatic change in the external environment. We do not claim that this is the only role of NE; but do see it as an important complement to other suggestions. 3 Inference and Learning in Adaptive Factor Analysis Our previous model of the role of ACh in cortical inference£ involved a generative scheme

with a discrete contextual variable ¤¦¥

 §¡©¨  ¡ ¢¨!"¨$#&%('

determined by

  ¡

 ¢¡

, evolving over time

, a discrete representational variable ) that was stochastically

, and a noisy observed9 variable¡ 0

¡!132 ¥ 46587@9BA

with¡ slow Markov dynamics (normal distribution). The

inferential task was to determine ) 0

¡  

because top-down ( ¢¡ ) and bottom-up (0 ) information have to be integrated.¡ Top down used to infer¤R¥¡ . We suggested, that ACh reports the uncertainty in the top-down context, information can be uncertain, in which case mainly bottom-up information 0 should be indicates the use of an approximation. ACh thereby reports expected uncertainty, as in the qualitative picture above, and appropriately controls cortical inference. However, if one

)

namely GIHQP  TS¡   0 D@D@DF0 ¡UV where  WS¡ ¤ is the most likely value of the context and P

¤¦¥

also considers learning, for instance if

is that

 WS¡

) ¡    ¡

is unknown, then the less certain the animal

¤¦¥

is the true contextual state, the less learning accorded to

) ¡   TS¡

. This is exactly

06CED@D@DF0 ; the HMM structure makes this interesting ¡

the opposite of what we should expect according to our empirically-supported arguments above. In fact, this way of viewing ACh is also not consistent with a more systematic reading5,16 of Holland & Gallagher's cholinergic results,14 which imply that ACh is better seen as a report of uncertainty in parameters rather than uncertainty in states. In order to model this more fitting picture of ACh, we need an explicit X¡Y¨model of parameter uncertainty. We constrain the problem to a single, implicit, context G . It is easiest (and perhaps more realistic) to develop the new picture in a continuous space, in which the parameter governing the known (hence the parameter¡ uncertainty, reported by ACh), and indeed can change. Again, ) stochastically specifies 0 through a normal distribution. ¡ Specifying how ` can change over time requires making an assumption about the nature ¡ of the context. In particular, novelty plays a critical role in model evolution. In general,

relationship between   ¡ ¨ G and ) is ` (scalar for convenience), which is imperfectly ¡ ¡


¥ "$%

y p(y; µ)

p(x|y)

15 10 5 0 -5 0

35 ' 70

"!#

x 20 10 0 -10 -10

"&#

 ¢¡£¥¤§¦

4 ¡   3 ¨©¤ 2 1 0 0

1  ¢¡£¦¤   2 3 4

0 10

Figure 1: Adaptive factor analysis model. (a) 2-layer adaptive factor analysis model, as specified by

Eq. 1 & 2. (b) Sample sequence of

IQP

2SR ,

IUT

2VC ,



WYX`2Sa#b

space, 99#e

,

IUc

(0) data points generated with parameters:

. 4 major shifts in e

1324)65 )87 99@2BADC#EFCHG ,

egf

,

2dR occurred (including initial

: hqp esrtCF) , u : hvp esrxw

99

, : y

), whose

projections into

hvp ert

h , are denoted as large circles.

8 projected into h

i hqp esr ,  :

. Small denotes



space and fall along the line

,  :   , :  h

. (c) Same sequence

viewed in



 space.

f f 998¥ 99#WX

: major shifts in , : e 

f 

h , where h

e  optimally projected into  space, ie

h29998sWX is the mean of the posterior distribution of

 

given only the

, :  ijf

h 

2k)5 )0 ,

rather than

observation

u e  f :i

 f 2kl65 7

h and flat priors. (d) Scatter plot of pdQeesf p vs. pd8Q hhg#p  itf . : 24CFw

, dashed line denotes parity. Larger i  f corresponds to greater reliance on

i  f 2kl5 7 for inferring    , while the intermediate value of exactly balances top-down uncertainty

with bottom-up uncertainty in the inference of §  .

we might expect small amounts of novelty, as models continually readjust, and we can allow for this by modeling continual small changes in ` . However, in order to allow for ¡ the possibility of macroscopic changes implied by substantial novelty (as reported by NE), which are of evident importance in many experiments, we must add a specific component to the model. The interaction between microscopic and macroscopic novelty is essentially

the interaction between ACh and NE. In¥ all,9¥q assume that

0

¥m

) ¡

9onp

C z

) `

¡ 5C

s

¡ 1 2 ¡ 1 2

¥ 9¥q y w

¡ 1 2 ¡ 1 2

¥ 9Hq y C ¤¦¥

{ u ¡ ¨

` ¡ ¨ `

¡U6rks!¡trvu ¡xw ¡

G H ¤¦¥ u ¡ ¨ V¨}| y G ¨

with the initial value `~

1 2 ¥ 9Hq y



C

(see Figure 1). We will see later that the binary

u ¡

(1) (2) is

the key to the model of NE; it comes from an assumption that there can occasionally (| G ) be dramatic changes in a model that force its radical revision. is another parameter; we assume it is known and fixed. Figure 1(b) & (c) shows a sample sequence of a particular setting of the model: the output 0 can be quite noisy, although there are clear underlying

mm

regularities in ) . At time , consider the case that we can make the approximation that `Y¡ £ where `v¡ is the estimate of ` and ¡

12 ¥`g¡ 9H ¡

 ,

¡

 is its variance (uncertainty), which is reported by

ACh. Here, the open circles indicate that this estimate is made before 0 is observed. We first consider how the ACh term influences inference about ) ; then go on to study learning.

For inference, it can easily be shownVthat )

q

¡ ¨ C q 5C 6 r  ¡



r mmqn p m

¡ 1 2

) ¡ ¨

¥%) 9q

¡ C ¡

 q

¡ , where

r  q 

C ¡ 5C ¡x 6 ` ¡ V r mm6n p

0 ¡ (3)

¡

whence the effect of ACh is exactly as in our qualitative picture. The more uncertainty Examples of just such effects can be found in Figure 1 (d). For learning,m start5C with the distribution of ` given 0 and assume 

(ie the larger ¡ ¡  ), the smaller the role of the top-down expectation `q¡ in determining )  . 

writing

  u

 ¨ ¥ 9o

y

y

. In this case,

q C

z

¨ mmm  q

n p y

 r , we get

¨ 2

¥m



V V V

 ¥

` 0     ¨ u 9  m  m W9 m  m 2

 0  G   g ~ r


with the obvious semantics for the product of two Gaussian distributions. This is almost exactly the standard form for a Kalman filter update for ` , and leads to standard results, such

as variance of the estimate going initiallyq like G

which balances the rate of change from C z

¡  £

, but ultimately reaching an asymptote

and the rate of new information from the 0 .

¡

¡

errors 0 H ` , but rather changes as a function only of time. Importantly,m in¡ this simple model, the uncertainty in ` does not depend on the prediction

However, if one takes into account the possibility that

for ` is the¤¦¥

two-component mixture

u y 

X 

m

W90

G

9  ¨



y

6 V

¥ V`

0

 ¨ #

m 

 ¥

 ` 0

!  ¨

¥m 

¤¦¥ u G  ¥

u

 ¨

G , then the posterior distribution

u  ¨ G

(4)

C



¡ mq

¢ 2

£

As

 

u m  Ur m

 g  

 ¨ GH

|  2

` 0 !  ¥ 9H

y

9

r q

~ z

C Qr | 2 ¥ 9o y r q

~ z

C r q {

increases, the¡ number of mixture components in the posterior distribution increases

exponentially as £ , since each setting of the H length binary string

£ u Fu  C D@D§D u ¡ is, barring

probability zero accidents, associated with a different component in the mixture. Thus, just as for switching state-space models,7 exact inference is impractical. One possibility would be to use a variational approximations.7,23 From the neural perspective of the involvement of neuromodulators, we propose an approximate learning algorithm in which signals reporting uncertainty, corresponding to our conceptual roles¥for¡U6ACh and, vation,  ) 0 . To control the exponential expansion in the hidden9H space,. we approximate

NE, control the interactions9 between the (approximate) distribution at H G ,  `

where ¤ ¡U6 ¦¥  ¨ 0 9 9 0 C D§D@D 0 ¡UV¨§ P

£  ¤ ¡U6

¥ , and bottom-up information relayed by the new obser-

¡ ¡



¥

the posterior P

¡UV ¤ ¡UV ¡UV"1 2 

is our best

¡U6 

` ¡UV ¡UV  ` ¡UV

uncertainty in our estimate ` 

estimate of `

¡U6`

after observing 0

¡UV

as a single£Gaussian, `

, and

¡UV

. In general, we might consider the NE level © as reporting

, corresponding to the ACh¡ level, is the

the posterior responsibility of the

T¡!¨ u

G componentof the equivalentmixture of equation 4.

Even more straightforwardly, we¡ can measure¡ a Z-score, namely prediction¡ errorm scaledand value  , ie 0 is unlikely to have come from an unmodified version of the current commodified version of Kalman filter:

uncertainty in our q estimates:r © ¡ ¨ mm0mm  ¡U6r 5C q C np

  r z 

¨  ¡ 0 H 0  

y

0 H0 , where 0

¡ ¡ ¨ mq

`

¡UV

, assuming that

u ¡

¡ . Whenever © exceeds a threshold

u

ponent, we assume

¡ ¨ G . Otherwise, u ¡ ¨ y . Now the learning problem reduces to a

prediction variance about ` Kalman gain correction variance estimated mean ¡ (5) (6) (7) (8)  ¡ ¨ ¡ ¨ ¡ ¨ ¡ ¨ 

 ¡U6r

m 



C

z

 ¡

r

mm ¡

  ¡ V

¡6 T¡ ¨

by



`



q mm

¡

r m 

mmmm  q



5C r n p 



`

¡

¡UVH 



¡UV



r



¡ ¡ 

0 H `

mmY ¡U6 

The difference from the conventional Kalman T¡

sition noise variance,

 ¡ u

, which depends on :

filter is they additionaly componentC of the tran&¡&¨ u q

, if

u ¡&¨

if¡

¡ ¨  ¡&¨

¡ { G .

Closer examination indicates that the ACh ( ) and NE (© ) signals have the desired se-



mantics. In the learning algorithm, large uncertainty about the mean estimate,



in large Kalman gain,

¡ , which causes a large shift in ` ¡  

. Large

¡

¡

, results

also weakens the

to¡ faster ylearning: large © means u

influence of top-down information in inference as in equation 3.q High NE levels also yleads NE levels also enhances the¡ dominance of bottom-up information in inference via its inreciprocal relationships between ACh and NE: higher ACh leads to smaller normalized prediction errors and therefore less active NE signalling, whereas greater NE would generally increase estimator uncertainty and thus ACh level. rameters as in Figure 1), and the estimated means using our approximate learning algorithm. The learning algorithm is clearly able to adjust to major changes in ` , although ¡

¡

u

¡ ¨

been ), ultimately resulting in a large Kalman gain and thus fast shifting of `  . High

G , which causes  ¡ ¨



(rather than  ¡ ¨ had

teractions with ACh: large © promotes large  ¡ . Note that this system predicts interesting

Figure 2(a) shows an example sequence of `  9 ` C D§D@D generated from a model (same pa9


¢¤£¦¥ ¢¥

5000 4000 3000 2000 1000 0 0

¢¨§©¥

15 10 5 0 -5 0 5 0 0

35 70

35   70 3 ¡ 10

Figure 2: Approximate learning algorithm. (a) : 

means



e   . General patterns of

: NE,  

e 

ht projected into

e  

 space, : actual  es  , : estimated

l . (b)   are captured by , though details may differ. k2 

: ACh,  : . ACh level rises whenver



  detected to be C (NE level exceeds ) and then 

smoothly falls. NE level is constant monitor of prediction error. (c) Mean summed square error over



)#) -step sequences trials (  0e s es  ) , as a function of . Error bars show standard errors of

the means over

error l6CFl

70)) trials. Mean square error for optimal 

 2}l

is §(0l , compared to exact learning

(lower line). Model parameters were same as in Figure 1.

more subtle changes in ` can miss¡ detection, such as the third large shift in ` . Figure 2(b) of `  . However, whereas NE is a constant monitor of prediction errors and fluctuates accordingly with every data point, ACh falls smoothly and predictably, and only depends on the observations when global changes in the environment have been detected. Figure 2(b) shows ladle-shaped dependence of estimation error, `  H ` , on the threshold value  . For   the particular setting of model parameters used here, learning is optimal for  around . 4 Differential Effects of Disrupting ACh and NE Signalling The different roles of the NE (© ) and ACh ( ) can be teased apart by disrupting each and observing the subsequent effects on learning in our model. We will examine several impairments observed in experimental manipulation of ACh or NE levels in animals. Of course, the complete experimental circumstances are far more complicated; we consider the general nature of the effects.

shows higher ACh ( ) and NE (© ¡

¡  levels both correspond to fast learning, ie fast shifting

¡  ¡

different manipulations of © and ¡  ¡ that disrupt normal learning, and relate the results to

First, we simulate depletion of cortical NE by setting ©

Figure 3(a). By ruling out the possibility of

u ¡ ¨

¡ ¨

y

. An example is shown in

¡

¡

G , the system is unableyy to cope with

abrupt, global changes in the world, ie when ` shifts. Mean error over

y

setting as in Figure 2(c)) without NE is

"

! trials (same

£$# , more than an order of magnitude larger than

full approximate learning (%&#' ) and exact learning ( G( ). This is consistent with the large errors of similar magnitude in Figure 2(c) for very large  , which effectively blocks the NE system from reporting global changes. However, as long as the underlying parameters remain the same, ie ` does not change greatly, the inference process functions normally, as tal observations: NE-lesioned animals are impaired in learning changes in reinforcement contingencies,26,28 but have little difficulty doing previously learned discrimination tasks.21 Figure 3(b) shows severe damage is caused the learning algorithm, but the inference sympin estimates of ` , thus making adaptation of that estimate slow, similar to NE depletion. ically differs from the prediction (which is often, since ` is slow to adapt and leaves little room for variance), and thus to base inference of ) directly on the bottom-up information ¡

¡ we can see in the first

y

steps in Figure 3(a). These results are consistent with experimen-



We can also simulate depletion of cortical ACh by setting ¡ to a small constant value.



toms are distinct from NE depletion. Permanently small ¡ corresponds to over-confidence

¡

However, because the NE system is still intact, the system is able to detect when

¡ ) dramat-

¥ ) 0 . Thus, inference is less impaired than learning, which has also been observed in ¡  ¡ 


¡§¦¨¤

10 0

¡£¢¥¤

1 35 70

10 0

1 35   70

Figure 3: Disrupting NE and ACh signals. (a) NE signal set to . (b) ACh signal set to

  :e ,   :8 , :projection of   hs into  space. Learning of

) © 

)65Cow .  :© , 

is poor in both manipulations, but

inference in ACh-depletion is less impaired. ACh-lesioned animals.31 Moreover, the system exhibits a peculiar hesitancy in inference, ie constantly switching back and forth between relying on top-down estimate of ) , based severe when the new ` is similar to the previous one, which can be thought of as a form of interference. Interestingly, hippocampal cholinergic deafferentation in animals also bring about a stronger susceptibility to interference compared with controls.10 time. The effect of¡ these two manipulations are similar, both cause the estimation of ` and inference of ) to base strongly on the observation 0 (data not shown). The perfordecrements in the estimation of ` and inference about ) are functions of the output gencies. Unfortunately, directly relevant experimental data is scarce. Administration of cholinergic agonists in the cortex has failed to induce impairments in tasks with changing contingencies, consistent with our predictions. However, to our knowledge, cholinergic and noradrenergic agonists have not yet been administered in combination with systematic manipulation of variability in the predictive consequences of stimuli and so the validity of our predictions remains to be tested. 5 Discussion We have suggested that ACh and NE report expected and unexpected uncertainty in representational learning and inference. As such, high levels of ACh and NE should both correspond to faster learning about the environment and enhancement of bottom-up processing in inference. However, whereas NE reports on dramatic changes, ACh has the subtler role of reporting on uncertainties in internal estimates. We formalized these ideas in an adaptive factor analysis model. The model is adaptive in that the mean of the hidden variable is allowed to alter greatly from time to time, capturing the idea of a generally stable context which occasionally undergoes large changes, leading to substantial novelty in inputs. As exact learning is intractable, we proposed an approximate learning algorithm in which the roles for ACh and NE are clear, and demonstrated that it performs learning and inference competently. Moreover, by disrupting one or both of ACh and NE signalling systems, we showed that the two systems have interacting but distinct patterns of malfunctioning that qualitatively resemble experimental results in animal studies. There is no single collection of definitive experimental studies, and teasing apart the effects of NE and ACh is tricky, since they appear to share many properties. Our model helps understand why, and should also help with the design of experiments to clarify the relationship.

mm  6

on ` and bottom-up estimate, based on ¡ ¡ V  m  m

¡  0 

¡ . This tendency is particularly



Saturation of ACh and NE are also easy to model, by setting ¡

and © very high all the¡ ¡

¡

mancenxp noise,

¡ ¡

9Hq 5C

in our model, and do not worsen when there are global changes in contin-


Of course, the adaptive factor analysis model is overly simple in many ways. In particular, it only considers one particular context; and so refers all the uncertainty to the parameters of that context. This is exactly the complement of our previous model,6,35 which referred all the uncertainty to the choice of context rather than the parameters within each context. The main conceptual difference is that the idea that ACh reports on the latter form of contextual uncertainty sits ill with the data on how uncertainty boosts learning; this fits better within the present model. Given multiple contexts, which could formally be handled within the framework of a mixture model, the tricky issue is to decide whether the parameters of the current context have changed, or a new (or pre-existing) context has taken over. Exploring this is important work for the future. More generally, a thoroughly hierarchical and non-linear model is clearly required as at a minimum as a way of addressing some of the complexities of cortical inference. Acknowledgement We are very grateful to Zoubin Ghahramani and Maneesh Sahani for helpful discussions. Funding was from the Gatsby Charitable Foundation and the NSF. References [1] Aston-Jones, G, Rajkowski, J, & Cohen, J (1999) Biol Psychiatry 46:1309-1320. [2] Carli, M, Robbins, TW, Evenden, JL, & Everitt, BJ (1983) Behav Brain Res 9:361-80. [3] Dalley, JW et al. (2001) J Neurosci 21:4908-4914. [4] Daw, ND, Kakade, S, & Dayan, P (2001) Neural Networks 15:603-616. [5] Dayan, P, Kakade, S, & Montague, PR (2000) In NIPS 2000:451-457. [6] Dayan, P & Yu, A (2002) In NIPS 2002. [7] Ghahramani, Z & Hinton, G (2000) Neural Computation 12:831-64. [8] Gil, Z, Conners, BW, & Amitai, Y (1997) Neuron 19:679-86. [9] Gu, Q (2002) Neuroscience, 111:815-835. [10] Hasselmo, ME (1995) Behavioural Brain Research 67:1-27. [11] Hasselmo, ME, Wyble, BP & Wallenstein, GV (1996) Hippocampus 6:693-708. [12] Hasselmo, ME & Cekic, M (1996) Behavioural Brain Research 79: 153-161. [13] Hasselmo, ME et al (1997) J Neurophysiology 78:393-408. [14] Holland, PC & Gallagher, M (1999) Trends In Cognitive Sciences 3:65-73. [15] Hsieh, CY, Cruikshank, SJ, & Metherate, R (2000) Brain Research 880:51064. [16] Kakade, S & Dayan, P (2002) Psychological Review 109:533-544. [17] Kimura, F, Fukuada, M, & Tsumoto, T (1999) Eur. Jour. of Neurosci. 11:3597-3609. [18] Kobayashi, M et al. (1999) European Journal of Neuroscience 12:264-272. [19] Mason, ST & Iversen, SD (1978) Brain Res150:135-48. [20] McCormick, DA (1989) Trends Neurosci 12:215-221. [21] McGaughy, J, Sandstrom, M, et al (1997) Behav Neurosci 111:646-52. [22] McGaughy, J & Sarter, M (1998) Behav Neurosci 112:1519-25. [23] Minka, TP (2001) A Family of Algorithms for Approximate Bayesian Inference. PhD, MIT. [24] Pearce, JM & Hall, G (1980) Psychological Review 87:532-552. [25] Rajkowski, J, Kubiak, P, & Aston-Jones, G (1994) Brain Res Bull 35:607-16. [26] Robbins, TW (1984) Psychological Medicine 14:13-21. [27] Robbins, TW, Everitt, BJ, & Cole, BJ (1985) Physiological Psychology 13:127-150. [28] Sara, SJ, Vankov, A, & Herve, A (1994) Brain Res Bull 35:457-65. [29] Sara, SJ (1998) Comptes Rendus de l'Academie des Sciences Serie III 321:193-198. [30] Sarter, M, Bruno, JP (1997) Brain Research Reviews 23:28-46. [31] Sarter, M, Holley, LA, & Matell, M (2000) In SFN 2000 abstracts. [32] Sullivan, RM (2001) Ingegrative Physiological and Behavioral Science 36:293-307. [33] Usher, M, et al. (1999) Science 5401:549-554. [34] Vankov, A, Herve-Minvielle, A, & Sara, SJ (1995) Eur J Neurosci109:903-911. [35] Yu, A & Dayan, P (2002) Neural Networks 15:719-730


