Half-Lives of EigenFlows for Spectral Clustering

Chakra Chennubhotla & Allan D. Jepson Department of Computer Science, University of Toronto, Canada M5S 3H5 chakra,jepson @cs.toronto.edu Abstract Using a Markov chain perspective of spectral clustering we present an algorithm to automatically find the number of stable clusters in a dataset. The Markov chain's behaviour is characterized by the spectral properties of the matrix of transition probabilities, from which we derive eigenflows along with their halflives. An eigenflow describes the flow of probability mass due to the Markov chain, and it is characterized by its eigenvalue, or equivalently, by the halflife of its decay as the Markov chain is iterated. A ideal stable cluster is one with zero eigenflow and infinite half-life. The key insight in this paper is that bottlenecks between weakly coupled clusters can be identified by computing the sensitivity of the eigenflow's halflife to variations in the edge weights. We propose a novel EIGENCUTS algorithm to perform clustering that removes these identified bottlenecks in an iterative fashion.

 

¡

1 Introduction We consider partitioning a weighted undirected graph-- corresponding to a given dataset-- into a set of discrete clusters. Ideally, the vertices (i.e. datapoints) in each cluster should be connected with high-affinity edges, while different clusters are either not connected or are connnected only by a few edges with low affinity. The practical problem is to identify these tightly coupled clusters, and cut the inter-cluster edges. Many techniques have been proposed for this problem, with some recent success being obtained through the use of spectral methods (see, for example, [2, 4, 5, 11, 12]). Here we use the random walk formulation of [4], where the edge weights are used to construct a Markov transition probability matrix, . This matrix defines a random walk on the graph to be partitioned. The eigenvalues and eigenvectors of provide the basis for deciding on a particular segmentation. In particular, it has been shown that for weakly coupled clusters, the leading eigenvectors of will be roughly piecewise constant [4, 13, 5]. This result motivates many of the current spectral clustering algorithms. For example in [5], the number of clusters must be known a priori, and the -means algorithm is used on the

¢ ¢

¢

£

£ ¢

£ £

leading eigenvectors of in an attempt to identify the appropriate piecewise constant £ ¢

regions. In this paper we investigate the form of the leading eigenvectors of the Markov matrix

¢

.

Using some simple image segmentation examples we confirm that the leading eigenvectors of are roughly piecewise constant for problems with well separated clusters. However, we observe that for several segmentation problems that we might wish to solve, the coupling between the clusters is significantly stronger and, as a result, the piecewise constant approximation breaks down.

¢


Unlike the piecewise constant approximation, a perfectly general view is that the eigenvectors of determine particular flows of probability along the edges in the graph. We refer to these as eigenflows since they are characterized by their associated eigenvalue   , which specifies the flow's overall rate of decay. Instead of measuring the decay rate in terms of the eigenvalue   , we find it more convenient to use the flow's halflife ¡ , which is simply the particular eigenflow to half its initial value. Note that as   approaches § the half-life defined by ¢ £¢¤¦¥¨§© . Here ¡ is the number of Markov chain steps needed to reduce approaches infinity. From the perspective of eigenflows, a graph representing a set of weakly coupled clusters produces eigenflows between the various clusters which decay with long halflives. In contrast, the eigenflows within each cluster decay much more rapidly. In order to identify clusters we therefore consider the eigenflows with long halflives. Given such a slowly decaying eigenflow, we identify particular bottleneck regions in the graph which critically restrict the flow (cf. [12]). To identify these bottlenecks we propose computing the sensitivity of the flow's halflife with respect to perturbations in the edge weights. We implement a simple spectral graph partitioning algorithm which is based on these ideas. We first compute the eigenvectors for the Markov transition matrix, and select those with long halflives. For each such eigenvector, we identify bottlenecks by computing the sensitivity of the flow's halflife with respect to perturbations in the edge weights. In the current algorithm, we simply select one of these eigenvectors in which a bottleneck has been identified, and cut edges within the bottleneck. The algorithm recomputes the eigenvectors and eigenvalues for the modified graph, and continues this iterative process until no further edges are cut. 2 From Affinities to Markov Chains Following the formulation in [4], we consider an undirected graph  with vertices  , for that is, 0 (') ¥20 )%' . A Markov chain is defined using these affinities by setting the transition sents the affinity of vertices   and  ) . The edge affinities are assumed to be symmetric, probability 34(') from vertex 5) to vertex   to be proportional to the edge affinity, 06(') .

¢

 ¥§ "!#!#!#%$ , and edges & (') with non-negativeweights 0 (') . Here the weight 0 1') repre-

That is, 341')7¥98A@CB 01') where 8)D¥FEFG ) IH

B

01') givesthenormalizingfactorwhichensures

matrix V , with elements 061') , and the transition probability matrix W¥YX13`1')a is given by E GIH 3P(')P¥Q§ . In matrix notation, the affinities are represented by a symmetric RTSUR ¢

b¥FVdc @eB fcg¥ diagX(8 #!"!#!#h8 ai!

B is not in general symmetric. G

¢

This transition probability matrix

(1) ¢

B Notice that the RpSPR matrix

¢

defines the random walk of a particle on the graph

Then, the probability of the particle being initially at vertex ) and taking edge &(') is  . Supposetheinitialprobabilityoftheparticlebeingat vertex) isqsr) ,fortu¥Y§v#!"!#!#%R . 341')wqsr) . In matrix notation, the probability of the particle ending up anyq7rx of, the vertices For analysis it is convenient to consider the matrix ¥Fc @CB% c B% , which is similar to

`¥yX( %"##w a after one step is given by the distributionqx B ¥ x

XqB #!"!#!iqG a . B G

x where q¥

¢

 

(where c is as given in Eq. (1)). The matrix  therefore has the same spectrum as

¢

of  must correspond to an eigenvector c B%

¢ ¢

and any eigenvector

x  x

of with the same

eigenvalue. Note that ¥gc @CB% c Bw ¥c @eBw Vdc @eB c B% ¥c @eBw Vdc @eBw , and

¢

therefore  is a symmetric RSPR matrix since V is symmetric while c is diagonal. ¢

The advantage of considering the matrix  over

is that the symmetric eigenvalue prob-

     

¢

lem is more stable to small perturbations, and is computationally much more tractable. Since the matrix  is symmetric, it has an orthogonal decomposition of the form: ¥Fdef (2)


(a) (b) (c) (d) (e)

Figure 1: (a-c) Three random images each having an occluder in front of a textured background. (d-e) A pair of eye images.

¡ xBx

#"#C

x

¢

ues   h sv##"Ch  where g¥

£x £  B G ¢ G

are the eigenvectors and  is a diagonal matrix of eigenval-

sorted in decreasing order. While the eigenvectors have unit length,

¥Y§ ,theeigenvaluesarerealandhaveanabsolutevalueboundedby1, ¢  ¢ ¦§ .



¥¤

The eigenvector representation provides a simple way to capture the Markovian relaxation process [12]. For example, consider propagating the Markov chain for ¡ iterations. The transition matrix after ¡ iterations, namely ¤ , can be represented as: Therefore the probability distribution for the particle being at vertex   after ¡ steps of the random walk, given that the initial probability distribution was qDr , is qd¤ ¥ ¦¤ q r ¥ c B% f¤ rx , where r ¥  e c @CB% qdrx provides the expansioncoefficients of the initial having non-zero weights, it is convenient to interpret the Markovian relaxation process as

¤ ¥9c Bw   ¤  e c @CB% !

 

¢

(3) ¢

 ¦x ¦x x x x

 ¢

distribution q7r in terms of the eigenvectors of  . As ¡

the stationary distribution ,

 ¥ x x ¨§©

, the Markov chain approaches

e . Assumingthegraph isconnectedwithedges

x ¦)  x),

¢

perturbations to the stationary distribution, q ¤¦¥

associated with the stationary distribution 3 EigenFlows

x x

x

 )¤ where   ¥ § is

and ) ¥2c Bw ) .

E2G x)%H

B

Let qdr be an initial probability distribution for a random particle to be at the vertices of x the graph  . By the definition of the Markov chain, recall that the probability of making conditional probability of taking edge & (') given that the particle is at vertex  ) , namely the transition from vertex  ) to   is the probability of starting in vertex  ) , times the The net flow of probability mass along edge &1') from 5) to  is therefore the difference 3 1') qsr) . Similarly,theprobabilityofmakingthetransitioninthereversedirectionis3 )h'qAr . is given by (') X qDr5a , where (') X q7r5a is the X  ta -element of the RpS R matrix 341')wqsr) 3)h'qAr .x Itthenfollowsthatthenetflowofprobabilitymass fromvertex ) to 

 ! x

x

Notice that

e ¥ !  !

!¨¥¡"  " " e for

!uXx !

q r af¥ diagX q r a x

 x diagX q r a ef!

!

¢ ¢

¥ (4) is antisymmetric (i.e.

). This expresses the fact that the flow )%' from   to  ) is just the opposite sign

¢

diagX q7ra , and therefore

!

of the flow in the reverse direction. Furthermore, it can be shown that



stationary distribution . Therefore, the flow is caused by the eigenvectors ) with  ) For illustration purposes we begin by considering an ensemble of random test images formed from two independent samples of 2D Gaussian filtered white noise (see Fig. 1a-c). of second sample is used for the foreground region. A small constant bias is added to the foreground region.

and hence we analyze the rate of decay of these eigenflows

One sample is used to form the

('§ (' S§ backgroundimage,andacropped S fragment ) 0)

!uXx)a.

!uXx $#

a¥ for any

&%¥§,


(a) (b) (c)

Figure 2: (a) Eigenmode (b) corresponding eigenflow (c) gray value at each pixel corresponds to the maximum of the absolute sensitivities of all the weights on edges connected to a pixel (not including itself). Dark pixels indicate high absolute sensitivities.

A graph clustering problem is formed where each pixel in a test image is associated with a vertex of the graph  . The edges in  are defined by the standard 8-neighbourhoodof each pixel (with pixels at the edges and corners of the image only having 5 and 3 neighbours, respectively). The edge weight between neighbouring vertices  ) and   is given by the

affinity 0 (') ¥

at pixel



and

¨x

¡ £¢¥¤©  §¦X  ¦X¨x) ©

X  a

¨x

awa ©6X a , where X a is the test image brightness



¥

 )



¡

 

 

¦

is a grey-level standard deviation. We use

¨x©

, where is the median

absolute difference of gray levels between all neighbouring pixels and

¥Y§v! .

This generative process provides an ensemble of clustering problems which we feel are representative of the structure of typical image segmentation problems. In particular, due to the smooth variation in gray-levels, there is some variability in the affinities within both foreground and background regions. Moreover, due to the use of independent samples for the two regions, there is often a significant step in gray-level across the boundary between the two regions. Finally, due to the small bias used, there is also a significant chance for pixels on opposite sides of the boundary to have similar gray-levels, and thus high affinities. This latter property ensures that there are some edges with significant weights between the

two clusters in the graph associated with the foreground and background pixels.

In Figure 2 we plot one eigenvector, of the matrix

x,

along with its eigenflow, uX  a .

!

x

¢

Notice that the displayed eigenmode is not in general piecewise constant. Rather, the eigenvector is more like vibrational mode of a non-uniform membrane (in fact, they can be modeled in precisely that way). Also, for all but the stationary distribution, there is a significant net flow between neighbours, especially in regions where the magnitude of the spatial gradient of the eigenmode is larger. 4 Perturbation Analysis of EigenFlows As discussed in the introduction, we seek to identify bottlenecks in the eigenflows associated with long halflives. This notion of identifying bottlenecks is similar to the well-known max-flow, min-cut theorem. In particular, for a graph whose edge weights represent maximum flow capacities between pairs of vertices, instead of the current conditional transition probabilities, the bottleneck edges can be identified as precisely those edges across which the maximum flow is equal to their maximum capacity. However, in the Markov framework, the flow of probability across an edge is only maximal in the extreme cases for which the initial probability of being at one of the edge's endpoints is equal to one, and zero at the other endpoint. Thus the max-flow criterion is not directly applicable here. Instead, we show that the desired bottleneck edges can be conveniently identified by considering the sensitivity of the flow's halflife to perturbations of the edge weights (see Fig. 2c). Intuitively, this sensitivity arises because the flow across a bottleneck will have fewer alternative routes to take and therefore will be particularly sensitive to changes in the edge weights within the bottleneck. In comparison, the flow between two vertices in a strongly coupled cluster will have many alternative routes and therefore will not be particularly


sensitive on the precise weight of any single edge. In order to pick out larger halflives, we will use one parameter, ¡ , which is a rough estimate of the smallest halflife that one wishes to consider. Since we are interested in perturbations r which significantly change the current halflife of a mode, we choose to use a logarithmic scale in halflife. A simple choice for a function which combines these two effects is X1¡ a ¥

¡£¢¥¤

X(¡ ¡ a ,where ¡ thehalflifexofofthe currenteigenmode.

r

  

a halflife of ¡p¥



derivative of

8 ¡¨¢©¤

Suppose we have an¡£¢¥¤eigenvector

Xva%©



¡£¢¥¤

0 1') for the X X1¡fXa -edge, to 0 1') t

  ¦

X(¡ ¡ a

8 1')

¦

r





, with eigenvalue   . This eigenvector decays with

Xh¢ £¢a .(')Considertheeffecton dX(¡ a ofperturbingtheaffinity

 

#,

§¦

. In particular, we show in the Appendix that the

1') awa withrespectto (') ,evaluatedat 1') ¥ ¦ ¦

 

¡¨¢©¤

Xva 



 8   

¡£¢¥¤

X  sa X   ¤ ©  a ¡¨¢©¤   

satisfies

¥  



 x

) 8 )

X §  sa

 

   8  8 )  )

(5)  

Here X w )5a are the X  ta elements of eigenvector

 

X  t a (Eq.1). In Figure 2, for a given eigenvector and its flow, we plot the maximum of absolute sensitivities of all the weights on edges connected to a pixel (not including itself). Note that the sensitivities are large in the bottlenecks at the border of the foreground and background. 5 EIGENCUTS: A Basic Clustering Algorithm We select a simple clustering algorithm to test our proposal of using the derivative of the eigenmode's halflife for identifying bottleneck edges. Given a value of ¡ , which is roughly r the minimum halflife to consider for any eigenmode, we iterate the following:

1. Form the symmetric

2. Set

!#"$! affinity matrix % , and initialize %'&)(0% .

1H254 2 13254 26(87@9ACBED % &2F4 A , and set a scale factor

DcbCd G

to be the median of . DcbCd for

d

I6(QPSRUTVTWTWRX! .

and X8%%8)5a are degrees of nodes

Form the symmetric matrix

3. Compute eigenvectors

4. For each eigenvector



x 254

A

(' X w CdpeFf g

egfh D Rfh

of hx f

Y & (01a`

d Y &

RWiWiWiRpfh

% & 1a`

of Yr& , with eigenvalues

xQ y

s t D spuvs t swTWTWT'uvs t s .

9pq 9

with halflife y , compute the halflife sensitivities,

 for each edge in the graph. Here we use (hPij .

5. Do non-maximalx suppression within each of the computed sensitivities.x That is, suppress

 x

the sensitivity

l

k

2F4

A 

if there is a strictly more negative value k 4

A 

or 2F4

.

for some vertex



for which x 2F4 A$s

in the neighbourhood of

x



l A

, or some

AVo &254 A

l

in the neighbourhood of

l 2 9

9

over all non-suppressed edges 6. Compute the sum

t

iG . We use

t

m

(hnvu¥TwP

of . n

x 254

pFIXRgqr

7. Select the eigenmode

8. Cut all edges pFIXRgqr in

h xVx f %y&

for which m xVx is maximal.



xVx

254

AHs (i.e. set their affinities to 0) for which t iG and for which

this sensitivity was not suppressed during non-maximal suppression. 9. If any new edges have been cut, go to 2. Otherwise stop.

Here steps §



@z are as described previously, other than computing the scaling constant , {

to provide a scale invariant threshold on the computed sensitivities. which is used in step

In step 4 we only consider eigenmodes with halflives larger than h¡| , with ¥§ ©} because this typically eliminates the need to compute the sensitivities for many modes with tiny

r |

X1¡ a , it is very rare for eigenvectors with

halflives smaller than %¡|

values of ¡ and, because of the ¡ term in  r

r

 

to produce any sensitivity less than .

~

'

In step 5 we perform a non-maximal suppression on the sensitivities for the g eigenvector.

We have observed that at strong borders the computed sensitivities can be less than ~ in a

band along the border few pixels thick. This non-maximal suppression allows us to thin this region. Otherwise, many small isolated fragments can be produced in the neighbourhood of such strong borders.


In step 6 we wish to select one particular eigenmode to base the edge cutting on at this iteration. The reason for not considering all the modes simultaneously is that we have found the locations of the cuts can vary by a few pixels for different modes. If nearby edges are cut as a result of different eigenmodes, then small isolated fragments can result in the final clustering. Therefore we wish to select just one eigenmode to base cuts on each iteration. The particular eigenmode selected can, of course, vary from one iteration to the next. The selection strategy in step 6 above picks out the mode which produces the largest

linearized increment in dX(¡ a ¥

¡£¢¥¤ ¦

¡£§ ¤



 

  ©¨ , where  

E 1') 0 (') 0¨1') ¥

be cut, and   0¨ ¥

e5f g

1')

#



¡¨¢©¤

X(¡



That is, we compute    

¥

0¨(') is the change of affinities for any edge left to



¡ a . r



otherwise. Other techniques for selecting a particular mode were

also tried, and they all produced similar results. This iterative cutting process must eventually terminate since, except for the last iteration, edges are cut each iteration and any cut edges are never uncut. When the process does terminate, the selected succession of cuts provides a modified affinity matrix V¨ which has well separated clusters. For the final clustering result, we can use either a connected

components algorithm or the having large halflives. 6 Experiments -means algorithm of [5] with set to the number of modes £ £

We compare the quality of EIGENCUTS with two other methods: a £ -means based spectral

clustering algorithm of [5] and an efficient segmentation algorithm proposed in [1] based on a pairwise region comparison function. Our strategy was to select thresholds that are likely to generate a small number of stable partitions. We then varied these thresholds to test the quality of partitions. To allow for comparison with -means, we needed to determine the number of clusters a priori. We therefore set to be the same as the number of clusters that EIGENCUTS generated. The cluster centers were initialized to be as orthogonal as possible [5]. The first two rows in Fig. 3 show results using EIGENCUTS. A crucial observation with EIGENCUTS is that, although the number of clusters changed slightly with a change in ¡ , the regions they defined were qualitatively preserved across the thresholds and correr sponded to a naive observer's intuitive segmentation of the image. Notice in the random images the occluder is found as a cluster clearly separated from the background. The performance on the eye images is also interesting in that the largely uniform regions around the center of the eye remain as part of one cluster. In comparison, both the -means algorithm and the image segmentation algorithm of [1] (rows 3-6 in Fig. 3) show a tendency to divide uniform regions and give partitions that are neither stable nor intuitive, despite multiple restarts. 7 Discussion We have demonstrated that the common piecewise constant approximation to eigenvectors arising in spectral clustering problems limits the applicability of previous methods to situations in which the clusters are only relatively weakly coupled. We have proposed a new edge cutting criterion which avoids this piecewise constant approximation. Bottleneck edges between distinct clusters are identified through the observed sensitivity of an eigenflow's halflife on changes in the edges' affinity weights. The basic algorithm we propose is computationally demanding in that the eigenvectors of the Markov matrix must be recomputed after each iteration of edge cutting. However, the point of this algorithm is to simply demonstrate the partitioning that can be achieved through the computation of the sensitivity of eigenflow halflives to changes in edge weights. More efficient updates of the eigenvalue computation, taking advantage of low-rank changes in the matrix ¨ from one iteration to the next, or a multi-scale technique, are important areas for further study.

£

£ £

£


(a) (b) (c) (d) (e)

Figure 3: Each column refers to a different image in the dataset shown in Fig. 1. Pairs

of rows correspond to results from applying: EIGENCUTS with

! § and ¡ ¥ r

(Rows 1&2),

 #

¥ §v! C| ¥ !

C~

¥

-Means spectral clustering where , the number £

£

of clusters, is determined by the results of EIGENCUTS (Rows 3&4) and Falsenszwalb &

Huttenlocher ~ (Rows 5&6).

¡ £¢¥ # '# # ) #

 ) # )6


Acknowledgements We have benefited from discussions with Sven Dickinson, Sam Roweis, Sageev Oore and Francisco Estrada. References [1] P. Felzenszalb and D. Huttenlocher Efficiently Computing a Good Segmentation Internation Journal on Computer Vision, 1999. [2] R. Kannan, S. Vempala and A. Vetta On clusterings­good, bad and spectral. Proc. 41st Annual Symposium on Foundations of Computer Science , 2000. [3] J. R. Kemeny and J. L. Snell Finite Markov Chains. Van Nostrand, New York, 1960. [4] M. Meila and J. Shi A random walks view of spectral segmentation. Proc. International Workshop on AI and Statistics , 2001. [5] A. Ng, M. Jordan and Y. Weiss On Spectral Clustering: analysis and an algorithm NIPS, 2001. [6] A. Ng, A. Zheng, and M. Jordan Stable algorithms for link analysis. Proc. 24th Intl. ACM SIGIR Conference, 2001. [7] A. Ng, A. Zheng, and M. Jordan Link analysis, eigenvectors and stability. Proc. 17th Intl. IJCAI, 2001. [8] P. Perona and W. Freeman A factorization approach to grouping. European Conference on Computer Vision, 1998. [9] A. Pothen Graph partitioning algorithms with applications to scientific computing. Parallel Numerical Algorithms, D. E. Keyes et al (eds.), Kluwer Academic Press, 1996. [10] G. L. Scott and H. C. Longuet-Higgins Feature grouping by relocalization of eigenvectors of the proximity matrix. Proc. British Machine Vision Conference, pg. 103-108, 1990. [11] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transaction on Pattern Analysis and Machine Intelligence , 2000. [12] N. Tishby and N. Slonim Data clustering by Markovian Relaxation and the Information Bottleneck Method. NIPS, v 13, MIT Press, 2001. [13] Y. Weiss Segmentation using eigenvectors: a unifying view. International Conference on Computer Vision, 1999. Appendix

We compute the derivative of the log of half-life

of the affinity matrix %

y of an eigenvalue t with respect to an element

t

o 2 A

. Half-life is defined as the power to which

t  ( PVi

must be raised to reduce the

eigenvalue to half, i.e.,

half-lives y

¡  . What we are interested is in seeing significant changes in those

which are relatively large compared to some minimum half-life y  . So eigenvectors with

¢

half-lives smaller than

¢¤£¦¥¡§

¢p

©¨

o 2 A

y

y  are effectively ignored. It is easy to show that,

£¦¥¡§

pgtr

£¦¥¡§ p

¢ ¢

yWr

( £¥§

t

r pgt

 i ¢ t

o A

¢ t R

o A (

f h ¢ Y

 

h f Y f (8t)fh

Sr h

2

Let be the corresponding eigenvector such that

Dcbwd

(Sec 2). As

%$'&

Y (@1 `

¢

¢ Y

o A

% 1 ` (@1 `

Dcbwd Dcbwd

, we can write for all ( I

"!

  Dcbwd

2 A ¨ A 2 1 `  n

q

and , where :

DcbCd

h

o A f T (6)

2 2

Y is the modified affinity matrix

Dcbwd

% 1 ` n 1 `

2

at location )(p 10 ; R r

#!)R ¢ ¢

p 2 R

%

where is a matrix of all zeros except for a value of P A r

(7) are degrees of  

32547618  d e 2 2 ¨ the nodes and I

 g d 2547618



A A

q (stacked as elements on the diagonal matrix 1 see Sec 2); and ! (

 having non-zero entries only on the diagonal. Simplifying the expression further, we get

¢ 

Dcbwd

 

h f  DcbCd Dcbwd DcbCd Dcbwd DcbCd ¢ Y h h  A A h  h

o 2 A f ( f 1 `



n8fh DcbCd

2 ¨ 2 1 `  f n8fh

h

! 1 1 ` % 1 `  f

 DcbCd DcbCd (8)

1 ` Dcbwd % 1 `

( Y f h

U1 ! f T

h t)fh Dcbwd DcbCd

Using the fact that 1 `

%y1 ` f (

, and !

Dcbwd h

1 ( 1 ! as both ! and 1 are

diagonal, the above equation reduces to:

¢

h f  DcbCd ¢ Y h h  A

o 2

2 A f ( f 1 `

( h f  DcbCd

    ¨

92 1 ` f n @  t)fh

t)fh 

 Dcbwd A ! 1 h f

A

 Dcbwd

1 ` 2 ¨ A 2 1 `  h f n

 ¢ D  ¨ ¢ D 

2` 2 2 A` A A  h f T (9)

The scalar form of this expression is used in Eq.5.


