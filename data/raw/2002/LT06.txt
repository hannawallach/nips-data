Dyadic Classification Trees via Structural Risk Minimization

Clayton Scott and Robert Nowak Department of Electrical and Computer Engineering Rice University

 

Houston, TX 77005 cscott,nowak¡ @rice.edu

Abstract Classification trees are one of the most popular types of classifiers, with ease of implementation and interpretation being among their attractive features. Despite the widespread use of classification trees, theoretical analysis of their performance is scarce. In this paper, we show that a new family of classification trees, called dyadic classification trees (DCTs), are near optimal (in a minimax sense) for a very broad range of classification problems. This demonstrates that other schemes (e.g., neural networks, support vector machines) cannot perform significantly better than DCTs in many cases. We also show that this near optimal performance is attained with linear (in the number of training data) complexity growing and pruning algorithms. Moreover, the performance of DCTs on benchmark datasets compares favorably to that of standard CART, which is generally more computationally intensive and which does not possess similar near optimality properties. Our analysis stems from theoretical results on structural risk minimization, on which the pruning rule for DCTs is based. 1 Introduction

  

Let ¢¤£¦¥¨§© ¥"!#¡ be a jointly distributed pair of random variables. In pattern

recognition, £ is called an input vector, and contains the measurements from an experi-

ment. The values in £ are referred to as features, attributes, or predictors. § is called a

response variable, and is thought of as a class label associated with

  

function $¦%& (' ¥)!#¡

£ . A classifier is a

that attempts to match an input vector with the appropriate class.

The performance of error: $

for a given distribution of the data is measured by the probability of

0  

¢1$2©4365 $7¢¤£8©@936§A¡CB

The classifier with the smallest probability of error, denoted fier. The Bayes classifier is given by

I

$ D ¢¤FG©H3 !

if PQ¢¤F2©SRUTV

$ED , is called the Bayes classi-

otherwise

¥


 

where PQ¢¤F2©43 5 § 3 ! £

¡

£¢

 

36F ¡ 3 §

¡

0F ¡

of error for the Bayes classifier is denoted D

is the regression of . § on £ . The probability

The true distribution on the data is generally unknown. In such cases, we may construct a

 

classifier $ based on a training dataset 3 ¢¤£ ¥ § © ¥)B"B)B"¥"¢¤£ 2¥¨§ © ¡ of independent,

T T

identically distributed samples. A procedure that constructs a classifier for all

discrimination rule. The0 performance of

¤ 0

3 ¢ $

¤

© 3 5

$ 3 $ ¢¤F © is measured by

¤

¡C¥

 

$

¤ ¦ ¤

0 ¢¤£ ©@936§

¤ ¤ ¦ ¤

©

is called a

¥¤ ¦§¤ §¤ ¨¤

¤

¡¦

the conditional probability of error. Note that is random, since

¦¤ is random.

In this paper, we examine a family of classifiers called dyadic classification trees (DCTs), built by recursive, dyadic partitioning of the input space. The appropriate tree from this family is obtained by building an initial tree (in a data-independent fashion), followed by a data-dependent pruning operation based on structural risk minimization (SRM). Thus, one important distinction between our approach and usual decision trees is that the initial tree is not adaptively grown to fit the data. The pruning strategy resembles that used by CART, except that the penalty assigned to a subtree is proportional to the square root of its size.

SRM penalized DCTs lead to a strongly consistent discrimination rule for input data



support in the unit cube ¥"! 

 

£ with

. We also derive bounds on the rate of convergence of DCTs

to the Bayes error. Under a modest regularity assumption (in terms of the box-counting dimension) on the underlying optimal Bayes decision boundary, we show that complexity-

© "!$# &%.

©' 

regularized DCTs converge to the Bayes decision at a rate of

minimax error rate for this class is at least T 

T  T Moreover, the

. This shows that dyadic classification trees

are near minimax-rate optimal, i.e., that no discrimination rule can perform significantly better in this minimax sense. We also present an efficient algorithm for implementing the

pruning strategy, which leads to an

algorithm requires ¢ ©

( 1032547680

( )©

¢ © algorithm for DCT construction. The pruning

operations to prune an initial tree with

0

terminal nodes,

and is based on the familiar pruning algorithm used by CART [1]. Finally, we compare DCTs with a CART-like tree classifier on four common datasets. 2 Dyadic Classification Trees Throughout this work we assume that the input data is restricted to the unit hypercube,

 "



¥)!  . This is a realistic assumption for real-world data, provided appropriate translation

 

A@ @CB

and scaling is applied. Let

space, where each an integer , let

@HD@HD

I9



E EF@CDdenote

9

3 ¥"B)B"B)¥ ¡ be a tree-structured partition of the input

is a hyperrectangle with sides parallel to the coordinate axes. Given

 

the element of ¡ that is congruent to modulo . If

T

PCQ &%.

@ D! @ D!

!&¥"B)B"B ¥

and T

!



P

G

V

is a cell at depth in the tree, let

%

E G

be the rectangles formed by splitting

at its midpoint along coordinate

points of

@HD

As a convention, assume



@ D!

&%

T contains those

that are less than or equal to the midpoint along the dimension being split.

 



¥"!

Definition 1 A sequential dyadic partition (SDP) is any partition of obtained by applying the following rules recursively:

  

1. The trivial partition

 

2. If

9

3 ¥"B)B"B)¥ T

A@ @CBR@

 

9  

3 ¥"!

¡

 ¡ is an SDP,

 that can be

is an SDP, then so is

¥)B)B"B)¥

&% ! % V

¥

.

T

where may be any integer,

VUT8UXW T !

@SD @ D! @ D @HD#

¥ T ¥

T ¥)B"B)B)¥

@ B

¡C¥ T

We define a dyadic classification tree (DCT) to be a sequential dyadic partition with a class label (0 or 1) assigned to each node in the tree.


The partitions are sequential because children must be split along the next coordinate after the coordinate where their parent was split. Such splits are referred to as forced splits, as opposed to free splits, in which any coordinate may be split. The partitions are dyadic

because we only allow midpoint splits. 0

By a complete DCT of depth , we mean a DCT such that every possible¡ split0 up to depth

has been made. In a complete DCT, every terminal node has volume  

of , then the terminal nodes of a complete DCT are hypercubes of sidelength  

G ¨

. If

is a¡ multiple



.

0

¨

3 SRM for DCTs Structural risk minimization (SRM) is an inductive principle for selecting a classifier from a sequence of sets of classifiers based on complexity regularization. It was introduced by Vapnik and Chervonenkis (see [2]), and later analyzed by Lugosi and Zeger [3], [4, Ch. 18]. We formulate structural risk minimization for dyadic classification trees by applying results from [4, Ch. 18]. SRM is formulated in terms of the VC dimension, which we briefly review. Let ¢ be a

  

can be correctly classified by some

¤

collection of classifiers possible labellings of £

shatters £ ¥)B)B"B)¥¦£

¤

$

%S ¥"B)B)B)¥¤£

R¤

 ' ¥"!#¡ , and let £

¤

  . If each of the  

$ ¥¢ , we say ¢ 

T

¥"B)B)B)¥¤£ 

. TheT Vapnik-Chervonenkis dimension (or VC dimension) of ¢ , denoted

T

by § , is the largest integer

T

¤

©

for which there exist £

© ©

T

¥)B"B)B ¦£ ¥

¤

   such that ¢ shatters

£ ¥"B)B)B)¥¤£ . If ¢ shatters some points for every , then § ©¨ by definition. The VC 3

dimension is a measure of the capacity of ¢ . As § increases, ¢ is able to separate more

complex patterns. If    for some integer  , we say  is dyadic. For dyadic  , and for

3

let 

G



!B %

denote the collection of all DCTs with

 , so that no terminal node has a side of length less than "!# !



!B % W

is

W

CUXW U

!   ,

terminal nodes and depth not exceeding

$  % . It is easily shown 3

D D D¤

¥ §0 © ¡ A

¤



that the VC dimension of   [5].

¤!B %

 

Given a dyadic integer  , and training data

$ 0)

¢¤£

'& , for

12436587¤9 @

B

D¤

'&

C

T

W ¥ A3!C¥¦  ¥"B)B"B (  , define

 3 arg min

Q¢ $2© ¥

where 0 A

¤ ! ¢ $2©43 © ¢ $7¢¤£ D D

©@936§ ©

is the empirical risk of . Thus, 0) is selected by empirical risk minimization over  $ $

Define the penalty term D

¤!B %



1W &© ¢ 2¥

T



!B %.

(1)

!B %,

© FE 3

 

W824©6 A© IH¢

D

©

G ¥

and for $ P   define the penalized risk 0 Q

¤ 0 A

Q¢ $2©43 Q¢1$2©

0 Q

¤ ¤!B %

$ D 

¤¤ Q 1W &©

¢ 2¥ © B

The SRM principle selects the classifier 0) from among

minimizes ¢ $ 0)  © . We refer to $QD  S)

¤

$ 0)

¤!B % W

 ¥ 3 !&¥R  ¥"B)B)B)¥(  , that

as a penalized or complexity-regularized dyadic

classification tree. We have the following risk bound.D

Theorem 1 For all

0

`Y 5

© W U

T  , and for all UVXW 0

12436587¤9 @ c'dSe ¢1$G© fUhg R

£U 

XH

¤

¢1$ D 0)

¤ and  ba

©

¢ 2¥ iqp

W ©

¦r V

T

© ,

Q ©  ts H

B ¤ uiIp V ¦v T ¥


and in particular, for all

©

and  ,

¢  ¤  U B

0 0

¢1$ D 0)  © a D

¡ ¢ ¢c

T

¥

W 24 6©§Q©

 

¥

§¦!

tW

Q 0 0 d ¤£ E 1243c'dSe@5 7(9

 

!B %

¢ $2© a

B

©¨¨ D B

Sketch of proof: Apply Theorem 18.3 in [4] with ¢

!&¥R  ¥"B)B)B)¥(  .

!B %

3 and § 3

W W for 3 

The first term on the right-hand side of the second bound is an upper bound on the expected estimation error. The second term is the approximation error. Even though the penalized though it does, because of the "min" in the expression. Nobel [6] gives similar results for classifiers based on initial trees that depend on the data. The next result demonstrates strong consistency for the penalized DCT, where strong con-

DCT does not know the value of

W that optimally balances the two terms, it performs as

0

sistency means

¤ 0

' D with probabilty one.

' ¨ , with  3  ¢ © assuming only dyadic integer values. If Theorem 2 Suppose (¥

 



3 ¢ ! ©

)© 25476'©

© )©

, then the penalized dyadic classification tree is strongly consistent for

all distributions supported on the unit hypercube. Sketch of proof: The proof follows from the first part of Theorem 1 and strong universal consistency of the regular histogram classifier. See [5] for details. 4 Rates of Convergence



In this section, we investigate bounds on the rate of convergence of complexity-regularized DCTs. First we obtain upper bounds on the rate of convergence for a particular class of of any data based classifier for this class. Most rate of convergence studies in pattern recognition place a constraint on the regression (e.g. Lipschitz, Besov, bounded variation). In contrast, the class we study is defined in terms of the regularity of the Bayes decision boundary, denoted . We allow to be arbitrarily irregular away from , so long  as it is well behaved near . The Bayes decision

distributions on ¢ £¦¥ § © . We then state a minimax lower bound on the rate of convergence

 

¡

! £ function PQ¢¤F2©43 5 § 3 36F ¡ by requiring it to belong to a certain smoothness class



boundary is informally defined as

 PQ¢¤F2©





3 F % PQ¢¤F2©3 "!4  ¡ . A more rigorous definition !

should take into account the fact that P might not take on the value #!u  [5]. !

denote a random pair, as before, where We now define a class of distributions. Let

£ takes on values in ¥)!



.



Definition 2 Let

¢¤£ ¥ §©

 ¥

V

 

R

¢ £¦¥ § ©

such that forT all dyadic integers  ,T if we subdivide the unit cube into cubes of side

. Define  ¢ ¥

V

© to be the collection of all distributions on

length "!# , !

A1 (Bounded marginal): For any such cube

 

5 £ 

 ! ¢ $

¡

#"T @©H3 !"

U 

intersecting the Bayes decision boundary,

T  , where "

denotes the Lebesgue measure.

A2 (Regularity): The Bayes decision boundary passes through at most

resulting   cubes.

 V  



T of the

Define  to be the class of all ¢ £¦¥ § © belonging to  ¢ ¥

T

V © for some  ¥

T

V .

The first condition holds, for example, if the density of £

U%

is essentially bounded with

. The second condition T respect to the Lebesgue measure, with essential supremum


can be shown to hold when one coordinate of the Bayes decision boundary is a Lipschitz

function of the others. See, for example, the boundary fragment class of [7] with therein. 3 !

 

The regularity condition A2 is closely related to the notion of box-counting dimension of

the Bayes decision boundary [8]. Roughly speaking, A2 holds for some

Bayes decision boundary has box-counting dimension

G



V

if and only if the

a ! . The box-counting dimension

is an upper bound on the Hausdorff dimension, and the two dimensions are equal for most

"reasonable" sets. For example, if has box-counting dimension . W is a smooth -dimensional submanifold of   , then

¡

W

¡

4.1 Upper Bounds on DCT Rate of Convergence

Theorem 3 Assume the distribution of ¢ £¦¥ § © belongs to ¢  ¥

T

¢  ¤  U!¦¤ 25476©© © !$# %

penalized dyadic classification tree, as described in Section 3. If 



then there exists a constant

0

R

such that for all a D &¢

0

8R   ,

¢ $ D  © 0) ! © T  T B

24 6 ©¨24 6 © 24 6'© &"!$# &%'Q

V  3 V ¢ ¢ ! © T  T ©

§¢ © 246 © !$# %

¢ ! © T  T

  is arbitrary.

, we mean

¥¤ £¢ © 25476¤© "!$# &% V ©

. Let

¢ !

$ D 0)  be the

© T  T ,

When we write 

, where

 ¦

Sketch of proof: It can be shown that for each0dyadic  , there exists a pruned DCT 0

3 T © leaf nodes, such that D

 

V

bound in Theorem 1 and minimizing over  produces Tthe desired result [5].

The minimal value of 

¤

in the above theorem tends to    

V

as

G

W ( 

¢   ¢ $2© a

U

$ with

!# . Plugging this into the risk



rate of convergence results for data-grown trees would beT more difficult to establish, since the approximation error is random in those cases. It is possible to eliminate the log factor in the upper bound by means of Alexander's in-

equality, as discussed in [4, Ch. 12]. This leads to a much larger value of improved asymptotic rate.

 ¤

, but an

' ¨ . Note that similar

To illustrate the significance of Theorem 3, consider a penalized histogram classifer, with bin width determined adaptively by structural risk minimization, as described in [4, Problem 18.6]. For that rule, the best exponent on the rate of convergence for our class is

#! ¢ f &© , compared with "! ¢ ! !

GVQ GVQ

! © for our rule. Intuitively, this is because the adaptive

resolution of dyadic classification trees enables them to focus on the decision boundary, rather than the dimensional regression function.

In the event that the data £ occupies a

G G

dimensional subset of

Theorem 3 follows through as before, but with an exponent of

G  Q

G

G

a ! dimensional

 "



¥)!  , the proof of

! instead of

G Q

! . Thus,

the penalized DCT is able to automatically adapt to the dimensionality of the input data. 4.2 Minimax Lower Bound The next result demonstrates that complexity-regularized DCTs nearly achieve the minimimax rate for our class of distributions.

!#"%$'&©(0)  1"¤ 

Theorem 4 Let

constant

 

R

¤

denote any discrimination rule based on training data. There exists a

such that for c'dSe

sufficiently large,

0 0 ¢ © a

D 

 ©  & T  B


Sketch of proof: This result follows from Theorem 2 in [7] (with proof of that result is in turn based on Assouad's lemma.

  ¡  3 3 ! therein). The 

Theorems 3 and 4, together with the above remark on Alexander's inequality, show that pect that the class studied by Tsybakov [7], used in our minimax proof, is more restrictive

complexity-regularized DCTs are close to minimax-rate optimal for the class

 . We sus-

than our class. Therefore, it may be that the exponent "!!

decreased to #!! ¢ 6! ©

GSQ

G

in the above theorem can be

, in which case we achieve the minimax rate.

Although bounds on the minimax rate of convergence in pattern recognition have been investigated in previous work [9, 10], the focus has been on placing regularity assumptions cases, for many common function spaces (e.g. Lipschitz, Besov, bounded variation), classification is not easier than regression function estimation [10]. This contrasts with the conventional wisdom that, in general, classification is easier than regression function estimation [4, Ch. 6]. Our approach is to study minimax rates for distributions defined in terms of the regularity of the Bayes decision boundary. With this framework, we see that mini-

 

¡

! £ on the regression function P2¢ FG©A3 5 § 3 3 F ¡ . Yang demonstrates that in such

max rates for classification can be orders of magnitude faster than for estimation of

since PQ¢¤FG©

PQ¢¤FG© ,

may be arbitrarily irregular away from the decision boundary for distributions

in our class. This view of minimax classification has also been adopted by Mammen and Tsybakov [7,11]. Our contribution with respect to their work is an implementable discrimination rule, with guaranteed computational complexity, that nearly achieves the minimax

lower bounds. We also remark that "fast rates" (e.g.,

( )©

¢ T © ) obtained by those authors

require much stronger assumptions on the smoothness of the decision boundary and than we employ in this paper. 5 An Efficient Pruning Algorithm PQ¢ FG©

In this section we describe an algorithm to compute the penalized DCT efficiently. We

switch notation, using

¢ 

to denote an arbitrary classification tree. Let

¢(possiblyarg¥§¦

itself). For 0 A

¨¥¢min ¤£

¢

  , define

¡¢ ¡

¢ ¢

denote that

is a pruned version of

£

¢

¢ ©43 T

¤£

¤  ©£Q 

2¢ ©

¤  ©£Q Q¢

¢

©

¢

¢ ¢  U

¥

and ¢

when

¢¡¢is

0 A

V ¢ ©43 arg min

£

¡

¥ ¦§¥¢

where denotes the number of leaf nodes of

a complete dyadic tree, and 83

G



 ¡¢ ¡  ¥

25476. A© R©

qH¢

£  £¢  £¢

T ¢ 7© 3

T

D¡

©



We are interested in computing

¢ V £ ¢ 7©

 

Breiman, et.al. [1] showed the existence of weights a ¨

 

and subtrees

 ¥

£ £ '£

  3

 

¡

T

D

© . Moreover, the weights and subtrees

D D

¢



operationsT [12, 13]. A similar result holds for the square-root penalty, and the trees produced are a subset of the trees produced by the additive penalty [5].

¢ ¢

Theorem 5 For each , there exists

¢

Therefore, pruning

£ £ 

such that V

¤£

¢ ©43

¤£ 

¢ © .

T

! " ! £

#

%$

¦! . 3 such that

D

may be found in

£ &(

¢

¥¨ 3

whenever

©

¡24 ¡¢ ¡ 6

with the square-root penalty always produces one of the0A trees

We may then determine the pruned tree

be performed in

£   ¡¢ ¡by

¢ ¢

( 24 6

DU&T

¥

minimizing the penalized risk

!C¥)B)B"B ¥

0

¢

¡¢ ¡ ¡¢ ¡

¤  Q

¢ ©

¢ )(.

minimizing this quantity over

©

3 . Thus, square-root pruning can

¢

operations.

In the context of constructing a penalized DCT, we start with an initial tree that is a

complete DCT. For the classifiers in Theorems 2 and 3, this initial tree has size

¡¢ ¡

3


Table 1: Comparison of a greedy tree growing procedure, with model selection based on holdout error estimate, and two DCT based methods. Numbers shown are test errors.

Pima Indian Diabetes Wisconsin Breast Cancer Ionosphere Waveform

CART-HOLD 26.8 % 4.7 % 12.88 % 19.8 % DCT-HOLD 27.2 % 6.4 % 18.6 % 29.1 % DCT-SRM 33.0 % 6.3 % 18.8 % 31.0 %

 

$

3 ¢

)© 24(6'©)©

!

also requires

© ¢ ©

, and so pruning requires

( )©

¢ © operations. Since the growing procedure

operations, the overall construction is

( ©

¢ © .

6 Experimental Comparison To gain a rough idea of the usefulness of dyadic classification trees in practice, we compared two DCT based classifiers with a greedy tree growing procedure, similar to that used by CART [1] or C4.5 [14], where each successive split is chosen to maximize an information gain defined in terms of an impurity function. We considered four two-class datasets, available on the web at http://www.ics.uci.edu/ mlearn/MLRepository.html. For each

¢

dataset, we randomly split the data into two halves to form training and testing datasets. For the greedy growing scheme, we used half of the training data to grow the tree, and constructed every possible pruning of the initial tree with an additive penalty. The best pruned tree was chosen to minimize the holdout error on the rest of the training data. We call this classifier CART-HOLD. The second classifier, DCT-HOLD, was constructed in a similar manner, except that the initial tree was a complete DCT, and all of the training data was used for computing the holdout error estimate. Finally, we implemented the complexityregularized DCT, denoted DCT-SRM, with square-root penalty determined by Equation 1. Table 1 shows the misclassification rate for each algorithm on each dataset. From these experiments, we might conclude two things: (i) The greedily-grown partition outperforms the dyadic partition; and (ii) Much of the discrepancy between CART-HOLD and DCT-SRM comes from the partitioning, and not from the model selection method (holdout versus SRM). Indeed, DCT-SRM beats or nearly equals DCT-HOLD on three of the four datasets. Conclusion (i) may be premature, for it is shown in [4, Ch. 20] that greedy partitioning based on impurity functions can perform arbitrarily poorly for some distributions, while this is never the case for complexity-regularized DCTs. In light of (ii), it may be possible to apply Nobel's pruning rules for data-grown trees [6], which can now be implemented with our algorithm, to equal or surpass the performance of CART, while avoiding the heuristic and computationally expensive cross-validation technique usually employed by CART to determine the appropriately pruned tree. 7 Conclusion Dyadic classification trees exhibit desirable theoretical properties (finite sample risk bounds, consistency, near minimax-rate optimality) and can be trained extremely rapidly. The minimax result demonstrates that other discrimination rules, such as neural networks or support vector machines, cannot significantly outperform DCTs (in this minimax sense). This minimax result is asymptotic, and considers worst-case distributions. From a practical standpoint, with finite samples and non-worst-case distributions, other rules may beat DCTs, which our experiments on benchmark datasets confirm. The sequential dyadic partitioning scheme is especially susceptible when many of the features are irrelevant, since


it must cycle through all features before splitting a feature again. Several modifications to the current dyadic partitioning scheme may be envisioned, such as free dyadic or median splits. Such modified tree induction strategies would still possess many of the desirable theoretical properties of DCTs. Indeed, Nobel has derived risk bounds and consistency results for classification trees grown according to data [6]. Our square-root pruning algorithm now provides a means of implementing his pruning schemes for comparison with other model selection techniques (e.g., holdout or cross-validation). It remains to be seen whether the rate of convergence analysis presented here extends to his work. Further details on this work, including full proofs, may be found in [5]. Acknowledgments This work was partially supported by the National Science Foundation, grant no. MIP­ 9701692, the Army Research Office, grant no. DAAD19-99-1-0349, and the Office of Naval Research, grant no. N00014-00-1-0390. References

[1] L. Breiman, J. Friedman, R. Olshen, and C. Stone, Wadsworth, Belmont, CA, 1984. Classification and Regression Trees,

[2] V. Vapnik, Estimation of Dependencies Based on Empirical Data, Springer-Verlag, New York, 1982. [3] G. Lugosi and K. Zeger, "Concept learning using complexity regularization," IEEE Transactions on Information Theory, vol. 42, no. 1, pp. 48­54, 1996. [4] L. Devroye, L. Gy¨orfi, and G. Lugosi, A Probabilistic Theory of Pattern Recognition, Springer, New York, 1996. [5] C. Scott and R. Nowak, "Complexity-regularized dyadic classification trees: Efficient pruning and rates of convergence," Tech. Rep. TREE0201, Rice University, 2002, available at http://www.dsp.rice.edu/ cscott. [6] A. Nobel, "Analysis of a complexity based pruning scheme for classification trees," IEEE Transactions on Information Theory, vol. 48, no. 8, pp. 2362­2368, 2002. [7] A. B. Tsybakov, "Optimal aggregation of classifiers in statistical learning," preprint, 2001, available at http://www.proba.jussieu.fr/mathdoc/preprints/. [8] K. Falconer, Fractal Geometry: Mathematical Foundations and Applications, Wiley, West Sussex, England, 1990. [9] J. S. Marron, "Optimal rates of convergence to Bayes risk in nonparametric discrimination," Annals of Statistics, vol. 11, no. 4, pp. 1142­1155, 1983. [10] Y. Yang, "Minimax nonparametric classification­Part I: Rates of convergence," IEEE Transactions on Information Theory, vol. 45, no. 7, pp. 2271­2284, 1999. [11] E. Mammen and A. B. Tsybakov, "Smooth discrimination analysis," Annals of Statistics, vol. 27, pp. 1808­1829, 1999. [12] P. Chou, T. Lookabaugh, and R. Gray, "Optimal pruning with applications to tree-structured source coding and modeling," IEEE Transactions on Information Theory, vol. 35, no. 2, pp. 299­315, 1989. [13] B. Ripley, Pattern Recognition and Neural Networks, Cambridge University Press, Cambridge, UK, 1996. [14] R. Quinlan, C4.5: Programs for Machine Learning, Morgan Kaufmann, San Mateo, 1993.

 


