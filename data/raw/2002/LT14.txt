PAC-Bayes & Margins
John Langford
IBM Research
Email: jcl@cs.cmu.edu
John Shawe-Taylor
Royal Holloway, University of London
Email: jst@cs.rhul.ac.uk
Abstract
We show two related things:
(1) Given a classier which consists of a weighted sum of features
with a large margin, we can construct a stochastic classier with
negligibly larger training error rate. The stochastic classier has
a future error rate bound that depends on the margin distribution
and is independent of the size of the base hypothesis class.
(2) A new true error bound for classiers with a margin which
is simpler, functionally tighter, and more data-dependent than all
previous bounds.
1 Introduction
PAC-Bayes bounds [8] (improved in [7] and again in [10]) are interesting for con-
structing bounds on future error rate in classication given only an assumption that
examples are drawn independently from some (unknown) distribution. One draw-
back of PAC-Bayes bounds is that they only apply strongly to stochastic classiers|
classiers which make randomized predictions. Most learning algorithms do not
produce stochastic predictors, so the PAC-Bayes bound can only be applied to clas-
siers altered to form a stochastic classier. This alteration can be done either by
a sensitivity analysis [5] or by using the Bayesian posterior [10] directly.
Although stochastic classiers are rare in practice, voting classiers are quite com-
mon. These include \Bayes optimal" classiers, Maximum Entropy classiers [4],
Adaboost [9], and others which has motivated analysis of true error bounds special-
ized to voting classiers. In particular, we know that when the vote is decided by
a margin of at least  on the training set, the true error rate is more tightly bound
than might be naively expected. Unfortunately, these bounds are still unsatisfying
for practical use because they often give the meaningless \future error rate is less
than 1" predictions.
Here we analyze the connection between PAC-Bayes and margin bounds. In partic-
ular, we show that the \build a stochastic classier" approach [5] to constructing a
non-vacuous PAC-Bayes bound is always possible given that some margin  holds
for the training set. This also shows that PAC-Bayes bounds are capable of captur-
ing the low eective \sample complexity" which exists when a large margin classier
is used.
An alteration to the PAC-Bayes result proves a new bound for margins which is a

shorter argument and much tighter than previous margin bounds.
There are two mathematical avors of margin bound dependent upon the weights
w i of the vote and the features x i that the vote is taken over.
1. Those ([12], [1]) with a bound on
P
i w 2
i and
P
i x 2
i (\l 2 =l 2 " bounds).
2. Those ([11], [6]) with a bound on
P
i w i and max i x i (\l 1 =l 1 " bounds).
The results here are of the \l 2 =l 2 " form. We improve on Shawe-Taylor et al. [12]
and Bartlett [1] by a log(m) 2 sample complexity factor and much tighter constants
(1000 or unstated versus 9 or 18 as suggested by Section 2.2). In addition, the
bound here covers margin errors without weakening the error-free case.
Herbrich and Graepel [3] moved signicantly towards the approach adopted in our
paper, but the methodology adopted meant that their result does not scale well to
high dimensional feature spaces as the bound here (and earlier results) do.
The layout of our paper is simple - we rst show how to construct a stochastic
classier with a good true error bound given a margin, and then construct a margin
bound.
2 Margin Implies PAC-Bayes Bound
2.1 Notation and theorem
Consider a feature space X which may be used to make predictions about the value
in an output space Y = f1;+1g. We use the notation x = (x 1 ; : : : ; xN ) to denote
an N dimensional vector. Let the vote of a voting classier be given by:
v w (x) = wx =
X
i
w i x i :
The classier is given by c(x) = sign (v w (x)). The number of \margin violations"
or \margin errors" at  is given by:
^ e  (c) = Pr
(x;y)U(S)
(yv w (x) < );
where U(S) is the uniform distribution over the sample set S.
For convenience, we assume vx (x)  1 and vw (w)  1. Without this assumption,
our results scale as
p
v x (x)
p
vw (w)= rather than 1=.
Any margin bound applies to a vector w in N dimensional space. For every example,
we can decompose the example into a portion which is parallel to w and a portion
which is perpendicular to w.
x> = x vw (x)
kwk 2
w x k = x x>
The argument is simple: we exhibit a \prior" over the weight space and a \posterior"
over the weight space with an analytical form for the KL-divergence. The stochastic
classier dened by the posterior has a slightly larger empirical error and a small
true error bound.
For the next theorem, let 
F (x) = 1
R x
1
1
p
2
e x 2 =2 dx be the tail probability of a
Gaussian with mean 0 and variance 1. Also let
e Q(w;;) = Pr
(x;y)D;hQ(w;;)
(h(x) 6= y)

be the true error rate of a stochastic classier with distribution Q(; w; ) dependent
on a free parameter , the weights w of an averaging classier, and a margin .
Theorem 2.1 There exists a function Q mapping a weight vector w, margin ,
and value  > 0 to a distribution Q(w; ; ) such that
Pr
SD m
0
B @8w; ;  : KL(^e  (c) + ke Q(w;;) ) 
ln 1

F


F 1 ()

 + ln m+1
Æ
m
1
C A  1 Æ
where KL(qkp) = q ln q
p + (1 q) ln 1 q
1 p = the Kullback-Leibler divergence between
two coins of bias q < p.
2.2 Discussion
Theorem 2.1 shows that when a margin exists it is always possible to nd a \pos-
terior" distribution (in the style of [5]) which introduces only a small amount of
additional training error rate. The true error bound for this stochastization of the
large-margin classier is not dependent on the dimensionality except via the margin.
Since the Gaussian tail decreases exponentially, the value of 
F 1 () is not very large
for any reasonable value of . In particular, at 
F (3), we have   0:01. Thus, for
the purpose of understanding, we can replace 
F 1 () with 3 and consider  ' 0.
One useful approximation for 
F (x) with large x is:

F (x) ' e x 2 =2
p
2
(1=x)
If there are no margin errors ^ e  (c) = 0, then these approximations, yield the ap-
proximate bound:
Pr
SD m
 
e Q(w;;0) 
9
2 2 + ln 3
p
2
 + ln m+1
Æ
m
!
 1 Æ
In particular, for large m the true error is approximately bounded by 9
2 2 m .
As an example, if  = 0:25, the bound is less than 1 around m = 100 examples and
less than 0:5 around m = 200 examples.
Later we show (see Lemmas 4.1 and 4.2 or Theorem 4.3) that the generalisation
error of the original averaging classier is only a factor 2 or 4 larger than that of the
stochastic classiers considered here. Hence, the bounds of Theorems 2.1 and 3.1
also give bounds on the averaging classiers w.
This theorem is robust in the presence of noise and margin errors. Since the PAC-
Bayes bound works for any \posterior" Q, we are free to choose Q dependent upon
the data in any way. In practice, it may be desirable to follow an approach similar
to [5] and allow the data to determine the \right" posterior Q. Using the data
rather than the margin  allows the bound to take into account a fortuitous data
distribution and robust behavior in the presence of a \soft margin" (a margin with
errors). This is developed (along with a full proof) in the next section.
3 Main Full Result
We now present the main result. Here we state a bound which can take into ac-
count the distribution of the training set. Theorem 2.1 is a simple consequence

of this result. This theorem demonstrates the exibility of the technique since it
incorporates signicantly more data-dependent information into the bound calcu-
lation. When applying the bound one would choose  to make the inequality (1)
an equality. Hence, any choice of  determines  and hence the overall bound. We
then have the freedom to choose  to optimise the bound.
As noted earlier, given a weight vector w, any particular feature vector x decom-
poses into a portion x k which is parallel to w and a portion x> which is perpen-
dicular to w. Hence, we can write x = x k e k + x>e> , where e k is a unit vector in
the direction of w and e> is a unit vector in the direction of x> . Note that we may
have yx k < 0, if x is misclassied by w.
Theorem 3.1 For all averaging classiers c with normalized weights w and for all
 > 0 stochastic error rates, If we choose  > 0 such that
Ex;yS 
F
 yx k
x>


=  (1)
then there exists a posterior distribution Q(w; ; ) such that
Pr
SD m
 
8; w;  : KL(ke Q(w;;) ) 
ln 1

F ()
+ ln m+1
Æ
m
!
 1 Æ
where KL(qkp) = q ln q
p + (1 q) ln 1 q
1 p = the Kullback-Leibler divergence between
two coins of bias q < p.
Proof. The proof uses the PAC-Bayes bound, which states that for all prior distri-
butions P ,
Pr
SD m

8Q : KL(^e Q keQ )  KL(QjjP ) + ln m+1
Æ
m

 1 Æ
We choose P = N(0; I), an isotropic Gaussian 1 .
A choice of the \posterior" Q completes the proof. The Q we choose depends upon
the direction w, the margin , and the stochastic error . In particular, Q equals
P in every direction perpendicular to w, and a rectied Gaussian tail in the w
direction 2 . The distribution of a rectied Gaussian tail is given by R() = 0 for
x <  and R() = 1

F ()
p
2
e x 2 =2 for x  .
The chain rule for relative entropy (Theorem 2.5.3 of [2]) and the independence of
draws in each dimension implies that:
KL(QkP ) = KL(Q k kP k ) + KL(Q> jjP > )
= KL(R()kN(0; 1)) + KL(P> kP> )
= KL(R()kN(0; 1)) + 0
=
Z 1

ln 1

F () R(x)dx
= ln 1

F ()
1 Later, the fact that an isotropic Gaussian has the same representation in all rotations
of the coordinate sytem will be useful.
2 Note that we use the invariance under rotation of N(0; I) here to line up one dimension
with w.

Thus, our choice of posterior implies the theorem if the empirical error rate is
^
e Q(w;x;)  Ex;yS 
F
 x k
x> 

  which we show next.
Given a point x, our choice of posterior implies that we can decompose the stochastic
weight vector, ^
w = ^
w k e k + ^
w>e> + ~
w, where e k is parallel to w, e> is parallel to x>
and ~
w is a residual vector perpendicular to both. By our denition of the stochastic
generation ^
w k  R() and ^
w>  N(0; 1). To avoid an error, we must have:
y = sign(v ^
w (x))
= sign( ^
w k x k + ^
w>x> ):
Then, since ^
w k  , no error occurs if:
y(x k + ^
w>x> ) > 0
Since ^
w> is drawn from N(0; 1) the probability of this event is:
Pr y(x k + ^
w>x> ) > 0

 1 
F
 yx k
x>


And so, the empirical error rate of the stochastic classier is bounded by:
^ e Q  Ex;yS 
F
 yx k
x>


= 
as required.
3.1 Proof of Theorem 2.1
Proof. (sketch) The theorem follows from a relaxation of Theorem 3.1. In par-
ticular, we treat every example with a margin less than  as an error and use the
bounds kx> k  1 and kx k k  .
3.2 Further results
Several aspects of the Theorem 3.1 appear arbitrary, but they are not. In particular,
the choice of \prior" is not that arbitrary as the following lemma indicates.
Lemma 3.2 The set of P satisfying 9P kk : P (x) = P kk (kxk 2 ) (rotational invari-
ance) and P (x) =
Q N
i=1 p i (x i ) (independence of each dimension) is N(0; I) for
 > 0.
Proof. Rotational invariance together with the dimension independence imply that
for all i; j; x : p i (x) = p j (x) which implies that:
P (x) =
N
Y
i=1
p(x i )
for some function p(). Applying rotational invariance, we have that:
P (x) = P kk (kxk 2 ) =
N
Y
i=1
p(x i )
This implies:
log P kk
  N
X
i=1
x 2
i
!
=
N
X
i=1
log p(x i ):

Taking the derivative of this equation with respect to x i gives
P 0
kk (kxk 2 )2x i
P kk (kxk 2 ) = p 0 (x i )
p(x i ) :
Since this holds for all values of x we must have
P kk (t) = P 0
kk (t)
for some constant , or P kk (t) = C exp(t), for some constant C. Hence, P (x) =
C exp(kxk 2 ); as required.
The constant  in the previous lemma is a free parameter. However, the results do
not depend upon the precise value of  so we choose 1 for simplicity. Some freedom
in the choice of the \posterior" Q does exist and the results are dependent on this
choice. A rectied gaussian appears simplest.
4 Margin Implies Margin Bound
There are two methods for constructing a margin bound for the original averaging
classier. The rst method is simplest while the second is sometimes signicantly
tighter.
4.1 Simple Margin Bound
First we note a trivial bound arising from a folk theorem and the relationship to
our result.
Lemma 4.1 (Simple Averaging bound) For any stochastic classier with distribu-
tion Q and true error rate e Q , the averaging classier,
c Q (x) = sign
Z
H
h(x)dQ(h)

has true error rate:
e(c Q )  2eQ
Proof. For every example (x; y), every time the averaging classier errs, the prob-
ability of the stochastic classier erring must be at least 1=2.
This result is interesting and of practical use when the empirical error rate of the
original averaging classier is low. Furthermore, we can prove that c Q (x) is the
original averaging classier.
Lemma 4.2 For Q = Q(w; ; ) derived according to Theorems 2.1 and 3.1 and
c Q (x) as in lemma 4.1:
c Q (x) = sign (v w (x))
Proof. For every x this equation holds because of two simple facts:
1. For any ^
w that classies an input x dierently from the averaging classier,
there is a unique equiprobable paired weight vector that agrees with the
averaging classier.
2. If v w (x) 6= 0, then there exists a nonzero measure of classier pairs which
always agrees with the averaging classier.

Condition (1) is met by reversing the sign of ^
w> and noting that either the orig-
inal random vector or the reversed random vector must agree with the averaging
classier.
Condition (2) is met by the randomly drawn classier ^
w = w and nearby classiers
for any  > 0. Since the example is not on the hyperplane, there exists some small
sphere of paired classiers (in the sense of condition (1)). This sphere has a positive
measure.
The simple averaging bound is elegant, but it breaks down when the empirical error
is large because:
e(c)  2eQ = 2(^e Q +m ) ' 2^e  (c) + 2m
where ^ e Q is the empirical error rate of a stochastic classier and m goes to zero
as m ! 1. Next, we construct a bound of the form e(c Q )  ^
e  (c) +  0
m where
 0
m > m but ^ e  (c)  2^e  (c).
4.2 A (Sometimes) Tighter Bound
By altering our choice of  and our notion of \error" we can construct a bound
which holds without randomization. In particular, we have the following theorem:
Theorem 4.3 For all averaging classiers c with normalized weights w for all  > 0
\extra" error rates and  > 0 margins:
Pr
SD m
0
B @8; w;  : KL(^e  (c) + ke(c) ) 
ln 1

F

2 
F 1 ()

 + 2 ln m+1
Æ
m
1
C A  1 Æ
where KL(qkp) = q ln q
p + (1 q) ln 1 q
1 p = the Kullback-Leibler divergence between
two coins of bias q < p.
The proof of this statement is strongly related to the proof given in [11] but notice-
ably simpler. It is also very related to the proof of theorem 2.1.
Proof. (sketch) Instead of choosing ^
w k so that the empirical error rate is increased
by , we instead choose ^
w k so that the number of margin violations at margin 
2 is
increased by at most . This can be done by drawing from a distribution such as
^
w k  R
 2 
F 1 ()


Applying the PAC-Bayes bound to this we reach a bound on the number of margin
violations at 
2 for the true distribution. In particular, we have:
Pr
SD m
0
B @KL

^
e  (c) + ke Q; 
2


ln 1

F

2 
F 1 ()

 + ln m+1
Æ
m
1
C A  1 Æ
The application is tricky because the bound does not hold uniformly for all . 3
Instead we can discretize  at scale 1=m and apply a union bound to get Æ ! Æ=m+1.
For any xed example, (x; y) with probability 1 Æ, we know that with probability
at least 1 e Q; 
2
, the example has a margin of at least 
2 . Since the example has
3 Thanks to David McAllester for pointing this out.

a margin of at least 
2 and our randomization doesn't change the margin by more
than 
2 with probability 1 , the averaging classier almost always predicts in the
same way as the stochastic classier implying the theorem.
4.3 Discussion & Open Problems
The bound we have obtained here is considerably tighter than previous bounds for
averaging classiers|in fact it is tight enough to consider applying to real learning
problems and using the results in decision making.
Can this argument be improved? The simple averaging bound (lemma 4.1) and
the margin bound (theorem 4.3) each have a regime in which they dominate. We
expect that there exists some natural theorem which does well in both regimes
simultaneously.
In order to verify that the margin bound is as tight as possible, it would also be
instructive to study lower bounds.
4.4 Acknowledgements
Many thanks to David McAllester for critical reading and comments.
References
[1] P. L. Bartlett, \The sample complexity of pattern classication with neural
networks: the size of the weights is more important than the size of the network,"
IEEE Transactions on Information Theory, vol. 44, no. 2, pp. 525{536, 1998.
[2] Thomas Cover and Joy Thomas, \Elements of Information Theory" Wiley, New
York 1991.
[3] Ralf Herbrich and Thore Graepel, A PAC-Bayesian Margin Bound for Linear
Classiers: Why SVMs work. In Advances in Neural Information Processing
Systems 13, pages 224-230. 2001.
[4] T. Jaakkola, M. Meila, T. Jebara, \Maximum Entropy D iscriminationnchar"
NIPS 1999.
[5] John Langford and Rich Caruana, (Not) Bounding the True Error NIPS2001.
[6] John Langford, Matthias Seeger, and Nimrod Megiddo, \An Improved Predic-
tive Accuracy Bound for Averaging Classiers" ICML2001.
[7] John Langford and Matthias Seeger, \Bounds for Averaging Classiers." CMU
tech report, CMU-CS-01-102, 2001.
[8] David McAllester, \PAC-Bayesian Model Averaging" COLT 1999.
[9] Yoav Freund and Robert E. Schapire, \A Decision Theoretic Generalization of
On-line Learning and an Application to Boosting" Eurocolt 1995.
[10] Matthias Seeger, \PAC-Bayesian Generalization Error Bounds for Gaussian
Processes", Tech Report, Division of Informatics report EDI-INF-RR-0094.
http://www.dai.ed.ac.uk/homes/seeger/papers/gpmcall-tr.ps.gz
[11] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee, \Boosting
the Margin: A new explanation for the eectiveness of voting methods" The
Annals of Statistics, 26(5):1651-1686, 1998.
[12] J. Shawe-Taylor, P. L. Bartlett, R. C. Williamson, and M. Anthony. Struc-
tural risk minimization over data-dependent hierarchies. IEEE Transactions on
Information Theory, 44(5):1926{1940, 1998.

