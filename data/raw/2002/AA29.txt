Dynamic Bayesian Networks with Deterministic Latent Tables

David Barber Institute for Adaptive and Neural Computation Edinburgh University 5 Forrest Hill, Edinburgh, EH1 2QL, U.K. dbarber@anc.ed.ac.uk

Abstract The application of latent/hidden variable Dynamic Bayesian Networks is constrained by the complexity of marginalising over latent variables. For this reason either small latent dimensions or Gaussian latent conditional tables linearly dependent on past states are typically considered in order that inference is tractable. We suggest an alternative approach in which the latent variables are modelled using deterministic conditional probability tables. This specialisation has the advantage of tractable inference even for highly complex non-linear/non-Gaussian visible conditional probability tables. This approach enables the consideration of highly complex latent dynamics whilst retaining the benefits of a tractable probabilistic model. Introduction

1

Dynamic Bayesian Networks are a powerful framework for temporal data models with widespread application in time series analysis[10, 2, 5]. A time series of length T is a sequence of observation vectors V = {v(1), v(2), . . . , v(T )}, where vi(t) represents the state of visible variable i at time t. For example, in a speech application V may represent a vector of cepstral coefficients through time, the aim being to classify the sequence as belonging to a particular phonene[2, 9]. The power in the Dynamic Bayesian Network is the assumption that the observations may be generated by some latent (hidden) process that cannot be directly experimentally observed. The basic structure of these models is shown in fig(1)[a] where network states are only dependent on a short time history of previous states (the Markov assumption). Representing the hidden variable sequence by H = {h(1), h(2), . . . , h(T )}, the joint distribution of a first order Dynamic Bayesian Network is p(V, H) = p(v(1))p(h(1)|v(1)) p(v(t+1)|v(t), h(t))p(h(t+1)|v(t), v(t+1), h(t))

T -1

t=1

This is a Hidden Markov Model (HMM), with additional connections from visible to hidden units[9]. The usage of such models is varied, but here we shall concentrate on unsupervised sequence learning. That is, given a set of training sequences


h(1) h(2) h(t)

v(1) v(2) v(t) h(1), h(2)

(a) Bayesian Network

h(2), h(3) (b) Hidden Inference h(t - 1), h(t)

Figure 1: (a) A first order Dynamic Bayesian Network containing a sequence of hidden (latent) variables h(1), h(2), . . . , h(T ) and a sequence of visible (observable) variables v(1), v(2), . . . , v(T ). In general, all conditional probability tables are stochastic ­ that is, more than one state can be realised. (b) Conditioning on the visible units forms an undirected chain in the hidden space. Hidden unit inference is achieved by propagating information along both directions of the chain to ensure normalisation. V1,...,VP we aim to capture the essential features of the underlying dynamical process that generated the data. Denoting the parameters of the model by , learning can be achieved using the EM algorithm which maximises a lower bound on the likelihood of a set of observed sequences by the procedure[5]:

P

new = arg max  p(Hµ|Vµ, old) log p(Hµ, Vµ, ). (1)

µ=1

This procedure contains expectations with respect to the distribution p(H|V) ­ that is, to do learning, we need to infer the hidden unit distribution conditional on the visible variables. p(H|V) is represented by the undirected clique graph, fig(1)[b], in which each node represents a function (dependent on the clamped visible units) of the hidden variables it contains, with p(H|V) being the product of these clique potentials. In order to do inference on such a graph, in general, it is necessary to carry out a message passing type procedure in which messages are first passed one way along the undirected graph, and then back, such as in the forward-backward algorithm in HMMs [5]. Only when messages have been passed along both directions of all links can the normalised conditional hidden unit distribution be numerically determined. The complexity of calculating messages is dominated by marginalisation of the clique functions over a hidden vector h(t). In the case of discrete hidden units with S states, this complexity is of the order S2, and the total complexity of inference is then O(T S2). For continuous hidden units, the analogous marginalisation requires integration of a clique function over a hidden vector. If the clique function is very low dimensional, this may be feasible. However, in high dimensions, this is typically intractable unless the clique functions are of a very specific form, such as Gaussians. This motivates the Kalman filter model[5] in which all conditional probability tables are Gaussian with means determined by a linear combination of previous states. There have been several attempts to generalise the Kalman filter to include non-linear/non-Gaussian conditional probability tables, but most rely on using approximate integration methods based on either sampling[3], perturbation or variational type methods[5]. In this paper we take a different approach. We consider specially constrained networks which, when conditioned on the visible variables, render the hidden unit


vout(1) vout(2) vout(t)

h(1) h(2) h(t) h(1) h(2) h(t)

v(1) v(2) v(t) vin(1) vin(2) vin(t)

(a) Deterministic Hiddens (b) Input-Output HMM

h(1) h(2) h(t) v(1) v(2) v(3) v(4)

(c) Hidden Inference (d) Visible Representation

Figure 2: (a) A first order Dynamic Bayesian Network with deterministic hidden CPTs (represented by diamonds) ­ that is, the hidden node is certainly in a single state, determined by its parents. (b) An input-output HMM with deterministic hidden variables. (c) Conditioning on the visible variables forms a directed chain in the hidden space which is deterministic. Hidden unit inference can be achieved by forward propagation alone. (d) Integrating out hidden variables gives a cascade style directed visible graph, shown here for only four time steps.

distribution trivial. The aim is then to be able to consider non-Gaussian and nonlinear conditional probability tables (CPTs), and hence richer dynamics in the hidden space. 2 Deterministic Latent Variables

The deterministic latent CPT case, fig(2)[a] defines conditional probabilities p(h(t + 1)|v(t + 1), v(t), h(t)) =  (h(t + 1) - f (v(t + 1), v(t), h(t), h))

(2)

where (x) represents the Dirac delta function for continuous hidden variables, and the Kronecker delta for discrete hidden variables. The vector function f parameterises the CPT, itself having parameters h. Whilst the restriction to deterministic CPTs appears severe, the model retains some attractive features : The marginal p(V) is non-Markovian, coupling all the variables in the sequence, see fig(2)[d]. The marginal p(H) is stochastic, whilst hidden unit inference is deterministic, as illustrated in fig(2)[c]. Although not considered explicitly here, input-output HMMs[7], see fig(2)[b], are easily dealt with by a trivial modification of this framework. For learning, we can dispense with the EM algorithm and calculate the log likelihood of a single training sequence V directly, L(v, h|V) = log p(v(1)|v) + log p(v(t + 1)|v(t), h(t), v) (3)

T -1

t=1


where the hidden unit values are calculated recursively using h(t + 1) = f (v(t + 1), v(t), h(t), h)

(4)

The adjustable parameters of the hidden and visible CPTs are represented by h and v respectively. The case of training multiple independently generated sequences Vµ, µ = 1, . . . P is straightforward and has likelihood L(v, h|Vµ). To maximise the log-likelihood, it is useful to evaluate the derivatives with respect to the model parameters. These can be calculated as follows :

µ

dL dv

T -1

=  log p(v(1)|v) v +  v log p(v(t + 1)|v(t), h(t), v) (5)

t=1

dL dh

T -1

=  h(t) h(t) log p(v(t + 1)|v(t), h(t), v)ddh (6)

t=1

dh(t) dh = f (t) h + f (t) h(t - 1) dh(t - 1) dh (7)

where f(t)  f(v(t), v(t - 1), h(t - 1), h). Hence the derivatives can be calculated by deterministic forward propagation of errors and highly complex functions f and CPTs p(v(t + 1)|v(t), h(t)) may be used. Whilst the training of such networks resembles back-propagation in neural networks [1, 6], the models have a stochastic interpretation and retain the benefits inherited from probability theory, including the possibility of a Bayesian treatment. 3 A Discrete Visible Illustration To make the above framework more explicit, we consider the case of continuous hidden units and discrete, binary visible units, vi(t)  {0, 1}. In particular, we restrict attention to the model:

V

p(v(t+1)|v(t), h(t)) = 

i=1



(2 vi(t + 1) - 1) uijj(t)

j

wijj(t) , hi(t+1) = 

j

where (x) = 1/(1 + e-x) and j(t) and j(t) represent fixed functions of the network state (h(t), v(t)). Normalisation is ensured since 1 - (x) = (-x). This model generalises a recurrent stochastic heteroassociative Hopfield network[4] to include deterministic hidden units dependent on past network states. The derivatives of the log likelihood are given by :

dL dwij = (1 - i(t)) (2vi(t+1)-1)j(t), dL duij = (1 - k(t)) (2vk(t+1)-1)wkll(t) dhl(t duij )

t t,k,l

where i(t)  ((2vi(t + 1) - 1) j wijj(t)), l(t)  dl(t)/dt and the hidden unit

derivatives are found from the recursions

dhl(t + 1) duij = ulk dk(t) duij + ilj(t), dk(t) duij k(t) dhm(t) hm(t) duij =

m k

We considered a network with the simple linear type influences, (t)  (t) 

h(t) v(t) , and restricted connectivity W = A 0 0 B , U = C 0 0 D , where the


h(t) h(t + 1)

v(t) v(t + 1)

(b) original (c) recalled (a) Network

Figure 3: (a) A temporal slice of the network. (b) The training sequence consists of a random set vectors (V = 3) over T = 10 time steps. (c) The reconstruction using H = 7 hidden units. The initial state v(t = 1) for the recalled sequence was set to the correct initial training value albeit with one of the values flipped. Note how the dynamics learned is an attractor for the original sequence.

parameters to learn are the matrices A, B, C, D. A slice of the network is illustrated in fig(3)[a]. We can easily iterate the hidden states in this case to give h(t + 1) = Ah(t) + Bv(t) = Ath(1) + At Bv(t - t )

t-1

t =0

which demonstrates how the hidden state depends on the full past history of the observations. We trained the network using 3 visible units and 7 hidden units to maximise the likelihood of the binary sequence in fig(3)[b]. Note that this sequence contains repeated patterns and therefore could not be recalled perfectly with a model which does not contain hidden units. We tested if the learned model had captured the dynamics of the training sequence by initialising the network in the first visible state in the training sequence, but with one of the values flipped. The network then generated the following hidden and visible states recursively, as plotted in fig(3)[c]. The learned network is an attractor with the training sequence as a stable point, demonstrating that such models are capable of learning attractor recurrent networks more powerful than those without hidden units. Learning is very fast in such networks, and we have successfully applied these models to cases of several hundred hidden and visible unit dimensions. 3.1 Recall Capacity What effect have the hidden units on the ability of Hopfield networks to recall sequences? By recall, we mean that a training sequence is correctly generated by the network given that only the initial state of the training sequence is presented to the trained network. For the analysis here, we will consider the retrieval dynamics to be completely deterministic, thus if we concatenate both hidden h(t) and visible variables v(t) into the vector x(t) and consider the deterministic hidden function f(y)  thresh(y) which is 1 if y > 0 and zero otherwise, then xi(t + 1) = thresh Mijxj(t). (8)

j

Here Mij are the elements of the weight matrix representing the transitions from time t to time t + 1. A desired sequence x(1), . . . , x(T ) can be recalled correctly if M [x(1), . . . , x(T - 1)] = [ (2), . . . , (T )] ~

~ we can find a matrix M and real numbers

i

~ (t) such that


where the i (t) are arbitrary real numbers for which thresh( (t)) = xi(t). This i ~

system of linear equations can be solved if the matrix [x(1), . . . , x(T - 1)] has rank T - 1. The use of hidden units therefore increases the length of temporal sequences that we can store by forming, during learning, appropriate hidden representations

h(t) such that the vectors h(2) v(2) , . . . , h(T ) v(T ) form a linearly independent set.

~ ~

Such vectors are clearly possible to generate if the matrix U is full rank. Thus recall can be achieved if (V + H)  T - 1. The reader might consider forming from a set of linearly dependent patterns v(1), . . . , v(T ) a linearly independent is by injecting the patterns into a higher dimensional space, v(t)  v(t) using a non-linear mapping. This would appear ^ to dispense with the need to use hidden units. However, if the same pattern in the training set is repeated at different times in the sequence (as in fig(3)[b]), no matter how complex this non-linear mapping, the resulting vectors v(1), . . . , v(T ) ^ ^ will be linearly dependent. This demonstrates that hidden units not only solve the linear dependence problem for non-repeated patterns, they also solve it for repeated patterns. They are therefore capable of sequence disambiguation since the hidden unit representations formed are dependent on the full history of the visible units. 4 A Continuous Visible Illustration To illustrate the use of the framework to continuous visible variables, we consider the simple Gaussian visible CPT model

1 2

p(v(t + 1)|v(t), h(t)) = exp -2 [v(t + 1) - g (Ah(t) - Bv(t))] /(22)V h(t + 1) = f (Ch(t) + Dv(t))

2

/2

(9)

where the functions f and g are in general non-linear functions of their arguments. In the case that f(x)  x, and g(x)  x this model is a special case of the Kalman filter[5]. Training of these models by learning A, B, C, D (2 was set to 0.02 throughout) is straightforward using the forward error propagation techniques outlined earlier in section (2). 4.1 Classifying Japanese vowels This UCI machine learning test problem consists of a set of multi-dimensional times series. Nine speakers uttered two Japanese vowels /ae/ successively to form discrete time series with 12 LPC cepstral coefficients. Each utterance forms a time series V whose length is in the range T = 7 to T = 29 and each vector v(t) of the time series contains 12 cepstral coefficients. The training data consists of 30 training utterances for each of the 9 speakers. The test data contains 370 time series, each uttered by one of the nine speakers. The task is to assign each of the test utterances to the correct speaker. We used the special settings f(x)  x and g(x)  x to see if such a simple network would be able to perform well. We split the training data into a 2/3 train and a 1/3 validation part, training then a set of 10 models for each of the 9 speakers, with hidden unit dimensions taking the values H = 1, 2, . . . , 10 and using 20 training iterations of conjugate gradient learning[1]. For simplicity, we used the same number of hidden units for each of the nine speaker models. To classify a test utterance, we chose the speaker model which had the highest likelihood of generating the test utterance, using an error of 0 if the utterance was assigned to the correct speaker and an error of 1 otherwise. The errors on the validation set for these 10 models


2 0 -2 20 0 -2 20 0 -2 20 0 -2 20 0

-2 0

5 10 15 20 25 30 35 40

5 10 15 20 25 30 35 40

5 10 15 20 25 30 35 40

5 10 15 20 25 30 35 40

2 0 -2 20 0 -2 20 0 -2 20 0 -2 20 0

-2 0

5 10 15 20 25 30 35 40

5 10 15 20 25 30 35 40

5 10 15 20 25 30 35 40

5 10 15 20 25 30 35 40

5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40

Figure 4: (Left)Five sequences from the model v(t) = sin(2(t - 1) +

(Right) Five sequences from the model v(t) = sin(5(t - 1) +

i

3

1 (t)) + 0.1 2 (t).

(t)) + 0.1 4 (t), where

(t) are zero mean unit variance Gaussian noise samples. These were combined

to form a training set of 10 unlabelled sequences. We performed unsupervised

learning by fitting a two component mixture model. The posterior probability

p(i = 1|Vµ) of the 5 sequences on the left belonging to class 1 are (from above) 0.99, 0.99, 0.83, 0.99, 0.96 and for the 5 sequences on the right belonging to class 2 are (from above) 0.95, 0.99, 0.97, 0.97, 0.95, in accord with the data generating process.

were 6, 6, 3, 5, 5, 5, 4, 5, 6, 3. Based on these validation results, we retrained a model with H = 3 hidden units on all available training data. On the final independent test set, the model achieved an accuracy of 97.3%. This compares favourably with the 96.2% reported for training using a continuous-output HMM with 5 (discrete) hidden states[8]. Although our model is not powerful in being able to reconstruct the training data, it does learn sufficient information in the data to be able to make reliable classification. This problem serves to illustrate that such simple models can perform well. An interesting alternative training method not explored here would be to use discriminative learning[7]. Also, not explored here, is the possibility of using Bayesian methods to set the number of hidden dimensions. 5 Mixture Models Since our models are probabilistic, we can apply standard statistical generalisations to them, including using them as part of a M component mixture model p(V|) = p (V|i, i) p (i) (10)

M

i=1

where p(i) denotes the prior mixing coefficients for model i, and each time series component model is represented by p (V|i, i). Training mixture models by maximum likelihood on a set of sequences V1, . . . , VP is straightforward using the standard EM recursions [1]:

p(Vµ|i, old)pold(i) i

p(Vµ|i, old)pold(i) i

pnew(i) =

P µ=1

M i=1 P

(11)

P µ=1

new = arg max i

i

p(Vµ|i, old) log p(Vµ|i, i) i (12)

µ=1

To illustrate this on a simple example, we trained a mixture model with component models of the form described in section (4). The data is a series of 10 one dimensional (V = 1) time series each of length T = 40. Two distinct models were used


to generate 10 training sequences, see fig(4). We fitted a two component mixture model using mixture components of the form (9) (with linear functions f and g) each model having H = 3 hidden units. After training, the model priors were found to be roughly equal 0.49, 0.51 and it was satisfying to find that the separation of the unlabelled training sequences is entirely consistent with the data generation process, see fig(4). An interesting observation is that, whilst the true data generating process is governed by effectively stochastic hidden transitions, the deterministic hidden model still performs admirably. 6 Discussion We have considered a class of models for temporal sequence processing which are a specially constrained version of Dynamic Bayesian Networks. The constraint was chosen to ensure that inference would be trivial even in high dimensional continuous hidden/latent spaces. Highly complex dynamics may therefore be postulated for the hidden space transitions, and also for the hidden to the visible transitions. However, unlike traditional neural networks the models remain probabilistic (generative models), and hence the full machinery of Bayesian inference is applicable to this class of models. Indeed, whilst not explored here, model selection issues, such as assessing the relevant hidden unit dimension, are greatly facilitated in this class of models. The potential use of this class of such models is therefore widespread. An area we are currently investigating is using these models for fast inference and learning in Independent Component Analysis and related areas. In the case that the hidden unit dynamics is known to be highly stochastic, this class of models is arguably less appropriate. However, stochastic hidden dynamics is often used in cases where one believes that the true hidden dynamics is too complex to model effectively (or, rather, deal with computationally) and one uses noise to `cover' for the lack of complexity in the assumed hidden dynamics. The models outlined here provide an alternative in the case that a potentially complex hidden dynamics form can be assumed, and may also still provide a reasonable solution even in cases where the underlying hidden dynamics is stochastic. This class of models is therefore a potential route to computationally tractable, yet powerful time series models. References [1] C.M. Bishop, Neural Networks for Pattern Recognition, Oxford University Press, 1995. [2] H.A. Bourlard and N. Morgan, Connectionist Speech Recognition. A Hybrid Approach., Kluwer, 1994. [3] A. Doucet, N. de Freitas, and N. J. Gordon, Sequential Monte Carlo Methods in Practice, Springer, 2001. [4] J. Hertz, A. Krogh, and R. Palmer, Introduction to the theory of neural computation., Addison-Wesley, 1991. [5] M. I. Jordan, Learning in Graphical Models, MIT Press, 1998. [6] J.F. Kolen and S.C. Kramer, Dynamic Recurrent Networks, IEEE Press, 2001. [7] A. Krogh and S.K. Riis, Hidden Neural Networks, Neural Computation 11 (1999), 541­563. [8] M. Kudo, J. Toyama, and M. Shimbo, Multidimensional Curve Classification Using Passing-Through Regions, Pattern Recognition Letters 20 (1999), no. 11-13, 1103­ 1111. [9] L.R. Rabiner and B.H. Juang, An introduction to hidden Markov models, IEEE Transactions on Acoustics Speech, Signal Processing 3 (1986), no. 1, 4­16. [10] M. West and J. Harrison, Bayesian forecasting and dynamic models, Springer, 1999.


