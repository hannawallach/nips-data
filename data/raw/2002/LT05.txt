Concentration Inequalities for the Missing Mass and for Histogram Rule Error

David McAllester Toyota Technological Institute at Chicago mcallester@tti-c.org Luis Ortiz University of Pennsylvania leo@cis.upenn.edu

Abstract This paper gives distribution-free concentration inequalities for the missing mass and the error rate of histogram rules. Negative association methods can be used to reduce these concentration problems to concentration questions about independent sums. Although the sums are independent, they are highly heterogeneous. Such highly heterogeneous independent sums cannot be analyzed using standard concentration inequalities such as Hoeffding's inequality, the Angluin-Valiant bound, Bernstein's inequality, Bennett's inequality, or McDiarmid's theorem. 1 Introduction The Good-Turing missing mass estimator was developed in the 1940s to estimate the probability that the next item drawn from a fixed distribution will be an item not seen before. Since the publication of the Good-Turing missing mass estimator in 1953 [9], this estimator has been used extensively in language modeling applications [4, 6, 12]. Recently a large deviation accuracy guarantee was proved for the missing mass estimator [15, 14]. The main technical result is that the missing mass itself concentrates -- [15] proves that

the probability that missing mass deviates from its expectation by more than

¡£¢¥¤§¦©¨

 

is at most

independent of the underlying distribution. Here we give a simpler proof of the

stronger result that the deviation probability is bounded by

¡¢¥¤§¦©¨

.

A histogram rule is defined by two things -- a given clustering of objects into classes and a given training sample. In a classification setting the histogram rule defined by a given clustering and sample assigns to each cluster the label that occurred most frequently for that cluster in the sample. In a decision-theoretic setting, such as that studied by Ortiz and Kaebling [16], the rule associates each cluster with the action choice of highest performance on the training data for that cluster. We show that the performance of a histogram rule (for a fixed clustering) concentrates near its expectation -- the probability that the performance clustering or the underlying data distribution. 2 The Exponential Moment Method All of the results in this paper are based on the exponential moment method of proving concentration inequalities. The exponential moment was perhaps first used by Bernstein

deviates from its expectation by more than   is bounded by ¡¢¤§¦¨ independent of the


but was popularized by Chernoff. Let

mean. Let

¡£¢¥¤¦ ¨§© ¢¥¤¦ © © ! ¢¥¤¦ #"$© ©% !.

be if and is

 

be any real-valued random variable with finite

The

following lemma is the central topic of Chernoff's classic paper [5].

Lemma 1 (Chernoff) For any real-valued variable

following for any

©

where the "entropy"

¡)¢¥¤¦ §0©12"

&'¤¦ §(©

¡ ¢

 

with finite mean

 ! we have the

is defined as below.

13547698@BA

&'¤C §©1ED F(GIHP ©RQ!SUTWVYX`¤C §0Qa

Xb¤¦ §0QcED ed¡ P 6gf

Lemma 1 follows, essentially, from the observation that for

¢¥¤¦ pq©1r"ets¡

P 4W6 1@uAwv

¢ D ¡ ¢

P @

ed¡ D

P QheiP

(1) (2) (3) we have the following.

6'f a47@ RxyR47698 A¦A ¡ ¢ ¢ P

(4)

Lemma 1 is called the exponential moment method because of the first inequality in (4). The following two observations provide a simple general tool.

Observation 2 Let

all

Qi



be any positive constant satisfying

  i &'¤w ! 9  ¤¦.

   

T7VYXb¤¦ §0Qc'"e QquQ for

. Formula (2) implies that for we have

Observation 3 If

 ) u, ¥ ,

are independent then

TWVYX`¤dtef  e §QagDhiejT7VYXb¤¦  e §0Qc .

Some further observations also prove useful. Let

 

be an arbitrary real-valued random

variable. For a discrete distribution the Gibbs distribution

¢ ¤¦ kDe©1gD P

¢ P

can be defined as follows.

P @ ¡

Xb¤¦ ¨§cQc l ¢¥¤C mDt©

There exists a unique largest open interval

such that for define the expectation of

Qr¤CQ n0oy §(Qs¤C U n0p(q

we have that

¤CQn0oX`¤C §Qc

at inverse temperature

y §0Q5n0p(qj

(possibly with infinite endpoints)

Qisas

finite. For

follows.

¡

P 6rf

Qr¤CQ n0oy §rQ n0p(q 

we

 s¤¦ Ud1D P

¢z¨{Y¤C¢¢

to

P

QP

Xb¤¦ ¨§0QaP

Equation (5) can be taken as the definition of let be

Gibbs-variance at inverse temperature . For

the KL-divergence from

P |W|¢`}Dt P

l  ds¤¦ U

Qtrh¤¦QRn0oy §uQ5n0p(qj v  ¤C §uQa  d¤¦ wSh¢  x 

Qyr¤CQP n0oy §Q n0pq 

for continuous distributions on

. The quantity

we let

z¨{Y¤C¢v ¢`

P|7|

f  ¤¦ §uQa 

which can be written as follows.

 !Q!S~T7VYXb¤¦ ¨§0Qa

(5) . For is the denote (6)

Let for

Qr¤CQ n0oy §Q n0p(qbethe

¤C©5n0oy §©Rn0pqj

smallest open interval containing all values of the form

. If the open interval

¤¦© n0oy §`© n0p(q 

is not empty then

For

Q



`¤C©gDts¤¦©1

 @ s¤w9  

satisfying

where

Qr¤CQP n0oy §bQ n0p(q. .

  !gD©

©r¤¦© n0oy §© n0p(qP 

  

P  

is a

monotonically increasing function of

Qg¤C©

to be the unique value

now define the double integral

¤CgDii

of

, and

define

For any continuous function

`w¤C© `¤¦©1

and

¥¤¦©1

¥¤CgDiwe,

s

to be the function satisfying

are the first and second derivatives

respectively. We now have the following general theorem.

Theorem 4 For any real-valued variable

¤¦Q n0oy §0Q n0p(q 

we have the following.

  , any © r ¤C© n0oy §(© n0p(q  , and Q r

(7) &'¤¦ ¨§c©1D ©5Qg¤C©0S~TWVYX`¤C §0Qg¤¦©1


D z¨{ ¤w¢ 47@uA ¢` P |W|

¡¢¡

@

£¥¤

6§¦ v  ¤C §0Qg¤P (

¨

v  ¤¦ §   

 ©¨

¡¡



(8)

D (9)

TWVYX`¤C §QaD } !uQ)

Formula (9) can be clarified by noting that for

&g¤¦ §(©}D ¡¢¡

@

£¥¤

 (10)

|©£Sh ! |

small we have the following.

6§¦ v  ¤C §0Qg¤¨ ( v  ¤¦ §}i  

 ¨ ¤C©£S~¨ !x 

Formula (7) is proved by showing that

Qg¤¦©1

is the the optimal

Q

in (2). Up to sign conven-

tions (7) is the equation for physical entropy in statistical mechanics. Equation (8) follows from (7) and (6). Equations (9) and (10) then follow from well known equations of statistical mechanics. An implicit derivation of (9) and (10) can be found in section six of Chernoff's original paper [5]. As a simple example of the use of (9), we derive Hoeffding's inequality. Consider a sum

  D  e    e 

Note that each

"$ ¨#

. We then have that

(1) and (9).

 where e

the

  e

are independent and

  e

is bounded to an interval of width ! .

remains bounded to this interval at all values of . Hence

v  ¤¦ §'Qc "  e%  e

$

 

Q

v ¤¦  e §gQar"e

! . Hoeffding's inequality now follows from

3 Negative Association The analysis of the missing mass and histogram rule error involve sums of variables that are not independent. However, these variables are negatively associated -- an increase in one variable is associated with decreases in the other variables. Formally, a set of real-valued

random variables

'

 ¥

of the integers (

l

is negatively associated if for any two disjoint subsets & and , and any two non-decreasing, or any two non-increasing,

s

 )§,uu,§0)1

functions

from 203453 to 2 and 6 from 273893 to 2 we have the following.

s¤¦  e §A@}rB&C6 ¤¦ EDj§GF¥r  c"s¤¦  e §A@ rH& 1¤C IDf§GF£r d '

6

'

Dubhasi and Ranjan [8] give a survey of methods for establishing and using negative association. This section states some basic facts about negative association.

Lemma 5 Let

   u,  

, be any set of negatively associated variables. Let

 ~ u, ¨

,

  e.

&'¤C §©1r&'¤C Uw§©

  D  ef  e  U1D  ef ¨e.

and

.

 e

be independent shadow variables, i.e., independent variables such that

identically to Let

variables we have

Lemma 6 Let &

is distributed

For any set of negatively associated

be any sample of P items (ball throws) drawn IID from a fixed distribution

on the integers (bins) ( sample. The variables j¤Sl  u,S ¤UQb. § u§RQ71

l ,

Let S

¤T@ 

be the number of times integer @ occurs in the

are negatively associated.

Lemma 7 For any negatively associated variables

functions

s  u,s 

, , we have that the quantities

ciated. This also holds if the functions

s e

s  ¤C     ,s  ¤¦   

, u, ¥

, and any non-decreasing

are negatively asso,

are non-increasing.


Lemma 8 Let

 ) ,  ¥

, be a negatively associated set of variables. Let

l

of

|   e then §    D ¢¥¤aree Dnegativelye  ¢¥¤is  D

§uu

 

¢ 

l  

. If ¢ 

  ,

u,   

|

e

associated. This also holds if

l

|

  e

  e,

¥u, 

 

¢¥¤ e D

 

¡  be 0-1 (Bernoulli) variables such that a stochastic function of i.e.,

is a non-decreasing function

¢¥¤ e D

¡ 

l   e

is

|

non-increasing.

4 The Missing Mass Suppose that we draw words (or any objects) independently from a fixed distribution over

a countable (but possibly infinite) set of words. We let the probability of drawing word

be denoted as

¢

¥¤ . For a sample

&

of P draws the missing mass of , denoted

total probability mass of the items not occurring in the sample, i.e.

Theorem 9 For the missing mass ing.  

as defined above, and for  

 kD& 

¤§¦¨

3 ¢

 

¤

£

, is the .

ti

, we have the follow-

&'¤¦ §}¨ !5S 2  

&'¤¦ §}¨ ! 2  

 © P   

P   

To prove theorem 9 let

 

¤ be a Bernoulli variable which is 1 if word £

(11) (12) does not occur in

  D  ¢  

¤ ¤ ¤ . the sample and 0 otherwise. The missing mass can now be written as

The variables have that the

variables

 

¤

  

¤ ¤ are monotonic functions of the word counts so by lemmas 6 and 7 we are negatively associated. By lemma 5 we can then assume that the

are independent. The analysis of this independent sum uses the following

general concentration inequalities for independent sums of bounded variables.



and each ! is a non-negative constant. Let

Lemma 10 Let have the following.

with

l   e rti5§   D  e% e e  e ! where  ) u,   ,



are independent random variables

 be . For

  i

we

e   e

&'¤C §( !5S 2  

&'¤C §( ! 2  

 

 

  e  e! e



 

 e  



xy

"

¨#

 #

(13)

(14)

Before proving (13) and (14) we first show how (13) and (14) imply (11) and (12) respec-

tively. For the missing mass

 ¤

 mD  ¢ c 

¤ ¤ ¤ we have the following.

Dt¢¥¤C  D }D¤ S~¢  " ¤

¤

l l

¤

¡ ¢ "! ¤

To prove (11) we note that formula (13) implies the following where we use the fact that

for

©!ei

we have

¡ ¢ "  ¤¡ ©

l

.

&'¤¦ ¨§( !5S 9  

 ¡

@

 ¢ 

 

¤ ¤



 

 ¡ ¢#$! ¤   ¢  ¤¡ 

¤ %¤ P

D  P   

To prove (12) we note that formula (14) implies the following.

&'¤¦ §   p       ¢   T7V

¤ ¤

   

&

 ¢  ¤

%¤



P

D P   

!


We now compare (13) and (14) to other well known bounds. Hoeffding's inequality [11]

yields the following.

&'¤¦ §   9  



 

 e% e  !



 e% e





! can be In the missing mass application we have that

  ¤  l

(15) which fails to yield (12).

The Srivistav-Stangier bound [17], which itself an improvement on the Angluin-Valiant

bound [1, 10], yields the following for

i " "    

where !

 



n0pq ¡£¢¥¤ e e.

is

!

n0p(q e e e   !



n0p(q  e e e





! 

&g¤¦ §  ! 9   © (16)

  ¤ 

!

It is possible to show that in the missing mass application ! can be

so this bound does not handle the missing mass. A weaker version of the lower-deviation inequality (13) can be derived from Bernstein's inequality [3] (see [7]). However, neither Bernstein"s inequality nor Bennett's inequality [2] can handle the upward deviation of the missing mass. To prove (13) and (14) we first note the following lemma. Lemma 11 Let 1 beaBernoulli

variable with

 Ube1Dt !.

 

a random variable with

  rhiI§

For any such variables

l 

and let

and

 ~ hand r BiI§ ( any lQ

and constant



we have the following.

T7VYXb¤C  §(Qa9"qTWVYX`¤w   §(Qa

l

This lemma follows from the observation that for any convex function

iI§ l

we have that

ed¡

P  6'f

s¤C©

is less than

¤ SU©1s¤wi y©s¤ 

l l

¡

P  f

s

on the interval

and so we have the following.

"idx¤ SU ~   l D¤ S~ !xy¨ ! Dits¡ ¡

l

P  P  6§¦xv

Lemma 11 and equation (2) now imply the following which implies that for the proof of

(13) and (14) we can assume without loss of generality that the variables

Lemma 12 Let have the following. .  ¨}D  e! e ¨e

where

To prove (13) let

Qh"i

  D   e! e  e e r Bi5§with1

( l

  e r iI§

  e

are Bernoulli.

 ¨e ¨  e

=

l

with the variables

  e

independent. Let

   U,

, and

 

we with . For any such

&'¤C §©1r&'¤C  §©

where the

  e   D e%  e  e



we have the following.

! are independent Bernoulli variables. For

v  ¤¦  e §0Qar"e¢ ¤C  e D 9" e 

v  ¤C §Qc¨"pie e e.

! 

l

Formula (13) now follows from (9). Formula (14)

P

So we have

follows from observations 2 and 3 and the following lemma of Kearns and Saul [13].

Lemma 13 (Kearns&Saul) For a Bernoulli variable

¢¥¤ D 

¢ 

l

.

  we have the following where  is

TWVYX`¤! g§0Qc2"  g uQ l}TWV   !  

"  g uQ !   }TWV!



¤ S `¢     ! &

&

Q   &

Q  (17)

(18)


5 Histogram Rule Error Now we consider the problem of learning a histogram rule from an IID sample of pairs

 

©a§¢¡¤£br¦¥¨§©

drawn from a fixed distribution

¡

on such pairs. The problem is to find

a rule  mapping ¥ to the two-element set (

loss 

¤ ¤C© §¡I

iI§

where  is a given loss function from (

l

1 so as to minimize the expectation of the

1§© to the interval

iI§

. In the

a¤¦©

l BiI§Bi5§¤1

l l

classification setting one typically takes © to be (

. In the decision-theoretic setting ¡

is the hidden state and can be arbitrarily complex and  



a¤¦©1 §¡

is the cost of taking action

in the presence of hidden state ¡ . In the general case (covering both settings) we

assume only 

a¤¦©19r iI§ (

l

1 and   j¤a¤¦©1 §¢¡IrrhiI§

l

.

We are interested in histogram rules with respect to a fixed clustering. We assume a given pairs drawn IID from a fixed distribution on ¥!§"©  . For any cluster index F , we define D

cluster function  mapping ¥ to the integers from

l

to . We consider a sample

such that 

& of P

to be the subset of the sample consisting of pairs

to be

|& |

D . For any cluster index F and

# D £

j¤}Dti

If S %F

¤ugD l S F   ¤

$

©§¢¡¤£ 3&% ¨

r Bi5§

( £

l

¤ `§¢¡I § ¢£

©§¢¡¤£

1 we define %D

j¤Y

¢£

¤¦©1gD

and # D

F . We define S %F

¤Y

£

D

as follows.

¤ b§¢¡I £

j¤& 

 D £ ¤YgDt  

©§¡'£(0) 47@BA 321

then we define # D

¤Y £ to be 1. We now define the rule # and 43 from class index

to labels as follows.

¤# }D %F ¤

¢6527A@¡98 WV ¨ 8 B j¤Y § # D £  3 %F ¤gD ¤ ¢C57A@¡D8 WV ¨ 8 2B ¤Y  D ¢£

Ties are broken stochastically with each outcome equally likely so that the rule E3 is a random variable only partially determined by the sample . We are interested in the gen-

eralization loss of the empirical rule # .

¤ RgDi #   ©a§¢¡¤£¢(E)

& s(¤ ¤# ¤¦©1 §¢¡I



v

Theorem 14 For F# defined as above we have the following for positive .

&HGI¤# §  s¤ 5# S v &HGI¤# §  s¤ 5#  v  2P  2P  P

  

Q

¤   

 P   R  (19) (20)

To prove this we need some additional terminology. For each class label F define D to be

 

the probability over selecting a pair

 D 43 F

¤ j¤

. In other words,

{AD

©§¡'£

that 

¤¦©1gD

F . Define D to be  D

{

is the additional loss on class F when # assigns the wrong

¢ ¤ SS43f¤%FcS l

label to this class. Define the random variable

The variable

 

 0D

to be 1 if # T

a¤ F DUV3j¤F

and 0 otherwise.

D represents the statement that the empirical rule is "wrong" (non-optimal)

on class F . We can now express the generalization loss of # as follows.

¤ }DX¤  

W# 3

$

e

¢ e{ e  e

The variable

 

D is a monotone stochastic function of the count

j¤S F

(21) -- the probability

of error declines monotonically in the count of the class. By lemma 8 we then have that the variables are negatively associated so we can treat them as independent. To prove

theorem 14 we start with an analysis of

Lemma 15

¢¥¤¦  D  D

l

.

  e

¢¥¤C  D 9" D l ©

¡ ¢`Yba ¤  %¢c % ¨


Proof: To prove this lemma we consider a threshold )

¢¥¤j¤S r" 2"

%F ) S F

| ¤9 2"

)

~" ¢ P D and show the following.

¢¥¤¦ ED D 2" ¢¥¤S ¤r"¤£¥¤ )¨ y¢¥¤C IDYD

l F

 §¦¨ %

¡ ¢ ¤ ¨

 ¢ ¡

 %

¡ ¢ ¢  ¨  % 



l

| j¤S 9 )

%F

¢¥¤C  D D

l

(22) (23) (24)

Formula (23) follows by the Angluin-Valiant bound [1, 7].1 To prove (24) we note that if a combination of Hoeffding's inequality and the union bound we have that the probability that one of these two conditions holds is bounded by the left hand side of (24). Lemma 15

 ED D l then either # D ¤43f¤%F (r ¤ ¤%F  {  f¤D S`43 ¤%F (r" ¤ S`V3 ¤IS£{   43 or # l l F  . By

now follows by setting ) to P D and noting that D

We now prove (19) using lemma 15 and (10). For

Qh"i

we have the following.

¨ ¢ { "

.

©" !

l

we have Qg¤¦©1"i and for

v  ¤C¢ e { e  IDj§0QcD ¢ e { e ¢ ¤¦ EDYD  ¤ S~¢ ¤¦ EDYD  " ¢ e { e ¢ ¤¦  D 9"¢ e { e ¢ ¤¦  D 9"e¢ e { e P

Since ED is bounded to the interval

by

l l l

D

iI§

l 

D

l

  ¢ e { e   l

we have that

Qa

. By (10) we then have the following for

P P ©

¡ ¢ Y ¤ C% c %¨ also bounded

 a

©

 T7V 

l

©

 @

¡ ¢

Q"i

deriving (27) we use the fact that

©  l

.

is a monotonically decreasing function of

v  ¤C¢ e{ e EDf§Dm¤isl

where

©

%XWY¥` G

G

I G

© . In for

 "!$#&%('0)21436567#98@'BADCE G

g $

a2b sq F

# c ¨#

F HGPI"QGSR Q

GR G I Q V 

G V P

A a2b

$

GHT¡U

$

g

g

3h5(7#98i'pA C Erqs 13h5(7#98i'BADCE 3h5(7#98i'BADCE 3h5(7#98i'BA C

$

a2b HG$

I utwvxy c# G ItvG x

¨#

Q

P

'

x

G

VC

I G

I

Y  a

x

I Q W Y¥` 

a2b GR G

dc# ¨# feP

#

x A $

Q a2b

c dc# wv iv

¨#

x WY¥` baY

' a2b Q

(25)

dc# ¨# fP ' Q

(26)

¨#

x G  a Y v fP  ' Q

(27)

V 0eP ' (28)

E V x G (29)

Formula (19) now follows from (29) and a downward variant of observation 2. The proof. of (20) is similar but uses (18). For

 "!$#&%('0)1 365(7#8i'BA C Eqs 1 365(7#8i'BAdCE

1

Qi

$

# c ¨#

we have the following where

I

x A

I

$

G © is

F

l

x E x

£X feg C  v ¦

x G2GQ

W

©

¤ )TWVIGRGR   ©

sq

a2b a2b

$

c#

utv utv I

G xyG

x G V P

¨#

GR G I Q V  A a2b

$

Ic GvF

¨#

x E

£X I Q

eh

#

e¡ ' Q

dc#

a2b iv ¦

¨#

' Q

The downward deviation Angluin-Valiant bound used here follows from (9) and the observation

that for a Bernoulli variable

i

and

'j14k l i Q ! %'0)X1 !i I

we have

g C )

.


g

3 5 7#8i'BA C E 3 5 7#8i'BAdCE F

$ G I

G

g x G V 0eP ' Q

(30)

G

V x P ' Q

(31)

Formula (20) now follows from (31) and observation 2. References [1] D. Anguluin and L. Valiant. Fast probabalistic algorithms for hamiltonian circuits. Journal of Computing Systems Science, 18:155­193, 1979. [2] G. Bennnett. Probability inequalities for the sum of independent ranndom variables. Journal of the American Statistical Association, 57:33­45, 1962. [3] S. Bernstein. The Theory of Probabilities. Gastehizdat Publishing House, Moscow, 1946. [4] Stanley Chen and Joshua Goodman. An empirical study of smoothing techniques for language modeling, August 1998. Technical report TR-10-98, Harvard University. [5] H. Chernoff. A measure of the asymptotic efficiency of tests of a hypothesis based on the sum of observations. Annals of Mathmematical Statistics, 23:493­507, 1952. [6] Kenneth W. Church and William A. Gale. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5:19­54, 1991. [7] Luc Devroye, L´aszl´o Gy¨orfi, and G´abor Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, 1996. [8] Devdatt P. Dubhashi and Desh Ranjan. Balls and bins: A study in negative dependence. Random Structures and Algorithms, 13(2):99­124, 1998. [9] I. J. Good. The population frequencies of species and the estimation of population parameters. Biometrika, 40(16):237­264, December 1953. [10] T. Hagerup and C. R¨ub. A guided tour of chernoff bounds. Information Processing Letters, 33:305­309, 1989. [11] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13­30, 1963. [12] Slava M. Katz. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing, ASSP35(3):400­401, March 1987. [13] Michael Kearns and Lawrence Saul. Large deviation methods for approximate probabilistic inference, with rates of convergence. In UAI-98, pages 311­319. Morgan Kaufmann, 1998. [14] Samuel Kutin. Algorithmic Stability and Ensemble-Based Learning. PhD thesis, University of Chicago, 2002. [15] David McAllester and Robert Schapire. On the convergence rate of good-turing estimators. In COLT00, 2000. [16] Luis E. Ortiz and Leslie Pack Kaelbling. Sampling methods for action selection in influence diagrams. In Proceedings of the Seventeenth National Conference on Artificial Intelligence, pages 378­385, 2000. [17] Anand Srivastav and Peter Stangier. Integer multicommodity flows with reduced demands. In European Symposium on Algorithms, pages 360­371, 1993.


