Ranking with Large Margin  Principle: Two Approaches

Amnon Shashua School of CS&E Hebrew University of Jerusalem Jerusalem 91904, Israel email: shashua@cs.huji.ac.il Anat Levin School of CS&E Hebrew University of Jerusalem Jerusalem 91904, Israel email: alevin@cs.huji.ac.il

Abstract We discuss the problem of ranking instances with the use of a "large margin" principle. We introduce two main approaches: the first is the "fixed margin" policy in which the margin of the closest neighboring classes is being maximized -- which turns out to be a direct generalizadifferent margins where the sum of margins is maximized. This approach

¡

¡£¢¥¤ tion of SVM to ranking learning. The second approach allows for

¡¨§©

is shown to reduce to -SVM when the number of classes

approaches are optimal in size of

¦ © 

. Both

where is the total number of training

examples. Experiments performed on visual classification and "collaborative filtering" show that both approaches outperform existing ordinal regression algorithms applied for ranking and multi-class SVM applied to general multi-class classification.

1 Introduction In this paper we investigate the problem of inductive learning from the point of view of predicting variables of ordinal scale [3, 7, 5], a setting referred to as ranking learning or ordinal regression. We consider the problem of applying the large margin principle used in Support Vector methods [12, 1] to the ordinal regression problem while maintaining an (optimal) problem size linear in the number of training examples.

Let

% §&¤!"#"be%theis  



set of training examples where



the index within each class. Let

§¤!"#"$¡ '§)( %  

denotes the class number, and be the total number of training

examples. A straight-forward generalization of the 2-class separating hyperplane problem,

where a single hyperplane determines the classification rule, is to define

hyperplanes which would separate the training data into

¡

separating

ordered classes by modeling the

¡0¢1¤

ranks as intervals on the real line -- an idea whose origins are with the classical cumulative

model [9], see also [7, 5]. The geometric interpretation of this approach is to look for

parallel hyperplanes represented by vector

and scalars

V

"" ACBED DFAHGPIQB 46587@9

(the dimension of the input vectors)

¡2¢3¤

defining the hyperplanes

 !"#"  RS4 APB!T RS4 AHGPIUBCT

, such that the

This work was done while A.S. was spending his sabbatical at the computer science department

of Stanford University.


2 |w| maximize the margin b1 - a1 | w | b2 - a2 | w |

(w,b1) (w,b2) (w,a1) (w,b1) (w,a2) (w,b2)

Fixed-margin Sum-of-margins

Figure 1: Lefthand display: fixed-margin policy for ranking learning. The margin to be maximized is associated with the two closest neighboring classes. As in conventional SVM, the margin is prescaled to be equal to  ¢¡¤£ ¥¦£ thus maximizing the margin is achieved by minimizing ¥¨§©¥ . The support vectors lie on the boundaries between the two closest classes. Righthand display: sum-of-margins policy for ranking learning. The objective is to maximize the sum of  margins. Each class is sandwiched between two hyperplanes, the norm of ¥ is set to unity as a constraint in the optimization on the boundaries among all neighboring classes (unlike the fixed-margin policy). When the number of classes &%'  , the dual functional is equivalent to ( -SVM.

(  problem and as a result the objective is to maximize $# "! . In this case, the support vectors lie

data are separated by dividing the space into equally ranked regions by the decision rule

)

R T § 02143

BA@©B©B©B©@G7C

D¢EGF

57698

¢ 4IHH A

5QPSRUT

In other words, all input vectors

E

(using the convention that

A G

§XWsatisfying  A IQB 4VH  A

5 P P 5

(1) are assigned the rank

). For instance, recently [5] proposed an "on-line"

algorithm (with similar principles to the classic "perceptron" used for 2-class separation) for finding the set of parallel hyperplanes which would comply with the separation rule above. To continue the analogy to 2-class learning, in addition to the separability constraints on lowering the "empirical risk" Q`badc eY (error measure on the training set) and lowering functions. The "structural risk minimization" (SRM) principle [12] minimizes a bound on the risk over a structure on the set of functions. The geometric interpretation for 2-class learning is to maximize the margin between the boundaries of the two sets [12, 1]. In our setting of ranking learning, there are margins to consider, thus there are two possible approaches to take on the "large margin" principle for ranking learning: "fixed margin" strategy: the margin to be maximized is the one defined by the closest

§ D  #" the variables Y 4 ACB D D)AHGPIUB T one would like to control the tradeoff between

7ih R T

RgY T the "confidence interval" f h controlled by the VC-dimension of the set of loss

¡ ¢1¤

(neighboring) pair of classes. Formally, let

 4 A7p

be the hyperplane separating the two

pairs of classes which are the closest among all the neighboring pairs of classes. Let

 4 A

p

be scaled such the distance of the boundary points from the hyperplane is 1, i.e., the margin

between the classes q qsr

u

is (see Fig. 1, lefthand display). Thus, the fixed margin

policy for ranking learning is to find the direction

4IH!4

4

and the scalars

 ¤

C#"" A B AHGPIUB

such that

 ¤ ©¤tvu4

is minimized (i.e., the margin between classes q qsr is maximized) subject to the

separability constraints (modulo margin errors in the non-separable case). "sum of margins" strategy: the sum of all margins are to be maximized. In this case, the margins are not necessarily equal (see Fig. 1, righthand display). Formally, the ranking ¡E¢ ¤


4 4 u u , rule employs a vector

¥  GPIQB¨D AHGPIUB D

"#

§ ¤

R A   T  

R4¢   Tand R4 A IQBPT 



4 H ¥ D8

¤D

, and a set of



other words, all the examples of class

hyperplanes and

therefore

 

© ¡£¢¥¤ R

such that and

D§

, where

©A

¡¢4

H



§¦T ¨ B

thresholds

A



  B@D8A!B ¡ £¢ED1A¤¢@D § ¤!"#"$¡ ¢ ¤ D

for



. In

W

are "sandwiched" between two parallel

W

and . The

( ¢   G

§

the large margin principle is to maximize

 ¡E¢¥¤ margins

R A   T are subject to

the separability constraints above.

It is also fairly straightforward to apply the SRM principle and derive the bounds on the actual risk functional -- see [11] for details. In the remainder of this paper we will introduce the algorithmic implications of these two strategies for implementing the large margin principle for ranking learning. The fixedmargin principle will turn out to be a direct generalization of the Support Vector Machine (SVM) algorithm -- in the sense that substituting in our proposed algorithm would produce the dual functional underlying conventional SVM. It is interesting to note that the

¡ § ©

¡ § ©

sum-of-margins principle reduces to -SVM (introduced by [10] and later [2]) when

2 Fixed Margin Strategy Recall that in the fixed margin policy

¦

.

unknown. The unknown variables

4 APB D D A!GPIQBis

R4



A T"#

p is a "canonical" hyperplane normalized such

that the margin between the closest classes q q&r

¤ ©9t 

4 . The index q is of course

(and the index q ) could be solved

in a two-stage optimization problem: a Quadratic Linear Programming (QLP) formulation followed by a Linear Programming (LP) formulation. The (primal) QLP formulation of the ("soft margin") fixed-margin policy for ranking learning takes the form:

 02143  @ @ @ "!£#

©¤ 4 C4H %$'&  &

r

4IHH A¢ D ) )B1



4IHH

§F¤!"#"$¡E¢ ¤ % §F¤!"#" %

B

and



¨  ¨A

R



, and

$

§¦ ¢

A 7698¤@A@B

354

(0) )¤1

r

¨¢ 1§¦ ¤¤  r ¢

))

B 

§¦ 2 B

 (2)

R

(3) (4) (5)

)

where

and

)¤1  §¦

is some predefined constant. The scalars

are positive for data points which are inside the margins or placed on the wrong

side of the respective hyperplane. Since the margin is maximized while maintaining separability, it will be governed by the closest pair of classes because otherwise the separability the margin size with possible margin errors -- but that is discussed later). The solution to this optimization problem is given by the saddle point of the Lagrange functional (Lagrangian):

conditions would cease to hold (modulo the choice of the constant

$ which would tradeoff

C § R T

H

r



©¤ 4IHH4 %$ &@

r

& R¤

@

FE

 ¢ )1§¦

( ) )1§¦

r

¢ r

B 2   ¢

¤ ¢ )

r

AD  r

T

where ,



, and

multipliers. Since the primal problem is convex, there exists a strong duality between the primal and dual optimization functions. By first minimizing the Lagrangian with respect

G G D E

  §¦    B  

1

& RS4 !H) AHG

A 4IHH T &@ 

§¦ B ¢ @ B  

AG ¢ &  §¦  §¦ 1 ) 1

B B

§ ¤ !""# ¡ ¢ ¤ % § ¤!"#" %

@

are all non-negative Lagrange


to

4 A

   B  ) )1§¦

we obtain the dual optimization function which then must be maximized

with respect to the Lagrange multipliers. From the minimization of the Lagrangian with

respect to

4

we obtain:

4

§ ¢

&  & 

@

 D r @  E

   §¦

B (6)

That is, the direction of the support vectors

4

of the parallel hyperplanes is described by a linear combination associated with the non-vanishing Lagrange multipliers. From the

Kuhn-Tucker theorem the support vectors are those vectors for which equality is achieved in the inequalities (3,4). These vectors lie on the two boundaries between the adjacent

 ¤

classes q q r (and other adjacent classes which have the same margin). From the mini-

mization of the Lagrangian with respect to

& 

D

 §

A 

we obtain the constraint:

)& )1§¦

 E 

and

§F¤ !""# ¡ ¢ ¤

yields the constraints:

(7)

and the minimization with respect to

$

B

¢ ¢ §  ¢ ¢

D G  

R

$

E G

  §¦ 1



R

D ¡$ D

D



B §

R

which in turn gives rise to the constraints

data point is a margin error (

likewise for

E

 G

R

respect to the class on its "left" and once with respect to the class on its "right". For the sake of presenting the dual functional in a compact form, we will introduce some

¢

£¥¤

 % 

§¦T

new notations. Let

 % §¤ C#"# %

multipliers multipliers

be the



matrix whose columns are the data points



,

DE 

 §  !"#" 

D D D

R B

. Let be the vector whose components are the Lagrange

. Note that a data point can count twice as a margin error -- once with

, thus from the Kuhn-Tucker theorem

where

D

§

§ $

(8) if the corresponding

) ¡

R ), and

vector holding all the

¨RD.



Note that

D B §andD

B !"#"

finally, let

GPIQB



corresponding to class . Likewise, let

corresponding to class r

 

. Let

¨ ©¨R B ¨ G IQB T¦ RE

and



¤

¢D§ ¢ !"#"Lagrange B GPIUB E ¢ § C#""

multipliers, and let

¨ RD

§ E C#"#R E

B

¨ ¢E

¦

 § B !""# 

D B §E B !""#E B §

¨ ©¨R B ¨ GPIQB T¦

be the



G IQB B E !"#"beGPIUB

 T¦

the Lagrange

T ¦

¨T¦

is a vector, and likewise so is

be the matrix holding two copies of the training data:

§ ©'¢ % ¢ % B G

 § ¢

¢ ¢ ¢

 9

¢ B C#""!¢ GPIUB  ¢ !""# G 

§ T E

the first and second halves of

. Let be the vector of 1's, and

where . For example, (6) becomes in the new notations

By substituting the expression for

4

§

¨ 

!¨

4

§

(9)

!¨

.

back into the Lagrangian and taking into

account the constraints (7,8) one obtains the dual functional which should be maximized with respect to the Lagrange multipliers :

#"%$& 0 354&

(' ¨ ¨ ¦ )R ¦  0¨T  2¨H  3¨H 

 ¢

1¨D D $ B §

BA

7698¤@A@B  

% §F¤!"#" ¢ §F¤!"#"$¡ ¢ ¤ 

(10)

R

(11) (12)

Note that

¡ §8©

, i.e., we have only two classes thus the ranking learning problem is equiv-

alent to the 2-class classification problem, the dual functional reduces and becomes equiv-

alent to the dual form of conventional SVM. In that case

denoting the class membership.

5 95 A@§ E¤ 

 H where

4R ¦ ET 65 5 §  



 87


Also worth noting is that since the dual functional is a function of the Lagrange multipliers

D E

and alone, the problem size (the number of unknown variables) is equal to twice the

§ © ¢ % ¢ %

¡0R

 ¢

T

B G



where is the number of training

number of training examples -- precisely examples. This favorably compares to the  

ordinal regression introduced in [7] or the to SVM [4, 8].

Further note that since the entries of

 ¦ 

 

required by the recent SVM approach to

required by the general multi-class approach

are the inner-products of the training examples,

they can be represented by the kernel inner-product in the input space dimension rather than by inner-products in the feature space dimension. The decision rule, in this case, given a

new instance vector

for which

&



E

would be the rank

 §¦ R

corresponding to the first smallest threshold

&

 

R  T D¥A  5 

A 5

B  ¢  T

¡£¢ 5§¦©¨ ¦ 5¡ ` ¤



ture" space

where

R T R T R T

H

R T

.

§ §! E

c c¥¤ ¡£¢ c c¤ 5§¦¨ ¦ 5¡ ` ¤

D

replaces the inner-products in the higher-dimensional "fea-



A

¨

Finally, from the dual form one can solve for the Lagrange multipliers

 ¤

classes q q r

4  ¨ §

and in turn obtain

the direction of the parallel hyperplanes. The scalar p (separating the adjacent

which are the closest apart) can be obtained from the support vectors, but

the remaining scalars

A



cannot. Therefore an additional stage is required which amounts

to a Linear Programming problem on the original primal functional (2) but this time already known (thus making this a linear problem instead of a quadratic one).

3 Sum-of-Margins Strategy In this section we propose an alternative large-margin policy which allows for

4

is

¡ ¢1¤ mar-

gins where the criteria function maximizes the sum of them. The challenge in formulating proach which is at the center of conventional SVM formulation and of the fixed-margin

the appropriate optimization functional is that one cannot adopt the "pre-scaling" of 4 ap-

policy for ranking learning described in the previous section. The approach we take is to represent the primal functional using

¡ ¢8¤

perplanes instead of

© ¡¨¢ ¤ R T

parallel hy-

. Each class would be "sandwiched" between two hyperplanes

(except the first and last classes). Formally, we seek a ranking rule which employs a vector

4

and a set of

§¦ ¨ B ¤ C#"# ¡ ¢ ¤

such

that

¤

and

A3D©

H



 for



. In other words, all the exam-

4  D  D

H



© ¡ ¢ ¤ R



§F¢4¡

, where

T

thresholds

ples of class

R4 A IQB T



and

  G

A§

are "sandwiched" between two parallel hyperplanes

W W

.

¤ ¢

  B DA!B§ D  7¢ DA¢ D D   G IQB D A!GPIQB

"#



R4   T 

and

 

The margin between two hyperplanes separating class

Thus, by setting the magnitude of

4



and Gr



is:

R A   T 4

 

t#"

to be of unit length (as a constraint in the optimization

problem), the margin which we would like to maximize is

(

  

R A   T 

for

¢ §F¤ !""# ¡ ¢3¤

which we can formulate in the following primal QLP (see also Fig. 1, righthand display):

GPIUB

  " 02143 @ $ @ &'  R A T %$'&  &

  ¢

BA

r

(5) )¤1§¦

r

B

2

(13)

 354A D¥A7608 

 



@A@B

4IHHD D  

4IHH4 D

  B )¨

¤

r

§¦

 §F¤!"#"$¡E¢ ©

) )1§¦

R

 ¢ 

A

)1§¦¨ B B D 4 !H §¦ B 

R

(14) (15) (16) (17)


where



§ ¤ !""# ¡£¢ ¤

(unless otherwise specified) and

1

% § ¤ !""# %  , and

$

is some prede-

fined constant (whose physical role would be explained later). Note that the (non-convex)

constraint

4 4

H is replaced by the convex constraint

that the optimal solution

4

4 H4H D

since it can be shown

would have unit magnitude in order to optimize the objective

§F¤ ¤

function (see [11] for details). We will proceed to derive the dual functional below. The Lagrangian takes the following form:

C § RbHT

&  R A T %$r &



¢  

( ) )1§¦

r

B

2

GPI ¢

¢

r

& R4IHC   T & RA  r

@

¢ ¤ ¢ r Y

where

 

 ¢     §   

Y

we will omit further derivations (those can be found in [11]) and move directly to the dual functional which takes the following form:

#"%$& 0 ¢ 354

     G G D E

 D RS4 H4H 1

¢ ¢ @ r &  R A T§¦ &' RA   B T ¡ )1§¦

r



B ¢ B

B

4IHH T

¢   

¢   §¦

 

)   )



T & G

¢ @

¤!"#"$¡ ¢ ¤@

&

@

AE  G

¢

 §¦   1 ) 1 B

(unless otherwise specified),

% § ¤ C#"# %  , and

are all non-negative Lagrange multipliers. Due to lack of space

¨ ¦7608 )R¨@A@B¦  0¨T 1¨DA D $

HH3¨3¨ BB B ¤ §

(18)

R

% § ¤ C#""

 ¨

and

GH2¨H¢¨

¨ ¢GPIQB ¤

where are defined in the previous section. The direction

linear combination of the support vectors:

Tucker theorem,

R

¨

 ¡ 4

§

t   ¨  ¨

4

(19) (20) (21) is represented by the

where, following the Kuhn-

for all vectors on the boundaries between the adjacent pairs of

 ¨



classes and margin errors. In other words, the vectors associated with non-vanishing

are those which lie on the hyperplanes or vectors tagged as margin errors. Therefore, all the thresholds can be recovered from the support vectors -- unlike the fixed-margin scheme which required another LP pass. The dual functional (18) is similar to the dual functional (10) but with some crucial differences: (i) the quadratic criteria functional is homogeneous, and (ii) constraints (20) lead to the constraint . These two differences are also what distinguishes between conventional SVM and -SVM for 2-class learning proposed recently by [10]. Indeed, if we set in the dual functional (18) we would be able to conclude that the two dual functionals are identical (by a suitable change of variables). Therefore, the role of the con-

(   ¨ ¨ ©

¡ §)© ¦

stant

$

complies with the findings of [10] by controlling the tradeoff between the number

©9t ©

of margin errors and support vectors and the size of the margins:

when when

$$

© §§ ©9t D $ D

such that

   A  

a single margin error is allowed (otherwise a duality gap would occur) and all vectors are allowed to become margin errors and support vectors (see

[11] for a detailed discussion on this point).

In the general case of

constant

$

classes (in the context of0 ranking learning) the role of the

carries the same meaning:

© ¡E¢ ¤ ¤£t ¥ $ D R T

T D $ D R © ¡E¢ ¤ T

¡ ¡ ©

where

£0 ¥ stand for "total

number of margin errors", thus

© ¡E¢ ¤ R

Since a data point can can count twice for a margin error, the total number of margin errors in the worst case is where is the total number of data points. § © ¢ % ¢ % B G 


Crammer & Singer 2001

fixed-m a r g in

Figure 2: The results of the fixed-margin principle plotted against the results of PRank of [5] which does not use a large-margin principle. The average error of PRank is about 1.25 compared to 0.7 with the fixed-margin algorithm.

4 Experiments Due to lack of space we describe only two sets of experiments we conducted on a "collaborative filtering" problem and visual data ranking. More details and further experiments are reported in [11]. In general, the goal in collaborative filtering is to predict a person's rating on new items such as movies given the person's past ratings on similar items and the ratings of other people of all the items (including the new item). The ratings are ordered, such as "highly recommended", "good",..., "very bad" thus collaborative filtering falls naturally under the domain of ordinal regression (rather than general multi-class learning). The "EachMovie" dataset [6] contains 1628 movies rated by 72,916 people arranged as a 2D array whose columns represent the movies and the rows represent the users -- about ratings. Given a new user, the ratings of the user on the 1628 movies (not all movies would

!"#"¡  5% of the entries of this array are filled-in with ratings between R totaling 2,811,983

5  

be rated) form the and the i'th column of the array forms the



which together form the

training data (for that particular user). Given a new movie represented by the vector



of

ratings of all the other 72,916 users (not all the users rated the new movie), the learning

)

task is to predict the rating

ratings were shifted by

¢£¢

¥¤

R T ¢2© !¢ ¤¤ !¢  C¤  ©

¥¤ ¥¤ ¤ to have the possible ratings of the new user. Since theD array¤ contains empty¥¤entries, theT R R

which allows to assign the value of zero to the empty entries of the array (movies which were not rated). For the training phase we chose users which ranked about 450 movies and selected a subset movies. We compared our results (collected over 100 runs) -- the average distance between the correct rating and the predicted rating -- to the best "on-line" algorithm of [5] called "PRank" (there is no use of large margin principle). In their work, PRank was compared to other known on-line approaches and was found to be superior, thus we limited our comparison to PRank alone. Attempts to compare our algorithms to other known ranking algorithms which use a large-margin principle ([7], for example) were not successful since those square the training set size which made the experiment with the Eachmovie dataset untractable computationally. The graph in Fig. 2 shows that the large margin principle makes a significant difference on the results compared to PRank. The results we obtained with PRank are consistent with the reported results of [5] (best average error of about 1.25), whereas our fixed-margin algorithm provided an average error of about 0.7). We have applied our algorithms to classification of "vehicle type" to one of three classes: "small" (passenger cars), "medium" (SUVs, minivans) and "large" (buses, trucks). There

D ¤ R C¤ !"#"¡¢

R R R R T of those movies for training and tested the prediction on the remaining


Correctly Classified Badly Classified

1

2

3

Figure 3: Classification of vehicle type: Small, Medium and Large (see text for details).

is a natural order Small, Medium, Large since making a mistake between Small and Large is worse than confusing Small and Medium, for example. We compared the classification error (counting the number of miss-classifications) to general multi-class learning using pair-wise SVM. The error over a test set of about 14,000 pictures was 20% compared to 25% when using general multi-class SVM. We also compared the error (averaging the PRank. The average error was 0.216 compared to 1.408 with PRank. Fig. 3 shows a typical collection of correctly classified and incorrectly classified pictures from the test set. References [1] B.E. Boser, I.M. Guyon, and V.N. Vapnik. A training algorithm for optimal margin classifers. In Proc. of the 5th ACM Workshop on Computational Learning Theory, pages 144­152. ACM Press, 1992. [2] C.C. Chang and C.J. Lin. Training ( ­Support Vector classifiers: Theory and Algorithms. In Neural Computations, 14(8), 2002. [3] W.W. Cohen, R.E. Schapire, and Y. Singer. Learning to order things. Journal of Artificial Intelligence Research (JAIR), 10:243­270, 1999. [4] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research, 2:265­292, 2001. [5] K. Crammer and Y. Singer. Pranking with ranking. In Proceedings of the conference on Neural Information Processing Systems (NIPS), 2001. [6] http://www.research.compaq.com/SRC/eachmovie/. [7] R. Herbrich, T. Graepel, and K. Obermayer. Large margin rank boundaries for ordinal regression. Advances in Large Margin Classifiers, 2000. pp. 115­132. [8] Y. Lee, Y. Lin, and G. Wahba. Multicategory support vector machines. Technical Report 1043, Univ. of Wisconsin, Dept. of Statistics, Sep. 2001. [9] P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman and Hall, London, 2nd edition edition, 1989. [10] B. Scholkopf, A. Smola, R.C. Williamson, and P.L. Bartless. New support vector algorithms. Neural Computation, 12:1207­1245, 2000. [11] A. Shashua and A. Levin. Taxonomy of Large Margin Principle Algorithms for Ordinal Regression Problems. Technical Report 2002-39, Leibniz Center for Research, School of Computer Science and Eng., the Hebrew University of Jerusalem. [12] V.N. Vapnik. The nature of statistical learning. Springer, 2nd edition, 1998. [13] J. Weston and C. Watkins. Support vector machines for multi-class pattern recognition. In Proc. of the 7th European Symposium on Artificial Neural Networks, April 1999.

D ¤ $© ¡¢

difference between the true rank T and the predicted rank using 2nd-order kernel) to


