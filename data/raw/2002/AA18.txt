Feature Selection in Mixture-Based Clustering

Martin H. Law, Anil K. Jain Dept. of Computer Science and Eng. Michigan State University, East Lansing, MI 48824 U.S.A. Mario A. T. Figueiredo ´ Instituto de Telecomunicac¸~oes, Instituto Superior T´ecnico 1049-001 Lisboa Portugal

Abstract There exist many approaches to clustering, but the important issue of feature selection, i.e., selecting the data attributes that are relevant for clustering, is rarely addressed. Feature selection for clustering is difficult due to the absence of class labels. We propose two approaches to feature selection in the context of Gaussian mixture-based clustering. In the first one, instead of making hard selections, we estimate feature saliencies. An expectation-maximization (EM) algorithm is derived for this task. The second approach extends Koller and Sahami's mutual-informationbased feature relevance criterion to the unsupervised case. Feature selection is then carried out by a backward search scheme. This scheme can be classified as a "wrapper", since it wraps mixture estimation in an outer layer that performs feature selection. Experimental results on synthetic and real data show that both methods have promising performance.

1 Introduction In partitional clustering, each pattern is represented by a vector of features. However, not all the features are useful in constructing the partitions: some features may be just noise, thus not contributing to (or even degrading) the clustering process. The task of selecting the "best" feature subset, known as feature selection (FS), is therefore an important task. In addition, FS may lead to more economical clustering algorithms (both in storage and computation) and, in many cases, it may contribute to the interpretability of the models. FS is particularly relevant for data sets with large numbers of features; e.g., on the order of thousands as seen in some molecular biology [22] and text clustering applications [21]. In supervised learning, FS has been widely studied, with most methods falling into two classes: filters, which work independently of the subsequent learning algorithm; wrappers, which use the learning algorithm to evaluate feature subsets [12]. In contrast, FS has received little attention in clustering, mainly because, without class labels, it is unclear how to assess feature relevance. The problem is even more difficult when the number of clusters is unknown, since the number of clusters and the best feature subset are inter-related [6]. Some approaches to FS in clustering have been proposed. Of course, any method not

Email addresses: lawhiu@cse.msu.edu, jain@cse.msu.edu, mtf@lx.it.pt This work was supported by the U.S. Office of Naval Research, grant no. 00014-01-1-0266, and by the Portuguese Foundation for Science and Technology, project POSI/33143/SRI/2000.


relying on class labels (e.g., [16]) can be used. Dy and Brodley [6] suggested a heuristic to compare feature subsets, using cluster separability. A Bayesian approach for multinomial mixtures was proposed in [21]; another Bayesian approach using a shrinkage prior was considered in [8]. Dash and Liu [4] assess the clustering tendency of each feature by

 

an entropy index. A genetic algorithm was used in [11] for FS in -means clustering. Talavera [19] addressed FS for symbolic data. Finally, Devaney and Ram [5] use a notion of "category utility" for FS in conceptual clustering, and Modha and Scott-Spangler [17] assign weights to feature groups with a score similar to Fisher discrimination. In this paper, we introduce two new FS approaches for mixture-based clustering [10, 15]. The first is based on a feature saliency measure which is obtained by an EM algorithm; unlike most FS methods, this does not involve any explicit search. The second approach extends the mutual-information based criterion of [13] to the unsupervised context; it is a wrapper, since FS is wrapped around a basic mixture estimation algorithm. 2 Finite Mixtures and the EM algorithm

Given

¡

i.i.d. samples

"#%$&('¢0)1324£ "#5$ &('§

¢¤£¦¥¨§©¨§  786

, the log-likelihood of a

7 )192@£

A78 

©

"#5$CBDE8A

©9F

&G'!

-component mixture is

§ )1 2H

1©

D 7 D

D DQPSR D D D

(1)

where: vector

1YX`¥1 © ab1F  ©TaF 

;

de © aEe F

7gf B 7hfiqpsr F B

I  £VU

; each is the set of parameters of the -th component; and

§

7

c

W

-dimensional feature is the full parameter set. Each is a

and all components have the same form (e.g., Gaussian).

Neither maximum likelihood (

(

t1

MAP

$y £uxw u  ¥

"#%$&('¢0)12( "#5$&G'$y132

t1 £vuxw u¥ ML

"#5$&('¢)132

) nor maximum a posteriori

) estimates can be found analytically. The

usual choice is the EM algorithm, which finds local maxima of these criteria. Let

¥7hf© £ ¨bR  

, for

&4be£Wa

set of

¡

missing labels, where with

, meaning that

§

7

is a sample of

A78 AB 

'hi© i© DE8

 7hfD

The complete log-likelihood is

"#5$&G'¢fEg)192@£

7gf &G'ed7 £dD © ab )1 2. "#5$ &('

7gfB p,

 £U£and 7gfD

(2)

EM produces a sequence of estimates

l

E-step: Computes

' 'gi "#%$& '

rt 1Gt1Xuosvw

7gfD

22@£

7hfD function

mn£poqdg)¢ bt1 2p,

'gi¢fEms)132Prt

'gi¥ht1

dF § )1 2  D 7 D p

followed by normalization so that

that

 £~U 7hfD

(i.e.,

§

7

)e¢  t1 2yx £T vw £`Uz){§.  t1 2{x}|

Notice that

belongs to cluster ) while

t 7hfD

2H £ ¨U5bjka"#5$&G'¢

R

using two alternating steps:

and plugs it into

§

7.

D 7hfD7hfDW £sU

'hi



bg)192 yielding the r -

. Since the elements of

7 are binary, we have 'hi &(' 'gi 2 § )t1 22H 7 D

F

FtD D

(3)

is the a priori probability

is the corresponding a posteriori

probability, after observing

l

M-step: Updates the parameter estimates,

in the case of MAP estimation, or without

"#5$&G''gi t1 gU2£Cuxw uxk0¥hr 1 t1 22

192

in the ML case.

$y

' 'hi "#%$&('192%

3 A Mixture Model with Feature Saliency In our first approach to FS, we assume conditionally independent features, given the component label (which in the Gaussian case corresponds to diagonal covariance matrices),

&G'§)¥

F D D H¥¨ 2£

DE8AB

©F D

&G'§)

D 2£

DE8AB 86i ©F © D &('e

 D ) 2H (4)


where

&('ed) D2

is the pdf of the -th feature in the -th component; in general, this could

  W

have any form, although we only consider Gaussian densities. In the sequel, we will use the indices , and to run through data points, mixture components, and features, respectively. Assume now that some features are irrelevant, in the following sense: if feature is

' '  

is the common (i.e., irrelevant, then

 D  

§¦' ¦

where

¢ e )¤ 2

 

independent of ) density of feature . Let such that if feature is relevant and

¥ a¥ %a¥¨ %a¥¨¤ h2£ F D D 

¦

£



£ UW &('§)

&('e ) 2 £¢ e )¤ 2, Wq£ U%aE!¨,

 

 for ¥¦ ££ ©otherwise;2

i

be a set of binary parameters,

then,

¡ W  

AB 6i © F ©

¦DE8's

D 8

 R

'&G'e

 D

) 22© ¢ e )¤ 2E2 ©   ' '

©

Our approach consists of: (i) treating the

(ii) estimating

£

  £ Uh2   §¦'

(5)

as missing variables rather than as parameters;

from the data;





is the probability that the -th feature is

 

useful, which we call its saliency. The resulting mixture model (see proof in [14]) is

&G'§)¥ ' '

The form of

¢F'w)2

D D   H¥¨ H¥¨¤ %a¥ h2£   D ) 2 !" #¢2 e )¤ #$2 % U   

 &G'e DE8AB 86i ©F © D

(6)

reflects our prior knowledge about the distribution of the non-salient fea-

tures. In principle, it can be any 1-D pdf (e.g., Gaussian or student-t); here we only consider mixture, we first select the component label by sampling from a multinomial distribution

to be a Gaussian. Equation (6) has a generative interpretation. As in a standard finite

' W

Then, for each feature with parameters

&('whoseD w) 2probabilityFof  

© a¨aa 2.

getting a head is

F

B



;

  £ U5abc

, we flip a biased coin

if we get a head, we use the mixture component

'

¢ s)2 '

to generate the -th feature; otherwise, the common component

'

D D  ¢

7gfi pwr

¢ s)¤ 2 

Given a set of observations

F

%t $y £uuxw &u 

% '

£ § © a¨aHE§  2,

is used.

£ ¥ H¥ H¥¤ H¥A78 h2©

with

§ £ de © ¨Ee 7 7gf

, the parameters

can be estimated by the maximum likelihood criterion,

"#5$ B DE8A ©F © D

86i

 &G'e

 7 D ) 2 !' #¢2 e )¤ $2 x U  7  ' ' (7)

In the absence of a closed-form solution, an EM algorithm can be derived by treating both

the



and the as missing data (see [14] for details).

7's

¦

's

3.1 Model Selection Standard EM for mixtures exhibits some weaknesses which also affect the EM algorithm ing a good local optimum. To overcome these difficulties, we adopt the approach in [9], which is based on the MML criterion [23, 24]. The MML criterion for the proposed model (see details in [14]) consists of minimizing, with respect to , the following cost function

just mentioned: it requires knowledge of !

, and a good initialization is essential for reach-

%

"#5$&G'¢)% ! j c "#5$ 2

where

( 0

and

¡s)(j

i A8 AB

© © DE8

"#5$9'¡

F D   2()0j

A8

i £ £ j

respectively. If

© "#%$ '¡ '

&G'

1" 22  U

(8)

are univariate Gaussians (arbitrary mean and variance),

are the number of parameters in



D

and

¤ , s)2 ¢ w)2

and

'

ter estimation viewpoint, this is equivalent to a MAP estimate with conjugate (improper) undergoes a minor modification in the M-step, which still has a closed form.

Dirichlet-type priors on the

F

's and (see details in [14]); thus, the EM algorithm

The terms in equation (8), in addition to the log-likelihood

interpretations. The term

responding to

!

F

D

!2B

values and

4 c

i "#%$ 3 ¡

"#%$3&('¢)%

2,

have simple

D  's

( 0

. From a parame-

is a standard MDL-type parameter code-length corvalues. For the -th feature in the -th component, the

W

 


"effective" number of data points for estimating

in each



the corresponding code-length is  

D,

£

Thus, there is a term

3 ¡ !" "#%$ ' '

U 22 

3  FF D 2.

is ¢¡

"#%$D ¡  ' D .

Since there are

Similarly, for the -th feature

(

in the common component, the number of effective data points for estimation is

in (8) for each feature.

 parameters ' ¡ U  2.

One key property of the EM algorithm for minimizing equation (8) is its pruning behavior, message length in (8) may become invalid at these boundary values can be circumvented

forcing some of the

F

by the arguments in [9]. When

and

 © aa¨aE 

B

to go to zero and some of the





to go to zero or one. Worries that the



goes to zero, the -th feature is no longer salient and

are removed. When

 

goes to 1,

¤  

and

 



 

are dropped.

D  

Finally, since the model selection algorithm determines the number of components, it can Because of this, as in [9], a component-wise version of EM [2] is adopted (see [14]). 3.2 Experiments and Results The first data set considered consists of 800points from a mixture of 4 equiprobable Gaus-

be initialized with a large value of

! , thus alleviating the need for a good initialization [9].

' ' © ' ' ¥¤¦ sians with mean vectors

2 2, 2, © 2,

, §



"noisy" features (sampled from a

'R ¥¨©

¤ and identity covariance matrices. Eight

¨U2

density) were appended to this data, yielding a

set of 800 10-D patterns. The proposed algorithm was run 10 times, each initialized with cies are initialized at 0.5. In all the 10 runs, the 4 components were always identified. The saliencies of all the ten features, together with their standard deviations (error bars), are shown in Fig. 1. We conclude that, in this case, the algorithm successfully locates the clusters and correctly assigns the feature saliencies. See [14] for more details on this experiment. ! £

 ; the common component is initialized to cover all data, and the feature salien-

1

R

0.8

0.6

Saliency

saliency 0.4

Feature

0.2 Feature

0

1 2 3 4 5 6 7 8 9 10

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2

Feature Number

Figure 1: Feature saliency for 10-D 4-component Gaussian mixture. Only the first two features are rel-

evant. The error bars show  one standard deviation.

5 10 Feature no 15 20

Figure 2: Feature saliency for the Trunk data. The smaller the feature number, the more important is the feature.

In the next experiment, we consider Trunk's data [20], which has two 20-dimensional

Gaussians classes with means



© £



£

!

3

   ¤ and 

£

 , and covariances

. Data is obtained by sampling 5000 points from each of these two Gaus-

© £ U% 3 aa¨a 3 2

' © © 3 ©

sians. Note that these features have a descending order of relevance. As above, the initial values of the feature saliencies are shown in Fig. 2. We see the general trend that as the feature number increases, the saliency decreases, following the true characteristics of the data. Feature saliency values were also computed for the "wine" data set (available at the UCI repository at www.ics.uci.edu/~mlearn/MLRepository.html), consisting of 178 13-dimensional points in three classes. After standardizing all features to zero mean and unit variance, we applied the LNKnet supervised feature selection algorithm (available at www.ll.mit.edu/IST/lnknet/). The nine features selected by LNKnet are 7, 13, 1, 5, 10, 2, 12, 6, 9. Our feature saliency algorithm (with no class labels) yielded the values !

is set to 30. In all the 10 runs performed, two components were always detected. The


Table 1: Feature saliency of wine data

5 0.14 6 0.99 7 1.00 8 0.66 9 0.94 10 0.85 1 0.94 2 0.77 3 0.10 4 0.59 11 0.88 12 1.00 13 0.83

in Table 1. Ranking the features in descending order of saliency, we get the ordering: 7, 12, 6, 1, 9, 11, 10, 13, 2, 8, 4, 5, 3. The top 5 features (7, 12, 6, 1, 9) are all in the subset selected by LNKnet. If we skip the sixth feature (11), the following three features (10, 13, 2) were also selected by LNKnet. Thus we can see that for this data set, our algorithm, though totally unsupervised, performs comparably with a supervised feature selection algorithm. 4 A Feature Selection Wrapper Our second approach is more traditional in the sense that it selects a feature subset, instead of estimating feature saliency. The number of mixture components is assumed known a priori, though no restriction on the covariance of the Gaussian components is imposed. 4.1 Irrelevant Features and Conditional Independence Assume that the class labels, , and the full feature vector, , follow some joint probability

function

&('

if it is conditionally independent of the label , given the remaining features

if

§

)§(2 £  )§¤¢§¥  2 £ )§¤¢@2,

where is split

¢ and "non-useful"features   (here,

§

§¦ 

 §

In supervised learning [13], a feature subset

&G'

§¥%U5¨EcYintois

§¡ 

is considered irrelevant

§£¢

, that is,

&('

&G'§(2.

two subsets: "useful" features the index set of the non-useful

features). It is easy to show that this implies ¢   &('§q)2£ &G'§ &G' )2 § ) § 2£ § )%2&('§ )§ 2H

¢ ¢   ¢

&G' (9)

t D

To generalize this notion to unsupervised learning, we propose to let the expectations

(a byproduct of the EM algorithm) play the role of the missing class labels. Recall that the probabilities based on all the features, and only on the useful features, respectively

(see (3)) are posterior class probabilities, Prob

d§©¨ class W9)§@b1 Consider the posterior

t 7hfD

Ft

D 7 D )t1 2H &G'§



7gfD

'

7Ft(of



2| D 7hf¢ Dbf¢ &('§

t D p.

7hf¢

|

where

§

is the subset of relevant features of sample

feature subset, then 

7gfD

D 7gfD £`U t 7gfD

D £`U§ §

t 7gfD

T T

course, the 

)t1 27hfD

and

t 7gfD (10) have

to be normalized such that and

). If   is a completely irrelevant

equals exactly, because of the conditional independence in (9),

applied to (3). In practice, such features rarely exist, though they do exhibit different de

grees of irrelevance. So we follow the suggestion in [13], and find

close to as possible. As both and 

2

t 7gf t 7gfD 7hfD '

that gives 

2

as

are probabilities, a natural criterion for

7gf '

assessing their closeness is the expected value of the Kullback-Leibler divergence (KLD, [3]). This criterion is computed as a sample mean

 '  2@£

2

 A78 AB

© © DE8 t 7hfD "#5$

7gfDt 7gfD'



2

in our case. A low value of

'



 

ally independent from the expected class labels, given the features in  .

In practice, we start by obtaining reasonable initial estimates of



using all the features, and set



that in

 

'

¨¢¥ h2

£ ¥

t 7hfD ¥ 

. At each stage, we find the feature





indicates that the features in

(11) are "almost" condition-

by running EM

¨



such

¢



is smallest and add it to

, to update the posterior probabilities

¥  t.7hfDEM.

is then run again, using the features not The process is then repeated until only

one feature remains, in what can be considered as a backward search algorithm that yields a sorting of the features by decreasing order of irrelevance.


4.2 The assignment entropy Given a method to sort the features in the order of relevance, we now require a method to measure how good each subset is. Unlike in supervised learning, we can not resort to classification accuracy. We adopt the criterion that a clustering is good if the clusters are "crisp", i.e., if, for every ,

2¦£ ¡ T 78 © T DE8B¥ ©

¡

t 7hfD¡ 

¢ '¥

t 7hfD

tU 7gfDfor

t 7gfD;

some . A natural way to formalize this that is, the clustering is considered to be

"#%$ t 7hfD

is small. In the sequel, we call

£

W

is to consider the mean entropy of the

good if

¢

© 

"the entropy of the assignment". An important characteristic of the entropy is that

it cannot increase when more features are used (because, for any random variables

, and

2,

¤

note that

¥ £S)7 ¦¥ ¨§2

2

¢,¢'¥

't ¤ ¢ '£)¤

¢ '¥ ¢ '¥

,

a fundamental inequality of information theory [3];

is a conditional entropy

t 7 7hf¢ )¥§ 2).

Moreover,

t 7

2

exhibits a diminishing returns behavior (decreasing abruptly as the most relevant features are included, but changing little when less relevant features are used). Our empirical results show that indeed has a strong relationship with the quality of the clusters. Of course, during the backward search, one can also consider picking the next feature whose removal least increases , rather than the one yielding the smallest KLD; both options are explored in the experiments. Finally, we mention that other minimum-entropy-type criteria have been recently used for clustering [7], [18], but not for feature selection.

¢

¢

4.3 Experiments We have conducted experiments on data sets commonly used for supervised learning tasks. Since we are doing unsupervised learning, the class labels are, of course, withheld and only used for evaluation. The two heuristics for selecting the next feature to be removed (based on minimum KLD and minimum entropy) are considered in different runs. To assess clustering quality, we assign each data point to the Gaussian component that most likely generated it and then compare this labelling with the ground-truth. Table 2 summarizes the characteristics of the data sets for which results are reported here (all available from the UCI repository); we have also performed tests on other data sets achieving similar results. The experimental results shown in Fig. 3 reveal that the general trend of the error rate agrees well with . The error rates either have a minimum close to the "knee" of the H curve, or the curve becomes flat. The two heuristics for selecting the feature to be removed perform comparably. For the cover type data set, the DKL heuristic yields lower error rates

¢

¢

than the one based on datasets. , while the contrary happens for image segmentation and WBC

5 Concluding Remarks and Future Work The two approaches for unsupervised feature selection herein proposed have different advantages and drawbacks. The first approach avoids explicit feature search and does not require a pre-specified number of clusters; however, it assumes that the features are conditionally independent, given the components. The second approach places no restriction on the covariances, but it does assume knowledge of the number of components. We believe that both approaches can be useful in different scenarios, depending on which set of assumptions fits the given data better. Several issues require further work: weakly relevant features (in the sense of [12]) are not removed by the first algorithm while the second approach relies on a good initial clustering. Overcoming these problems will make the methods more generally applicable. We also need to investigate the scalability of the proposed algorithms; ideas such as those in [1] can be exploited.


Table 2: Some details of the data sets (WBC stands for Wisconsin breast cancer).

Name No. points used No. of features No. of classes cover type 2000 10 4 image segmentation 1000 18 7 WBC 569 30 2 wine 178 13 3

Entropy

3000 2500 2000 1500 1000 500 0 60 55 50 45 40 35 30 25

Erorr

% Entropy

4000 3500 3000 2500 2000 1500 1000 500 0 65 60 55 50 45 40 35

Error

%

2 4 6 8 10

2 4 6 8 10

No. of features

(a) No. of features (b)

Entropy

900 800 700 600 500 400 300 200 100 0 70 65 60 55 50 45 40 35 1200 1000 800 600 400 200 0

Error

% Entropy

60 55 50 45 40 35 30 25

Error

%

5

10 No. of features

(c) 15 5 10 No. of features (d) 15

Entropy

450 400 350 300 250 200 150 100 50 0

5 10 15 20 25

22 20 18 16 14 12 10 8 6 30

500 16

400 14

300 12

Error Error

% Entropy 200 10 %

100 8

0 5 10 15 20 25 6 30

No. of features

(e) No. of features (f)

Entropy

80 70 60 50 40 30 20 10 0 35 30 25 20 15 10 5 0

Error

% Entropy

70 60 50 40 30 20 10 0 35 30 25 20 15 10 5 0

Error

%

2 4 6 8 10 12 2 4 6 8 10 12

No. of features

(g) No. of features (h)

Figure 3: (a) and (b): cover type; (c) and (d): image segmentation; (e) and (f): WBC; (g)

and (h): wine. Feature removal by minimum KLD (left column) and minimum

column). Solid lines: error rates; dotted lines: deviation over 10 runs. . Error bars correspond to

¢

(right

one standard  

¢


References

[1] P. Bradley, U. Fayyad, and C. Reina. Clustering very large database using EM mixture models. In Proc. 15th Intern. Conf. on Pattern Recognition, pp. 76­80, 2000. [2] G. Celeux, S. Chr´etien, F. Forbes, and A. Mkhadri. A component-wise EM algorithm for mixtures. Journal of Computational and Graphical Statistics, 10:699­712, 2001. [3] T. Cover and J. Thomas. Elements of Information Theory. John Wiley & Sons, 1991. [4] M. Dash and H. Liu. Feature selection for clustering. In Proc. of Pacific-Asia Conference on Knowledge Discovery and Data Mining, 2000, pp. 110­121.

[5] M. Devaney and A. Ram. ICML'1997, pp. 92­97, 1997. Efficient feature selection in conceptual clustering. In Proc.

[6] J. Dy and C. Brodley. Feature subset selection and order identification for unsupervised learning. In Proc. ICML'2000, pp. 247­254, 2000. [7] E. Gokcay and J. Principe. Information Theoretic Clustering. IEEE Trans. on PAMI, 24(2):158171, 2002. [8] P. Gustafson, P. Carbonetto, N. Thompson, and N. de Freitas. Bayesian feature weighting for unsupervised learning, with application to object recognition. In Proc. of the 9th Intern. Workshop on Artificial Intelligence and Statistics, 2003. [9] M. Figueiredo and A. Jain. Unsupervised learning of finite mixture models. IEEE Trans. on PAMI, 24(3):381­396, 2002. [10] A. K. Jain and R. C. Dubes. Algorithms for Clustering Data. Prentice Hall, 1988. [11] Y. Kim, W. Street, and F. Menczer. Feature Selection in Unsupervised Learning via Evolutionary Search. In Proc. ACM SIGKDD, pp. 365­369, 2000. [12] R. Kohavi and G. John. Wrappers for feature subset selection. Artificial Intelligence, 97(12):273­324, 1997. [13] D. Koller and M. Sahami. Toward optimal feature selection. In Proc. ICML'1996, pp. 284­292, 1996. [14] M. Law, M. Figueiredo, and A. Jain. Feature Saliency in Unsupervised Learning. Tech.

Rep., Dept. Computer Science and Eng., Michigan State Univ., 2002. http://www.cse.msu.edu/ lawhiu/papers/TR02.ps.gz.

 

Available at

[15] G. McLachlan and K. Basford. Mixture Models: Inference and Application to Clustering. Marcel Dekker, New York, 1988. [16] P. Mitra and C. A. Murthy. Unsupervised feature selection using feature similarity. IEEE Trans. on PAMI, 24(3):301­312, 2002. [17] D. Modha and W. Scott-Spangler. Feature weighting in k-means clustering. Machine Learning, 2002. to appear. [18] S. Roberts, C. Holmes, and D. Denison. Minimum-entropy data partitioning using RJ-MCMC. IEEE Trans. on PAMI, 23(8):909-914, 2001. [19] L. Talavera. Dependency-based feature selection for clustering symbolic data. Intelligent Data Analysis, 4:19­28, 2000. [20] G. Trunk. A problem of dimensionality: A simple example. IEEE Trans. on PAMI, 1(3):306­ 307, 1979. [21] S. Vaithyanathan and B. Dom. Generalized model selection for unsupervised learning in high dimensions. In S. Solla, T. Leen, and K. Muller, eds, Proc. of NIPS'12. MIT Press, 2000. [22] E. Xing, M. Jordan, and R. Karp. Feature selection for high-dimensional genomic microarray data. In Proc. ICML'2001, pp. 601­608, 2001. [23] C. Wallace and P. Freeman. Estimation and inference via compact coding. Journal of the Royal Statistical Society (B), 49(3):241­252, 1987. [24] C.S. Wallace and D.L. Dowe. MML clustering of multi-state, Poisson, von Mises circular and Gaussian distributions. Statistics and Computing, 10:73­83, 2000.


