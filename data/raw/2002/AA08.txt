Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems

Sepp Hochreiter, Michael C. Mozer, and Klaus Obermayer



Department of Electrical Engineering and Computer Science Technische Universit¨at Berlin, 10587 Berlin, Germany University of Colorado, Boulder, CO 80309­0430, USA {hochreit,oby}@cs.tu-berlin.de, mozer@cs.colorado.edu

 Department of Computer Science

Abstract We introduce a family of classifiers based on a physical analogy to an electrostatic system of charged conductors. The family, called Coulomb classifiers, includes the two best-known support-vector machines (SVMs), the ­SVM and the C­SVM. In the electrostatics analogy, a training example corresponds to a charged conductor at a given location in space, the classification function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy. The electrostatic framework provides not only a novel interpretation of existing algorithms and their interrelationships, but it suggests a variety of new methods for SVMs including kernels that bridge the gap between polynomial and radial-basis functions, objective functions that do not require positive-definite kernels, regularization techniques that allow for the construction of an optimal classifier in Minkowski space. Based on the framework, we propose novel SVMs and perform simulation studies to show that they are comparable or superior to standard SVMs. The experiments include classification tasks on data which are represented in terms of their pairwise proximities, where a Coulomb Classifier outperformed standard SVMs. Introduction

1

Recently, Support Vector Machines (SVMs) [2, 11, 9] have attracted much interest in the machine-learning community and are considered state of the art for classification and regression problems. One appealing property of SVMs is that they are based on a convex optimization problem, which means that a single minimum exists and can be computed efficiently. In this paper, we present a new derivation of SVMs by analogy to an electrostatic system of charged conductors. The electrostatic framework not only provides a physical interpretation of SVMs, but it also gives insight into some of the seemingly arbitrary aspects of SVMs (e.g., the diagonal of the quadratic form), and it allows us to derive novel SVM approaches. Although we


are the first to make the analogy between SVMs and electrostatic systems, previous researchers have used electrostatic nonlinearities in pattern recognition [1] and a mechanical interpretation of SVMs was introduced in [9]. In this paper, we focus on the classification of an input vector x  X into one of two categories, labeled "+" and "-". We assume a supervised learning paradigm in which N training examples are available, each example i consisting of an input xi and a label yi  {-1,+1}. We will introduce three electrostatic models that are directly analogous to existing machine-learning (ML) classifiers, each of which builds on and generalizes the previous. For each model, we describe the physical system upon which it is based and show its correspondence to an ML classifier. 1.1 Electrostatic model 1: Uncoupled point charges Consider an electrostatic system of point charges populating a space X homologous to X. Each point charge corresponds to a particular training example; point charge i is fixed at location xi in X , and has a charge of sign yi. We define two sets of fixed charges: S+ = xi | yi = +1 and S- = xi | yi = -1 . The charge of point i is Qi  yi i, where i  0 is the amount of charge, to be discussed below. We briefly review some elementary physics. If a unit positive charge is at x in X , it will be attracted to all charges in S- and repelled by all charges in S+. To move the charge from x to some other location x, the attractive and repelling forces ~ must be overcome at every point along the trajectory; the path integral of the force along the trajectory is called the work and does not depend on the trajectory. The potential at x is the work that must be done to move a unit positive charge from a reference point (usually infinity) to x.

The potential at x is  (x) = N j=1 Qj G xj, x , where G is a function of the

distance. In electrostatic systems with point charges, G (a, b) = 1/ a - b . From

2

this definition, one can see that the potential at x is negative (positive) if x is in a neighborhood of many negative (positive) charges. Thus, the potential indicates the sign and amount of charge in the local neighborhood. Turning back to the ML classifier, one might propose a classification rule for some input x that assigns the label "+" if (x) > 0 or "-" otherwise. Abstracting from the electrostatic system, if i = 1 and G is a function that decreases sufficiently steeply with distance, we obtain a nearest-neighbor classifier. This potential classifier can be also interpreted as Parzen windows classifier [9]. 1.2 Electrostatic model 2: Coupled point charges Consider now an electrostatic model that extends the previous model in two respects. First, the point charges are replaced by conductors, e.g., metal spheres. Each conductor i has a self­potential coefficient, denoted Pii, which is a measure of how much charge it can easily hold; for a metal sphere, Pii is related to sphere's diameter. Second, the conductors in S+ are coupled, as are the conductors in S-. "Coupling" means that charge is free to flow between the conductors. Technically, S+ and S- can each be viewed as a single conductor. In this model, we initially place the same charge /N on each conductor, and allow charges within S+ and S- to flow freely (we assume no resistance in the coupling and no polarization of the conductors). After the charges redistribute, charge will tend to end up on the periphery of a homogeneous neighborhood of conductors, because like charges repel. Charge will also tend to end up along the S+­S- boundary because opposite charges attract. Figure 1 depicts the redistribution of charges, where the shading is proportional to the magnitude i. An ML classifier can be built based on this model, once again using (x) > 0 as the decision rule


for classifying an input x. In this model, however, the i are not uniform; the conductors with large i will have the greatest influence on the potential function. Consequently, one can think of i as the weight or importance of example i. As we will show shortly, the examples with i > 0 are exactly support vectors of an SVM.

+

+ +

+ +

+ + +

+ +

+ +

+

+ + + +

+

-

+

-

-

-

-

+ + + -

Figure 1: Coupled conductor system following charge redistribution. Shading reflects the charge magnitude, and the contour indicates a zero potential. The redistribution of charges in the electrostatic system is achieved via minimization of the Coulomb energy. Imagine placing the same total charge magnitude, m, on S+ and S- by dividing it uniformly among the conductors, i.e., i = m/ |Syi|. The free charge flow in S+ and S- yields a distribution of charges, the i, such that Coulomb energy is minimized. To introduce Coulomb energy, we begin with some preliminaries. The potential at conductor i, (xi), which we will denote more compactly as i, can be described potential induced on conductor i by charge Qj on conductor j; Pii  Pij  0 and Pij = Pji. If each conductor i is a metal sphere centered at xi and has radius ri (radii are enforced to be small enough so that the spheres do not touch each other), the system can be modeled by a point charge Qi at xi, and Pij = G xi, xj as in the previous section [10]. The self-potential, Pii, is defined as a function of ri. The Coulomb energy is defined in terms of the potential on the conductors, i:

in terms of the coefficients of potential Pij [10]: i = N j=1 Pij Qj, where Pij is the

N N

E = 1 2 i Qi = 1 2 QT P Q = 1 2 Pij yi yj i j .

i=1 i,j=1

When the energy minimum is reached, the potential i will be the same for all connected i  S+ (i  S-); we denote this potential S+ (S-). Two additional constraints on the system of coupled conductors are necessary in order to interpret the system in terms of existing machine learning models. First, the positive and negative potentials must be balanced, i.e., S+ = -S-. This constraint is achieved by setting the reference point of the potentials through b, Second, the conductors must be prevented from reversing the sign of their charge, i.e., i  0, and from holding more than a quantity C of charge, i.e., i  C. These

b = -0.5 (S+ + S-), into the potential function:  (x) = N i=1 Qi G xi, x + b.


requirements can be satisfied in the electrostatic model by disconnecting a conductor i from the charge flow in S+ or S- when i reaches a bound, which will subsequently freeze its charge. Mathematically, the requirements are satisfied by treating energy minimization as a constrained optimization problem with 0  i  C. The electrostatic system corresponds to a ­support vector machine (­SVM) [9]

with kernel G if we set C = 1/N. The electrostatic system assures that

iS-

iS+

i =

i = 0.5 . The identity holds because the Coulomb energy is exactly the

­SVM quadratic objective function, and the thresholded electrostatic potential evaluated at a location is exactly the SVM decision rule. The minimization of potentials differences in the systems S+ and S- corresponds to the minimization of slack variables in the SVM (slack variables express missing potential due to the upper bound on i). Mercer's condition [6], the essence of the nonlinear SVM theory, is equivalent to the fact that continuous electrostatic energy is positive, i.e., E = G (x, z) h (x) h (z) dx dz  0. The self-potentials of the electrostatic system provide an interpretation to the diagonal elements in the quadratic objective function of the SVM. This interpretation of the diagonal elements allows us to introduce novel kernels and novel SVM methods, as we discuss later. 1.3 Electrostatic model 3: Coupled point charges with battery In electrostatic model 2, we control the magnitude of charge applied to S+ and S-. Although we apply the same charge magnitude to each, we do not have to control the resulting potentials S+ and S-, which may be imbalanced. We compensate for this imbalance via the potential offset b. In electrostatic model 3, we control the potentials S+ and S+ directly by adding a battery to the system. We connect S+ to the positive pole of the battery with potential +1 and S- to the negative pole with potential -1. The battery ensures that S+ = +1 and S- = -1 because charges flow from the battery into or out of the system until the systems take on the potential of the battery poles. The battery can then be removed. The potential i = yi is forced by the battery on conductor i. The total Coulomb energy is the energy from model 2 minus the work done by the battery. The work done by the

battery is iN yiQi =

iN N

i. The Coulomb energy is

N

1 2

N

QT P Q - i =

1 2 Pij yi yj i j - i .

i=1 i,j=1 i=1

This physical system corresponds to a C­support vector machine (C­SVM) [2, 11]. in the system described here, it can be enforced by a slightly different system [4]. A more straightforward relation to the C­SVM is given in [9] where the authors show that every ­SVM has the same class boundaries as a C­SVM with appropriate C. 2 Comparison of existing and novel models 2.1 Novel Kernels The electrostatic perspective makes it easy to understand why SVM algorithms can break down in high-dimensional spaces: Kernels with rapid fall-off induce small potentials and consequently, almost every conductor retains charge. Because a charged conductor corresponds to a support vector, the number of support vectors is large, which leads to two disadvantages: (1) the classification procedure is slow, and (2) the expected generalization error increases with the number of support vectors [11]. We therefore should use kernels that do not drop off exponentially. The self­potential The C­SVM requires that

i yii = 0; although this constraint may not be fulfilled


permits the use of kernels that would otherwise be invalid, such as a generalization ri the radius of the ith sphere. The ris are increased to their maximal values, i.e. until they hit other conductors (ri = 0.5 minj xi -xj ). These kernels, called "Coulomb kernels", are invariant to scaling of the input space in the sense that scaling does not change the minimum of the objective function. Consequently, such kernels are appropriate for input data with varying local densities. Figure 2 depicts a classification task with input regions of varying density. The optimal class boundary is smooth in the low data density regions and has high curvature in regions, where the data density is high. The classification boundary was constructed using

of the electric field: G xi, xj := xi - xj -l 2 and G xi, xi -l := ri = Pii, where

2

-l/2

a C-SVM with a Plummer kernel G xi, xj := xi -xj 2 2 + 2 , which is an

approximation to our novel Coulomb kernel but lacks its weak singularities.

Figure 2: Two class data with a dense region and trained with a SVM using the new kernel. Gray-scales indicate the weights -- support vectors are dark. Boundary curves are given for the novel kernel (solid), best RBF-kernel SVM which overfits at high density regions where the resulting boundary goes through a dark circle (dashed), and optimal boundary (dotted).


2.2 Novel SVM models Our electrostatic framework can be used to derive novel SVM approaches [4], two representative examples of which we illustrate here. 2.2.1 ­Support Vector Machine (­SVM): We can exploit the physical interpretation of Pii as conductor i's self­potential. The Pii's determine the smoothness of the charge distribution at the energy minimum.  controls the complexity of the corresponding SVM. With this modification, and with C = , electrostatic model 3 becomes what we call the ­SVM. 2.2.2 p­Support Vector Machine (p­SVM): At the Coulomb energy minimum the electrostatic potentials equalize: i - yi = 0, i (y is the label vector). This motivates the introduction of potential difference,

We can introduce a parameter  to rescale the self potential ­ Pii new =  Pii . old

1 2

PQ + y = QT P T PQ + QT P T y + yT y as the objective. We obtain min T Y PT P Y  - 1TY P Y 



subject to

1 2

1T P Y  = 0 , |i|  C,

2 2 1 2 1 2

where 1 is the vector of ones and Y := diag(y). We call this variant of the optimization problem the potential-SVM (p-SVM). Note that the p-SVM is similar to the "empirical kernel map" [9]. However P appears in the objective's linear term and the constraints. We classify in a space where P is a dot product matrix. The constraint 1T P Y  = 0 ensures that the average potential for each class is equal. By construction, P T P is positive definite; consequently, this formulation does not require positive definite kernels. This characteristic is useful for problems in which the properties of the objects to be classified are described by their pairwise proximities. That is, suppose that instead of representing each input object by an explicit feature vector, the objects are represented by a matrix which contains a real number indicating the similarity of each object to each other object. We can interpret the entries of the matrix as being produced by an unknown kernel operating on unknown feature vectors. In such a matrix, however, positive definiteness cannot be assured, and the optimal hyperplane must be constructed in Minkowski space. 3 Experiments UCI Benchmark Repository. For the representative models we have introduced, we perform simulations and make comparisons to standard SVM variants. All datasets (except "banana" from [7]) are from the UCI Benchmark Repository and were preprocessed in [7]. We did 100-fold validation on each data set, restricting the training set to 200 examples, and using the remainder of examples for testing. We compared two standard architectures, the C­SVM and the ­SVM, to our novel architectures: to the ­SVM, to the p­SVM, and to a combination of them, the ­p­SVM. The ­p­SVM is a p­SVM regularized like a ­SVM. We explored the use of radial basis function (RBF), polynomial (POL), and Plummer (PLU) kernels. Hyperparameters were determined by 5­fold cross validation on the first 5 training sets. The search for hyperparameter was not as intensive as in [7]. Table 1 shows the results of our comparisons on the UCI Benchmarks. Our two novel architectures, the ­SVM and the p­SVM, performed well against the two existing architectures (note that the differences between the C­ and the ­SVM are due to model selection). As anticipated, the p­SVM requires far fewer support vectors. Additionally, the Plummer kernel appears to be more robust against hyperparameter and SVM choices than the RBF or polynomial kernels.


RBF POL PLU RBF POL PLU RBF POL PLU

C 6.4 22.8 6.1 33.6 36.0 33.4 28.7 33.7 28.8

 9.4 12.6 6.2  thyroid 7.7 7.0 6.1 p 5.4 13.3 5.7

breast­cancer

31.6 25.7 33.1 29.3 29.6 28.5 33.8 29.6 33.4 german 29.0 26.2 33.3

32.4

27.1 29.1 30.6 33.4 27.8 28.8 31.8 26.2 27.1 33.3

-p 8.6 6.9 6.1 33.7

C 21.4 20.4 16.3  19.1 20.4 16.3

 heart 17.9 19.3 16.3 banana

p 22.4 23.0 -p 17.8 19.3

17.4 16.3

13.2 36.7

35.3 15.7 35.0 15.7

13.2 11.6 13.4

11.5 15.7 22.4 21.9 11.5 15.7

Table 1: Mean % misclassification on 5 UCI Repository data sets. Each cell in the table is obtained via 100 replications splitting the data into training and test sets. The comparison is among five SVMs (the table columns) using three kernel functions (the table rows). Cells in bold face are the best result for a given data set and italicized the second and third best.

Pairwise Proximity Data. We applied our p­SVM and the generalized SVM

(G­SVM) [3] to two pairwise-proximity data sets. The first data set, the "cat cortex" data, is a matrix of connection strengths between 65 cat cortical areas and was provided by [8], where the available anatomical literature was used to determine proximity values between cortical areas. These areas belong to four different coarse brain regions: auditory (A), visual (V), somatosensory (SS), and frontolimbic (FL). The goal was to classify a given cortical area as belonging to a given region or not. The second data set, the "protein" data, is the evolutionary distance of 226 sequences of amino acids of proteins obtained by a structural comparison [5] (provided by M. Vingron). Most of the proteins are from four classes of globins: hemoglobin-ff (H-ff), hemoglobin-fi (H-fi), myoglobin (M), and heterogenous globins (GH). The goal was to classify a protein as belonging to a given globin class or not. As Table 2 shows, our novel architecture, the p­SVM, beats out an existing architecture in the literature, the G­SVM, on 5 of 8 classification tasks, and ties the G­SVM on 2 of 8; it loses out on only 1 of 8. cat cortex protein data

Size G-SVM G-SVM G-SVM p-SVM p-SVM p-SVM

Reg. -- 0.05 0.1 0.2 0.6 0.7 0.8 V 18 4.6 4.6 6.1 3.1 3.1 3.1 A 10 3.1 3.1 1.5 1.5 3.1 3.1 SS 18 3.1 6.1 3.1 6.1 4.6 4.6 FL 19 1.5 1.5 3.1 3.1 1.5 1.5 Reg. -- 0.05 0.1 0.2 300 400 500 H- 72 1.3 1.8 2.2 0.4 0.4 0.4 H- 72 4.0 4.5 8.9 3.5 3.1 3.5 M 39 0.5 0.5 0.5 0.0 0.0 0.0 GH 30 0.5 0.9 0.9 0.4 0.9 1.3

Table 2: Mean % misclassifications for the cat-cortex and protein data sets using the p­SVM and the G­SVM and a range of regularization parameters (indicated in the column labeled "Reg."). The result obtained for the cat-cortex data is via leaveone-out cross validation, and for the protein data is via ten-fold cross validation. The best result for a given classification problem is printed in bold face.


4 Conclusion

The electrostatic framework and its analogy to SVMs has led to several important ideas. First, it suggests SVM methods for kernels that are not positive definite. Second, it suggests novel approaches and kernels that perform as well as standard methods (will undoubtably perform better on some problems). Third, we demonstrated a new classification technique working in Minkowski space which can be used for data in form of pairwise proximities. The novel approach treats the proximity matrix as an SVM Gram matrix which lead to excellent experimental results. We argued that the electrostatic framework not only characterizes a family of support-vector machines, but it also characterizes other techniques such as nearest neighbor classification. Perhaps the most important contribution of the electrostatic framework is that, by interrelating and encompassing a variety of methods, it lays out a broad space of possible algorithms. At present, the space is sparsely populated and has barely been explored. But by making the dimensions of this space explicit, the electrostatic framework allows one to easily explore the space and discover novel algorithms. In the history of machine learning, such general frameworks have led to important advances in the field. Acknowledgments We thank G. Hinton and J. Schmidhuber for stimulating conversations leading to this research and an anonymous reviewer who provided helpful advice on the paper. References [1] M. A. Aizerman, E. M. Braverman, and L. I. Rozono´er. Theoretical foundations of the potential function method in pattern recognition learning. Automation and Remote Control, 25:821­837, 1964. [2] C. J. C. Burges. A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, 2(2):1­47, 1998. [3] T. Graepel, R. Herbrich, B. Sch¨olkopf, A. J. Smola, P. L. Bartlett, K.-R. M¨uller, K. Obermayer, and R. C. Williamson. Classification on proximity data with LP­machines. In Proceedings of the Ninth International Conference on Artificial Neural Networks, pages 304­309, 1999. [4] S. Hochreiter and M. C. Mozer. Coulomb classifiers: Reinterpreting SVMs as electrostatic systems. Technical Report CU-CS-921-01, Department of Computer Science, University of Colorado, Boulder, 2001. [5] T. Hofmann and J. Buhmann. Pairwise data clustering by deterministic annealing. IEEE Trans. Pattern Anal. and Mach. Intelligence, 19(1):1­14, 1997. [6] J. Mercer. Functions of positive and negative type and their connection with the theory of integral equations. Philosophical Transactions of the Royal Society of London A, 209:415­446, 1909. [7] G. R¨atsch, T. Onoda, and K.-R. M¨uller. Soft margins for AdaBoost. Technical Report NC-TR-1998-021, Dep. of Comp. Science, Univ. of London, 1998. [8] J. W. Scannell, C. Blakemore, and M. P. Young. Analysis of connectivity in the cat cerebral cortex. The Journal of Neuroscience, 15(2):1463­1483, 1995. [9] B. Sch¨olkopf and A. J. Smola. Learning with Kernels -- Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, 2002. [10] M. Schwartz. Principles of Electrodynamics. Dover Publications, NY, 1987. Republication of McGraw-Hill Book 1972. [11] V. Vapnik. The nature of statistical learning theory. Springer, NY, 1995.


