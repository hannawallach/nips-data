Regularized Greedy Importance Sampling

Finnegan Southey

 

Dale Schuurmans Ali Ghodsi

School of Computer Science University of Waterloo fdjsouth,dale,aghodsib @cs.uwaterloo.ca

¡ Abstract

Greedy importance sampling is an unbiased estimation technique that reduces the variance of standard importance sampling by explicitly searching for modes in the estimation objective. Previous work has demonstrated the feasibility of implementing this method and proved that the technique is unbiased in both discrete and continuous domains. In this paper we present a reformulation of greedy importance sampling that eliminates the free parameters from the original estimator, and introduces a new regularization strategy that further reduces variance without compromising unbiasedness. The resulting estimator is shown to be effective for difficult estimation problems arising in Markov random field inference. In particular, improvements are achieved over standard MCMC estimators when the distribution has multiple peaked modes.

1 Introduction Many inference problems in graphical models can be cast as determining the expected value of a random variable of interest, , given observations drawn according to a tarnatural situations is usually not in a form that we can sample from efficiently. For example, in standard Bayesian network inference corresponds to for a given

get distribution £

¢

. That is, we are interested in computing

£

¤¦¥¨§©¢ . Unfortunately, in

assignment to evidence variables

0

£ !"

in a given network

2

#%$&!')(01

. It is usually not possible to sam-

ple from this distribution directly, nor efficiently evaluate or even approximate

# $ ')(01

at

given points [2]. It is therefore necessary to consider restricted architectures or heuristic and approximate algorithms to perform these tasks [6, 3]. Among the most convenient and successful techniques for performing inference are stochastic methods which are guaranteed to converge to a correct solution in the limit of large random samples [7, 14, 4]. These methods can be easily applied to complex inference problems that overwhelm deterministic approaches. The family of stochastic inference methods can be grouped into the independent Monte Carlo methods (importance sampling and rejection sampling [7, 4]) and the dependent Markov Chain Monte Carlo (MCMC) methods (Gibbs sampling, Metropolis sampling, and Hybrid Monte Carlo) [7, 5, 8, 14]. The goal of all these methods is to simu-

late drawing a random sample from a target distribution that is hard to sample from directly.

£ 

defined by a graphical model

In this paper we improve the greedy importance sampling (GIS) technique introduced in [12, 11]. GIS attempts to improve the variance of importance sampling by explicitly search-

ing for important regions in the target distribution

£

. Previous work has shown that search


can be incorporated in an importance sampler while maintaining unbiasedness, leading to improved estimation in simple problems. However, the drawbacks of the previous GIS method are that it has free parameters whose settings affect estimation performance, and its importance weights are directed at achieving unbiasedness without necessarily being directed at reducing variance. In this paper, we introduce a new, parameterless form of greedy importance sampling that performs comparably to the previous method given its best parameter settings. We then introduce a new weight calculation scheme that preserves unbiasedness, but provides further variance reduction by "regularizing" the contributions each search path gives to the estimator. We find that the new procedure significantly improves the original technique and achieves competitive results on difficult estimation problems arising in large discrete domains, such as those posed by Boltzmann machines. Below we first review the generalized importance sampling procedure that forms the core of our estimators before describing the innovations that lead to improved estimators. 2 Generalized importance sampling

Importance sampling is a useful technique for estimating

¤¦¥§© ¢

be sampled from directly. The basic idea is to draw independent points



cording to a simple proposal distribution



 £ ! 



when¡£¢¥¤¦¤¦¤§¢ cannot but then weight the points according to  ©¨

ac-

£

. Assuming that we can evaluate

£ !

the weighted sample can

be used to estimate desired expectations (Figure 1).1 The unbiasedness of this proce-

dure is easy to establish, since for a random variable

¢ under



©£!"# ¢ £ !¤

is  & 

§©¢ !

¤)¥§© ¢





¢

the expected weighted value of

©"!"# ¢ ! !"$ 



©£!"# ¢! ¨§© %



¥§©  

. (For simplicity we will focus on the discrete case in this

paper.) The main difficultywith importance sampling is that even though it is an effective type results in a high variance estimator since the sample will almost always contains unrepresentative points but will intermittently be dominated by a few high weight points. The idea behind greedy importance sampling (GIS) [11, 12] is to avoid generating under-weight To develop a provably unbiased GIS procedure it is useful to first consider a generalization of standard importance sampling that can be proved to yield unbiased estimates: The generalized importance sampling procedure introduced in [12] operates by sampling deterministic blocks of points instead of individual points (Figure 1). Here, to each domain

estimation technique when



when

approximates

£

over most of the domain, it performs poorly

does not have reasonable mass in high probability regions of

£

. A mismatch of this

samples by explicitly searching for significant regions in the target distribution £ .

point

2@'

('

we associate a fixed block

. When ©' is drawn from the proposal distribution 

  2)'0 ©'21¡3¢4¤§¤¦¤¦¢ ©'21 ¡ 576 , where 89' is the length of block

we recover block A' and add the 2

block points to the sample.2 Ensuring unbiasedness then reduces to weighting the sampled points appropriately. To this end, [12] introduces an auxiliary weighting scheme that can be used to obtain unbiased estimates: To each pair of¢ points , (such that ) one

associates a weight G



' assigns to sample point C in its block

1

!(' HC 

¢

B' DC DCFE 2@'

, where intuitively G (' DC¢ is the weight that initiating point 2

' . The G

!  

' C values can be arbitrary as long

!  

Unfortunately, for standard inference problems in graphical models it is usually not possible to

evaluate I)PRQDS directly but rather just I)PRQHSVUWI)PRQDSYX for some unknown constant X . However it is T still possible to apply the "indirect" importance sampling procedure shown in Figure 1 by assigning indirect weights `aPRQDSVUbTI)PRQDSYc¥dFPRQDS and renormalizing. The drawback of the indirect procedure is that it is no longer unbiased at small sample sizes, but instead only becomes unbiased in the large sample limit [4]. To keep the presentation simple we will focus on the "direct" form of importance sampling described in Figure 1 and establish unbiasedness for that case--keeping in mind that every extended form of importance sampling we discuss below can be converted to an "indirect" form. not even contain their initiating point QDe --however their union has to cover the sample space f , and d cannot put zero probability on initiating points which leaves sample points uncovered.

2 There is no restriction on the blocks other than that they be finite--blocks can overlap and need


"Direct" importance sampling

 

 

Draw ¢¡¤£¤¥¦¥¦¥¦£ ¨§ indep. according to d .

Q Q Weight each point by © PRQHe S U

Estimate 

"

U

¦

! PRQHS by

§¡ $#e ¡ 

§

PRQ e !©S PRQ e S .

 66

.

 

"Generalized" importance sampling

  

Draw ¢¡¤£¤¥¦¥¦¥¦£ ¨§ indep. according to d . Q Q For each QHe , recover its block Create a large sample out of the blocks Q ¡ C ¡ £¤¥¦¥H¥¦£ Q ¡ CE 2 £I¥H¥¦¥ Q § C ¡ £¤¥¦¥¦¥H£ Q § CE .

@ e BA DCe U Q ¡¤£¤¥¦¥¦¥¦£ Q DCE . e GF6

Weight QPSR Q

@

e by © e PRQTP4S U ¤U'WV  ¦

6

(¦

Estimate X ! PRQHS by ¦

"

Y

U `

a ab § E 6 1#e ¡ # ¡

b b

 

 

 

Draw Q ¡ £¤¥¦¥¦¥¦£ Q § indep. according to d . where I T U I X for some unknown X .

Weight each point by `aPRQ e S &% U ¦¦ '6'6

Estimate 

"

U

¦

)(103254  6 7($082  6

 PRQHS¦ by

6 ¦

6 C  ¤U

"Indirect" importance sampling

 

 

 

PRQHe9C !©S e PRQHeDC S

(direct form)

6 96 '16¦ '6 .

Figure 1: Basic importance sampling procedures

as they satisfy



!(' HC 

¢

© 6 !"# G   dc!  

' C ¢

for every

 piE 2 DC . (Here c

indicates c

(' HC  fe HC 2&'

¢

E

' C ¢   if

e

and c !(' HC  hg

¢



(1) if

sum to e' . In fact, it is quite easy to prove that this yields unbiased estimates [12] since the

expected weighted value of



 ¦ a 6 'rqRa

¢ when sampling initiating B'



under

¤UtsTu PRQ P !©S e PRQ P vS U 6   sxw 6  ¤UtsTu PRQ P S   6

a a ¤U¤sw  sxw 6

¦¤U¤y

6

is PRQ e £ Q P S dFPRQ e S

C .) That is, for each destination point C , the total of the incoming G -weight has to

PRQHe!£YQTP4S PRQQP¥S I)PRQQP¥S PRQHe£YQQP¥S U y U

 sw ¤Utsxw 6

PRQHe£YQQP¥S PRQQP¥S I)PRQQP S PRQHe£YQQP4S y

U   U sw PRQQP4S I)PRQTP4S  sw PRQHe£YQQP¥S PRQHe£YQQP¥S U  6



y   U sxw PRQQP¥S I)PRQQP¥S  U

¦ ! PRQDS

Crucially, this argument does not depend on how the block decomposition is chosen or how the G -weights are set, so long as they satisfy (1). That is, one could fix any block decomposition and weighting scheme, even one that depends on the target distribution and random variable , without affecting the unbiasedness of the procedure. Intuitively, this works because the block structure and weighting scheme are fixed a priori, and unbiasedness is achieved by sampling blocks and assigning fair weights to the points. The generality of this outcome allows one to consider using a wide range of alternative importance sampling schemes, while employing appropriate G -weights to cancel any bias. In particular, we will determine blocks on-line by following deterministic greedy search paths. 3 Parameter-free greedy importance sampling Our first contribution in this paper is to derive an efficient greedy importance sampling (GIS) procedure that involves no free parameters, unlike the proposal in [12]. One key motivating principle behind GIS is to realize that the optimal proposal dis-

¢ £

tribution for estimating 

(¢! £ !" ( ©£!"# (¢!"¤£  (,

¥§©  ¢  

with standard importance sampling is

 

which minimizes the resulting variance [10]. GIS at-

tempts to overcome a poor proposal distribution by explicitly searching for points that

maximally increase the objective

(¢ £ (

(Figure 2). The primary difficulty in imple-

menting GIS is finding ways to assign the auxiliary weights G

!  

' C so that they satisfy ¢

the constraint (1). If this can be achieved, the resulting GIS procedure will be unbiased via the arguments of the previous section. However, the G -weights must not only satisfy the constraint (1), they must also be efficiently calculable from a given sample.


"Greedy" importance sampling

  

Draw ¢¡¤£¤¥¦¥¦¥¦£ ¨§ independently from d . Q Q

For each QHe , let QHeDC ¡ U QHe and: @

Compute block

e U A Q DCe ¡¤£YQ DCe £¤¥¦¥¦¥¦£ Q DCE 6 F

¡ e

¤¥¥ y

§

£   U

¦

by taking local steps in¢ the direction of maximum PRQDS I)PRQDS until a local max.

¢

Weight each Q P R

¦ U6  



V

¦ ¤U 6 C 

@ e by © e PRQ P S U

V

 ¤U 6 C 

is defined in (2). where

§

¡ ¡ ... yy ¡¨¡ ... ¡ ¡

§

  ¤¥¥

  

P S I)P S  P S I)P S

... P S I)P S `  `

Y

¥H¥¦¥¥H¥¦¥ ... ¥H¥¦¥ Y yy ¡'§§ ©

¡

y

©

. .. §x§ 

Create the final sample from the blocks ¢¡ C¡W£¤¥¦¥H¥¦£ ¢¡¦ 2!£I¥H¥¦¥ ¨§Q C¡¤£¤¥¦¥¦¥H£ ¨§Qb CE .

Estimate 

"

U

Q Q CE

PRQHS by b (

§¡ 1#e ¡



§



E 6 # ¡

U ¦

b  

PRQ 9Ce !©S e PRQ 9Ce S .

Figure 2: "Greedy" importance sampling procedure (left); Section 4  matrix (right) A computationally efficient G -weighting scheme can be determined by distributing weight in a search tree in a top down manner: Note that to verify (1) for a domain point we have to consider every search path that starts at some other point B' and passes through C . If the search is deterministic (which we assume) then the set of search paths entering C will ¢form In principle, the tree will have unbounded depth since the greedy search procedure does not stop until it has reached a local maximum. Therefore, to ensure G BC & e we distribute



a tree. Let  C denote the tree of points that lead into C and let G    C   ©  !"!

U

(C

G $# C .   

 

by a convergent series;

 

C#

, to be G

 

C#



weight down the tree from level g (the root, C ) to levels e 

¢&%H¢¥¤¦¤¦¤

where¡ for simplicity we set the total weightC # allocated at level ' , G

§#)(  § 

¡ #)(10 . This trivially ensures 32#)465 G   e 

.3 (Finite depth bounds will be handled

automatically below.) Having established the total weight at level ' , G 

  C#

, we must then determine how much

of that weight is allocated to a particular point at that level. Given the entire search tree this would be trivial, but the greedy search paths will typically provide only a single branch of the tree. We accomplish the allocation by recursively dividing the weight equally amongst branches, starting at# thebyroot of the tree. Thus, if ¥'7 (8# is the inward branching factor at the

root, we divide G

©'

  C

7 ' (8# at the first level. Then, following the path to a desired point

, we successively divide the remaining weight at each point by the observed branching

factor 7 ' (9#A@ , 7 ' (8# @$0 , etc. until we reach ¡

('

. In the case 7 '  g ,

has no descendants

B'

©'

and we compensate by adding the mass of the missing subtree to 's weight. This scheme

is efficient to compute because we require only the branching factors along a given search path to correctly allocate the weight. This yields the following weighting scheme that runs in linear¢ time and exactly satisfies the constraint (1): Given a start¢ point ' and a search

path ©' ('(

 ¡£¢¥¤¦¤¦¤§¢ ©'(8# DC

 G ©' DC

   ¢

from (' to HC , we assign a weight G a' HC by

¡

C C C ¡

 B C 6ED82 6FD"GIHPHPH¡ 6FD  #)( 6ED82 6FD"GIHPHPH 6FD  #)(

C C

§§  §#I(10 

¡

if 7 'RQ g if 7 ' 7g 

  !  

(2)

where 7 ' (TS denotes the inward branching factor of point ' (TS . A simple induction proof can an efficient unbiased method for implementing GIS that does not use any free parameters. 4 Variance reduction While GIS reduces variance by searching, the G -weight correction scheme outlined above is designed only to correct bias and does not specifically address variance issues. However, be used to show that  ©   

6 G ©' DC  e . Therefore, the new G -weighting scheme provides ¢

3 We merely chose the simplest heavy tailed convergent series available.




there is a lot of leeway in setting the G -weights since the normalization constraint (1) is quite weak. In fact, one can exploit this additional flexibility to determine minimum variance unbiased estimators in simple cases. To illustrate, consider a toy domain consisting constrained to move between adjacent points so that from every initial point the greedy search will move to the right until it hits point . Any G -weighting scheme for this domain ¢ can be expressed as a matrix,  , shown in Figure 2, where row ¦ corresponds to the search block retrieved by starting at point ¦ . Note that the constraint (1) amounts to requiring that the columns of  sum to e . However, it is the rows of  that correspond to search blocks then  gives the column vector of block estimates that correspond to each start point. The variance of the overall estimator then becomes equal to the variance of the column vector  . In particular, if each row produces the same estimate, the estimator will have zero variance. We conclude that zero variance is achieved iff  equals a constant. Thus, the unbiasedness constraints behave orthogonally to the zero variance constraints: unbiasedness imposes a constraint on columns of  whereas zero variance imposes a constraint

of points e ¢&%H¢¡ D¢¥¤¦¤¦¤§¢£¢ , where g¥¤ ¢§¦  £ §¦ ©¨ ¢¦ e  £ ¦ e  . Assume the search is

sampled during estimation. If we assume a uniform proposal distribution

 ¡ ¨ ¢¥¤¦¤§¤¦¢ ¡ £

¢ ¨



¢ 

¢ 

on rows of  . An optimal estimator will satisfy both sets of constraints. Since there are

constraints in total and unbiased estimator (for

¢ ¢

¢

 7e 



Q %

%

¢ %

variables, one can apparently solve for a zero variance

). However, it turns out that the constraint matrix does not

have full rank, and it is not always possible to achieve zero bias and variance for given  . Nevertheless, one can obtain an optimal GIS estimator by solving a quadratic program for the  which minimizes variance subject to satisfying the linear unbiasedness constraints. The point of this simple example is not to propose a technique that explicitly enumerates the domain in order to construct a minimum variance GIS estimator. (Although the above discussion applies to any finite domain--all one needs to do is encode the search topology in the weight matrix  .) Rather, the point is to show that a significant amount of flexibility remains in setting the G -weights--even after the unbiasedness constraints have been satisfied--and that this additional flexibility can be exploited to reduce variance. We can now extend these ideas to a more realistic, general situation: To reduce the variance of the GIS estimator developed in Section 3, our idea is to equalize the block totals among different search paths. The main challenge is to adjust G -weights in a way that equalizes block totals without introducing bias, and without requiring excessive computational overhead. Here we follow the style of local correction employed in Section 3. First note that when traversing a path6FD from ' to" C6 ,1 the blocks sampled by GIS produce estimates of the search. This point will have been arrived at via some predecessor ' (TS @ , but we could the block totals that would have been obtained by arriving via any one of these predecessor points. The key to maintaining unbiasedness is to ensure that any weight calculation performed at a point in a search tree is consistent, regardless of the path taken to reach that point. Since we cannot anticipate the initial points, it is only convenient to equalize note the total sum obtained by points after ' (TS ; i.e. from ' (TS¨( to C . We equalize the different predecessor totals by determining factors ' # which satisfy the constraints

6FD 

form  '0 # S 415 §© !¥§©  ! ¨§© 

 6 ! §© © . 6FD! Now consider an intermediate point a'(TS in the

¡

have arrived at a'(TS via any one of its possible predecessors $#



. We would like to equalize

the subtotals from the predecessors %# , through , and up to the root

 ('(TS  DC . Let & ' (6S de¡ 

¢  £  (¥'  ¢  £ ! )0& 

# # # ' (TS ' (TS ' (TS

over the predecessors # . This scales the parent quantity



 1

¢!  £  (2&

' (6S ' (TS ' (TS on each

path to compensate for differences between predecessors. The equalization and unbiasedness constraints form a linear system whose solution we rescale to obtain positive '3# . The ' # are computed starting at the end of the block and working backwards. The results can be easily incorporated into the GIS procedure by multiplying the original G -weights in (2) by the product D' ( H' (10 ' ' (TS @ . Importantly, at a given search point, any of its predecessors will calculate the same ' -correction scheme locally, regardless of which predecessor

' ¡ ' ¤¦¤§¤ ¡


is actually sampled. This means that the correction scheme is not sample-dependent but fixed ahead of time. It is easy to prove that any fixed ' -weighting scheme that satisfies of this scheme is that it reduces variance while preserving unbiasedness.4 5 Empirical results: Markov random field estimation To investigate the utility of the GIS estimators we conducted experiments on inference problems in Markov random fields. Markov random fields are an important class of undirected graphical model which include Boltzmann machines as a special case [1]. These models are known to pose intractable inference problems for exact methods. Typically, standard MCMC methods such as Gibbs sampling and Metropolis sampling are applied to such problems, but their success is limited owing to the fact that these estimators tend to get trapped in local modes [7]. Moreover, improved MCMC methods such as Hybrid Monte Carlo [8] cannot be directly applied to these models because they require continuous sample spaces, whereas Boltzmann machines and other random field models define distributions on a discrete domain. Standard importance sampling is also a poor estimation strategy for these models because a simple proposal distribution (like uniform) has almost no chance of sampling in relevant regions of the target distribution [7]. Explicitly searching for modes would seem to provide an effective estimation strategy for these problems. We consider a generalization of Boltzmann machines that defines a joint distribution over

 C # 4 6FD!¡ ' #  7 ' (TS , and is applied to an unbiased G -weighting, will satisfy (1). The benefit

a set of discrete variables

£ '  ¥¤§¦©¨ 

 ¢ ¡ 

!



¡£¢¥¤¦¤§¤¦¢ ¡

 ('

,

 £¢ E e e ¡ ,accordingto

¢



!' 



 

' 

'21CC a'! ' C ' C

!  ) ¢ !' 0§

where   "' ' '

  ¤

Here  is the "temperature" of the model and

'

defines the "energy" of configuration

; the functions ' C and ' define the local energy between pairs of variables and individ-

ual variables respectively; and  is a normalization constant. Exact inference in such a model is difficult because the normalization constant  is typically unknown. Moreover,  is usually not possible to obtain exactly because it is defined as an exponentially large sum that is not prone simplification.5 We experimented with two classes of generalized Boltzmann machines: generalized Ising models, where the underlying graph is a 2 dimensional grid, and random models, where the graph is generated by randomly choosing links between variables. For each model, the function values were chosen randomly from a standard normal distribution. We considered the objective functions (ex-

pected energy);

¢!'   

¢!' 

' 1CC ('%#

!

  ' 

 $# 'C

(expected number of 1's in a configuration); and  e (expected number of pairwise "and's" in a configura-

!('& e  ¢!' 



 ' 

tion). The latter two objectives are summaries of the quantities needed to estimate gradients in standard Boltzmann machine learning algorithms [1]. This would seem to be an ideal model on which to test our methods. We conducted experiments by fixing a model and temperature and ran the estimators for a fixed amount of CPU time. Each estimator was re-run 1000 times to estimate their root mean squared error (RMSE) on small models where exact answers could be calculated, or standard deviation (STD) on large models where no such exact answer is feasible. We compared estimators by controlling their run time (given a reasonable C implementation) not just their sample size, because the different estimators use different computational overheads, and run time is the only convenient way to draw a fair comparison. For example, GIS methods require a substantial amount of additional computation to find the greedy search

4 This variance reduction scheme applies naturally to unbiased direct estimators. With indirect

estimators, bias is typically more problematic than variance. Therefore, for indirect GIS we employ an alternative & -weighting scheme that attempts to maximize total block weight. ods for the special case of Ising models [9, 15, 13].

5 Interesting recent progress has been made on developing exact and approximate sampling meth-


E(energy) IS GISold GISnew GISreg Gibbs Metro

GISreg 4x4 GISreg 5x5 GISreg 6x6 GISreg 7x7 GISreg 8x8

Avg SS 5094 1139 1015 1015 36524 35885 RMSE @ T=1.0 27.75 13.89 14.31 3.01 0.21 0.28 T=0.5 68.96 12.93 13.73 4.10 0.37 0.53 T=0.25 145.97 12.96 13.94 5.57 4.44 5.75 T=0.1 374.04 13.35 15.25 6.61 21.86 24.56 T=0.05 749.42 10.46 11.78 6.20 53.44 56.16 T=0.025 1503.73 12.59 11.03 7.72 108.13 122.46

25 25

20 20

15 15

RMSE RMSE 10 10

5 5

Gibbs 4x4 Gibbs 5x5 Gibbs 6x6 Gibbs 7x7 Gibbs 8x8

0 0 1

0.1 Temperature 0.01 1 0.1 Temperature 0.01

Figure 3: Estimating average energy in a random field model (table shows results for  ¢¡£  ).

E(and's) IS GISold GISnew GISreg Gibbs Metro

GISreg 4x4 GISreg 5x5 GISreg 6x6 GISreg 7x7 GISreg 8x8

Avg SS 4764 1125 1015 1015 22730 25789 RMSE @ T=1.0 6.10 6.33 6.09 3.56 0.33 0.37 T=0.5 8.42 5.16 5.16 3.06 0.36 0.43 T=0.25 9.60 4.03 4.30 2.43 0.59 0.63 T=0.1 10.45 2.57 2.85 0.90 0.70 0.76 T=0.05 10.15 0.64 0.61 0.17 1.41 1.30 T=0.025 10.15 0.43 0.15 0.05 1.54 1.41

RMSE

8 7 6 5 4 3 2 1 0

RMSE

8 7 6 5 4 3 2 1 0

Gibbs 4x4 Gibbs 5x5 Gibbs 6x6 Gibbs 7x7 Gibbs 8x8

1

0.1 Temperature 0.01 1 0.1 Temperature 0.01

Figure 4: Estimating average "sum of and's" in a random field model (table shows  ¤¡¥  ).

paths and calculate inward branching factors, and consequently they must use substantially smaller sample sizes than their counterparts to ensure a fair comparison. However, the GIS estimators still seem to obtain reasonable results despite their sample size disadvantage.

For the GIS procedures we implemented a simple search that only ascends in

(¢!'  £ '  (,

£ ' 

not

and we only used a uniform proposal distribution in all our experiments. We

also only report results for the indirect versions of all importance samplers (cf. Figure 1). Figures 3 and 4 show typical outcomes of our experiments. Table 3 shows results for estimating expected energy in an  ¦¡§  generalized Ising model when temperature is dropped from 1.0 to 0.025. Figure 4 shows comparable results for estimating the "sum of and's". Standard importance sampling (IS) is a poor estimator in this domain, even when it is able to use 4.5 times as many data points as the GIS estimators. IS becomes particularly poor when the temperature drops. Among GIS estimators, the new, parameter-free version introduced in Section 3 (GIS new) compares favorably to the previous technique of [12] (GIS old). The regularized GIS from Section 4 (GIS reg) is clearly superior to either. Next, to compare the importance sampling approaches to the MCMC methods, we see the dramatic effect of temperature reduction. Owing to their simplicity (and an efficient implementation), the MCMC samplers were able to gather about 20 to 30 times as many data


points as the GIS estimators in the same amount of time. The effect of this substantial sample size advantage is that the MCMC methods demonstrate far better performance at high temperatures; apparently owing to an evidential advantage. However, as the temperature is lowered, a well known effect takes hold as the the low energy configurations begin to dominate the distribution. At low temperatures the modes around the low energy configurations become increasingly peaked and standard MCMC estimators become trapped in modes from which they are unable to escape [8, 7]. This results in a very poor estimate that is dominated by arbitrary modes. Figures 3 and 4 show the RMSE curves of Gibbs sampling and GIS reg, side by side, as temperature is decreased in different models. By contrast to MCMC procedures, the GIS procedures exhibit almost no accuracy loss as the temperature is lowered, and in fact sometimes improve their performance. There seems to be a clear advantage for GIS procedures in sharply peaked distributions. Also they appear to have much more robustness against varying steepness in the underlying distribution. However, at warmer temperatures the MCMC methods are clearly superior. It is important to note that greedy importance sampling is not equivalent to adaptive importance sampling. Sample blocks are completely independent in GIS, but sample points are not independent in AIS. Nevertheless, GIS can benefit from adapting the proposal distribution in the same way as standard IS. Clearly we cannot propose GIS methods as a replacement for MCMC approaches, and in fact believe that useful hybrid combinations are possible. Our goal in this research is to better understand a novel approach to estimation that appears to be worth investigating. Much work remains to be done in reducing computational overhead and investigating additional variance reduction techniques. References [1] D. Ackley, G. Hinton, and T. Sejnowski. A learning algorithm for Boltzmann machines. Cognitive Science, 9:147­169, 1985. [2] P. Dagum and M. Luby. Approximating probabilistic inference in Bayesian belief networks is NP-hard. Artificial Intelligence, 60:141­153, 1993. [3] P. Dagum and M. Luby. An optimal approximation algorithm for Bayesian inference. Artificial Intelligence, 93:1­27, 1997. [4] J. Geweke. Baysian inference in econometric models using Monte Carlo integration. Econometrica, 57:1317­1339, 1989. [5] W. Gilks, S. Richardson, and D. Spiegelhalter. Markov Chain Monte Carlo in Practice. Chapman and Hall, 1996. [6] M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. An introduction to variational methods for graphical models. In Learning in Graphical Models. Kluwer, 1998. [7] D. MacKay. Intro to Monte Carlo methods. In Learning in Graphical Models. Kluwer, 1998. [8] R. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Tech report, 1993. [9] J. Propp and D. Wilson. Exact sampling with coupled Markov chains and applications to statistical mechanics. Random Structures and Algorithms, 9:223­253, 1996. [10] R. Rubinstein. Simulation and the Monte Carlo Method. Wiley, New York, 1981. [11] D. Schuurmans. Greedy importance sampling. In Proceedings NIPS-12, 1999. [12] D. Schuurmans and F. Southey. Monte Carlo inference via greedy importance sampling. In Proceedings UAI, 2000. [13] R. Swendsen, J. Wang, and A. Ferrenberg. New Monte Carlo methods for improved efficiency of computer simulations in statistical mechanics. In The Monte Carlo Method in Condensed Matter Physics. Springer, 1992. [14] M. Tanner. Tools for Statistical Inference: Methods for Exploration of Posterior Distributions and Likelihood Functions. Springer, New York, 1993. [15] D. Wilson. Sampling configurations of an Ising system. In Proceedings SODA, 1999.


