Clustering with the Fisher Score

  ¡ ¢¡

Koji Tsuda, Motoaki Kawanabe and Klaus-Robert M¨uller

 

AIST CBRC, 2-41-6, Aomi, Koto-ku, Tokyo, 135-0064, Japan

Fraunhofer FIRST, Kekul´estr. 7, 12489 Berlin, Germany

¡

¢

Dept. of CS, University of Potsdam, A.-Bebel-Str. 89, 14482 Potsdam, Germany koji.tsuda@aist.go.jp, £ nabe,klaus¤ @first.fhg.de Abstract Recently the Fisher score (or the Fisher kernel) is increasingly used as a feature extractor for classification problems. The Fisher score is a vector of parameter derivatives of loglikelihood of a probabilistic model. This paper gives a theoretical analysis about how class information is preserved in the space of the Fisher score, which turns out that the Fisher score consists of a few important dimensions with class information and many nuisance dimensions. When we perform clustering with the Fisher score, K-Means type methods are obviously inappropriate because they make use of all dimensions. So we will develop a novel but simple clustering algorithm specialized for the Fisher score, which can exploit important dimensions. This algorithm is successfully tested in experiments with artificial data and real data (amino acid sequences).

1 Introduction Clustering is widely used in exploratory analysis for various kinds of data [6]. Among them, discrete data such as biological sequences [2] are especially challenging, because efficient clustering algorithms e.g. K-Means [6] cannot be used directly. In such cases, one naturally considers to map data to a vector space and perform clustering there. We call the mapping a "feature extractor". Recently, the Fisher score has been successfully applied as a feature extractor in supervised classification [5, 15, 14, 13, 16]. The Fisher score is derived as follows: Let us assume that a probabilistic model ¥§¦©¨ ¨! is available. Given a parameter estimate " from training samples, the Fisher score vector is obtained as

#

(0)2143 (C)2143

¦©¨$&%'¦ (8749 ¥§¦©¨5"6 A@A@B@ (E7GF ¥§¦D¨D"6 IHP@

The Fisher kernel refers to the inner product in this space [5]. When combined with high performance classifiers such as SVMs, the Fisher kernel often shows superb results [5, 14]. For successful clustering with the Fisher score, one has to investigate how original classes are mapped into the feature space, and select a proper clustering algorithm. In this paper, it will be claimed that the Fisher score has only a few dimensions which contains the class information and a lot of unnecessary nuisance dimensions. So K-Means type clustering [6] is obviously inappropriate because it takes all dimensions into account. We will propose a clustering method specialized for the Fisher score, which exploits important dimensions with class information. This method has an efficient EM-like alternating procedure to learn, and has the favorable property that the resultant clusters are invariant to any invertible linear


transformation. Two experiments with an artificial data and an biological sequence data will be shown to illustrate the effectiveness of our approach. 2 Preservation of Cluster Structure

Before starting, let us determine several notations. Denote by

crete or continuous) and by is denoted as ¦D¨8 §

#

% A@B@A@

. Let ¦©¨

¦  F ¢¡¤£ ¦¥¨§

the domain of objects (dis-

the set of class labels. The feature extraction be the underlying joint distribution and assume

that the class distributions ¦D¨

© 

 %

 are well separated,i.e. ¦ % ¨8 is close to 0 or 1.

¦©¨8

  

is known. Then the problem First of all, let us assume that the marginal distribution

is how to find a good feature extractor, which can preserve class information, based on the prior knowledge of ¦D¨8 . In the Fisher score, it amounts to finding a good parametric model

¥§¦©¨ 6  



F

. This problem is by no means trivial, since it is in general hard to infer

anything about the possible [12].

¦!¨8

from the marginal

¦©¨$

without additional assumptions



A loss function for feature extraction In order to investigate how the cluster structure is preserved, we first have to define what the class information is. We regard that the class information is completely preserved, if a set of predictors in the feature space can recover to recover the posteriors when classes are totally mixed up. As a predictor of posterior probability in the feature space, we adopt the simplest one, i.e. a linear estimator:

the true posterior probability

 ¦ 6¨8 . This view makes sense, because it is impossible

#H "$# F

¦©¨$&%

"¤#&%¦D¨8

#

¦D¨8

1% 2 3) 4

# #

@

The prediction accuracy of

eters and

# # 6 )

 ('0)5#

for ¦ % ¨8 is difficult to formulate, because param-

are learned from samples. To make the theoretical analysis possible, we

#

consider the best possible linear predictors. Thus the loss of feature extractor

class is defined as

#

¦ &% #

CED¤FHGPIQRD¤FSUTWV%

8@9BA7

H ¦©¨$ #

7

for -th (2.1)



where

is just the sum over all classes

S T

denote the expectation with the true marginal distribution ¦©¨$ . The overall loss

¦  % # 9 # ¦  .

7 fe g #ih #

7 X'0)`Ya¦! ¨8Rbdc

% 

Even when the full class information is preserved, i.e.

# ¦  %

r

qp¥

, clustering in the fea-

ture space may not be easy, because of nuisance dimensions which do not contribute to clustering at all. The posterior predictors make use of an at most dimensional subspace out of the -dimensional Fisher score, and the complementary subspace may not have any information about classes. K-means type methods [6] assume a cluster to be hyperspherical, which means that every dimension should contribute to cluster discrimination. For When nuisance dimensions cannot be excluded, we will need a different clustering method that is robust to nuisance dimensions. This issue will be discussed in Sec. 3. Optimal Feature Extraction In the following, we will discuss how to determine ¥§¦©¨ 6 . First, a simple but unrealistic example is shown to achieve ¦ C% , without producing nuisance dimensions at all. Let us assume that ¥§¦©¨ 6 is determined as a mixture model of

7

such methods, we have to try to minimize the dimensionality

r while keeping # ¦  small.

7 # 5p

#ihwg¦v #ihwg¦v

true class distributions:

ts¥ u

¦©¨   %

 ¡du5© e g¦v %

9 9

the true marginal distribution ¦©¨8 , when

x

#ih #yx9

99

#¦©¨  % (' £Y¦

x9

#

  ¥A 

¦©¨ %

x  (2.2)

where

x x

#

% ! % ©  d£% ¦¥eYf£4@

¦ %

x

4B@A@B@

£4

p  £% ¥`Y£$§.

A@B@A@

# 

Obviously this model realizes


#

¦©¨$&%

)2143

¥ ¦©¨   achieves

 

7 7

(proof) To prove the lemma, it is sufficient to show the existence of ¦

and dimensional vector

)2143

¥ ¦©¨ 

¦¨  ¢ s u§  (' ©§  ¦ d£

such that % ¦

% ¨$ B@A@A@B ¦ %

¦  % . # When the Fisher score is derived at the true parameter, it achieves

Lemma 1. The Fisher score

¥Y0£

¡ £¢ s u

¦  % .

#

¥eY£p¥¤ 2r p

¨$I H @

matrix

¦

(0)2143

 "!

sxg¦vUs usg¦v ¦¦x£ 9 %B@A@A@B ¦! ¥%

 

u  

 ! ¥Y0£



(2.3)

The Fisher score for ¥ ¦D¨  is

(¥

¦D¨ 

# % where

y9

3

 ¡Us' I % 9

C%

denotes

9

x # ¨$

Y g¦v £

£Ye g¦v

9  

#ih4¨899 #  x

 %

 5£% ¥Y0£ 4A@B@A@ @

Let

)2143

#$¤&%

x

£Y e g¦vh £

99

x  

and

 '¢ s u ( ¦! d£9

 

matrix filled with ones. Then

¦ (% v § 12 v  I

and %

g¦v

9 9

¥ ¦©¨ 9 &% ¦ % ¨8A@B@A@A ¦ %

, (2.3) holds.

  &¥eYf£ 1Y)0 I

¨8 IH g¦v 9 9 @

When we set

Loose Models and Nuisance Dimensions We assumed that ¦©¨$ is known but still we do not know the true class distributions  . Thus the model ¥ ¦©¨   in Lemma 1 is never available. In the following, the result of Lemma 1 is relaxed to a more general class of probability models by means of the chain rule of derivatives. However, in this case, we have to pay the price: nuisance dimensions.

Denote by

to the information geometry [1],

4

3

a set of probability distributions

3

3 ¡ Ps¥ ts¥ u ¦u  §

%

is regarded as a manifold in a Riemannian space. Let ¦  % , whichis answeredbythe followingtheorem.

¥  ¥§¦©¨ 6   @ Now the question is how to

#%

p

F ¦D¨ 

 §

. According

¦©¨

 ds u

denote the manifold of ¥§¦©¨ 6 :

determine a manifold

4

74 ¡

such that

Theorem 1. Assume that the true distribution ¦©¨$ is contained in



¦D¨8&% ¥§¦©¨   % ¥ ¦D¨   ¨ 

   



 

s u

4

:

4

where  is the true parameter. If the tangent space of

of

3# 7

¦  % @

at ¦D¨8 contains the tangent space #   derived from ¥§¦D¨   satisfies



at the same point (Fig. 1), then the Fisher score

p

(proof) To prove the theorem, it is sufficient to show the existence of ¦

and dimensional vector

¦¨ '5 )2143

§

such that

('©§  ¦ 5£%

% ¦ ¥§¦©¨ 6

4  ! ¥Yf£

¥Y0£

¥ Y£ ¤ r

¨8I H @

matrix

¦

3

¨8A@A@B@ ¦ %

When the tangent space of by the chain rule:

(C) 1 3

(

is contained in that of

(2.4)

around ¦©¨$ , we have the following

    (87

(

x 66 7BA9

ts¥ u ¦©¨   

x

# %

Let

C ©EDFHG % PI

where

) 1 3

FQG SRUTWVR 67@Xh 7 XA

 % 7YX

66

w h F ( )2143

9 (87¥§¦D¨



 # 6687@9h

@ (2.5)

@ With this notation, (2.5) is rewritten as

C` '5

¥§¦©¨   %

 

  !¦¦ £

¦ % ¨$ B@A@A@A ¦ % 9

% v C § (2 v  and %

  ¥Y0£9¨$IIH1Y9I9) I

9 9

g¦v g¦v

. The equation (2.4) holds by setting


M

Figure 1: Information geometric picture of a probabilistic model whose Fisher score can fully extract the

class information. When the tangent space of

contained in

4 7

3

is

, the Fisher score can fully extract the x p

Q

¦ &% . Details explained in

#

class information, i.e. the text.

p

Important

Figure 2: Feature space constructed by the Fisher score from the samples with two distinct clusters. The ¨ and mension, respectively. When the Euclidean metric is used as in K-Means, it is difficult to recover the two "lines" as clusters. 

-axis corresponds to an nuisance and an important di-

Nuisance

In determination of ¥§¦D¨ 6 , we face the following dilemma: For capturing important di-

mensions (i.e. the tangent space of

larger than But a large

¥. r

3

), the number of parameters should be sufficiently

leads to a lot of nuisance dimensions, which are harmful for

r

clustering in the feature space. In typical supervised classification experiments with hidden markov models [5, 15, 14], the number of parameters is much larger than the number of classes. However, in supervised scenarios, the existence of nuisance dimensions is not a serious problem, because advanced supervised classifiers such as the support vector machine have a built-in feature selector [7]. However in unsupervised scenarios without class labels, it is much more difficult to ignore nuisance dimensions. Fig. 2 shows how the feature space looks like, when the number of clusters is two and only one nuisance dimension is involved. Projected on the important dimension, clusters will be concentrated into two distinct points. However, when the Euclidean distance is adopted as in K-Means, it is difficult to recover true clusters because two "lines" are close to each other. 3 Clustering Algorithm for the Fisher score In this section, we will develop a new clustering algorithm for the Fisher score. Let

¡P G§ G  

9

!h

be a set of class labels assigned to ¨

9

pose of clustering is to obtain

¡d G§ G !h

¡ G§ G !h

9



only from samples ¨

¡ G§ G !h

9,. respectively. The purAs mensioned before,

in clustering with the Fisher score, it is necessary to capture important dimensions. So far, it has been implemented as projection pursuit methods [3], which use general measures for interestingness, e.g. nongaussianity. However, from the last section's analysis, we know more than nongaussianity about important dimensions of the Fisher score. Thus we will

construct a method specially tuned for the Fisher score. Let us assume that the underlying classes are well separated, i.e.

or 1 for each sample ¨ 

¦! f% G

¨ is close to 0

7

¥ G





. When the class information is fully preserved, i.e.

¦  % , #

p

there are bases in the space of the Fisher score, such that the samples in the -th cluster are projected close to 1 on the -th basis and the others are projected close to 0. The objective function of our clustering algorithm is designed to detect such bases:

#H # #   ¢¡ ¤£¤£¤£  ¦¥ §¡ ¤£¤£¤£ ©¨ ¡ ¤£¤£¤£ ¨ iI8@9AI C I IC QiI IQ 89A 8@9BA

#ihw h! V% g wG

9 9 ¦©¨ 

G('0) Y ¦YG  bPc %   (3.1)

where ¦I is the indicator function which is 1 if the condition  holds and 0 otherwise.  Notice that the optimal result of (3.1) is invariant to any invertible linear transformation transformation or data normalization [6]. When linear transformation is notoriously set,

# ¦©¨$ %  U'E§ #

¦©¨$ . In contrast, K-means type methods are quite sensitive to linear


K-means can end up with a false result which may not reflect the underlying structure.1 The objective function (3.1) can be minimized by the following EM-like alternating procedure:

!

9 #

Y §¦H v

1. Initialization: Set

%

¡P G§ G

9 #

¡

v £¢!9 e Gh G Gto ¤ ¥ 

9 !

¡P@G§#G

!h

9 initial values. Compute 9

¡P¡P%G§#!G§h#ihg

  !9 e Gh

%

¡ )# § g #ih 9

¦D¨8 and

% ) # #

¦©¨  ¦©¨  H

obtained as the solution of the following problem:

D% ) ¡©¨3 8@9A CUIQ %



¡ £ wh!

I % 9 V

This problem is analytically solved as

%

#

9

# #

H ¦D¨  #

for later use.

9

.

9 2. Repeat 3. and 4. until the convergence of 3. Fix and minimize with respect to

9

wGh!

 ¦©¨ 

!h and . Each is

G('0)3Y !¦ G  bPc  %  @

% h! £ wG

9

4. Fix

## !9,e% GGh !¦andG 

% ¦ 9 v  !¦ G % #

G Y  P 3) % Y # # #

#ih% #! #ih9



9

!%¦

where

by solving the following problem

¡P% § g ¡¨) § g

9 9 % @ minimize with respect to

¡P G§ G !h

. Each

#  G

IH ¦©¨  # G

is obtained

 G 8@9BA % G('0)# Y !¦  b c

%

#H # 

  9 ¦©¨  % 

The solution can be obtained by exhaustive search.

Steps 1, 3, 4 take

 % r  % ¥r  % ¥cr

¦  , ¦

c

 , ¦  computational costs, respectively. Since

the computational cost of algorithm is linear in , it can be applied to problems with large sample sizes. This algorithm requires ¦  time for inverting the matrix , which may only be an obstacle for an application in an extremely high dimensional data setting. 4 Clustering Artificial Data We will perform a clustering experiment with artificially generated data (Fig. 3). Since this

h # components 

¡ ¡

¡

data has a complicated structure, the Gaussian mixture with

probabilistic model for the Fisher score: ¥§¦©¨ 6&%

denotes the Gaussian distribution with mean

 

9

e   

%

is used as a

¦©¨    where ¦D¨  

 

 r %

#thw V g

¡

and covariance matrix . The parameters

are learned with the EM algorithm and the marginal distribution is accurately estimated as shown in Fig. 3 (upperleft). We applied the proposed algorithm and K-Means to the partition, we first divided the points into 8 subclusters by the posterior probability to each Gaussian. In K-means and our approach defined in Sec. 3, initial clusters are constructed by randomly combining these subclusters. For each method, we chose the best result which achieved the minimum loss among the local minima obtained from 100 clustering experiments. As a result, the proposed method obtained clearly separated clusters (Fig. 3, upper right) but K-Means failed to recover the "correct" clusters, which is considered as the effect of nuisance dimensions (Fig. 3, lower left). When the Fisher score is whitened (i.e. linear transformation to have mean 0 and unit covariance matrix), the result of K-Means changed to Fig. 3 (lowerright) but the solution of our method stayed the same as discussed in Sec. 3. Of course, this kind of problem can be solved by many state-of-the-art methods (e.g. [9, 8]) invariant to normalization. However this method in turn causes singularities, where a cluster shrinks to the delta distribution, and difficult to learn in high dimensional spaces.

. Fisher score calculated by taking derivatives with respect to  In order to have an initial

1 When the covariance matrix of each cluster is allowed to be different in K-Means, it becomes


Figure 3: (Upperleft) Toy dataset used for clustering. Contours show the estimated density with the mixture of 8 Gaussians. (Upperright) Clustering result of the proposed algorithm. (Lowerleft) Result of K-Means with the Fisher score. (Lowerright) Result of K-Means with the whitened Fisher score.

because it is only two dimensional. However these methods typically do not scale to large dimensional or discrete problems. Standard mixture modeling methods have difficulties in modeling such complicated cluster shapes [9, 10]. One straightforward way is to model special care needs to be taken for such a "mixture of mixtures" problem. When the parameters are jointly optimized in a maximum likelihood process, the solution is not unique. In order to have meaningful results e.g. in our dataset, one has to constrain the parameters such that 8 Gaussians form 2 groups. In the Bayesian framework, this can be done by specifying an appropriate prior distributions on parameters, which can become rather involved. Roberts et. al. [10] tackled this problem by means of the minimum entropy principle using MCMC which is somewhat more complicated than our approach. 5 Clustering Amino Acid Sequences In this section, we will apply our method to cluster bacterial gyrB amino acid sequences, where the hidden markov model (HMM) is used to derive the Fisher score. gyrB - gyrase subunit B - is a DNA topoisomerase (type II) which plays essential roles in fundamental mechanisms of living organisms such as DNA replication, transcription, recombination and repair etc. One more important feature of gyrB is its capability of being an evolutionary and taxonomic marker alternating popular 16S rRNA [17]. Our data set consists of 55 amino acid sequences containing three clusters (9,32,14). The three clusters correspond to three genera of high GC-content gram-positive bacteria, that is, Corynebacteria, Mycobacteria, Rhodococcus, respectively. Each sequence is represented as a sequence of 20 characters, each of which represents an amino acid. The length of each sequence is different from 408 to 442, which makes it difficult to convert a sequence into a vector of fixed dimensionality. In order to evaluate the partitions we use the Adjusted Rand Index (ARI) [4, 18]. Let

¡ x  ¡ 9

each cluster as a Gaussian Mixture:

x

¦©¨8 eg#th e % # h9¢¡9 #  #  #    ¦©¨     @ However,

# #  #¤£ #¥£

 ¡ 

 

and

 C

9 A@B@A@A

C g

be the obtained clusters and ¦ A@A@B@§¦©¨ be the ground truth clusters. Let 9

e GI

% G

be

the number of samples which belongs to both

of samples in

C G

C G and ¦ . Also let



% G£ % %!

and

£



be the number

and ¦ , respectively. ARI is defined as



% G



 

Ye G e %

%"£  



%G£

¦

9 



%"£  %G£ 

%#G 

£  

 £

 

c

 e G 'fe  Y e G e

  

%!

The attractive point of ARI is that it can measure the difference of two partitions even when


0.8

Proposed K-Means

0.6

0.4 ARI

0.2

0 2

3 4 5

Number of HMM States

Figure 4: Adjusted Rand indices of K-Means and the proposed method in a sequence classification experiment.

the number of clusters is different. When the two partitions are exactly the same, ARI is 1, and the expected value of ARI over random partitions is 0 (see [4] for details). In order to derive the Fisher score, we trained complete-connection HMMs via the Baum-

Welch algorithm, where the number of states

emits one of  % state probabilities,

 

%

¡



 

pc

 

is changed from 2 to 5, and each state

characters. This HMM has transition probabilities and

  

initial state probabilities,

 

terminal

for example, a HMM has 75 parameters in total, which is much larger than the

emission probabilities. Thus when

number of potential classes (i.e. 3). The derivative is taken with respect to all paramaters as described in detail in [15]. Notice that we did not perform any normalization to the Fisher score vectors. In order to avoid local minima, we tried 1000 different initial values and chose the one which achieved the minimum loss both in K-means and our method. In KMeans, initial centers are sampled from the uniform distribution in the smallest hypercube

#

which contains all samples. In the proposed method, every

#isis

sampled from the normal

initially set to zero. distribution with mean 0 and standard deviation 0.001. Every

6 G)

Fig. 4 shows the ARIs of two methods against the number of HMM states. Our method shows the highest ARI (0.754) when the number of HMM states is 3, which shows that important dimensions are successfully discovered from the "sea" of nuisance dimensions. In contrast, the ARI of K-Means decreases monotonically as the number of HMM states increases, which shows the K-Means is not robust against nuisance dimensions. But when false clusters which happened to appear in nuisance dimensions. This result suggests that prior dimensionality reduction may be effective (cf.[11]), but it is beyond the scope of this paper. 6 Concluding Remarks In this paper, we illustrated how the class information is encoded in the Fisher score: most information is packed in a few dimensions and there are a lot of nuisance dimensions. Advanced supervised classifiers such as the support vector machine have a built-in feature selector [7], so they can detect important dimensions automatically. However in unsupervised learning, it is not easy to detect important dimensions because of the lack of class labels. We proposed a novel very simple clustering algorithm that can ignore nuisance dimensions and tested it in artificial and real data experiments. An interesting aspect of our gyrB experiment is that the ideal scenario assumed in the theory section is not fulfilled anymore as clusters might overlap. Nevertheless our algorithm is robust in this respect and achieves highly promising results.

the number of nuisance dimensions are too many (i.e.

 C%£¢ ¥¤ ), our method was caught in


The Fisher score derives features using the prior knowledge of the marginal distribution.

In general, it is impossible to infer anything about the conditional distribution

the marginal

¦©¨$

6¨8 from

[12] without any further assumptions. However, when one knows the

 ¦

directions that the marginal distribution can move (i.e. the model of marginal distribution), it is possible to extract information about even though it may be corrupted by many nuisance dimensions. Our method is straightforwardly applicable to the objects to which the Fisher kernel has been applied (e.g. speech signals [13] and documents [16]). Acknowledgement The authors gratefully acknowledge that the bacterial gyrB amino acid sequences are offered by courtesy of Identification and Classification of Bacteria (ICB) database team [17]. KRM thanks for partial support by DFG grant # MU 987/1-1. References [1] S. Amari and H. Nagaoka. Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs. American Mathematical Society, 2001. [2] R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press, 1998. [3] P.J. Huber. Projection pursuit. Annals of Statistics, 13:435­475, 1985. [4] L. Hubert and P. Arabie. Comparing partitions. J. Classif., pages 193­218, 1985. [5] T.S. Jaakkola and D. Haussler. Exploiting generative models in discriminative classifiers. In M.S. Kearns, S.A. Solla, and D.A. Cohn, editors, Advances in Neural Information Processing Systems 11, pages 487­493. MIT Press, 1999. [6] A.K. Jain and R.C. Dubes. Algorithms for Clustering Data. Prentice Hall, 1988. [7] K.-R. M¨uller, S. Mika, G. R¨atsch, K. Tsuda, and B. Sch¨olkopf. An introduction to kernel-based learning algorithms. IEEE Trans. Neural Networks, 12(2):181­201, 2001. [8] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14. MIT Press, 2002. [9] M. Rattray. A model-based distance for clustering. In Proc. IJCNN'00, 2000. [10] S.J. Roberts, C. Holmes, and D. Denison. Minimum entropy data partitioning using reversible jump markov chain monte carlo. IEEE Trans. Patt. Anal. Mach. Intell., 23(8):909­915, 2001. [11] V. Roth, J. Laub, J.M. Buhmann, and K.-R. M¨uller. Going metric: Denoising pairwise data. In NIPS02, 2003. to appear. [12] M. Seeger. Learning with labeled and unlabeled data. Technical report, Institute for Adaptive and Neural Computation, University of Edinburgh, 2001. http://www.dai.ed.ac.uk/homes/seeger/papers/review.ps.gz. [13] N. Smith and M. Gales. Speech recognition using SVMs. In T.G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14. MIT Press, 2002. [14] S. Sonnenburg, G. R¨atsch, A. Jagota, and K.-R. M¨uller. New methods for splice site recognition. In ICANN'02, pages 329­336, 2002. [15] K. Tsuda, M. Kawanabe, G. R¨atsch, S. Sonnenburg, and K.-R. M¨uller. A new discriminative kernel from probabilistic models. Neural Computation, 14(10):2397­2414, 2002. [16] A. Vinokourov and M. Girolami. A probabilistic framework for the hierarchic organization and classification of document collections. Journal of Intelligent Information Systems, 18(2/3):153­ 172, 2002. [17] K. Watanabe, J.S. Nelson, S. Harayama, and H. Kasai. ICB database: the gyrB database for identification and classification of bacteria. Nucleic Acids Res., 29:344­345, 2001. [18] K.Y. Yeung and W.L. Ruzzo. Principal component analysis for clustering gene expression data. Bioinformatics, 17(9):763­774, 2001.

¦!¨8,


