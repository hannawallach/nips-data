Learning Graphical Models with Mercer Kernels

Francis R. Bach Division of Computer Science University of California Berkeley, CA 94720 fbach@cs.berkeley.edu Michael I. Jordan Computer Science and Statistics University of California Berkeley, CA 94720 jordan@cs.berkeley.edu

Abstract We present a class of algorithms for learning the structure of graphical models from data. The algorithms are based on a measure known as the kernel generalized variance (KGV), which essentially allows us to treat all variables on an equal footing as Gaussians in a feature space obtained from Mercer kernels. Thus we are able to learn hybrid graphs involving discrete and continuous variables of arbitrary type. We explore the computational properties of our approach, showing how to use the kernel trick to compute the relevant statistics in linear time. We illustrate our framework with experiments involving discrete and continuous data. 1 Introduction Graphical models are a compact and efficient way of representing a joint probability distribution of a set of variables. In recent years, there has been a growing interest in learning the structure of graphical models directly from data, either in the directed case [1, 2, 3, 4] or the undirected case [5]. Current algorithms deal reasonably well with models involving discrete variables or Gaussian variables having only limited interaction with discrete neighbors. However, applications to general hybrid graphs and to domains with general continuous variables are few, and are generally based on discretization. In this paper, we present a general framework that can be applied to any type of variable. We make use of a relationship between kernel-based measures of "generalized variance" in a feature space, and quantities such as mutual information and pairwise independence in

the input space. In particular, suppose that each variable

high-dimensional space

variables ¥ ¡

¡

via a map ¤ . Let ¥ ¡

£

¡§¦ ¡©¨ ¡ ¢¡ ¤

in our domain is mapped into a and consider the set of random

in feature space. Suppose that we compute the mean and covariance matrix

of these variables and consider a set of Gaussian variables, ¥¢ , that have the same mean and covariance. We showed in [6] that a canonical correlation analysis of ¥ yields a measure, known as "kernel generalized variance," that characterizes pairwise independence the original variables. This link led to a new set of algorithms for independent component analysis. In the current paper we pursue this idea in a different direction, considering the use of the kernel generalized variance as a surrogate for the mutual information in model selection problems. Effectively, we map data into a feature space via a set of Mercer kernels, with different kernels for different data types, and treat all data on an equal footing

¡ 

among the original variables    ¡  , and is closely related to the mutual information among

¡ 


as Gaussian in feature space. We briefly review the structure-learning problem in Section 2, and in Section 4 and Section 5 we show how classical approaches to the problem, based on MDL/BIC and conditional independence tests, can be extended to our kernel-based approach. In Section 3 we show that by making use of the "kernel trick" we are able to compute the sample covariance matrix in feature space in linear time in the number of samples. Section 6 presents experimental results. 2 Learning graphical models Structure learning algorithms generally use one of two equivalent interpretations of graphical models [7]: the compact factorization of the joint probability distribution function leads to local search algorithms while conditional independence relationships suggest methods based on conditional independence tests. Local search. In this approach, structure learning is explicitly cast as a model selection problem. For directed graphical models, in the MDL/BIC setting of [2], the likelihood is decomposed

¢¤£¦¥¨§

penalized by a model selection term that is equal to ¡  times the number of param-

eters necessary to encode the local distributions. The likelihood term can be§('

and expressed as follows:

!

where

¡

¦ ¨ ¡ "!

©

¡

© , with ©



is the set of parents of node in the graph to be scored and

¨#$"!¡  ¦&%¨ 

'

and the vector

 

0 2

¨  ¡)  1032 

¡   

 0 2

,

empirical mutual information between the variable

  ¡

is the

. These mutual in-

formation terms and the number of parameters for each local conditional distributions are easily computable in discrete models, as well as in Gaussian models. Alternatively, in a full Bayesian framework, under assumptions about parameter independence, parameter modularity, and prior distributions (Dirichlet for discrete networks, inverse Wishart for Gaussian networks), the log-posterior probability of a graph given the data can be decomposed in a similar way [1, 3]. Given that our approach is based on the assumption of Gaussianity in feature space, we could base our development on either the MDL/BIC approach or the full Bayesian approach. In this paper, we extend the MDL/BIC approach, as detailed in Section 4. Conditional independence tests. In this approach, conditional independence tests are performed to constrain the structure of possible graphs. For undirected models, going from the graph to the set of conditional independences is relatively easy: there is an edge In Section 5, we show how our approach could be used to perform independence tests and learn an undirected graphical model. We also show how this approach can be used to prune the search space for the local search of a directed model. 3 Gaussians in feature space In this section, we introduce our Gaussianity assumption and show how to approximate the mutual information, as required for the structure learning algorithms. 3.1 Mercer Kernels

between   ¡ and  54 if and only if   ¡ and  64 are independent given all other variables [7].

A Mercer kernel on a space

points    CBDBCBC 1E   

  in 7

¨ 9)@ 

I

¡

7 is a function

§GFH§

8 from 7 to A for any set of

, the

¨ 9)@ 

matrix , defined by

¡ ¦such¨ that   4 ¡)

8

4

I

8

¨  P)@ 

8

is usually referred to as the Gram matrix of the points 

£ , it is possible to find a space

£ and a map ¤ from

¨#@ 

7

, is positive ¡

  

£

to

. ,

semidefinite. The matrix

Given a Mercer kernel

such that £

I

is the dot product in

between ¤ ¨    and ¤

(see, e.g., [8]). The space

is usually referred to as the feature space and the map ¤ as the feature map. We will use


the notation  ¢¡¤£ to denote the dot product of   and £ in feature space

notation  ¢¡ to denote the representative of   in the dual space of

For a discrete variable which takes values in ¦¥

©¢

¨

© 



CBDBCB 

¨§ 

£ . £

. We also use the

8

, which corresponds to a feature space of dimension . The feature map is ¤

CBDBCBD

© 



, we use the trivial kernel §

¨ 9)@¨  ¦    ¦

 

. Note that this mapping corresponds to the usual embedding of a multi

nomial variable of order

§

in the vector space A .

8

¨ P"@  !" ¦  $#&%(' % 0)¡

For continuous variables, we use the Gaussian kernel . The feature

space has infinite dimension, but as we will show, the data only occupy a small linear manifold and this linear subspace can be determined adaptively in linear time. Note that

an alternative is to use the kernel data as Gaussian in input space. 3.2 Notation

Let   DBCBCBD 21 

¨  P)@  ¦   @

8 , which corresponds to simply modeling the

 

a Mercer kernel

be 3 random variables with values in spaces

¡ ¡

8

to each of the input spaces

7

DBCBDBC 7

 

£

7

1

, with feature space

CBCBDB 

¥  ¤

1¡ 54 ¨ ¨   and

CBDBCBD

4    

¡ . Let us assign feature map

¤

1 ¨ 61  

¤ . The random vector of feature images ¥

¡ ¦ ¨ ¥

 

has a covariance matrix 7 defined by blocks, with block 7

between ¥ ¡ ¦ ¡ ¨  ¡  ¤ and ¥

4

¦

¤

4 54

¨   

. Let ¥  ¦ ¨

vector with the same mean and covariance as ¥

¦ ¨

¥ 

¥ 

1being  the covariance matrix

CBDBCB  DBCBDB 

¥  denoteajointlyGaussian ¥ . The vector ¥  will be 1 

 

used as the random vector on which the learning of graphical model structure is based. herently pairwise. No dependency involving strictly more than two variables is modeled explicitly, which makes our scoring metric easy to compute. In Section 6, we present empirical evidence that good models can be learned using only pairwise information. 3.3 Computing sample covariances using kernel trick

Note that the sufficient statistics for this vector are  ¤ ¡ ¨  ¡  ¡ ¨  ¡  ¨     ¤ ¤ 4 54 ¡ , and are in-

We are given a random sample    CBDBCB  1E   

F BCBCB F 1

 § of elements of 7 7

¦ ¡ ¨  ¡  ¤

. By mapping

3 elements ¢8¡



¥

have been centered, i.e.,

¡4 ¦

 E

 ¡ ¨ 

4

E

 

8 . We assume that for each

 E

¥ 8

¡ 9 ¦

 

into the feature spaces, we define

the data in feature space  ¥ ¡ 

CBCBDBC



. The sample

¡

¥

¡E

8

A8¥ A8¥ ¡ . NotethataGaussianwith

8

covariance matrix 7 @ is then equal to 7 @

4  

covariance matrix 7 @ has zero variance along directions that are orthogonal to the images of the data. Consequently, in order to compute the mutual information, we only need to compute the covariance matrix of the projection of ¥ onto the linear span of the data, that

is, for all   CB ED 0F ¦ :

¨ ¡  ¡

 

4 4

¦ § ¥ I E

 ¨ ¡  ¡ ¨  AG¥ ¡ ¥ 8 ¥ 8 ¡ 6H¥ 4 4 ¦ § ¥ I E AG¥ ¡ 7 @ 6H¥

© where

G

 ¨ ¡  ¨  ¦ I G 8 I 4 H 8 § ¥ ©

§

8 F 

¥ vectorswithonlyzerosexceptatpositionD ,andI istheGram

¡



8

G¡ I ¡ 4 © 

I

H (1)

 

denotes the

¡ see that the sample covariance matrix of ¥ in the "data basis" has blocks

matrix of the centered points, the so-called centered Gram matrix of the -th component,

defined from the Gram matrix P ofF(§the original (non-centered) points as

' %  §

, where is a

¡ ¦ ¨ ' %

I

matrix composed of ones [8]. From Eq. (1), we

¡

4 .

E   I I

E

RQ    ¡ ¨ P RQ  

E

Q

3.4 Regularization When the feature space has infinite dimension (as in the case of a Gaussian kernel on ),

A

then the covariance we are implicitly fitting with a kernel method has an infinite number of parameters. In order to avoid overfitting and control the capacity of our models, we


regularize by smoothing the Gaussian ¥¢ by another Gaussian with small variance (for

 

be a small constant. We

add to ¥  an isotropic Gaussian with covariance

an alternative interpretation and further details, see' [6]). Let

 

¡

in an orthonormal basis. In the data

basis, the covariance of this Gaussian is exactly the block diagonal matrix with blocks

¤£ ¦ B

1I

  § '



¡ . Consequently, our regularized Gaussian covariance 7 has blocks 7

¢

¡ ¡ ¦

  I

¤¥ ¡¡

 

1I

  ¡

¡ ¡

 ¦ ¡

and 7

¥

  I

¡ ¡

 

1I Since

¡

£¦, B

 

¡4 ¦ ¢

¡ §¦ ¡

¡

4

E I



is a small constant, we can use 7

  I E

 

¨ ¡ ¥if

I

¡ ¢ ¢

, with blocks

¡ ¡

E

E

¡ ¦ 4

¥©¨ ¨.  

4

for



which leads to a more compact correlation matrix

' § '

, and

¡ ¡ ¦

, where

§

¡ ¦ ¡ ¨ ¡

I

¥ E

I

 

  .

 

These cross-correlation matrices have exact dimension

¡

I

§ ¡  ¨ ¡ ¨ ¡ ¥ ¦

, but since the eigenvalues of

are softly thresholded to zero or one by the regularization, the effective dimension is

§ E 

 

§ ¡

I I

 

'

. This dimensionality will be used as the dimension of our

Gaussian variables for the MDL/BIC criterion, in Section 4. 3.5 Efficient implementation Direct manipulation of matrices would lead to algorithms that scale as

§ F §

 ¨ ¨  §

Gram matrices, however, are known to be well approximated by matrices of low rank

The approximation is exact when the feature space has finite dimension

crete kernels), and



§



. .

(e.g., with dis-

can be chosen less than . In the case of continuous data with the §

Gaussian kernel,§ we have shown that

independent of



can be chosen to be upper bounded by a constant

[6]. Finding a low-rank decomposition can thus be done through incom-

§

plete Cholesky decomposition in linear time in see [6]).

(for a detailed treatment of this issue,

Using the incomplete Cholesky decomposition, for each matrix

§ F

ization I

¡ , where

is an



matrix with rank

§ F

to obtainF an perform a singular value decomposition' of

onal columns (i.e., such that

¡

I



! " ! ¡

¡ ¦ ¨ ¡ ¥

I

.

§ '

#¦$ %¡ ¡¡ ¦ ¡ ¡ ¡

! ¡ !

E   I

¡ ¡ ¦

¡ ¦

¡¡



¡ ¦© ¡  ¡  ¡ I

¡

), and an

'&¡ ¡ ¡ ! ! ¡

  ¡ ¡

¡¡

we obtain the§ factor-

, where

matrix

!

 ¡

§¡

diagonal matrix

& ¡

"withsuch ¡

. We orthog-

that

We have

 

elements. Thus ¥  has a correlation matrix with blocks

basis defined by the columns of the matrices the various mutual information terms. 3.6 KGV-mutual information

! ¡

¡

"

¡

, where where is the diagonal matrix

§



¡ 5& ¡ ¦

(0)12(43¡ (&

! ¡ !

4

¨ ¥

obtained from the diagonal matrix by applying the function

4 4

 



to its

in the new

, and these blocks will be used to compute

We now show how to compute the mutual information between ¥

a link with the mutual information of the original variables

Let

@ CBDBCB "@

1

  DBCBDB  

 

CBDBCBD

21 

.

¥  ,andwemake 1

@

  CBCBDBC"@

be 3 jointly Gaussian random vectors with covariance matrix @9

£

in terms of blocks

1

6

¡ 87 ¨#@ ¡  4 ¦ "@34

6

, defined

. The mutual information between the variables

is equal to (see, e.g., [9]):

' ¨@

CBDBCB "@

1  ¦

 

%

 

¡¥ ¢¤£¦¥

where

AGHA

denotes the determinant of the matrix

A6 A AG6 ACBDBEBFA6 A

 $ 

. The ratio of determinants in this ex-

1 1  (2)

pression is usually referred to as the generalized variance, and is independent of the basis which is chosen to compute .

6

Following Eq. (2), the mutual information between ¥  the distribution of , is equal to  

PI

' ¨  

CBCBDBC

61   ¦ % ¡¥ ¢¤£¦¥

 

CBDBCBC

¥  , which depends solely on 1

1 1 B (3)

A A A ACBDBEBDA A

 " 


We refer to this quantity as the I

¡  -mutual information (KGV stands for kernel gen-

eralized variance). It is always nonnegative and can also be defined for partitions of the variables into subsets, by simply partitioning the correlation matrix accordingly. The KGV has an interesting relationship to the mutual information among the original the KGV is equal to the mutual information up to second order, when expanding around the

variables,   DBCBDB    1 . In particular, as shown in [6], in the case of two discrete variables,

 

manifold of distributions that factorize in the trivial graphical model (i.e. with independent Gaussian kernel tend to zero, the KGV necessarily tends to a limit, and also provides a second-order expansion of the mutual information around independence. This suggests that the KGV-mutual information might also provide a useful, computationally-tractable surrogate for the mutual information more generally, and in particular substitute for mutual information terms in objective functions for model selection, where even a rough approximation might suffice to rank models. In the remainder of the paper, we investigate this possibility empirically. 4 Structure learning using local search components). Moreover, in the case of continuous variables, when the width ¢ of the

In this approach, an objective function

directed graphical model

  ¤£ )1

© A measures the goodness of fit of the

, and is minimized. The MDL/BIC objective function for our





Gaussian variables is easily derived. Let

We have

 # )!

© © , with

§

¡ ¢ £ ¥

¨ ¡  ¦ )!

# §

"

4



A

!

%¨  ¦ ¡ ¨ ¡ 

© 0 2

¡ ¦ ¡ %¨  !

¨§©¡ ¥ ¨§©¡  ¡ ¡ 0 2 0 2 0 2 0 2 AA A

be the set of parents of node in

¥ § § ¡ 032

¡ %¨ 

¢¤£¦¥ § 



.

§032 ¦ 

¦¥ A A

where  4

. Given the scoring metric ©

(4)

hard optimization problem on the space of directed acyclic graphs [10]. Because the score decomposes as a sum of local scores, local greedy search heuristics are usually exploited. We adopt such heuristics in our simulations, using hillclimbing. It is also possible to use Markov-chain Monte Carlo (MCMC) techniques to sample from the posterior distribution defined by within our framework; this would in principle allow us to output several high-scoring networks. 5 Conditional independence tests using KGV In this section, we indicate how conditional independence tests can be performed using the KGV, and show how these tests can be used to estimate Markov blankets of nodes. Likelihood ratio criterion. In the case of marginal independence, the likelihood ratio criterion is exactly equal to a power of the mutual information (see, e.g, [11] in the case of Gaussian variables). This generalizes easily to conditional independence, where the

'¨ & ¤!#" ¨ '¨    A  )% ©

likelihood' ratio criterion to test'the conditional independence of % § P)@ 

$

'

$

§

, where

!#"&% ¨ ¨   % ¨  P"@ % ¨ P  ('

@

and

$

given

 

is equal to

is the number of samples and the mutual

information terms are computed using empirical distributions. Applied to our Gaussian variables ¥  , we obtain% a' test statistic' based on. linear combination old values exist for conditional independence tests with Gaussian variables [7], but instead,

'

I I I of KGV-mutual information terms: ¨  P)@   $ ¨  P)@  % ¨  P  $

Theoretical thresh-



we prefer to use the value given by the MDL/BIC criterion, i.e.,

§ 3

0)12

¡ 

E

E

 § 43§

(where

§

and

are the dimensions of the Gaussians), so that the same decision regarding conditional

, we are faced with an NP-

independence is made in the two approaches (scoring metric or independence tests) [12]. Markov blankets. For Gaussian variables, it is well-known that some conditional independencies can be read out from the inverse of the joint covariance matrix [7]. More precisely,


If

@ CBDBCBC"@

1

6

are 3 jointly Gaussian random vectors with dimensions

@9 £ defined in terms of blocks

6 ¡ 07 ¨#@ ¡¨ 

4

¦ )@34

CB 

§ ¡

¡

, and with covari-

ance  matrix , then

of I

@

¦

and

6

given all the other variables if and only if the block

@34  

are independent is equal to zero.

Thus in the sample case, we can read out the edges of2 the undirected model directly2 from 2

¡I £¥£ ¡

2 2

using the test statistic

 ¡ 4 ¦ %

¢ £ ¥

¡  with the threshold value ¡

)12

E

@

Applied to the variables

¡ ¦

¢¡¤£¥£ I I ¤£I §¦©¨ I £ ¡

¥¡ and for all pairs of nodes, we can find an undirected

 

£

I

E

, .

graphical model in polynomial time, and thus a set of Markov blankets [4]. We may also be interested in constructing a directed model from the Markov blankets; however, this transformation is not always possible [7]. Consequently, most approaches use heuristics to define a directed model from a set of conditional independencies [4, 13]. Alternatively, as a pruning step in learning a directed graphical model, the Markov blanket can be safely used by only considering directed models whose moral graph is covered by the undirected graph. 6 Experiments We compare the performance of three hillclimbing algorithms for directed graphical mod-

els, one using the KGV metric (with

 

¦ 96B9

¥ and ¢

¦

¥ ),oneusingtheMDL/BICmetric

§



of [2] and one using the BDe metric of [1] (with equivalent prior sample size

¦ ¥ ).

When the domain includes continuous variables, we used two discretization strategies; the first one is to use K-means with a given number of clusters, the second one uses the adaptive discretization scheme for the MDL/BIC scoring metric of [14]. Also, to parameterize the local conditional probabilities we used mixture models (mixture of Gaussians, mixture of softmax regressions, mixture of linear regressions), which provide enough flexibility at reasonable cost. These models were fitted using penalized maximum likelihood, and invoking the EM algorithm whenever necessary. The number of mixture components was less than four and determined using the minimum description length (MDL) principle. When the true generating network is known, we measure the performance of algorithms by the KL divergence to the true distribution; otherwise, we report log-likelihood on held-out test data. We use as a baseline the log-likelihood for the maximum likelihood solution to a model with independent components and multinomial or Gaussian densities as appropriate (i.e., for discrete and continuous variables respectively). Toy examples. We tested all three algorithms on a very simple generative model on 3

binary nodes, where nodes ¥ through 3

3

%

¥ parents, we set 

¨ 21 ¦

%

¥ A@ by sampling uniformly at random in



¥ pointtonode 3 . Foreachassignment@ ofthe %9 '

6

¥ . We also

studied a linear Gaussian generative model with the identical§ topology, with regression We report average results (over 20 replications) in Figure 1 (left), for 3 ranging from ¥ to ¥ . We see that on the discrete networks, the performance of all three algorithms is 9 similar, degrading slightly as 3 increases. On the linear networks, on the other hand, the discretization methods degrade significantly as 3 increases. The KGV approach is the only approach of the three capable of discovering these simple dependencies in both kinds of networks. Discrete networks. We used three networks commonly used as benchmarks1, the ALARM network (37 variables), the INSURANCE network (27 variables) and the HAILFINDER network (56 variables). We tested various numbers of samples . We performed 40 replications and report average results in Figure 1 (right). We see that the performance of our metric lies between the (approximate Bayesian) BIC metric and the (full Bayesian) BDe

weights chosen uniformly at random in % ¥ ¥ . We generated 

¦ 9 9 9 ¥ samples.

§

1 Available at http://www.cs.huji.ac.il/labs/compbio/Repository/.

% '


1

Network ALARM N (  

0.5

0 1 INSURANCE 2 4 6 8 10 m

0.5 HAILFINDER

0

¡£¢¥¤ 0.5 1 4 16 0.5 1 4 16 0.5 1 4 16 ) BIC 0.85 0.42 0.17 0.04 1.84 0.93 0.27 0.05 2.98 1.70 0.63 0.25 BDe 0.47 0.25 0.07 0.02 0.92 0.52 0.15 0.04 2.29 1.32 0.48 0.17 KGV 0.66 0.39 0.15 0.06 1.53 0.83 0.40 0.19 2.99 1.77 0.63 0.32

2 4 6 8 10 m

Figure 1: (Top left) KL divergence vs. size of discrete network 3 : KGV (plain), BDe (dashed), MDL/BIC (dotted). (Bottom left) KL divergence vs. size of linear Gaussian network: KGV (plain), BDe with discretized data (dashed), MDL/BIC with discretized data (dotted x), MDL/BIC with adaptive discretization (dotted +). (Right) KL divergence for discrete network benchmarks.

Network ABALONE VEHICLE PIMA AUSTRALIAN BREAST BALANCE HOUSING CARS1 CLEVE HEART N 4175 846 768 690 683 625 506 392 296 270 D 1 1 1 9 1 1 1 1 8 9 C 8 18 8 6 10 4 13 7 6 5 d-5 10.68 21.92 3.18 5.26 15.00 1.97 14.71 6.93 2.66 1.34 d-10 10.53 21.12 3.14 5.11 15.03 2.03 14.25 6.58 2.57 1.36 KGV 11.16 22.71 3.30 5.40 15.04 1.88 14.16 6.85 2.68 1.32

§

Table 1: Performance for hybrid networks. is the number of samples, and

&

and 7

are the number of discrete and continuous variables, respectively. The best performance in each row is indicated in bold font. metric. Thus the performance of the new metric appears to be competitive with standard metrics for discrete data, providing some assurance that even in this case pairwise sufficient statistics in feature space seem to provide a reasonable characterization of Bayesian network structure. Hybrid networks. It is the case of hybrid discrete/continuous networks that is our principal interest--in this case the KGV metric can be applied directly, without discretization of the continuous variables. We investigated performance on several hybrid datasets from the UCI machine learning repository, dividing them into two subsets, 4/5 for training and 1/5 for testing. We also log-transformed all continuous variables that represent rates or counts. We report average results (over 10 replications) in Table 1 for the KGV metric and for the BDe metric--continuous variables are discretized using K-means with 5 clusters (d-5) or 10 clusters (d-10). We see that although the BDe methods perform well in some problems, their performance overall is not as consistent as that of the KGV metric. 7 Conclusion We have presented a general method for learning the structure of graphical models, based on treating variables as Gaussians in a high-dimensional feature space. The method seamlessly integrates discrete and continuous variables in a unified framework, and can provide


improvements in performance when compared to approaches based on discretization of continuous variables. The method also has appealing computational properties; in particular, the Gaussianity assumption enables us make only a single pass over the data in order to compute the pairwise sufficient statistics. The Gaussianity assumption also provides a direct way to approximate Markov blankets for undirected graphical models, based on the classical link between conditional independence and zeros in the precision matrix. While the use of the KGV as a scoring metric is inspired by the relationship between the KGV and the mutual information, it must be emphasized that this relationship is a local one, based on an expansion of the mutual information around independence. While our empirical results suggest that the KGV is also an effective surrogate for the mutual information more generally, further theoretical work is needed to provide a deeper understanding of the KGV in models that are far from independence. Finally, our algorithms have free parameters, in particular the regularization parameter and the width of the Gaussian kernel for continuous variables. Although the performance is empirically robust to the setting of these parameters, learning those parameters from data would not only provide better and more consistent performance, but it would also provide a principled way to learn graphical models with local structure [15]. Acknowledgments The simulations were performed using Kevin Murphy's Bayes Net Toolbox for MATLAB. We would like to acknowledge support from NSF grant IIS-9988642, ONR MURI N0001400-1-0637 and a grant from Intel Corporation. References [1] D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20(3):197­243, 1995. [2] W. Lam and F. Bacchus. Learning Bayesian belief networks: An approach based on the MDL principle. Computational Intelligence, 10(4):269­293, 1994. [3] D. Geiger and D. Heckerman. Learning Gaussian networks. In Proc. UAI, 1994. [4] J. Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, 2000. [5] S. Della Pietra, V. J. Della Pietra, and J. D. Lafferty. Inducing features of random fields. IEEE Trans. PAMI, 19(4):380­393, 1997. [6] F. R. Bach and M. I. Jordan. Kernel independent component analysis. Journal of Machine Learning Research, 3:1­48, 2002. [7] S. L. Lauritzen. Graphical Models. Clarendon Press, 1996. [8] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT Press, 2001. [9] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley & Sons, 1991. [10] D. M. Chickering. Learning Bayesian networks is NP-complete. In Learning from Data: Artificial Intelligence and Statistics 5. Springer-Verlag, 1996. [11] T. W. Anderson. An Introduction to Multivariate Statistical Analysis. Wiley & Sons, 1984. [12] R. G. Cowell. Conditions under which conditional independence and scoring methods lead to identical selection of Bayesian network models. In Proc. UAI, 2001. [13] D. Margaritis and S. Thrun. Bayesian network induction via local neighborhoods. In Adv. NIPS 12, 2000. [14] N. Friedman and M. Goldszmidt. Discretizing continuous attributes while learning Bayesian networks. In Proc. ICML, 1996. [15] N. Friedman and M. Goldszmidt. Learning Bayesian networks with local structure. In Learning in Graphical Models. MIT Press, 1998.


