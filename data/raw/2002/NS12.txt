Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution

 

Matthias Bethge, David Rotermund, and Klaus Pawelzik Institute of Theoretical Physics University of Bremen 28334 Bremen mbethge,davrot,pawelzik @physik.uni-bremen.de

¢

¡

Abstract Here we derive optimal gain functions for minimum mean square reconstruction from neural rate responses subjected to Poisson noise. The shape of these functions strongly depends on the length of the time window within which spikes are counted in order to estimate the underlying firing rate. A phase transition towards pure binary encoding occurs if the maximum mean spike count becomes smaller than approximately three provided the minimum firing rate is zero. For a particular function class, we were able to prove the existence of a second-order phase transition analytically. The critical decoding time window length obtained from the analytical derivation is in precise agreement with the numerical results. We conclude that under most circumstances relevant to information processing in the brain, rate coding can be better ascribed to a binary (low-entropy) code than to the other extreme of rich analog coding.

£

1 Optimal neuronal gain functions for short decoding time windows The use of action potentials (spikes) as a means of communication is the striking feature of neurons in the central nervous system. Since the discovery by Adrian [1] that action potentials are generated by sensory neurons with a frequency that is substantially determined by the stimulus, the idea of rate coding has become a prevalent paradigm in neuroscience [2]. In particular, today the coding properties of many neurons from various areas in the cortex have been characterized by tuning curves, which describe the average firing rate response as a function of certain stimulus parameters. This way of description is closely related to the idea of analog coding, which constitutes the basis for many neural network models. Reliable inference from the observed number of spikes about the underlying firing rate of a neuronal response, however, requires a sufficiently long time interval, while integration times of neurons in vivo [3] as well as reaction times of humans or animals when performing classification tasks [4, 5] are known to be rather short. Therefore, it is important to understand, how neural rate coding is affected by a limited time window available for decoding. While rate codes are usually characterized by tuning functions relating the intensity of the

¤

http://www.neuro.uni-bremen.de/~mbethge


neuronal response to a particular stimulus parameter, the question, how relevant the idea of analog coding actually is does not depend on the particular entity represented by a neuron. Instead it suffices to determine the shape of the gain function, which displays the mean firing rate as a function of the actual analog signal to be sent to subsequent neurons. Here we seek for optimal gain functions that minimize the minimum average squared reconstruction error for a uniform source signal transmitted through a Poisson channel as a function of the maximum mean number of spikes. In formal terms, the issue is to optimally encode a real random variable   in the number of pulses emitted by a neuron within a certain time window. Thereby,   stands for the intended analog output of the neuron that shall be signaled to subsequent neurons. The

¡

latter, however, can only observe a number of spikes

length

. The statistical dependency between   and

¡ integrated within a time interval of is specified by the assumption of

£

Poisson noise

¢¤£ £ ¡¦¥§  ©¨¨

§ £ £¡ ¨¨

! #" § ¡ £  ©¨ $

¢

and the choice of the gain function %  ©¨ , which together with §

count £

£

 ©¨ &%  ©¨ . Animportantadditionalconstraintisthelimitedoutputrangeofthe

£

£

£

(1) determines the mean spike

neuronal firing rate, which can be included by the requirement of a bounded gain function (%('0)21435%  ©¨637%('98A@0$!B&  ). Since inhibition can reliably prevent a neuron from firing, £ we will here consider the case %C'0)21DFE only. Instead of specifying %G'98A@ , we impose meaningful constraint only in conjunction with a fixed time window length . As objective function we consider the minimum mean squared error (MMSE) with respect

§ a bound directly on the mean spike count (i.e. £  ¨H3PI§ ), because % '98A@ constitutes a

£

to Lebesgue measure for  HQSRET$VUW , R§  ¨`W¦DabR  W abR`c  W dU

X¤Y £ Y " Y ¢v£ £ ¡¦¥§ ¢¤£ £ ¡¦¥§  ¨¨!w# yx  ©¨¨!w#  Y

£ ¡ ¥¡

"feg Ahpi qsrut ir t   i

$ (2)

where   c ¨abR  W denotes the mean square estimator, which is the conditional expectation (see e.g. [6]). 1.1 Tunings and errors As derived in [7] on the basis of Fisher information the optimal gain function for a single neuron in the asymptotic limit D has a parabolic shape:

£

%

8' £

 ¨%G'08@( 

Y

(3)

For any finite I , however, this gain function is not necessarily optimal, and in the limit § DE ,itisstraightforwardtoshowthattheoptimaltuningcurveisastepfunction

£

%

` £

  ¨% '98A@  

¥ £ " 

¨$

where 

£d

¨ denotes the Heaviside function that equals one, if £

§I ¨ of the step tuning curve depends on I and can be determined

 §I £ ¨uhU "

d "4i jCk#lnmop q "fk lnmo

£ U U

¨ (5)

dfe§ E and zero if d4g (4) E .

The optimal threshold analytically

as well as the corresponding MMSE [8]:

XvY ` R% W U UsrSt U "

R U £ "  §I £ £ ¨¨ U

£ d  §I Y"fk#lumo ¨ l " t 

¨`W UTv (6)


w±

1 0.5 0

-1

10

-2

10

-3

10 0.1

2 

deviation

rel. 0

-1 0

10 1 2 10

and

 §I p £ ¨

10 - 10 µ

Figure 1: The upper panel shows a bifurcation plot for

optimal gain function in

t ¢

as a function of I illustrating the phase transition from binary

§

 §I " £ ¨ ¡   

of the

to continuous encoding. The dotted line separates the regions before and after the phase Eq. 4+5 is optimal. The middle panel shows the MMSE of this step function (dashed) and

transition in all three panels. Left from this line (i.e. for I § g ) the step function given by

¢

of the optimal gain function in

¢ ¢

the phase transition. The relative deviation between the minimal errors of £ X Y X Y X Y

¥§¦ ¥©¨ ¥©¨)

" ¨ is displayed in the lower panel and has a maximum below E E .

Y (solid), which becomes smaller than the first oneY after

t and  §d (i.e.

¤£§I

§ §

there has to be a transition from discrete to analog encoding with increasing I . UnfortuThe binary shape for small I and the continuous parabolic shape for large I implies that § nately it is not possible to determine the optimal gain function within the set of all bounded

¥

functions

 % %

¡

RET$ U Wu RET$ % '08@ W and hence, one has to choose a certain param-

 

¢

in advance that is feasible for the optimization. In [8], we

¢

eterized function space

investigated various such function spaces and for I

§

g 

r , we did not find any gain func-

tion with an error smaller than the MMSE of the step function. Furthermore, we always only slightly on the function space. As one can see in Fig. 1 (upper) I is approximately observed a phase transition from binary to analog encoding at a critical I that depends three.

!£§

§ £


In this paper, we consider two function classes

¢ ¢ t

$ Y , which bothcontainthe binarygain

function as well as the asymptotic optimal parabolic function as special cases. Furthermore

t

is a proper subset of

Y . Ourinterestin

t

results from the fact that we can analyze the

¢

phase transition in this subset analytically, while

Y is the most general parameterization

¢ ¢ ¢

for which we have determined the optimal encoding numerically. The latter has six free parameters 3 3 RET$VUW , %G'0) Q ET$ %('98A@C¨ , $ Q RET$A ¨ andthe parameterization £

of the gain functions is given by

¡   £¢6Q

¥¤ ¦ ¨§

ll x 8 '98A@ % '0) ¨ @ 8

"

 ¥¤¥¤ £%  E q

% '0) % '0) %('98A@

p

$ E  

 ¡

g

g

g g

%   y$ $ G$ %('0) C$ $ ¤¨

¥ ¨ ¥ £ ¥¤ £ll  !x q

@

$   g

 ¡

  ©¢ ¤ ¦ ©§ ¡  $

¢¥©¨ g

The integrals entering Eq. 2 for the MMSE in case of the gain function %

" t ¡ "

7¡

  g

g

¢

$   U

(7)

then read

¢¤£ ¡¦¥ ©¨!w#  ¡U

   !¨¦)

£ "

£ ¢ " ¡

§ D£

¨i

0)Y

%('98A@ %('98A@ % '98A@ 

i D

% '0)

¡ "

$#¡  &%r

Y

('i

p £

p

A1'2¨30465¤ %G') p

 !¨¦0)Yt$A£ i

q

@9

¡ p

1'2¨3046587¥¤ % '0) ¨

Y x

¡ p

  

p

 ii i

i Y

@9Y

2304¥5"'23$B©CiD

¤" ¢

£

k l

i

D"

%('0) s¨

"

¢ ¡%¨D

i

Y

¡¥¤¨¥¤

%('0) s¨ v

Y (8)

ED£ i t

p 7U

r 9

£ ¢ " p

§

¡ D£ ¨i )

q

2¨30465('2"3$B¨CiD %('98A@

¡ p t x

%('0) s¨

¥¤

23$B©CF

 !¨

q

1'2iiA 3046587¡ ¦) ¥¤

¡ p

%('0) t

x

" t ¢¤£ ¡¦¥ ¨!w#   ¡pU

¢

i

0#   §

"



('i)

p £ ¡ "



where

)HR 'S £d

r

timization leads to the minimal MMSE as a function of I as displayed in Fig. 1 (middle).

The parameterization of the gain functions in

t

is given by

¨ 

R&T(U S 

£ l k l t

p t

 9

2G3$465('2"3HB¨CiD

% '08@

(9)

p

¤

% ') ¨

p £ U " ¢ ¨% '08@  k l £

23$B¨CIPQ $

w denotes the truncated§Gamma function. Numerical op-

¢

T !WYXYda`cb0d

l m o ¥§¦   £ ¥ E GV q @ $ E    §I "  §I p £

£ ¨

¨

g g £

  

$

 §I " ¨  

% with

  $ ¦¨

 

%('98A@ % '98A@ Eex $

g g £

g g  

 

U

 §I p ¨  

$ (10)

Q RE $ UW and SQ RE $  ¨ . The integrals enteringEq. 2 forthe MMSE in case of the

V


gain function %

" t

 

¥ ¦

¢¤£

read ¡¦¥ ©¨!w# 

 ¡U

r

 

  §I " £ £

¨

r

 §I " £ £ ¨

"  §I £ £



¨

  ¨ % ('i p

i



p

  ¨ )

q k l

p U p  

¨

% '98A@ Y

% '98A@ 

1'2i 3$B¨C

q ¡ p

  Y ) q Y

('2i£¡ i3HB¨C%

¡ p

V

'08@ ¨ Y

eY x

t

V

r

" t ¡¦¥ ©¨!w#   ¡pU

£ U

 §I " £ £ ¨  

 

% ('i p r

 

¢¤£

e 23$B©C )

x

i  (11)

¢

q

k l

('2iV 3HB¨Ci 

%('98A@

23HB¨C¤£

¡ p t

i

 e x ¨

p "  §I " £ ¨

¨% '98A@ 

The minimal MMSE for these gain functions is only slightly worse than that for

¢

(12) Y . The

relative difference between both is plotted in Fig. 1 (lower) showing a maximum deviation

of

r¦¥

. In particular, the relative deviation is extremely small around the phase transition.

¢

This comparison suggests that a restriction to

t

, which is a necessary simplification for

d

the following analytical investigation, does not change the qualitative results. 2 A phase transition The phase transition from binary to analog encoding corresponds to a structural change of that with increasing I this minimum changes into a local maximum at a certain critical point I  I . Therefore, the critical point can be determined by a local expansion of

  the objective function

implies that

X Y £   X Y £ GV$ ¨V$ p¨ . Inparticular,theoptimalityofbinaryencodingfor§I

¦¨ hasaminimumat

  E . Theexistenceofaphasetransitionimplies g ¤£§I

§ §

£ §

 

$ I ¨ ¡ §  X Y £

 

¨V$ v$ I ¨ § " X Y £ ET$ u$#I§ ¨ GV

g e

t¨§  £© (13)

around

  Ah

DE ,becausethesignofitsleadingcoefficient I ¨ (i.e.X thecoefficient

that does not vanish identically) determines, whether

  e Y £ GV$

£ §

¡

minimal

minimum or maximum at

 £

e

¨  E . §I

E . Accordingly,thecriticalpointis givenas thesolutionof

 

  with u$CI§ ¨ has§a local

With quite a bit of efforts one can prove that the first derivative of §

X§ Y £

all I . The second derivative, however, is a decreasing function of I and hence constitutes the wanted leading coefficient



e £ §I ¨  q k!mo "

£ U U  " i

p

p q

U j "(k om

7r 7

p

k om k om Y p

j#k#lumY Us¨

o

p

U

GV$ § u$ I ¨ vanishesfor

k om " q jCk om " q k  om i Y p

k om " d k om

p

U

£ 

p k om

¨

p jCk#lnmo q k om "fj q k om

7l ¨

7 p 9 9

 " d i p j#kClnmo k om I x x §

x

§I l

p q jCk om Y p q   ) Y om U

9(9

¦ V ('i V0v   )

om U

t

r U V ('i Vv Y t


-c µ

5 4 3 2

1 2

§

3 

V

U determinesthephasetransitionin

"

U  r

"

k om k om "

7

k om k om " Y

7

k l om  T

q U i



 VhQ

¡

evaluation at

     

¢t.

Figure 2: The critical maximum mean spike count

E $ E E $E UC$ $     

d

£ is shown as a function of (numerical

§

¢

). The minimum

£  r r ¡  UVE at    j VU £¢l

p

U p

9

§ jCk#lumo " d I x l

q U i U p

§ jCk#lnmo " d I

l

¦ x

  ) ('i om l 

¨ Vl 1'i V0v  

 

Y

) om r

¦¦¥   

U §I

T

¨ t (14)

9 V

" ¤

t v t v

VU w T F 

i

Obviously, it is not possible to write the zeros of 

evaluation of the critical point I §

have treated

V

£ £

phase transition in all subsets

of I

to show that the critical point I with respect to the entire class

§ £

V¦¨.

We determined this value up to a precision of

£

§

¢

V¦¨

£t(£ Vp¨

of

¢

eV£§Iis

¨ in a closed form. The numerical

displayed in Fig. 2. Note, that we as a function of

as a fixed parameter, which means that we determine the critical point of the

t

that correspond to a fixed . It is straight forward

t

is given by the minimum

to be I r



.

 E E#ECETU 

¤£§ 

j 



¢

V

3 Conclusion Our study reveals that optimal encoding with respect to the minimum mean squared error is binary for maximum mean spike counts smaller than approximately three. Within the encoding analytically. With respect to mutual information the advantage of binary encoding holds even up to a maximum mean spike count of about 3.5 (results not shown) and remains discrete also for larger I . In a related work [9], Softky compared the informa§ tion capacity of the Poisson channel with the information rate of a (noiseless) binary pulse code. The rate of the latter turned out to exceed the capacity of the former at a factor of at least 72 demonstrating a clear superiority of binary coding over analog rate coding. Our rate-distortion analysis of the Poisson channel differs from that comparison in a twofold way: First, we do not change the noise model and second, the MMSE is often more appropriate to account for the coding efficiency than the channel capacity [10]. In particular, the assumption of a real random variable to be encoded with minimal mean squared error loss appears to introduce a bias for analog coding rather than for binary coding. Nevertheless,

¢ function class t we determined a second-order phase transition from binary to continuous


assuming a high temporal precision (i.e. small integration times £ ), our results hint into a

similar direction, namely that binary coding seems to be a more reasonable choice even if one supposes that the only means of neuronal communication would be the transmission of Poisson distributed spike counts. Methodologically, our analysis is similar to many theoretical studies of population coding with respect to a stimulus parameter   . Though conceptually different, some readers may therefore wish to know whether binary coding is still advantageous if many neurons, say , together encode for a single analog value. While the approach chosen in this paper is not feasible in case of large , a partial answer can be given: For the efficiency of population coding redundancy reduction is most important [7, 8, 11]. Smooth tuning curves, which have a dynamic range at about the same size as the signal range always lead to a large

 ¨ isnotinterpretedas theneuron'sgainfunction,but as atuningfunction  if %  ©¨ £ § £

£

 

 

l t

amount of redundancy so that the MMSE can not decrease faster than

MMSE of binary tuning functions scales proportional to

Y

l

. In contrast the  

or even faster. This holds  

also true for tuning functions, which are not perfectly binary, but have a dynamic range that a small dynamic range is always advantageous in case of population coding. In contrast, most experimental studies do not report on binary or steep tuning functions, but show smooth tuning curves only. However, the shape of a tuning function always depends on the stimulus set used. Only recently, experimental studies under natural stimulus conditions provided evidence for the idea that neuronal encoding is essentially binary [12]. Particularly striking is this observation for the H1 neuron of the fly [13], for which the functional role is probably better understood than for most other neurons that have been characterized by tuning functions. While the noise level of the Poisson channel studied in this paper is rather large, the H1 neuron can respond very reliably under optimal stimulus conditions [13]. Another example of a low-noise binary code has been found in the auditory cortex [14]. If we drop the restriction to Poisson noise and impose a hard constraint on the maximum number of spikes

is at least smaller than the signal range divided by  

§

instead, optimal encoding is always discrete with

is easy to grasp, because any rational

§

£

 ¨ takingintegervaluesonly[15]. This

can not serve to increase the entropy of the available

§ . Independent from I this implies that

symbol set (i.e. the candidate spike counts), but only increases the noise entropy instead. In other words, it is the simple fact that spike counts are discrete by nature, which already severely limits the possibility of graded rate coding. Clearly, this is not so obvious in case of the Poisson channel, if there is no hard constraint imposed on the maximum spike count. A remarkable aspect of the neuronal response of H1 shown in [13] is that it becomes the more binary the less noisy the stimulus conditions are (the noise level is determined by the different light conditions at midday, half an hour before, and half an hour after sunset). This suggests an interesting hypothesis why choosing a binary code with very high temporal precision might be advantageous even if the signal of interest by itself does not change at that time scale: the sensory input may sometimes be too noisy, so that repeated, independent samples from the signal of interest may sometimes lead to neuronal firing and sometimes not. In other words, a binary code at the short time scale is useful independent from the correlation time of the signal to be encoded, if uncertainties have to be taken into account, because any surplus available amount of temporal precision is maximally used for uncertainty representation in a self-adjusting manner. Furthermore, this Monte-Carlo type of uncertainty representation features several computational advantages [16]. Finally, it is a remarkable fact that this property is unique for a binary code, because the representation of uncertainty is necessary for many information processing tasks solved by the brain. Additional support for the potential relevance of a binary neural code comes from intracellular recordings in vivo revealing that the subthreshold membrane potential of many cortical cells switches between up and down states [17] depending on the stimulus. Furthermore,


the dynamics of bursting cells plays an important role for neuronal signal transmission [18] and may also be seen as evidence for binary rate coding. In light of these experimental facts, we conclude from our results that the idea of binary tuning constitutes an important hypothesis for neural coding. Acknowledgments This work was supported by the Deutsche Forschungsgesellschaft SFB 517. References [1] E.D. Adrian. The impulses produced by sensory nerve endings: Part i. J. Physiol. (London), 61:49­72, 1926. [2] D.H. Perkel and T.H. Bullock. Neural coding: a report based on an nrp work session. Neurosci. Research Prog. Bull., 6:220­349, 1968. [3] W.R. Softky and C. Koch. The hihgly irregular firing of cortical cells is inconsistent with temporal integration of random epsps. J. Neurosci., 13:334­350, 1993. [4] C. Keysers, D. Xiao, P. Foldiak, and D. Perrett. The speed of sight. J. Cog. Neurosci., 13:90­101, 2001. [5] S. Thorpe, D. Fize, and Marlot. Speed of processing in the human visual system. Nature, 381:520­522, 1996. [6] E.L. Lehmann and G. Casella. Theory of point estimation. Springer, New York, 1999. [7] M. Bethge, D. Rotermund, and K. Pawelzik. Optimal short-term population coding: when fisher information fails. Neural Comput., 14(10):2317­2351, 2002. [8] M. Bethge, D. Rotermund, and K. Pawelzik. Optimal neural rate coding leads to bimodal firing rate distributions. Network: Comput. Neural Syst., 2002. in press. [9] W.R. Softky. Fine analog coding minimizes information transmission. Neural Networks, 9:15­24, 1996. [10] D.H. Johnson. Point process models of single-neuron discharges. J. Comput. Neurosci., 3:275­299, 1996. [11] M. Bethge and K. Pawelzik. Population coding with unreliable spikes. Neurocomputing, 44-46:323­328, 2002. [12] P. Reinagel. How do visual neurons respond in the real world. Curr. Op. Neurobiol., 11:437­442, 2001. [13] G.D. Lewen, W. Bialek, and R.R. de Ruyter van Steveninck. Neural coding of natural stimuli. Network: Comput. Neural Syst., 12:317­329, 2001. [14] M.R. DeWeese and A.M. Zador. Binary coding in auditory cortex. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems, volume 15, 2002. [15] A. Gersho and R.M. Grey. Vector quantization and signal compression. Kluwer, Boston, 1992. [16] P.O. Hoyer and A. Hyvarinen. Interpreting neural response variability as monte carlo sampling of the posterior. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems, volume 15, 2002. [17] J. Anderson, I. Lampl, I. Reichova, M. Carandini, and D. Ferster. Stimulus dependence of two-state fluctuations of membrane potential in cat visual cortex. Nature Neurosci., 3:617­621, 2000. [18] J.E. Lisman. Bursts as a unit of neural information processing: making unreliable synapses reliable. TINS, 20:38­43, 1997.


