The Stability of Kernel Principal
Components Analysis and its Relation to
the Process Eigenspectrum
John Shawe-Taylor
Royal Holloway
University of London
john@cs.rhul.ac.uk
Christopher K. I. Williams
School of Informatics
University of Edinburgh
c.k.i.williams@ed.ac.uk
Abstract
In this paper we analyze the relationships between the eigenvalues
of the mm Gram matrix K for a kernel k(; ) corresponding to a
sample x 1 ; : : : ; xm drawn from a density p(x) and the eigenvalues
of the corresponding continuous eigenproblem. We bound the dif-
ferences between the two spectra and provide a performance bound
on kernel PCA.
1 Introduction
Over recent years there has been a considerable amount of interest in kernel methods
for supervised learning (e.g. Support Vector Machines and Gaussian Process predic-
tion) and for unsupervised learning (e.g. kernel PCA, Scholkopf et al. (1998)). In
this paper we study the stability of the subspace of feature space extracted by kernel
PCA with respect to the sample of size m, and relate this to the feature space that
would be extracted in the innite sample-size limit. This analysis essentially \lifts"
into (a potentially innite dimensional) feature space an analysis which can also
be carried out for PCA, comparing the k-dimensional eigenspace extracted from
a sample covariance matrix and the k-dimensional eigenspace extracted from the
population covariance matrix, and comparing the residuals from the k-dimensional
compression for the m-sample and the population.
Earlier work by Shawe-Taylor et al. (2002) discussed the concentration of spectral
properties of Gram matrices and of the residuals of xed projections. However,
these results gave deviation bounds on the sampling variability the eigenvalues of
the Gram matrix, but did not address the relationship of sample and population
eigenvalues, or the estimation problem of the residual of PCA on new data.
The structure the remainder of the paper is as follows. In section 2 we provide
background on the continuous kernel eigenproblem, and the relationship between
the eigenvalues of certain matrices and the expected residuals when projecting into
spaces of dimension k. Section 3 provides inequality relationships between the
process eigenvalues and the expectation of the Gram matrix eigenvalues. Section 4
presents some concentration results and uses these to develop an approximate chain
of inequalities. In section 5 we obtain a performance bound on kernel PCA, relating
the performance on the training sample to the expected performance wrt p(x).

2 Background
2.1 The kernel eigenproblem
For a given kernel function k(; ) the mm Gram matrix K has entries k(x i ; x j ),
i; j = 1; : : : ; m, where fx i : i = 1; : : : ; mg is a given dataset. For Mercer kernels K
is symmetric positive semi-denite. We denote the eigenvalues of the Gram matrix
as ^  1  ^  2 : : :  ^ m  0 and write its eigendecomposition as K = Z ^
Z 0 where ^ 
is a diagonal matrix of the eigenvalues and Z 0 denotes the transpose of matrix Z.
The eigenvalues are also referred to as the spectrum of the Gram matrix.
We now describe the relationship between the eigenvalues of the Gram matrix and
those of the underlying process. For a given kernel function and density p(x) on a
space X , we can also write down the eigenfunction problem
Z
X
k(x; y)p(x) i (x) dx =  i  i (y): (1)
Note that the eigenfunctions are orthonormal with respect to p(x), i.e.
R
X  i (x)p(x) j (x)dx = Æ ij . Let the eigenvalues be ordered so that  1   2  : : :.
This continuous eigenproblem can be approximated in the following way. Let
fx i : i = 1; : : : ; mg be a sample drawn according to p(x). Then as pointed out in
Williams and Seeger (2000), we can approximate the integral with weight function
p(x) by an average over the sample points, and then plug in y = x j for j = 1; : : : ; m
to obtain the matrix eigenproblem.
1
m
m
X
k=1
k(x k ; x j ) i (x k ) =  i  i (x j ):
Thus we see that  i
def
= 1
m
^
 i is an obvious estimator for the ith eigenvalue of the
continuous problem. The theory of the numerical solution of eigenvalue problems
(Baker 1977, Theorem 3.4) shows that for a xed k,  k will converge to  k in the
limit as m !1.
For the case that X is one dimensional, p(x) is Gaussian and k(x; y) = exp b(x
y) 2 , there are analytic results for the eigenvalues and eigenfunctions of equation (1)
as given in section 4 of Zhu et al. (1998). A plot in Williams and Seeger (2000) for
m = 500 with b = 3 and p(x)  N(0; 1=4) shows good agreement between  i and  i
for small i, but that for larger i the matrix eigenvalues underestimate the process
eigenvalues. One of the by-products of this paper will be bounds on the degree of
underestimation for this estimation problem in a fully general setting.
Koltchinskii and Gine (2000) discuss a number of results including rates of conver-
gence of the -spectrum to the -spectrum. The measure they use compares the
whole spectrum rather than individual eigenvalues or subsets of eigenvalues. They
also do not deal with the estimation problem for PCA residuals.
2.2 Projections, residuals and eigenvalues
The approach adopted in the proofs of the next section is to relate the eigenvalues
to the sums of squares of residuals. Let x be a random variable in d dimensions,
and let X be a d  m matrix containing m sample vectors x 1 ; : : : ; xm . Consider
the m  m matrix M = X 0 X with eigendecomposition M = Z ^
Z 0 . Then taking
X = Z
p
^
 we obtain a nite dimensional version of Mercer's theorem. To set the
scene, we now present a short description of the residuals viewpoint.
The starting point is the singular value decomposition of X = UZ 0 , where U
and Z are orthonormal matrices and  is a diagonal matrix containing the singular

values (in descending order). We can now reconstruct the eigenvalue decomposition
of M = X 0 X = ZU 0 UZ 0 = Z ^
Z 0 , where ^
 =  2 . But equally we can construct
a d  d matrix N = XX 0 = UZ 0 ZU 0 = U ^
U 0 , with the same eigenvalues as M .
We have made a slight abuse of notation by using ^
 to represent two matrices of
potentially dierent dimensions, but the larger is simply an extension of the smaller
with 0's. Note that N = mCX , where CX is the sample correlation matrix.
Let V be a linear space spanned by k linearly independent vectors. Let P V (x)
(P ?
V (x)) be the projection of x onto V (space perpendicular to V ), so that kxk 2 =
kP V (x)k 2 +kP ?
V (x)k 2 . Using the Courant-Fisher minimax theorem it can be proved
(Shawe-Taylor et al., 2002, equation 4) that
k
X
i=1
^  i (M) = max
dim(V )=k
m
X
j=1
kP V (x j )k 2 =
m
X
j=1
kx j k 2 min
dim(V )=k
m
X
j=1
kP ?
V (x j )k 2 ;
m
X
i=k+1
^  i (M) =
m
X
j=1
kx j k 2
k
X
i=1
^  i (M) = min
dim(V )=k
m
X
j=1
kP ?
V (x j )k 2 : (2)
Hence the subspace spanned by the rst k eigenvectors is characterised as that for
which the sum of the squares of the residuals is minimal. We can also obtain similar
results for the population case, e.g.
P k
i=1  i = max dim(V )=k E [kP V (x)k 2 ].
2.3 Residuals in feature space
Frequently, we consider all of the above as occurring in a kernel dened feature
space, so that wherever we have written a vector x we should have put  (x),
where   is the corresponding feature map   : x 2 X 7!  (x) 2 F to a feature
space F . Hence, the matrix M has entries M ij = h (x i );  (x j )i. The kernel
function computes the composition of the inner product with the feature maps,
k(x; z) = h (x);  (z)i =  (x) 0  (z), which can in many cases be computed without
explicitly evaluating the mapping  . We would also like to evaluate the projections
into eigenspaces without explicitly computing the feature mapping  . This can be
done as follows. Let u i be the i-th singular vector in the feature space, that is
the i-th eigenvector of the matrix N , with the corresponding singular value being
 i =
p ^  i and the corresponding eigenvector of M being z i . The projection of an
input x onto u i is given by
 (x) 0 u i = ( (x) 0 U) i = ( (x) 0 XZ) i  1
i = k 0 z i  1
i ;
where we have used the fact that X = UZ 0 and k j =  (x) 0  (x j ) = k(x; x j ).
Our nal background observation concerns the kernel operator and its eigenspaces.
The operator in question is
K(f)(x) =
Z
X
k(x; z)f(z)p(z)dz:
Provided the operator is positive semi-denite, by Mercer's theorem we can de-
compose k(x; z) as a sum of eigenfunctions, k(x; z) =
P 1
i=1  i  i (x) i (z) =
h (x);  (z)i; where the functions ( i (x)) 1
i=1 form a complete orthonormal basis
with respect to the inner product hf; gi p =
R
X f(x)g(x)p(x)dx and  (x) is the
feature space mapping
  : x ! (  i (x)) 1
i=1 =
p
 i  i (x)
 1
i=1
2 F:
Note that  i (x) has norm 1 and satises  i  i (x) =
R
X k(x; z) i (z)p(z)dz (equation
1), so that
 i =
Z
X 2
k(y; z) i (y) i (z)p(z)p(y)dydz: (3)

If we let (x) = ( i (x)) 1
i=1 2 F , we can dene the unit vector u i 2 F corresponding
to  i by u i =
R
X  i (x)(x)p(x)dx. For a general function f(x) we can similarly
dene the vector f =
R
X f(x)(x)p(x)dx. Now the expected square of the norm of
the projection P f ( (x)) onto the vector f (assumed to be of norm 1) of an input
 (x) drawn according to p(x) is given by
E

kP f ( (x))k 2

=
Z
X
kP f ( (x))k 2 p(x)dx =
Z
X
(f 0  (x)) 2
p(x)dx
=
Z
X
Z
X
Z
X
f(y)(y) 0  (x)p(y)dyf(z)(z) 0  (x)p(z)dzp(x)dx
=
Z
X 3
f(y)f(z)
1
X
j=1
p
 j  j (y) j (x)p(y)dy
1
X
`=1
p
 `  ` (z) ` (x)p(z)dzp(x)dx
=
Z
X 2
f(y)f(z)
1
X
j;`=1
p
 j  j (y)p(y)dy
p
 `  ` (z)p(z)dz
Z
X
 j (x) ` (x)p(x)dx
=
Z
X 2
f(y)f(z)
1
X
j=1
 j  j (y) j (z)p(y)dyp(z)dz
=
Z
X 2
f(y)f(z)k(y; z)p(y)p(z)dydz:
Since all vectors f in the subspace spanned by the image of the input space in F
can be expressed in this fashion, it follows using (3) that the sum of the nite case
characterisation of eigenvalues and eigenvectors is replaced by an expectation
 k = max
dim(V )=k
min
06=v2V
E [kP v ( (x))k 2 ]; (4)
where V is a linear subspace of the feature space F . Similarly,
k
X
i=1
 i = max
dim(V )=k
E

kP V ( (x))k 2

= E

k (x)k 2

min
dim(V )=k
E

kP ?
V ( (x))k 2

;
1
X
i=k+1
 i = E

k (x)k 2
 k
X
i=1
 i = min
dim(V )=k
E

kP ?
V ( (x))k 2

: (5)
where P V ( (x)) (P ?
V ( (x))) is the projection of  (x) into the subspace V (the
projection of  (x) into the space orthogonal to V ).
2.4 Plan of campaign
We are now in a position to motivate the main results of the paper. We consider the
general case of a kernel dened feature space with input space X and probability
density p(x). We x a sample size m and a draw of m examples S = (x 1 ; x 2 ; : : : ; xm )
according to p. Further we x a feature dimension k. Let ^
V k be the space spanned by
the rst k eigenvectors of the sample kernel matrix K with corresponding eigenvalues
^
 1 ; ^
 2 ; : : : ; ^  k , while V k is the space spanned by the rst k process eigenvectors with
corresponding eigenvalues  1 ;  2 ; : : : ;  k . Similarly, let ^
E [f(x)] denote expectation
with respect to the sample, ^ E [f(x)] = 1
m
P m
i=1 f(x i ), while as before E [] denotes
expectation with respect to p.
We are interested in the relationships between the following quantities: (i)
^
E
h
kP ^
Vk (x)k 2
i
= 1
m
P k
i=1
^
 i =
P k
i=1  i , (ii) E

kP Vk (x)k 2

=
P k
i=1  i (iii)

E
h
kP ^
Vk (x)k 2
i
and (iv) ^
E

kP Vk (x)k 2

. Bounding the dierence between the rst
and second will relate the process eigenvalues to the sample eigenvalues, while the
dierence between the rst and third will bound the expected performance of the
space identied by kernel PCA when used on new data.
Our rst two observations follow simply from equation (5),
^
E
h
kP ^
Vk (x)k 2
i
= 1
m
k
X
i=1
^
 i  ^ E

kP Vk (x)k 2

; (6)
and E

kP Vk (x)k 2

=
k
X
i=1
 i  E
h
kP ^
Vk (x)k 2
i
: (7)
Our strategy will be to show that the right hand side of inequality (6) and the left
hand side of inequality (7) are close in value making the two inequalities approxi-
mately a chain of inequalities. We then bound the dierence between the rst and
last entries in the chain.
3 Averaging over Samples and Population Eigenvalues
The sample correlation matrix is CX = 1
m XX 0 with eigenvalues  1   2 : : :   d .
In the notation of the section 2  i = (1=m) ^
 i . The corresponding population
correlation matrix has eigenvalues  1   2 : : :   d and eigenvectors u 1 ; : : : ; u d .
Again by the observations above these are the process eigenvalues. Let E m [] denote
averages over random samples of size m.
The following proposition describes how E m [ 1 ] is related to  1 and E m [ d ] is related
to  d . It requires no assumption of Gaussianity.
Proposition 1 (Anderson, 1963, pp 145-146) E m [ 1 ]   1 and E m [ d ]   d .
Proof : By the results of the previous section we have
 1 = max
06=c
m
X
i=1
1
m
kP c (x i )k 2  1
m
m
X
i=1
kPu1 (x i )k 2 = ^
E

kPu1 (x)k 2
 :
We now apply the expectation operator E m to both sides. On the RHS we get
E m ^
E

kPu1 (x)k 2
 = E

kPu1 (x)k 2
 =  1
by equation (5), which completes the proof. Correspondingly  d is characterized by
 d = min 06=c ^
E

kP c (x i )k 2
 (minor components analysis).
Interpreting this result, we see that E m [ 1 ] overestimates  1 , while E m [ d ] under-
estimates  d .
Proposition 1 can be generalized to give the following result where we have also
allowed for a kernel dened feature space of dimension NF  1.
Proposition 2 Using the above notation, for any k, 1  k  m, E m [
P k
i=1  i ] 
P k
i=1  i and E m [
P m
i=k+1  i ] 
P NF
i=k+1  i .
Proof : Let V k be the space spanned by the rst k process eigenvectors. Then from
the derivations above we have
k
X
i=1
 i = max
V : dimV =k
^
E

kP V ( (x))k 2

 ^
E

kP Vk ( (x))k 2

:

Again, applying the expectation operator E m to both sides of this equation and
taking equation (5) into account, the rst inequality follows. To prove the second we
turn max into min, P into P ? and reverse the inequality. Again taking expectations
of both sides proves the second part.
Applying the results obtained in this section, it follows that E m [ 1 ] will overestimate
 1 , and the cumulative sum
P k
i=1 E m [ i ] will overestimate
P k
i=1  i . At the other
end, clearly for NF  k > m,  k  0 is an underestimate of  k .
4 Concentration of eigenvalues
We now make use of results from Shawe-Taylor et al. (2002) concerning the concen-
tration of the eigenvalue spectrum of the Gram matrix. We have
Theorem 3 Let K(x; z) be a positive semi-denite kernel function on a space X,
and let p be a probability density function on X. Fix natural numbers m and 1 
k < m and let S = (x 1 ; : : : ; xm ) 2 X m be a sample of m points drawn according to
p. Then for all  > 0,
P
    1
m
^  k (S) E m
 1
m
^
 k (S)
     

 2 exp
 2 2 m
R 4

;
where ^  k (S) is the sum of the largest k eigenvalues of the matrix K(S) with entries
K(S) ij = K(x i ; x j ) and R 2 = max x2X K(x; x).
This follows by a similar derivation to Theorem 5 in Shawe-Taylor et al. (2002).
Our next result concerns the concentration of the residuals with respect to a xed
subspace. For a subspace V and training set S, we introduce the notation

P V (S) = ^
E

kP V ( (x))k 2

:
Theorem 4 Let p be a probability density function on X. Fix natural numbers m
and a subspace V and let S = (x 1 ; : : : ; xm ) 2 X m be a sample of m points drawn
according to a probability density function p. Then for all  > 0,
Pf 
P V (S) E m [ 
P V (S)]j  g  2 exp
  2 m
2R 4

:
This is theorem 6 in Shawe-Taylor et al. (2002).
The concentration results of this section are very tight. In the notation of the earlier
sections they show that with high probability
^
E
h
kP ^
Vk ( (x))k 2
i
= 1
m
k
X
i=1
^  i  E m
h
^ E
h
kP ^
Vk ( (x))k 2
ii
= E m
"
1
m
k
X
i=1
^  i
#
(8)
and
E

kP Vk ( (x))k 2

=
k
X
i=1
 i  ^
E

kP Vk ( (x))k 2

; (9)
where we have used Theorem 3 to obtain the rst approximate equality and Theo-
rem 4 with V = V k to obtain the second approximate equality.
This gives the sought relationship to create an approximate chain of inequalities
^
E
h
kP ^
Vk ( (x))k 2
i
= 1
m
k
X
i=1
^  i  ^
E

kP Vk ( (x))k 2

 E

kP Vk ( (x))k 2

=
k
X
i=1
 i  E
h
kP ^
Vk ( (x))k 2
i
: (10)

This approximate chain of inequalities could also have been obtained using Propo-
sition 2. It remains to bound the dierence between the rst and last entries in this
chain. This together with the concentration results of this section will deliver the
required bounds on the dierences between empirical and process eigenvalues, as
well as providing a performance bound on kernel PCA.
5 Learning a projection matrix
The key observation that enables the analysis bounding the dierence between
^
E
h
kP ^
Vk ( (x))k 2
i
and E
h
kP ^
Vk ( (x))k 2
i
is that we can view the projection norm
kP ^
Vk ( (x))k 2 as a linear function of pairs of features from the feature space F .
Proposition 5 The projection norm kP ^
Vk ( (x))k 2 is a linear function ^
f in a fea-
ture space ^
F for which the kernel function is given by ^ k(x; z) = k(x; z) 2 : Further-
more the 2-norm of the function ^
f is
p
k.
Proof : Let X = UZ 0 be the singular value decomposition of the sample matrix X
in the feature space. The projection norm is then given by ^
f(x) = kP ^
Vk ( (x))k 2 =
 (x) 0 U k U 0
k  (x), where U k is the matrix containing the rst k columns of U . Hence
we can write
kP ^
Vk ( (x))k 2 =
NF
X
ij=1
 ij  (x) i  (x) j =
NF
X
ij=1
 ij ^
 (x) ij ;
where ^
  is the projection mapping into the feature space ^
F consisting of all pairs
of F features and  ij = (U k U 0
k ) ij . The standard polynomial construction gives
^ k(x; z) = k(x; z) 2 =
  NF
X
i=1
 (x) i  (z) i
! 2
=
NF
X
i;j=1
 (x) i  (z) i  (x) j  (z) j =
NF
X
i;j=1
( (x) i  (x) j )( (z) i  (z) j )
=
D
^
 (x); ^
 (z)
E
^
F
:
It remains to show that the norm of the linear function is k. The norm satises
(note that k  kF denotes the Frobenius norm and u i the columns of U)
k ^
fk 2 =
NF
X
i;j=1
 2
ij = kU k U 0
k k 2
F =
* k
X
i=1
u i u 0
i ;
k
X
j=1
u j u 0
j
+
F
=
k
X
i;j=1
(u 0
i u j ) 2 = k
as required.
We are now in a position to apply a learning theory bound where we consider a
regression problem for which the target output is the square of the norm of the
sample point k (x)k 2 . We restrict the linear function in the space ^
F to have norm
p
k. The loss function is then the shortfall between the output of ^
f and the squared
norm.
Using Rademacher complexity theory we can obtain the following theorems:
Theorem 6 If we perform PCA in the feature space dened by a kernel k(x; z)
then with probability greater than 1 Æ, for all 1  k  m, if we project new data

onto the space ^
V k , the expected squared residual is bounded by
 >k  E
h
kP ?
^
Vk ( (x))k 2
i
 min
1`k
2
4 1
m
^  >` (S) + 1 +
p
`
p
m
v u u t 2
m
m
X
i=1
k(x i ; x i ) 2
3
5
+R 2
s
18
m ln
 2m
Æ

where the support of the distribution is in a ball of radius R in the feature space and
 i and ^  i are the process and empirical eigenvalues respectively.
Theorem 7 If we perform PCA in the feature space dened by a kernel k(x; z)
then with probability greater than 1 Æ, for all 1  k  m, if we project new data
onto the space ^
V k , the sum of the largest k process eigenvalues is bounded by
 k  E
h
kP ^
Vk ( (x))k 2
i
 max
1`k
2
4 1
m
^  ` (S) 1 +
p
`
p
m
v u u t 2
m
m
X
i=1
k(x i ; x i ) 2
3
5
R 2
s
19
m ln
 2(m + 1)
Æ

where the support of the distribution is in a ball of radius R in the feature space and
 i and ^  i are the process and empirical eigenvalues respectively.
The proofs of these results are given in Shawe-Taylor et al. (2003). Theorem 6
implies that if k  m the expected residual E
h
kP ?
^
Vk
( (x))k 2
i
closely matches the
average sample residual of ^
E
h
kP ?
^
Vk
( (x))k 2
i
= (1=m)
P m
i=k+1
^
 i , thus providing
a bound for kernel PCA on new data. Theorem 7 implies a good t between the
partial sums of the largest k empirical and process eigenvalues when
p
k=m is small.
References
Anderson, T. W. (1963). Asymptotic Theory for Principal Component Analysis. Annals
of Mathematical Statistics, 34(1):122{148.
Baker, C. T. H. (1977). The numerical treatment of integral equations. Clarendon Press,
Oxford.
Koltchinskii, V. and Gine, E. (2000). Random matrix approximation of spectra of integral
operators. Bernoulli, 6(1):113{167.
Scholkopf, B., Smola, A., and Muller, K.-R. (1998). Nonlinear component analysis as a
kernel eigenvalue problem. Neural Computation, 10:1299{1319.
Shawe-Taylor, J., Cristianini, N., and Kandola, J. (2002). On the Concentration of Spectral
Properties. In Diettrich, T. G., Becker, S., and Ghahramani, Z., editors, Advances in
Neural Information Processing Systems 14. MIT Press.
Shawe-Taylor, J., Williams, C. K. I., Cristianini, N., and Kandola, J. (2003). On the
Eigenspectrum of the Gram Matrix and the Generalisation Error of Kernel PCA. Tech-
nical Report NC2-TR-2003-143, Dept of Computer Science, Royal Holloway, University
of London. Available from http://www.neurocolt.com/archive.html.
Williams, C. K. I. and Seeger, M. (2000). The Eect of the Input Density Distribution on
Kernel-based Classiers. In Langley, P., editor, Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning (ICML 2000). Morgan Kaufmann.
Zhu, H., Williams, C. K. I., Rohwer, R. J., and Morciniec, M. (1998). Gaussian regression
and optimal nite dimensional linear models. In Bishop, C. M., editor, Neural Networks
and Machine Learning. Springer-Verlag, Berlin.

