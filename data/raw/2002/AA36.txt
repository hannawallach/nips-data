A Formulation for Minimax Probability Machine Regression

Thomas Strohmann Department of Computer Science University of Colorado, Boulder strohman@cs.colorado.edu Gregory Z. Grudic Department of Computer Science University of Colorado, Boulder grudic@cs.colorado.edu

Abstract We formulate the regression problem as one of maximizing the minimum probability, symbolized by , that future predicted outputs of the regression model will be within some ± bound of the true regression function. Our formulation is unique in that we obtain a direct estimate of this lower probability bound . The proposed framework, minimax probability machine regression (MPMR), is based on the recently described minimax probability machine classification algorithm [Lanckriet et al.] and uses Mercer Kernels to obtain nonlinear regression models. MPMR is tested on both toy and real world data, verifying the accuracy of the  bound, and the efficacy of the regression models.

1 Introduction The problem of constructing a regression model can be posed as maximizing the minimum probability of future predictions being within some bound of the true regression function. We refer to this regression framework as minimax probability machine regression (MPMR). For MPMR to be useful in practice, it must make minimal assumptions about the distributions underlying the true regression function, since accurate estimation of these distribution is prohibitive on anything but the most trivial regression problems. As with the minimax probability machine classification (MPMC) framework proposed in [1], we avoid the use of detailed distribution knowledge by obtaining a worst case bound on the probability that the regression model is within some  > 0 of the true regression function. Our regression formulation closely follows the classification formulation in [1] by making use of the following theorem due to Isii [2] and extended by Bertsimas and Sethuraman [3]: where a and b are constants, z is a random vector, and the supremum is taken over all distributions having mean ¯z and covariance matrix z. This theorem assumes linear boundaries, however, as shown in [1], Mercer kernels can be used to obtain nonlinear versions of this theorem, giving one the ability to estimate upper and lower bounds on probability that points generated form any distribution having mean ¯z and covariance z, will be on one side of a nonlinear boundary. In [1], this formulation is used to construct nonlinear classifiers (MPMC) that maximize the minimum probability of correct classification on future data.

supE 1 [z]=¯z,Cov[z]=z P r{aT z  b} = 1 1 + 2 , 2 = infa

T zb

(z - ¯z)T - (z - ¯z) (1) z


In this paper we exploit the above theorem (??) for building nonlinear regression functions which maximize the minimum probability that the future predictions will be within an  to the true regression function. We propose to implement MPMR by using MPMC to construct a classifier that separates two sets of points: the first set is obtained by shifting all of the regression data + along the dependent variable axis; and the second set is obtained by surface (i.e. classification boundary) between these two classes corresponds to a regression shifting all of the regression data - along the dependent variable axis. The the separating surface, which we term the minimix probability machine regression model. The proposed MPMR formulation is unique because it directly computes a bound on the probability that the regression model is within ± of the true regression function (see Theorem 1 below). The theoretical foundations of MPMR are formalized in Section 2. Experimental results on synthetic and real data are given in Section 3, verifying the accuracy of the minimax probability regression bound and the efficacy of the regression models. Proofs of the two theorems presented in this paper are given in the appendix. Matlab and C source code for generating MPMR models can be downloaded from http://www.cs.colorado.edu/grudic/software. 2 Regression Model We assume that learning data is generated from some unknown regression function f :

d

 that has the form:

y = f(x) + 

where x 

d

are generated according to some bounded distribution , y



(2) ,

E[] = 0, V ar[] = 2, and  

is finite. We are given N learning examples

 = {(x1, y1), ..., (xN , yN )}, where i  {1, ..., N }, xi = (xi , ..., xid)  1

d

is gen-

we wish to use  to construct an approximation f^of f, such that, for any x generated from erated from the distribution , and yi  . The goal of our formulation is two-fold: first the distribution , we can approximate y^ using The second goal of our formulation is, for any   ,  > 0, estimate the bound on the (4) Our proposed formulation of the regression problem is unique because we obtain direct  = inf Pr {|y^ - y|  } estimates of . Therefore we can estimate the predictive power of a regression function by a bound on the minimum probability that we are within  of the true regression function. We refer to a regression function that directly estimates (4) as a mimimax probability machine regression (MPMR) model. The proposed MPMR formulation is based on the kernel formulation for mimimax probability machine classification (MPMC) presented in [1]. Therefore, the MPMR model has y^ = f^(x) = iK (xi, x) + b (5)

(3)

y^ = f^(x) minimum probability, symbolized by , that f^(x) is within  of y (define in (2)):

the form: N

i=1

where, K (xi, x) = (xi)(x) is a kernel satisfying Mercer's Conditions, xi, i  learning algorithm. {1,...,N}, are obtained from the learning data , and i,b  are outputs of the MPMR 2.1 Kernel Based MPM Classification Before formalizing the MPMR algorithm for calculating i and b from the training data , we first describe the MPMC formulation upon which it is based. In [1], the binary classification problem is posed as one of maximizing the probability of correctly classifying future


data. Specifically, two sets of points are considered, here symbolized by {u1, ..., uNu}, be generated from a distribution that has mean u and a covariance matrix u, and correi  {1,...,Nv},vi  spondingly, the points vi are assumed to be generated from a distribution that has mean v and a covariance matrix v. For the nonlinear kernel formulation, these points are mapped

where i  {1, ..., Nu}, um,belonging i m , belonging to the first class, and {v1, ..., vNv}, where

to the second class. The points ui are assumed to

into a higher dimensional space  :

m

mean and covariance matrix ((u),  (u)

),

h

as follows: u  (u) with corresponding

and v  (v) with corresponding mean and

covariance matrix ((v), 

(v)

for the first class and c = +1 for the second): c = sign iKc (zi, z) + bc

Nu+Nv

i=1

where Kc (zi, z) = (zi)(z), zi = ui for i = 1, ..., Nu, zi = vi- Nu

Nu + 1, ..., Nu + Nv, and  = (1, ..., Nu+Nv ), bc obtained by solving the following optimization problem:

min  KNu u ~ + KNv v ~ s.t.T ku - kv ~ ~ = 1

). The binary classifier derived in [1] has the form (c = -1

(6) for i =

2 2

where Ku = Ku - 1NuKc(vj, zi)

1 Nv

Nv j=1

~ as: [kv]i = ~

ku; where Kv = Kv - 1N1kv; Nu v

and [ku]i = ~

Nu

j=1

~ ~ ~

where kv, ku 

~ ~ Nu+Nv

(7) defined

Kc(uj, zi); where 1k is a k

dimensional column vector of ones; where Ku contains the first Nu rows of the Gram matrix K (i.e. a square matrix consisting of the elements Kij = Kc(zi, zj)); and finally Kv contains the last Nv rows of the Gram matrix K. Given that  solves the minimization problem in (7), bc can be calculated using:

bc = T ku -  ~ 1 Nu

1 Nu

T Ku Ku = T kv +  ~ T ~ ~ 1 Nv T Kv Kv ~ T ~

-1

(8)

where,

 = T Ku Ku + ~ T ~ 1 Nv T Kv Kv ~ T ~ (9)

One significant advantage of this framework for binary classification is that, given perfect knowledge of the statistics u, u, v, v, the maximum probability of incorrect classification is bounded by 1 - , where  can be directly calculated from  as follows: This result is used below to formulate a lower bound on the probability that that the approximated regression function is within  of the true regression function. 2.2 Kernel Based MPM Regression In order to use the above MPMC formulation for our proposed MPMR framework, we first

 = 2 1 + 2 (10)

d+1

for i = 1, ..., N, as follows: take the original learning data  and create two classes of points ui  ui = (yi + , xi , xi , ..., xid)

1 1 2 2

and vi 

d+1

,

(11)

Given these two sets of points, we obtain  by minimizing equation (7). Then, from (6), vi = (yi - , xi , xi , ..., xid) the MPM classification boundary between points ui and vi is given by iKc (zi, z) + bc = 0 (12)

2N

i=1

We interpret this classification boundary as a regression surface because it acts to separate points which are  above the y values in the learning set , and  below the y values


in . Furthermore, given any point x = (x1, ..., xd) generated from the distribution , calculating y^ the regression model output (5), involves finding a y^ that solves equation (12), where z = (^ x1, ..., xd), and, recalling from above, zi = ui for i = 1, ..., N, zi = vi- for i = N + 1, ..., 2N (note that Nu = Nv = N). If Kc (zi, z) is nonlinear, solving (12) for y^ is in general a nonlinear single variable optimization problem, which can be solved using a root finding algorithm (for example the Newton-Raphson Method outlined in [4]). However, below we present a specific form of nonlinear Kc (zi, z) that allows (12) to be solved analytically. It is interesting to note that the above formulation of a regression model can be derived using any binary classification algorithm, and is not limited to the MPMC algorithm. Specifically, if a binary classifier is built to separate any two sets of points (11), then finding a crossing point y^ at where the classifier separates these classes for some input x = (x1, ..., xd), is equivalent to finding the output of the regression model for input x = (x1, ..., xd). It would be interesting to explore the efficacy of various classification algorithms for this type of regression model formulation. However, as formalized in Theorem 1 below, using the MPM framework gives us one clear advantage over other techniques. We now state the main result of this paper: Theorem 1: For any x = (x1, ..., xd) generated according to the distribution , assume that there exists only one y^ that solves equation (12). Assume also perfect knowledge of the statistics u, u, v, v. Then, the minimum probability that y^ is within  of y (as defined in (2)) is given by:

y, N

 = inf Pr {|y^ - y|  } = 2 1 + 2 (13)

where  is defined in (9). Proof: See Appendix. Therefore, from the above theorem, the MPMC framework directly computes the lower bound on the probability that the regression model is within  of the function that generated the learning data  (i.e. the true regression function). However, one key requirement of the theorem is perfect knowledge of the statistics u, u, v, v. In the actual implementation of MPMR, these statistics are estimated from , and it is an open question (which we address in Section 3) as to how accurately  can be estimated from real data. In order to avoid the use of nonlinear optimizations techniques to solve (12) for y^, we restrict the form of the kernel Kc (zi, z) to the following: Kc (zi, z) = yiy^ + K (xi, x) (14) where K (xi, x) = (xi)(x) is a kernel satisfying Mercer's Conditions; where z = yi - for i = N + 1, ..., 2N. Given this restriction on Kc (zi, z), we now state our final Lemma 1:

for i = 1, ..., N; and where zi = vi- , yi- = N N

(^ x1, ..., xd); where zi = ui, yi = yi + y, theorem which uses the following lemma:

k~u - k~v = 2 y (15)

Proof: See Appendix.

Theorem 2: Assume that (14) is true. Then all of the following are true: Part 1: Equation (12) has an analytical solution as defined in (5), where

i = -2 (i + i b = -2 bc +N )

Part 2: Ku = Kv ~ ~


Table 1: Results over 100 random trials for sinc data: mean squared errors and the stan-

dard deviation; MPTD: fraction of test points that are within predicted probability that the model is within  = 0.2 of y.

2 = 0 2 = 0.5 2 = 1.0 mean (std) mean (std) mean (std)

mean squared error 0.0 (0.0) 0.0524 (0.0386) 0.2592 (0.3118) MPTD 1.0 (0.0) 0.6888 (0.1133) 0.3870 (0.1110)

= 0.2 of y; predicted : predicted  1.0 (0.0)

0.1610 (0.0229) 0.0463 (0.0071)

Part 3: The problem of finding an optimal  in (7) is reduced to solving the following

linear least squares problem for t  min

t

where  = o + Ft , o =

2N-1

:

Ku (o + Ft) ~ 2

2

ku - kv / ku - kv

~ ~ ~ ~

, and F 

2N×(2N-1) is an

orthogonal matrix whose columns span the subspace of vectors orthogonal to ku - kv . Proof: See Appendix. Therefore, Theorem 2 establishes that the MPMR formulation proposed in this paper has a closed form analytical solution, and its computational complexity is equivalent to solving a linear system of 2N - 1 equations in 2N - 1 unknowns. 3 Experimental Results For complete implementation details of the MPMR algorithm used in the following experiments, see the Matlab and C source code available at http://www.cs.colorado.edu/grudic/software. Toy Sinc Data: Our toy example uses the noisy sinc function yi = sin(xi)/(xi) + i i = 1, ..., N, where i is drawn from a Gaussian distribution with mean 0 and variance 2 [5]. We use a RBF kernel K(a, b) = exp(-|a - b|2) and N = 100 training examples. Figure 1 (a), (b), and (c), and Table 1 show the results for different variances 2 and a   2 affect the mean squared error (on 100 random test points), the predicted  and constant value of  = 0.2. Figure 1 (d) and (e) illustrate how different tube sizes 0.05  measured percentage of test data within  (here called MPTD) of the regression model. Each experiment consists of 100 random trials. The average mean squared error in (e) has a small deviation (0.0453) over all tested  and always was within the range 0.19 to 0.35. This indicates that the accuracy of the regression model is essentially independent from the choice of . Also note that the mean predicted  is a lower bound on the mean MPTD. The tightness of this lower bound varies for different amounts of noise (Table 1) and different choices of  (Figure 1 d). Boston Housing Data: We test MPMR on the widely used Boston housing regression data available from the UCI repository. Following the experiments done in [5], we use the this data set. No attempt was made to pick optimal values for  using cross validation. RBF kernel K(a, b) = exp(- a - b /(22)), where (22)) = 0.3 · d and d = 13 for The Boston housing data contains 506 training examples, which we randomly divided into N = 481 training examples and 25 testing examples for each test run. 100 such random tests where run for each of  = 0.1, 1.0, 2.0, ..., 10.0. Results are reported in Table 2 for 1) average mean squared errors and the standard deviation; 2) MPTD: fraction of test points that are within of y and the standard deviation; 3) predicted : predicted probability that the model is within  of y and standard deviation. We first note that the results compare favorably to those reported for other state of the art regression algorithms [5], even though ~ ~


2 3 4

1.5

learning examples true regression function MPMR function MPMR function +  MPMR function - 

2.5

2

learning examples true regression function MPMR function MPMR function +  MPMR function - 

3

learning examples true regression function MPMR function MPMR function +  MPMR function - 

2

1.5

1 1

1

y

0.5 y 0

0.5 y

0

-1

-0.5

0

-2 -1

-3 -2 -1 0

x

a)  = 0.2, 2 = 0

1

1 2 3

-1.5 -3 -2 -1 0

x

1 2 3 -3 -3 -2 -1

Percentage of Test Data within± 

0 x 1 2 3

b)  = 0.2, 2 = 0.5

1

0.9

c)  = 0.2, 2 = 1.0

0.8

0.7

0.8

MPTD ± std runs)

0.6

(100

Probability 0.5 0.6

MSE 0.4

0.4 estimated  ± std

0.3

Average

0.2

0.2 0.1

0 0 0.2 0.4 0.6 0.8



e) mean squared error on test data w.r.t. , 2 = 1.0

1 1.2 1.4 1.6

0 0 0.2 0.4 0.6 0.8

 1 1.2 1.4 1.6

d) MPTD and predicted  w.r.t. , 2 = 1.0

Figure 1: Experimental results on toy sinc data.

Table 2: Results over 100 random trials for the Boston Housing Data for  =

0.1, 1.0, 2.0, ..., 10.0: mean squared errors and the standard deviation; MPDT: fraction probability that the model is within  of y and standard deviation. Average MSE (100 runs)

of y and the standard deviation; predicted : predicted of test points that are within

 MSE STD MPDT STD  STD 0.1 9.9 5.9 0.05 0.04 0.002 0.0005 1.0 10.5 9.5 0.33 0.09 0.19 0.03 2.0 10.9 8.6 0.58 0.09 0.51 0.06 3.0 9.5 5.9 0.76 0.08 0.69 0.05 4.0 10.3 8.1 0.84 0.07 0.80 0.04 4.0 9.9 8.0 0.89 0.06 0.87 0.03 6.0 10.5 8.5 0.93 0.05 0.90 0.01 7.0 10.5 8.1 0.95 0.04 0.92 0.01 8.0 9.2 5.3 0.97 0.03 0.94 0.009 9.0 10.1 6.9 0.97 0.03 0.95 0.009 10.0 10.6 7.6 0.98 0.02 0.96 0.008

no attempt was made to optimize for . Second, as with the toy data, the errors are relatively independent of . Finally, we note that the mean predicted  is lower than the measured average MPTD, thus validating the the MPMR algorithm does indeed predict an effective lower bound on the probability that the regression model is within  of the true regression function. 4 Discussion and Conclusion We formalize the regression problem as one of maximizing the minimum probability, , that the regression model is within ± of the true regression function. By estimating mean and covariance matrix statistics of the regression data (and making no other assumptions on the underlying true regression function distributions), the proposed minimax probability machine regression (MPMR) algorithm obtains a direct estimate of . Two theorems are presented proving that, given perfect knowledge of the mean and covariance statistics of the true regression function, the proposed MPMR algorithm directly computes the exact lower probability bound . We are unaware of any other nonlinear regression model formulation that has this property.


Experimental results are given showing: 1) the regression models produced are competitive with existing state of the art models; 2) the mean squared error on test data is relatively independent of the choice of ; and 3) estimating mean and covariance statistics directly from the learning data gives accurate lower probability bound  estimates that the regression model is within ± of the true regression function - thus supporting our theoretical results. Future research will focus on a theoretical analysis of the conditions under which the accuracy of the regression model is independent of . Also, we are analyzing the rate, as a function of sample size, at which estimates of the lower probability bound  converge to the true value. Finally, the proposed minimax probability machine regression framework is a new formulation of the regression problem, and therefore its properties can only be fully understood through extensive experimentation. We are currently applying MPMR to a wide variety of regression problems and have made Matlab / C source code available (http://www.cs.colorado.edu/grudic/software) for others to do the same. References [1] G. R. G. Lanckriet, L. E. Ghaoui, C. Bhattacharyya, and M. I. Jordan. Minimax probability machine. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press. [2] A. W. Marshall and I. Olkin. Multivariate chebyshev inequalities. Annals of Mathematical Statistics, 31(4):1001­1014, 1960. [3] I. Popescu and D. Bertsimas. Optimal inequalities in probability theory: A convex optimization approach. Technical Report TM62, INSEAD, Dept. Math. O.R., Cambridge, Mass, 2001. [4] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes in C. Cambridge University Press, New York NY, 1988. [5] Bernhard Scholkopf, Peter L. Bartlett, Alex J. Smola, and Robert Williamson. Shrink¨ ing the tube: A new support vector regression algorithm. In D. A. Cohn M. S. Kearns, S. A. Solla, editor, Advances in Neural Information Processing Systems, volume 11, Cambridge, MA, 1999. The MIT Press. Appendix: Proofs of Theorems 1 and 2 Proof of Theorem 1: Consider any point x = (x1, ..., xd) generated according to the distribution . This point will have a corresponding y (defined in (2)), and from (10), the probability that z+ = (y + , x1, ..., xd) will be classified correctly (as belonging to class u) by (6) is . Furthermore, the classification boundary occurs uniquely at the point where z = (^ x1, ..., xd), where, y, from the assumptions, y^ is the unique solution to (12). Similarly, for the same point y, the probability that z- = (y - , x1, ..., xd) will be classified correctly (as belonging to class v) by (6) is also , and the classifications boundary occurs uniquely at the point where z = (^ x1, ..., xd). Therefore, both z+ = (y + , x1, ..., xd) and z- = (y - , x1, ..., xd) are, with probability , on the correct side of the regression surface, defined by z = (^ x1, ..., xd). Therefore, z+ differs from z by at most + in the first dimension, y, and z- differs from z by at most - in the first dimension. Thus, the minimum bound on completes the proof. the probability that |y - y^|   is  (defined in (10)), which has the same form as . This

y, 


Proof of Lemma 1:

[k~u]i - [k~v]i =

1 N N l=1

1 N ( N l=1

Kc(ul, zi)) - (

N

1

N l=1 Kc(vl, zi)) =

(yl + )yi + K(xl, xi) - ((yl - )yi + K(xl, xi)) = 1 N N2 yi = 2 yi

Proof of Theorem 2: Part 1: Plugging (14) into (12), we get:

0 = 0 = 0 =

2N i=1 N i=1 N i=1

i [yiy^ + K (xi, x)] + bc i [(yi + ) y^ + K (xi, x)] +

{(i + i +N

N i=1 i

+N [(yi - ) y^ + K (xi, x)] + bc

) [yiy^ + K (xi, x)] + (i - i +N ) y^} + bc

When we solve analytically for y^, giving (5), the coefficients i and the offset b have a

denominator that looks like: - i for the denominator of i and b. Part 2: The values zi are defined as: z1 = u1, ..., zN = uN, zN the following term for a single matrix entry:

N =1

[(i + i +N ) yi + (i - i +N ) ] = -Ty

Applying Lemma 1 and (7) we obtain: 1 = T(~(ku) - k~v) = T2 y  -Ty = -2 1

+1

(2 , 0, ··· , 0)T , ..., z2N = vN = uN - (2 , 0, ··· , 0)T . Since K~u = Ku - 1N k~u we have

[K~u]i,j = Kc(ui, zj) - 1 N N l=1 Kc(ul, zj) i = 1, .., N j = 1, ..., 2N

= v1 = u1 -

Similarly the matrix entries for K~v look like:

[K~v]i,j = Kc(vi, zj) -

1 N N l=1 Kc(vl, zj) i = 1, .., N j = 1, ..., 2N

We show that these entries are the same for all i and j:

[K~u]i,j = Kc(vi + (2 0 ··· 0)T , zj) -

Kc(vi, zj) + 2 [zj]1 - (

Kc(vi, zj) + 2 [zj]1 - Kc(vi, zj) + 2 [zj]1 -

Kc(vi, zj) - 1 N N l=1

1 N 1 N 1 N

N l=1 N

l=1 N l=1

1 N N l=1 Kc(vl + (2 0 ··· 0)T , zj) =

Kc(vl, zj) + 2 [zj]1) =

Kc(vl, zj) -

Kc(vl, zj) - N2 [zj]1 =

N

1 N 1

N l=1 2 [zj]1 =

Kc(vl, zj) = [K~v]i,j

This completes the proof of Part 2. Part 3: From Part 2 we know that K~u = K~v. Therefore, the minimization problem Formulating this minimization with the use of the orthogonal matrix F and an initial vector

(7) collapses to min K~u 2 2 with respect to  (the N is constant and can be removed).

o this becomes (see [1]): min K~u(o + Ft)

h(t) = K~u( + Ft)

linear equations: 0 =

2 2 d

2 2

with respect to t 

2N-1 . We set

. Therefore in order to find the minimum we must solve 2N - 1 h(t) i = 1, ..., 2N - 1. This completes the proof of Part 3.

dti


