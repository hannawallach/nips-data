Categorization Under Complexity: A Unified MDLAccount ofHuman Learning ofRegular and Irregular Categories

David Fass Department of Psychology Center for Cognitive Science Rutgers University Piscataway,NJ08854 dfass@ruccs.rutgers.edu Jacob Feldman Department of Psychology Center for Cognitive Science Rutgers University Piscataway,NJ08854 jacob@ruccs.rutgers.edu

Abstract We present an account of human concept learning--that is,learning of categories from examples--basedon the principle of minimum description length (MDL).In support of this theory,we testeda wide range of two-dimensional concept types,including both regular (simple)and highly irregular (complex)structures,andfoundthe MDL theory to give a goodaccount of subjects'performance.This suggests that the intrinsiccomplexityof a concept (that is,its description length)systematically influences its learnability.

1 The Structure ofCategories A number of different principles have been advancedto explain the manner in which humans learn to categorize objects.It has been variously suggestedthat the underlying principle might be the similaritystructureof objects [1],the manipulability of decision boundaries [2],or Bayesian inference [3][4].While many of these theories are mathematically well-groundedandhave been successful in explaining a range of experimental findings, they have commonly only been testedon a narrow collection of concept types similar to the simple unimodal categories of Figure 1(a­c).

Figure 1:Categories similar to those previously studied.Lines represent contours of equal probability.All except (e)are unimodal. http://ruccs.rutgers.edu/~jacob/feldman.html


Moreover, in the scarce research that has ventured to look beyond simple category types, the goal has largely been to investigate categorization performance for isolated irregular distributions, rather than to present a survey ofperformance across a range ofinteresting distributions.For example, Nosofsky has previously examined the "criss-cross"category ofFigure 1(d)and a diagonal category similar to Concept 3ofFigure 2, as well as some othermultimodalcategories[5][6].Whiletheseindividualcategorystructuresarenodoubt theoretically important, they in no way exhaust the range ofpossible concept structures. Indeed, ifwe view -dimensional Cartesian space as the canvas upon which a category may be represented, then any set ofmanifolds in that space may be considered as a potential category [7].It is therefore natural to ask whether one such category-manifold is in principle easier or more difficult to learn than another.Since previous investigations have never considered any reasonably broad range ofcategory structures, they have never been in a position to answer this question. In this paper we present a theory for human categorization, based on the MDLprinciple, that is much better equipped to answer questions about the intrinsic learnability of both structurally regular and structurally irregular categories.In support ofthis theory we briefly present an experiment testing human subjects'learning ofa range ofconcept types defined over a continuous two-dimensional feature space, including both highly regular and highly irregular structures.We find that our MDL-based theory gives a good account ofhuman learning for these concepts, and that descriptive complexity accurately predicts the subjective difficulty ofthe various concept types tested. 2 Previous Investigations of Category Structure The role ofcategory structure in determining learnability has not been overlooked entirely in the literature;in fact, the intrinsic structure ofbinary-featured categories has been investigated quite thoroughly.The classic work by Shepard et al.[8]showed that human performance in learning such Boolean categories varies greatly depending on the intrinsic logical structure ofthe concept.More recently, we have shown that this performance is well-predicted by the intrinsic Booleancomplexityofeach concept, given by the length ofthe shortest Boolean formula that describes the objects in the category [9].This result suggests that a principle ofsimplicity or parsimony, manifested as a minimization of complexity, might play an important role in human category learning. The details ofBoolean complexity analysis do not generalize easily to the type ofcontinuous feature spaces we wish to investigate here.Thus a newapproach is required, similar in general spirit but differing in the mathematics.Our goals are therefore (1)to deploy a complexity minimization technique such as MDLto quantify the complexity ofcategories defined over continuous features, and (2)to investigate the influence ofcomplexity onhumancategorylearningbytestingarangeofconcepttypesdifferingwidelyinintrinsic complexity.

3 Experiment While the MDLprinciple that we plan to employ is applicable to concepts ofany dimension, for reasons ofconvenience this experiment is limited to category structures that can be formed within a two-dimensional feature space.This feature space is discretized into a 4×4gridfromwhichalegitimatecategorycanbespecifiedbytheselectionofanyfourgrid squares.Our motivation for discretizing the feature space is to place a constraint on possible category structure that will facilitate the computation ofa complexity measure;this doesnotrestricttherangeofpossiblefeaturevaluesthatcanbeadoptedbystimuli.Inprinciple, feature values are limited only by machine precision, but as a matter ofconvenience


we restrict features to adopting one of 1000 possible values in the range [0,1].

Figure 2:Abstract concepts used in experiment. The particular 12abstract category structures ("concepts")examined in the experiment are shown in Figure 2. These concepts were considered to be individually interesting (from a cross-theoretical perspective)and jointly representative of the broader range of available concepts. The two categories in each concept are referred to as "positive"and "negative." The positive category is represented by the dark-shaded regions, and the corresponding negative category is its complement. Note that in many cases the categories are "disconnected"or multimodal. Nevertheless, these categories are not in any sense "probabilistic" or "ill-defined";a given point in feature space is always either positive or negative. During the experiment, each stimulus is drawn randomly from the feature space and is labeled "positive"or "negative"based on the region from which it was drawn. Uniform sampling is used, so all 12categories of Figure 2have the same base rate for positives, The experiment itself was clothed as a video game that required subjects to discriminate between two classes of spaceships, "Ally"and "Enemy,"by destroying Enemy ships and quick-landing Allied ships. Each subject (14total)played 12five-minute games in which thedistributionofAlliesandEnemiescorresponded(inrandomorder)tothe12conceptsof Figure 2. The physical features of the spaceships in all cases were the height of the "tube" and the radius of the "pod."As shown in Figure 3, these physical features are mapped randomly onto the abstract feature space such that the experimental concepts may be any rigid rotation or reflection of the abstract concepts in Figure 2.

(positive)= 4 16 = . 1 4

4 Derivation of the MDL Principle TheMDLprincipleislargelyduetoRissanen[10]andiseasilyshowntobeaconsequence of optimal Bayesian inference [11]. While several Bayesian algorithms have previously been proposed as models of human concept learning [3][4], the implications of the MDL principle for human learning have only recently come under scrutiny [12][13]. We briefly review the relevant theory. AccordingtoBayesrule,alearneroughttoselectthecategoryhypothesis thatmaximizes

Figure 3:(a)Aspaceship. (b­d)Three possible instantiations of Concept 6from Figure 2.


the posterior ( | ), where is the data, and

( | ) = ( | ) ( ) ( ) (1)

Takingnegative logarithms ofboth sides, we obtain log ( | ) = log ( | ) log ( )+ log ( )

(2)

The problem ofmaximizing ( | ) is thus identicalto the problem ofminimizing log ( | ). Since log ( ) is constant for allhypotheses, its value does not enter into the minimizationproblem, andwe canstate that the hypothesis ofchoice ought to be such as to minimize the quantity log ( | ) log ( ) (3) Ifwe follow Rissanenandregardthe quantity log ( ) as the descriptionlength of , ( ), thenEquation3 instructs us to select the hypothesis that minimizes the totaldescriptionlength ( | )+ ( ) (4) What this means is that the hypothesis that is optimalfrom the standpoint ofthe Bayesian decisionmakeris the same hypothesis thatyieldsthemost compacttwo-partcode inEquation4. Thus,besidesthemeritsofbrevityforitsownsake,weseethatmaximaldescriptive compactness also corresponds to maximalinferentialpower. It is this equivalence between descriptionlengthandinferencethatleadsustoinvestigatetheroleofdescriptivecomplexityinthe domainofconcept learning. 5 Theory Inorder to investigate the complexityofthe 12 concepts ofFigure 2, Equation4indicates thatweneedtoanalyze(1)thedescriptionlengthofahypothesisforeachconcept, ( ),

and(2)thedescriptionlengthoftheconceptgiventhehypothesis, these insequence. 5.1 The HypothesisDescriptionLength, ( )

Inorder to compute

( | ). Wediscuss

( ), we first fixa language1 withinwhich hypotheses about the

categorystructure canbe expressed. We choose to use the "rectangle language"whose alphabet (Table 1)consists of10classesofsymbols representingthe 10different sizes of rectangle that canbe compositedwithina 4×4grid:1×1, 1×2, 1×3, 1×4, 2×2, 2×3, 2×4, 3×3, 3×4, and4×4.2 Each member ofthe class " × "is an × or × rectangle situatedat a particular positioninthe 4×4grid. We allowa givenhypothesis to be representedbyup to four distinct rectangles (i.e., four symbols). Havingspecifiedalanguage,theissueisnowthelengthofthehypothesiscode.Thederivationabove suggests that a codelength of log ( ) be assignedto each symbol , which corresponds to the so-calledShannoncode. We therefore proceedto compute the Shannon codelengths for the rectangle alphabet ofTable 1.3

1 Equivalently, a modelclass. The particular choice oflanguage (modelclass)is obviouslyanim-

portantdeterminantoftheultimatehypothesisdescriptionlength. WementionthattheMDLanalysis inthis paper might be replacedbyanother theoreticalapproach, such as a Bayesianframework, although we have not pursuedthis possibility. We adopt the MDLformulationpartlybecause its emphasis onrepresentation (i.e., description)seems apt for a studyofcomplexity.

2 3 The class " × "contains allrectangles ofdimension × and × . We use the noninteger value log ( ) rather thanthe integer d log ( )e. Logs are base-2.


Table 1: Rectangle alphabet. The third and fourth columns show the probability that the source generates a given member of the class " × "and the corresponding codelength. Rectangle Class Possible Locations Probability Codelength

1×1 1×2 1×3 1×4 2×2 2×3 2×4 3×3 3×4 4×4 16 24 16 8 9 12 6 4 4 1

1

·

1

10 16 10 24 10 16 10 8 10 9 10 12 10 6 10 4 10 4

1 · 1

1 · 1

1 · 1

1 · 1

1 · 1

1 · 1

1 · 1

1 · 1

1 10 ·1

log

log¡¡160¢¢ log¡¡160¢¢ log¡¡120¢ log¡80¢¢

1 1 240

log

1 1

1 90 1 1 60

log

log

log¡¡40¢¢ log¡40¢

1 1

1 10

Computing these codelengths requires that we specify the probability mass function of a source, ( ). It is convenient for this purpose (and compatible with the subject's perspective)to imagine that the concepts in Figure 2 are produced by a "concept generator,"an information source whose parameters are essentially unknown. A reasonable assumption is that the source randomly selects a rectangle class with uniform probability,and then selects an individual member of the chosen class also with uniform probability. Since there are 10classes,the assumption regarding class selection places a prior on each rectangle class of ( × ) = .

1 10

Moreover,the assumption of uniform within-class sampling means that in order to encode any individual rectangle,we need only consider the cardinality of the class to which it belongs. We now recall that the individual rectangles of the class " × "differ only in their positions within the 4×4 grid. Therefore,the cardinality of the class " × "is equal

tothenumber ofuniqueways from a 4×4 grid,where

×

×

½

in whichan × or × rectanglecanbeselected

(5 2(5 )(5 )(5 ) ) = 6= = (5)

Thus,the probability associated with an individual rectangle of class " × "is

( × )

×

.

The corresponding Shannon codelengths are shown next to these probabilities in Table 1. The description length of a particular hypothesis is the summed codeword lengths for all the rectangles (up to four)that are comprised by the hypothesis. 5.2 The LikelihoodDescription Length, ( | ) The second part of the two-part MDLcode is the description of the concept with respect to the selected hypothesis,corresponding to the Bayes likelihood. There are several possible

approaches to computing We recall that a hypothesis ( | );we discuss one that is particularly straightforward. is composed of up to four rectangular regions. Computing

( | ) therefore involves describing that portion of the positive category that falls within each rectangular hypothesis region. This is conceptually the same problem that we faced in computing ( ) above,except that the region of interest for ( ) was fixed


Table 2: Minimum description lengths for the 12 abstract concepts. Concept MDLCodelength MDLHypothesis MDLConcept

1 2 3 4 5 6 7 8 9 10 11 12 8.0768 bits 8.3219 bits 27.3236 bits 17.8138 bits 16.5216 bits 14.4919 bits 17.1357 bits 22.5687 bits 14.4919 bits 15.0768 bits 27.1946 bits 28.1536 bits

at 4×4,while the regions for ( | ) can be of anydimension 4×4 and smaller.

Guided bythis analogy,we follow the procedure of the previous section to compute an appropriate probabilitymass function. Since ( | ) must capture just the positive squares in the hypothesis region (a maximum of four squares),the onlyrectangle classes needed in the alphabet are those of size four: 1×1,1×2,1×3,1×4,and 2×2.

6 Minimum Description LengthsforExperimentalConcepts Applying the MDLanalysis above to the concepts in Figure 2 requires that we compute the total description length ( | ) + ( ) corresponding to all viable hypotheses for each concept. The hypothesis corresponding to the shortest total codelength ( | )+ ( )foreach concept is theMDLhypothesis.4 The MDLhypothesesfor all 12 concepts are shown in Table 2 along with the corresponding minimum codelengths. It can be observed that while for some concepts the MDLhypothesis preciselyconforms to the true positive category(meaning that almost all of the concept information is carried in the hypothesis code),for the majorityof concepts the MDLhypothesis is broaderthan the true categoryregion (meaning that the concept information is distributed between the hypothesis and likelihood codes).

4 Note that the MDLhypothesis is not in general the most compact hypothesis,i.e.,the hypoth-

esis for which ( | )+ ( ) is a minimum. Rather,the MDLhypothesis is the one for which the sum ( ) is minimum.


7 Results For each game played by the subject (i.e., each concept in Figure 2), an overall measure ofperformance ( ) is computed.5 Figure 4shows performance for all subjects and all concepts as a function ofthe concept complexities (MDLcodelengths) in Table 2. There is anevidentdecreaseinperformancewithincreasingcomplexity,whicharegressionanalysis that the linear trend in the plot is very unlikely to be a statistical accident. Thus, the MDL complexity predicts the subjective difficulty oflearning across a broad range ofconcepts.

0

shows to be highly significant ( 2 = 384, (1 166)= 103375, 000001), meaning

3.5 3 2.5 2 1.5 1 0.5 0 -0.5

-1

)'d(ecnam rofreP

5 10 15 20 25 30

Complexity, DL(H) + DL(D|H)

Figure 4: Performance vs. complexity for all 14subjects. The performance for each concept is indicated by a `+'and the mean for each concept is indicated by an `O'. We mention that the MDLapproach described here can be further modified to make "realtime"predictions ofhowsubjects will categorizeeach newstimulus. In the most simplistic approach, the prediction for each new stimulus is made based on the MDLhypothesis prevailing at the time that stimulus is observed. Correlation between this MDLprediction 0

and the subject's actual decision is found to be highly significant ( 12 concept types. The Pearson statistics are given below: Concept #: 1 2 3 4 5 6 7 8 002) for each ofthe 9 10 11 12

0

Pearsonr: 46 47 19 18 20 51 18 14 34 32 32 05 Figure 5 illustrates the behavior ofthe real-time MDLalgorithm. Simulations for a variety ofdata sets can be found at http://ruccs.rutgers.edu/~dfass/mdlmovies.html.

Figure 5: Real-time MDLhypothesis evolution for actual Concept 11data. As the size ofthe data set grows beyond 150, there is oscillation between the one-rectangle (2×4) hypothesis shown in Step 169and the two-rectangle (1×3) hypothesis shown in Step 190.

5 0 (discriminability) gives a measure ofsubjects'intrinsic capacity to discriminate categories,

i.e., one that is independent oftheir criterion for responding "positive" 14.


8 Conclusions As discussed above, MDL bears a tight relationship with Bayesian inference, and hence serves as a reasonable basis for a theory of learning.The data presented above suggest that human learners are indeed guided by something very much like Rissanen's principle when classifying objects.While it is premature to conclude that humans construct anything precisely corresponding to the two-part code of Equation 4, it seems likely that they employ some closely related complexity-minimization principle--and an associated "cognitivecode"stilltobediscovered.Thisfindingisconsistentwithmanyearlierobservations of minimum principles guiding human inference, especially in perception (e.g., the Gestalt principleof Prägnanz).Moreover, our findings suggest aprincipled approach to predicting the subjective difficulty of concepts defined over continuous features.As we had previously found with Boolean concepts, subjective difficulty correlates with intrinsic complexity:Thatwhichisincompressible is,in turn,incomprehensible.The MDL approach is an elegant frameworkinwhichto makethisobservationrigorousandconcrete, andonewhich apparently accords well with human performance. Acknowledgments This research was supported by NSF SBR-9875175. References

[1]Nosofsky, R.M., "Exemplar-based accounts of relations between classification, recognition, and typicality,"JournalofExperimentalPsychology: Learning,Memory,andCognition, Vol.14, No.4, 1988, pp.700­708. [2]Ashby, F.G.and Alfonso-Reese, L.A., "Categorization as probability density estimation," JournalofMathematicalPsychology, Vol.39, 1995, pp.216­233. [3]Anderson,J.R., "Theadaptivenatureofhumancategorization,"PsychologicalReview, Vol.98, No.3, 1991, pp.409­429. [4]Tenenbaum, J.B., "Bayesian modeling of human concept learning,"Advancesin NeuralInformation Processing Systems, edited by M.S.Kearns, S.A.Solla, and D.A.Cohn, Vol.11, MIT Press, Cambridge, MA, 1999. [5]Nosofsky, R.M., "Optimal performanceandexemplar modelsof classification,"RationalModelsofCognition, edited by M.Oaksford and N.Chater, chap.11, Oxford University Press, Oxford, 1998, pp.218­247. [6]Nosofsky,R.M.,"Furthertestsofanexemplar-similarityapproachtorelatingidentificationand categorization,"Perception andPsychophysics, Vol.45, 1989, pp.279­290. [7]Feldman, J., "The structure of perceptual categories,"JournalofMathematicalPsychology, Vol.41, No.2, 1997, pp.145­170. [8]Shepard, R.N., Hovland, C.I., and Jenkins, H.M., "Learning and memorization of classifications,"PsychologicalMonographs:GeneralandApplied, Vol.75, No.13, 1961, pp.1­42. [9]Feldman, J., "Minimization of Boolean complexity in human concept learning," Nature, Vol.407, 2000, pp.630­632. [10]Rissanen, J., "Modeling by shortest data description,"Automatica, Vol.14, 1978, pp.465­471. [11]Li, M.and Vitányi, P., An Introduction toKolmogorovComplexity andItsApplications, Springer, New York, 2nd ed., 1997. [12]Pothos, E.M.and Chater, N., "Categorization by simplicity:A minimum description length approach to unsupervised clustering,"Similarity andCategorization, edited by U.Hahn and M.Ramscar, chap.4, Oxford University Press, Oxford, 2001, pp.51­72. [13]Myung, I.J., "Maximum entropy interpretation of decision bound and context models of categorization,"JournalofMathematicalPsychology, Vol.38, 1994, pp.335­365. [14]Wickens, T.D., Elementary SignalDetection Theory, Oxford University Press, Oxford, 2002.


