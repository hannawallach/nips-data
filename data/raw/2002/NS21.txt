Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals

  ¡  £¢ ¤

  Tatyana Sharpee , Nicole C. Rust , and William Bialek Sloan­Swartz Center for Theoretical Neurobiology, Department of Physiology

University of California at San Francisco, San Francisco, California 94143­0444 ¡

¤

Center for Neural Science, New York University, New York, NY 10003 Department of Physics, Princeton University, Princeton, New Jersey 08544 sharpee@phy.ucsf.edu, rust@cns.nyu.edu, wbialek@princeton.edu

We propose a method that allows for a rigorous statistical analysis of neural responses to natural stimuli, which are non-Gaussian and exhibit strong correlations. We have in mind a model in which neurons are selective for a small number of stimulus dimensions out of the high dimensional stimulus space, but within this subspace the responses can be arbitrarily nonlinear. Therefore we maximize the mutual information between the sequence of elicited neural responses and an ensemble of stimuli that has been projected on trial directions in the stimulus space. The procedure can be done iteratively by increasing the number of directions with respect to which information is maximized. Those directions that allow the recovery of all of the information between spikes and the full unprojected stimuli describe the relevant subspace. If the dimensionality of the relevant subspace indeed is much smaller than that of the overall stimulus space, it may become experimentally feasible to map out the neuron's input-output function even under fully natural stimulus conditions. This contrasts with methods based on correlations functions (reverse correlation, spike-triggered covariance, ...) which all require simplified stimulus statistics if we are to use them rigorously. 1 Introduction From olfaction to vision and audition, there is an increasing need, and a growing number of experiments [1]-[8] that study responses of sensory neurons to natural stimuli. Natural stimuli have specific statistical properties [9, 10], and therefore sample only a subspace of all possible spatial and temporal frequencies explored during stimulation with white noise. Observing the full dynamic range of neural responses may require using stimulus ensembles which approximate those occurring in nature, and it is an attractive hypothesis that the neural representation of these natural signals may be optimized in some way. Finally, some neuron responses are strongly nonlinear and adaptive, and may not be predicted from a combination of responses to simple stimuli. It has also been shown that the variability in neural response decreases substantially when dynamical, rather than static, stimuli are used [11, 12]. For all these reasons, it would be attractive to have a rigorous method of analyzing neural responses to complex, naturalistic inputs. The stimuli analyzed by sensory neurons are intrinsically high-dimensional, with dimen-


sions

 ¢¡¤£¦¥ ¨§¡ £©¥ ¤

. For example, in the case of visual neurons, input is specified as light

intensity on a grid of at least

£¦¥£¦¥

pixels. The dimensionality increases further if the

time dependence is to be explored as well. Full exploration of such a large parameter space is beyond the constraints of experimental data collection. However, progress can be made provided we make certain assumptions about how the response has been generated. In the simplest model, the probability of response can be described by one receptive field (RF)

[13]. The receptive field can be thought of as a special direction



in the stimulus space

such that the neuron's response depends only on a projection of a given stimulus

This special direction

 

onto .



is the one found by the reverse correlation method [13, 14]. In a

more general case, the probability of the response depends¡ on projections

#

, of the stimulus on a set of vectors

©VW#

DFEHGCIQP'RTS3U

DFEHGCIQP'RTS E 2 2`4'4'4

:

2

and

1

£32544'4276

89&   !,&  !  A$!CB

2@44'4

&

"!$#%& '"!)(0

,

DFEaGYIQP'RTSbU0V

VYX    !  ¡ !  A$! V

where is the probability of a spike given a stimulus



DFEaGYIcPR3S

V

(1) is the av-

erage firing rate. In what follows we will call the subspace spanned by the set of vectors analyze input-output functions with respect to different neural responses, we settle on a

the relevant subspace (RS). Even though the ideas developed below can be used to

single spike as the response of interest. Eq. (1) in itself is not yet a simplification if the dimensionality

dimensionality

d

6 of the RS is equal to the

89& "!7B

of the stimulus space. In this paper we will use the idea of dimensionality

reduction [15, 16] and assume that

6fe

d

. The input-output function

X

in Eq. (1) can be

strongly nonlinear, but it is presumed to depend only on a small number of projections. This assumption appears to be less stringent than that of approximate linearity which one makes when characterizing neuron's response in terms of Wiener kernels. The most difficult part

6hgi£

in reconstructing the input-output function is to find the RS. For

terms of any linear combination of vectors

89& "!CB

, a description in

is just as valid, since we did not make any

assumptions as to a particular form of nonlinear function . We might however prefer one coordinate system over another if it, for example, leads to sparser probability distributions or more statistically independent variables. only few parameters, and it becomes feasible to map this function experimentally, inverting the probability distributions according to Bayes' rule:

Once the relevant subspace is known, the probability

DFEaGYIcPR3SbU©V becomes a function of

X 80 "! B"Vp# DFE E

DFE

80 "!80 B"! V

UGCIQPR3S B"V

(2)

X

If stimuli are correlated Gaussian noise, then the neural response can be characterized by the spike-triggered covariance method [15, 16]. It can be shown that the dimensionality of the RS is equal to the number of non-zero eigenvalues of a matrix given by a difference between covariance matrices of all presented stimuli and stimuli conditional on a spike. Moreover, the RS is spanned by the eigenvectors associated with the non-zero eigenvalues multiplied by the inverse of the a priori covariance matrix. Compared to the reverse correlation method, we are no longer limited to finding only one of the relevant directions triggered covariance method requires better sampling of distributions of inputs conditional on a spike. In this paper we investigate whether it is possible to lift the requirement for stimuli to be Gaussian. When using natural stimuli, which are certainly non-Gaussian, the RS cannot be found by the spike-triggered covariance method. Similarly, the reverse correlation method does not give the correct RF, even in the simplest case where the input-output function (1) depends only on one projection. However, vectors that span the RS are clearly special directions in the stimulus space. This notion can be quantified by Shannon information, and

& "! . However because of the necessity to probe a two-point correlation function, the spike-


an optimization problem can be formulated to find the RS. Therefore the current implementation of the dimensionality reduction idea is complimentary to the clustering of stimuli done in the information bottleneck method [17]; see also Ref. [18]. Non­information also been proposed [19]. We illustrate how the optimization scheme of maximizing information as function of direction in the stimulus space works with natural stimuli for model orientation sensitive cells with one and two relevant directions, much like simple and complex cells found in primary visual cortex. It is also possible to estimate average errors in the reconstruction. The advantage of this optimization scheme is that it does not rely on any specific statistical properties of the stimulus ensemble, and can be used with natural stimuli. 2 Information as an objective function When analyzing neural responses, we compare the a priori probability distribution of all presented stimuli with the probability distribution of stimuli which lead to a spike. For Gaussian signals, the probability distribution can be characterized by its second moment, the covariance matrix. However, an ensemble of natural stimuli is not Gaussian, so that neither second nor any other finite number of moments is sufficient to describe the probability distribution. In this situation, the Shannon information provides a convenient way of comparing two probability distributions. The average information carried by the arrival time of one spike is given by [20]

based measures of similarity between probability distributions

DFE0V DFE UGYIQP'RTS and V have

 ¢¡¤£¦¥ §©¨

#



DFE UGYIQP'RTS

V ¡

DFE UGYIcPR3S DFE 4  V© 0V"!

(3) 

The information per spike, as written in (3) is difficult to estimate experimentally, since a model of how spikes were generated, i.e. the knowledge of low-dimensional RS. However it is possible to calculate in a model-independent way, if stimuli are presented

it requires either sampling of the high-dimensional probability distribution V or

 #¡¤£¦¥ §©¨

multiple times to estimate the probability distribution

 ¢¡$£%¥ §&¨

#(' DFEaGYIcPR3S DFEaGYIcPR3SbU©VV  ¡0)

DFEHGCIQPR3SbU©VV21435 DFEHGCIQP'RTS

DFEHGCIQP'RTSbU0V

. Then,

2

DFE UGCIQP'RTS

(4)

where the average is taken over #¡¤£¦¥ presented stimuli. Note that for a finite dataset of 6

 ¦¡$£¦¥ §&¨ 87 is the number of different stimuli,

repetitions, the obtained value

¡@9¤¥ ACB¦D ¥ ¡$£¦¥ §&¨FE

difference

¡¤£¦¥ §©¨

and 6  ¢¡$£%¥ §©¨

6

¡ E6 all§©¨ E

G

6 V

will be¡@9¤¥ on average larger than ACB¦D ¥ V , with

, where 6

is the number of elicited spikes [21] across7 all of the repetitions. The true value

"E V

"E

can also be found by extrapolating to 6IH [22]. The knowledge of the total

information per spike will characterize the quality of the reconstruction of the neuron's input-output relation. Having in mind a model in which spikes are generated according to projection onto a lowdimensional subspace, we start by projecting all of the presented stimuli on a particular

DQP E¤R UGYIcPR3S E$R §

(5)

direction

$( V 

UGCIQP'RTSWVin D E¤R

theP stimulus space, andV form probability distributions

,  . The information

    9DR E$R UGCIQPR3S

P

V ¡5

E¤R § V #YX VE#TS¤U

 $( V V #TS¤U

D E¤R UGYIcPR3S D E¤R 2

P

V© P V"!

provides an invariant measure of how much the occurrence of a spike is determined by projection on the direction . It is a function only of direction in the stimulus space and does not change when vector is multiplied by a constant. This  can be seen by noting that



D5a"PE$R DeP E¤R

for any probability distribution and any constant ` ,

  ¡$£%¥ §©¨

 &.

along any vector,

Vhg

9E  

  ¡$£¦¥ §&¨

V #b`dc

f` . When evaluated

¦V

total information can be recovered along one



particular direction only if

#   Theand !,

the RS is one-dimensional.


By analogy with (5), one could also calculate information

directions

D P ¢ ¤£¤£¤£ ¢ ¢ P ¥

8

   

B

2¦44'4

9E  2¦44'42   

 

¡ 

V

along a set of several

based on the multi-point probability distributions:  

V # ¨ ¦© E¤R § §¦ ¦ UGCIQPR3SWV 2 D



directions

P P

E ¦ 8 B"V # R

S U

b( V

¢ ¤£¤£¤£ ¢ ¢ ¥

 

§¦ R

 

E UGYIcPR3S 8 B S ¨ ¦© U E$R § ¦

 

b( V ¦ V  4

If we are successful in finding all of the

6

& '"!

in the input-output relation (1),

then the information evaluated along the found set will be equal to the total information RS, the answer is, of course, smaller than and is quadratic in deviations U . One can therefore find the RS by maximizing information with respect to  vectors simultaneously. The information does not increase if more vectors outside the RS are included into the calculation. On the other hand, the result of optimization with respect to the number of vectors  may deviate from the RS if stimuli are correlated. The deviation is also   ¡$£%¥ §©¨

. When we calculate information along a set of  vectors that are slightly off from the  ¦¡$£%¥ §&¨  ¦

6

proportional to a weighted average of

DFEaGYIcPR3S3Ub  2`4'44'2 ! T !YV § DFEaGYIcPR3S3UT  2`4'4'42

 

E

!  'A$! V

. For

uncorrelated stimuli, any vector or a set of vectors that maximizes

To find the RS, we first maximize

V

9E   V

belongs to the RS.

  ¡$£¦¥ §&¨

, and compare this maximum with , which is

estimated according to (4). If the difference exceeds that expected from finite sampling corrections, we increment the number of directions with respect to which information is simultaneously maximized.

9E 

Pd 



The information computed 

V

as defined by (5) is a continuous function, whose gradient can be

  9DR E CP $R

D E$R)UGCIQP'RTS

P

VPf 

V 4 X V HS  UR 2 GCIQP'RTSWV § URFV aS ! R

   DePE$R #



Since information does not change with the length of the vector,

 (6) (which can

also be seen from (6) directly), unnecessary evaluations of information for multiples of used a combination of gradient ascent and simulated annealing algorithms: successive line maximizations were done along the direction of the gradient. During line maximizations, a point with a smaller value of¦# information%$was accepted according to Boltzmann statistics,

are avoided by maximizing along the gradient. As an optimization algorithm, we have

S"! I E¤  E § E   ¦

with probability   

V VYV© ! . The effective temperature T is reduced upon

completion of each line maximization. 3 Discussion



We tested the scheme of looking for the most informative directions on model neurons that respond to stimuli derived from natural scenes. As stimuli we used patches of digitized to 8-bit scale photos, in which no corrections were made for camera's light intensity transformation function. Our goal is to demonstrate that even though spatial correlations present in natural scenes are non-Gaussian, they can be successfully removed from the estimate of vectors defining the RS. 3.1 Simple Cell Our first example is taken to mimic properties of simple cells found in the primary visual

cortex. A model phase and orientation sensitive cell has a single relevant direction

shown in Fig. 1(a). A given frame



leads to a spike if projection

&   !

reaches a

threshold value & in the presence of noise:

DFEHGCIQPR3S3U©VV(' DFEHGCIQPR3S

X    ! V # E 0)S

E §    ! &2143 TV V

  !W#  (9&   ! 2

(7)


Figure 1:¡@9¡ Analysis of a model simple cell with RF shown in (a). The spike-triggered average is shown in (b). Panel¢ (c)  shows¡$9¡ an attempt to remove correlations accord-

¦©¨ ¦ ¦  A 

ing to reverse correlation method, £¥¤§¦ c ; (d) vector



£

found by maximizing

information; (e) The probability of a spike

§

and &

F#

!EV

A 

DFEHGCIQP'RTS3U¥c4 

DFEHGCIQPR3S3U (p V

(crosses) is compared to

A  AQ¥ 

b  !

¥c4£ E

§



used in generating spikesAQ¥(solid line). Parameters

 V 

AQ¥  [ A  and







#   V

are the maximum and minimum values of

over the ensemble of presented stimuli.] (f) Convergence of the algorithm according to

information

V

and projection

  as a function of inverse effective temperature $ c .

9E  

 & (3   !

where GaussianR random variable 3 of variance

)

VF#

for

E¤R £ gi¥

 models additive noise, and function

, and zero otherwise. Together with the RF

&   !

, the parameters &

for threshold and the noise variance  determine the input-output function.

The spike-triggered average (STA), shown in Fig. 1(b), is broadened because of spatial correlations present in natural stimuli. If stimuli were drawn from a Gaussian probability distribution, they could be decorrelated by multiplying by the inverse of the a priori covariance matrix, according to the reverse correlation method. The procedure is not valid for non-Gaussian stimuli and nonlinear input-output functions (1). The result of such a decorrelation is shown in Fig. 1(c). It is clearly missing the structure of the model filter. However, it is possible to obtain a good estimate of it by maximizing information directly, see panel (d). A typical progress of the simulated annealing algorithm with decreasing temperature $ is shown  in panel (e). There we plot both the information along the vector,

 ¡$9¡ 

and its projection on

& ¥Q27¥T¥3¥

set, see below. In the example shown in Fig. 1 there were

probability of spike !

¥Q4!E

. The final value of projection depends on the size of the data per frame. Having reconstructed the RF, one can proceed to

!#" spikes with average

sampleA  nonlinear input-output function. This is done by constructing histograms for A  A 

and

V

of projections onto vector



A 

information, and taking their ratio. In Fig. 1(e) we compare

with the probability

3  ! V

DFEaGYIcPR3SbU

found by maximizing

(  V

(crosses)

DFE (5 V

the

DFEDFEaGYIcPR3S3UcUGYIcPR3S  ( 



used in the model (solid line).

3.2 Estimated deviation from the optimal direction When information is calculated with respect to a finite data set, the vector

 

mizes will deviate from the true RF

&   !

. The deviation U

  & § #     !



which maxi-

arises because the

probability distributions are estimated from experimental histograms and differ from the


1

0.95 e  vmax 0.9

1

0.85

0.8 0 1 2

N-1 spike 3 10-5

Figure 2: Projection of vector

 A 

that maximizes information on¡$£%¥RF §&¨

function of the number of spikes to show the linear scaling in #6

£ &   !

is plotted as a

(solid line is a fit).

distributions found in the limit on infinite data size. For a simple cell, the quality of recon-

¡  

  !

struction can be characterized by the projection

¡ 

U , where both



¡

are normalized, and U is by definition orthogonal to

 2¦£¢is# where

 



(   !p#&

The deviation U

¢

 and  c ,

 &

£ §   !.



¡  ¢

&

the Hessian of information. Its structure is similar to that of a covariance matrix:

  DFE$R)UGCIQP'RTS

R

V E

¥

¡

¦  

URFV § UR URFV aS aS V

¦ V

 

E X G $R S (8)  G

£ V   R

¤

DFE$RDFEUGCIQPR3SV

the optimal direction. Here in order to evaluate S¤U  

 §¦©¨ #



 

When averaged over possible outcomes of N trials, ¡the gradient  of information  is zero for to know the varianceR of the gradient of . By discretizing both the space of stimuli and possible projections , and assuming that the probability of generating a spike is indepenan expected error in the reconstruction of the optimal filter is inversely proportional to the number of spikes and is given by:

V     V c S c ! , we need



¢   £¢¦

¡ E ¡$£%¥ §&¨ E dent for different bins, one could obtain that S   ¦   fV 6 G V . Therefore

£ §

 & (Q   !

£

E S U



 

¡ V

¦¨



2

!

# E ¡$£%¥ §©¨ E (9) 6

  c ! G

  

where

¦¨

means that  the trace is taken in the subspace orthogonal to the model filter, since

by definition U

 (@&  !F#

¥



. In Fig. 2 we plot the average projection of the normalized

reconstructed vector 3.3 Complex Cell on the RF

&

  !

, and show that it scales with the number of spikes.

A sequence of spikes from a model cell with two relevant directions was simulated by projecting each of the stimuli on vectors that differ by e in their spatial phase, taken to mimic properties of complex cells,  see Fig. 3.  A particular¡ frame leads to a spike according

E

& & §

to a logical OR, that is if either

threshold value

DFEHGCIQP'RTSbU0VV

!

3 ! #  (   !    !, T ! #  (  ¡ !

,

§

, or T¡ !

¡ V

exceeds a

in the presence of noise. Similarly to (7),

 #"V

DFEHGCIQP'RTS # X    !  ¡ ! V #TS    ! )

E 2 E7U U § §

& 3   ()

E7U U § § ¡ ! & 3 V 2

(10)

where 3 and 3 are independent Gaussian variables. The sampling of this input-output function by our particular set of natural stimuli is shown in Fig. 3(c). Some, especially

large, combinations of values of

b  ! T¡ !

and are not present in the ensemble.

  ¡


We start by maximizing information with respect to one direction. Contrary to analysis for a simple cell, one optimal direction recovers only about 60% of the total information per spike. This is significantly different from the total for stimuli drawn from natural scenes, where due to correlations even a random vector has a high probability of explaining 60% of total information per spike. We therefore go on to maximize information with respect to two directions. An example of the reconstruction of input-output function of a

  ¡$£¦¥ §&¨

 9E  2    

complex cell is given in Fig. 3. Vectors

nal, and are also rotated with respect to

&

 

  !  ¡thatHowever,

and

!

.

and

& ¡

maximize

¡

V

are not orthogo-

the quality of reconstruction

is independent of a particular choice of basis with the RS. The appropriate measure of similarity betweenP the two planes is the dot product of their normals. In the example of Fig. 3, Maximizing information with respect to two directions requires a significantly slower cooling rate, and consequently longer computational times.  However, an expected error in the roughly twice that for a simple cell given the same number of spikes. In this calculation

¥c4  ¢¤£ ¢¤£¨§

 

 ¡  ¢¦¥ ¢ ¥ ! ©  ! ¢   § (c ¢ P ! .

£ § P P

reconstruction,  ¢ £ ¢  £§  ¡  ¢¥ ¢ ¥ ! ¡  !, ¢

  §

(9 ¢ follows a 6 c ¡$£%¥ §©¨ behavior, similarly to (9), and is

£¦¥

(a)

10

there were ! spikes.

e(1) (b) e(2) (c) P(spike|s(1),s(2))

10

model

20 20

30 30 10 20 30 10 20 30

v 1 v (d) (e) 2 (f) P(spike|s v1,s v2)

10 10

20 20

reconstruction

30 30 10 20 30 10 20 30

Figure 3: Analysis of a model complex cell with relevant directions

¥c4 E §

with the threshold &

&

(a) and (b). Spikes are generated according to an "OR" input-output functionA 

# 

A 

 V

AQ¥  and noise variance



#

"



 ¥Q4¥!  ¡ !

and

E

&

X shown¡in!VV.

§ 

! T

AQ¥ 

E 2

Panel (c) shows how the input-output function is sampled by our ensemble of stimuli. Dark

  ¡

pixels for large values of

we show vectors

 

and



3 !foundTby!

and ¡

  ¡

correspond to cases where

maximizing information

9E  2

$(

 3 



! V #

¡

!V Ttogether

 . Below, with the

DFE 2

and corresponding input-output function with respect to projections

¥

$( ¡ .

In conclusion, features of the stimulus that are most relevant for generating the response of a neuron can be found by maximizing information between the sequence of responses and the projection of stimuli on trial vectors within the stimulus space. Calculated in this manner, information becomes a function of direction in a stimulus space. Those directions that maximize the information and account for the total information per response of interest span the relevant subspace. This analysis allows the reconstruction of the relevant subspace without assuming a particular form of the input-output function. It can be strongly nonlinear within the relevant subspace, and is to be estimated from experimental histograms. Most importantly, this method can be used with any stimulus ensemble, even those that are strongly non-Gaussian as in the case of natural images. Acknowledgments We thank K. D. Miller for many helpful discussions. Work at UCSF was supported in part by the Sloan and Swartz Foundations and by a training grant from the NIH. Our collab-


oration began at the Marine Biological Laboratory in a course supported by grants from NIMH and the Howard Hughes Medical Institute. References [1] F. Rieke, D. A. Bodnar, and W. Bialek. Naturalistic stimuli increase the rate and efficiency of information transmission by primary auditory afferents. Proc. R. Soc. Lond. B, 262:259­265, (1995). [2] W. E. Vinje and J. L. Gallant. Sparse coding and decorrelation in primary visual cortex during natural vision. Science, 287:1273­1276, 2000. [3] F. E. Theunissen, K. Sen, and A. J. Doupe. Spectral-temporal receptive fields of nonlinear auditory neurons obtained using natural sounds. J. Neurosci., 20:2315­2331, 2000. [4] G. D. Lewen, W. Bialek, and R. R. de Ruyter van Steveninck. Neural coding of naturalistic motion stimuli. Network: Comput. Neural Syst., 12:317­329, 2001. [5] N. J. Vickers, T. A. Christensen, T. Baker, and J. G. Hildebrand. Odour-plume dynamics influence the brain's olfactory code. Nature, 410:466­470, 2001. [6] K. Sen, F. E. Theunissen, and A. J. Doupe. Feature analysis of natural sounds in the songbird auditory forebrain. J. Neurophysiol., 86:1445­1458, 2001. [7] D. L. Ringach, M. J. Hawken, and R. Shapley. Receptive field structure of neurons in monkey visual cortex revealed by stimulation with natural image sequences. Journal of Vision, 2:12­24, 2002. [8] W. E. Vinje and J. L. Gallant. Natural stimulation of the nonclassical receptive field increases information transmission efficiency in V1. J. Neurosci., 22:2904­2915, 2002. [9] D. L. Ruderman and W. Bialek. Statistics of natural images: scaling in the woods. Phys. Rev. Lett., 73:814­817, 1994. [10] D. J. Field. Relations between the statistics of natural images and the response properties of cortical cells. J. Opt. Soc. Am. A, 4:2379­2394, 1987. [11] P. Kara, P. Reinagel, and R. C. Reid. Low response variability in simultaneously recorded retinal, thalamic, and cortical neurons. Neuron, 27:635­646, 2000. [12] R. R. de Ruyter van Steveninck, G. D. Lewen, S. P. Strong, R. Koberle, and W. Bialek. Reproducibility and variability in neural spike trains. Science, 275:1805­1808, 1997. [13] F. Rieke, D. Warland, R. R. de Ruyter van Steveninck, and W. Bialek. Spikes: Exploring the neural code. MIT Press, Cambridge, 1997. [14] E. de Boer and P. Kuyper. Triggered correlation. IEEE Trans. Biomed. Eng., 15:169­179, 1968. [15] N. Brenner, W. Bialek, and R. R. de Ruyter van Steveninck. Adaptive rescaling maximizes information transmission. Neuron, 26:695­702, 2000. [16] R. R. de Ruyter van Steveninck and W. Bialek. Real-time performance of a movement-sensitive neuron in the blowfly visual system: coding and information transfer in short spike sequences. Proc. R. Soc. Lond. B, 234:379­414, 1988. [17] N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. In Proceedings of the 37th Allerton Conference on Communication, Control and Computing, edited by B. Hajek & R. S. Sreenivas. University of Illinois, 368­377, 1999. [18] A. G. Dimitrov and J. P. Miller. Neural coding and decoding: communication channels and quantization. Network: Comput. Neural Syst., 12:441­472, 2001. [19] L. Paninski. Convergence properties of some spike-triggered analysis techniques. In Advances in Neural Information Processing 15, edited by S. Becker, S. Thrun, and K. Obermayer, 2003. [20] N. Brenner, S. P. Strong, R. Koberle, W Bialek, and R. R. de Ruyter van Steveninck. Synergy in a neural code. Neural Comp., 12:1531-1552, 2000. [21] A. Treves and S. Panzeri. The upward bias in measures of information derived from limited data samples. Neural Comp., 7:399, 1995. [22] S. P. Strong, R. Koberle, R. R. de Ruyter van Steveninck, and W. Bialek. Entropy and information in neural spike trains. Phys. Rev. Lett., 80:197­200, 1998.


