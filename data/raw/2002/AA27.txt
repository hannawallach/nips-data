On the Dirichlet Prior and Bayesian
Regularization
Harald Steck
Articial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139
harald@ai.mit.edu
Tommi S. Jaakkola
Articial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139
tommi@ai.mit.edu
Abstract
A common objective in learning a model from data is to recover
its network structure, while the model parameters are of minor in-
terest. For example, we may wish to recover regulatory networks
from high-throughput data sources. In this paper we examine how
Bayesian regularization using a product of independent Dirichlet
priors over the model parameters aects the learned model struc-
ture in a domain with discrete variables. We show that a small
scale parameter { often interpreted as "equivalent sample size" or
"prior strength" { leads to a strong regularization of the model
structure (sparse graph) given a suÆciently large data set. In par-
ticular, the empty graph is obtained in the limit of a vanishing scale
parameter. This is diametrically opposite to what one may expect
in this limit, namely the complete graph from an (unregularized)
maximum likelihood estimate. Since the prior aects the parame-
ters as expected, the scale parameter balances a trade-o between
regularizing the parameters vs. the structure of the model. We
demonstrate the benets of optimizing this trade-o in the sense
of predictive accuracy.
1 Introduction
Regularization is essential when learning from nite data sets. In the Bayesian ap-
proach, regularization is achieved by specifying a prior distribution over the parame-
ters and subsequently averaging over the posterior distribution. This regularization
provides not only smoother estimates of the parameters compared to maximum
likelihood but also guides the selection of model structures.
It was pointed out in [6] that a very large scale parameter of the Dirichlet prior can
degrade predictive accuracy due to severe regularization of the parameter estimates.
We complement this discussion here and show that a very small scale parameter
can lead to poor over-regularized structures when a product of (conjugate) Dirichlet
priors is used over multinomial conditional distributions (Section 3). Section 4
demonstrates the eect of the scale parameter and how it can be calibrated. We
focus on the class of Bayesian network models throughout this paper.

2 Regularization of Parameters
We briey review Bayesian regularization of parameters. We follow the assump-
tions outlined in [6]: multinomial sample, complete data, parameter modularity,
parameter independence, and Dirichlet prior. Note that the Dirichlet prior over
the parameters is often used for two reasons: (1) the conjugate prior permits an-
alytical calculations, and (2) the Dirichlet prior is intimately tied to the desirable
likelihood-equivalence property of network structures [6]. The Dirichlet prior over
the parameters  j i is given by
p( X i j i ) =
(
P
x i  x i ; i )
Q
x i
( x i ; i )
Y
x i
 x i ; i 1
x i j i
; (1)
where  x i j i pertains to variable X i in state x i given that its parents  i are in joint
state  i . The number of variables in the domain is denoted by n, and i = 1; :::; n.
The normalization terms in Eq. 1 involve the Gamma function (). There are
a number of approaches to specifying the positive hyper-parameters  x i ; i of the
Dirichlet prior [2, 1, 6]; we adopt the common choice,
 x i ; i =   p(x i ;  i ); (2)
where p is a (marginal) prior distribution over the (joint) states, as this assignment
ensures likelihood equivalence of the network structures [6]. Due to lack of prior
knowledge, p is often chosen to be uniform, p(x i ;  i ) = 1= (jX i j  j i j), where jX i j,
j i j denote the number of (joint) states [1]. The scale parameter  of the Dirichlet
prior is positive and independent of i, i.e.,  =
P
x i ; i  x i ; i .
The average parameter value   x i j i , which typically serves as the regularized pa-
rameter estimate given a network structure m, is given by

 x i j i  E p( X i j i jD;m) [ x i j i ] = N x i ; i +  x i ; i
N  i +   i
; (3)
where N x i ; i are the cell-counts from data D; E[] is the expectation. Positive
hyper-parameters  x i ; i lead to regularized parameter estimates, i.e., the estimated
parameters become "smoother" or "less extreme" when the prior distribution p is
close to uniform. An increasing scale parameter  leads to a stronger regulariza-
tion, while in the limit  ! 0, the (unregularized) maximum likelihood estimate is
obtained, as expected.
3 Regularization of Structure
In the remainder of this paper, we outline eects due to Bayesian regularization of
the Bayesian network structure when using a product of Dirichlet priors. Let us
briey introduce relevant notation.
In the Bayesian approach to structure learning, the posterior probability of the
network structure m is given by p(mjD) = p(Djm)p(m)=p(D), where p(D) is the
(unknown) probability of given data D, and p(m) denotes the prior distribution over
the network structures; we assume p(m) > 0 for all m. Following the assumptions
outlined in [6], including the Dirichlet prior over the parameters , the marginal
likelihood p(Djm) = E p(jm) [p(Djm; )] can be calculated analytically. Pretending
that the (i.i.d.) data arrived in a sequential manner, it can be written as
p(Djm) =
N
Y
k=1
n
Y
i=1
N (k 1)
x k
i ; k
i
+  x k
i ; k
i
N (k 1)
 k
i
+   k
i
; (4)

where N (k 1) denotes the counts implied by data D (k 1) seen before step k along
the sequence (k = 1; :::; N ). The (joint) state of variable X i and its parents  i
occurring in the k th data point is denoted by x k
i ;  k
i . In Eq. 4, we also decomposed
the joint probability into a product of conditional probabilities according to the
Bayesian network structure m. Eq. 4 is independent of the sequential ordering of
the data points, and the ratio in Eq. 3 corresponds to the one in Eq. 4 when based
on data D (k 1) at each step k along the sequence.
3.1 Limit of Vanishing Scale-Parameter
This section is concerned with the limit of a vanishing scale parameter of the Dirich-
let prior,  ! 0. In this limit Bayesian regularization depends crucially on the
number of zero-cell-counts in the contingency table implied by the data, or in other
words, on the number of dierent congurations (data points) contained in the
data. Let the Eective Number of Parameters (EP) be dened as
d (m)
EP =
n
X
i=1
[
X
x i ; i
I(N x i ; i )
X
 i
I(N  i ) ]; (5)
where N x i ; i ; N  i are the (marginal) cell counts in the contingency table implied
by data D; m refers to the Bayesian network structure, and I() is an indicator
function such that I(z) = 0 if z = 0 and I(z) = 1 otherwise. When all cell
counts are positive, EP is identical to the well-known number of parameters (P),
d (m)
EP = d (m)
P =
P
i (jX i j 1)j i j, which play an important role in regularizing the
learned network structure. The key dierence is that EP accounts for zero-cell-
counts implied by the data.
Let us now consider the behavior of the marginal likelihood (cf. Eq. 4) in the limit
of a small scale parameter . We nd
Proposition 1: Under the assumptions concerning the prior distribution outlined
in Section 2, the marginal likelihood of a Bayesian network structure m vanishes in
the limit  ! 0 if the data D contain two or more dierent congurations. This
property is independent of the network structure. The leading polynomial order is
given by
p(Djm)   d (m)
EP as  ! 0; (6)
which depends both on the network structure and the data. However, the dependence
on the data is through the number of dierent data points only. This holds inde-
pendently of a particular choice of strictly positive prior distributions p(X i ;  i ). If
the prior over the network structures is strictly positive, this limiting behavior also
holds for the posterior probability p(mjD).
In the following we give a derivation of Proposition 1 that also facilitates the intuitive
understanding of the result. First, let us consider the behavior of the Dirichlet
distribution in the limit  ! 0. The hyper-parameters  x i ; i vanish when  ! 0,
and thus the Dirichlet prior converges to a discrete distribution over the parameter
simplex in the sense that the probability mass concentrates at a particular, randomly
chosen corner of the simplex containing  j i (cf. [9]). Since the randomly chosen
points (for dierent  i ; i) do not change when sampling (several) data points from
the distribution implied by the model, it follows immediately that the marginal
likelihood of any network structure vanishes whenever there are two or more dierent
congurations contained in the data.
This well-known fact also shows that the limit  ! 0 actually corresponds to a very
strong prior belief [9, 12]. This is in contrast to many traditional interpretations
where the limit  ! 0 is considered as "no prior information", often motivated
by Eq. 3. As pointed out in [9, 12], the interpretation of the scale parameter 

as "equivalent sample size" or as the "strength" of prior belief may be misleading,
particularly in the case where  x i ; i < 1 for some congurations x i ;  i . A review
of dierent notions of "noninformative" priors (including their limitations) can be
found in [7]. Note that the noninformative prior in the sense of entropy is achieved
by setting  x i ; i = 1 for each x i ;  i and for all i = 1; :::; n. This is the assignment
originally proposed in [2]; however, this assignment generally is inconsistent with
Eq. 2, and hence with likelihood equivalence [6].
In order to explain the behavior of the marginal likelihood in leading order of the
scale parameter , the properties of the Dirichlet distribution are not suÆcient by
themselves. Additionally, it is essential that the probability distribution described
by a Bayesian network decomposes into a product of conditional probabilities, and
that there is a Dirichlet prior pertaining to each variable for each parent congu-
ration. All these Dirichlet priors are independent of each other under the standard
assumption of parameter independence. Obviously, the ratio (for given k and i) in
Eq. 4 can only vanish in the limit  ! 0 if N (k 1)
x k
i ; k
i
= 0 while N (k 1)
 k
i
> 0; in other
words, the parent-conguration  k
i must already have occurred previously along
the sequence ( k
i is "old"), while the child-state x k
i occurs simultaneously with this
parent-state for the rst time (x k
i ;  k
i is "new"). In this case, the leading polyno-
mial order of the ratio (for given k and i) is linear in , assuming p(X i ;  i ) > 0;
otherwise the ratio (for given k and i) converges to a nite positive value in the
limit  ! 0. Consequently, the dependence of the marginal likelihood in leading
polynomial order on  is completely determined by the number of dierent cong-
urations in the data. It follows immediately that the leading polynomial order in
 is given by EP (cf. Eq. 5). This is because the rst term counts the number of
all the dierent joint congurations of X i ;  i in the data, while the second term
ensures that EP counts only those congurations where (x k
i ;  k
i ) is "new" while  k
i
is "old".
Note that the behavior of the marginal likelihood in Proposition 1 is not entirely
determined by the network structure in the limit  ! 0, as it still depends on the
data. This is illustrated in the following example. First, let us consider two binary
variables, X 0 and X 1 , and the data D containing only two data points, say (0; 0)
and (1; 1). Given data D, three Dirichlet priors are relevant regarding graph m 1 ,
X 0 ! X 1 , but only two Dirichlet priors pertain to the empty graph, m 0 . The
resulting additional "exibility" due to an increased number of priors favours more
complex models: p(Djm 1 )  , while p(Djm 0 )   2 . Second, let us now assume
that all possible congurations occur in data D. Then we still have p(Djm 0 )   2
for the empty graph. Concerning graph m 1 , however, the marginal likelihood now
also involves the vanishing terms due to the two priors pertaining to  X1 jX0=0 and
 X1 jX0=1 , and it hence becomes p(Djm 1 )   3 .
This dependence on the data can be formalized as follows. Let us compare the
marginal likelihoods of two graphs, say m + and m . In particular, let us consider
two graphs that are identical except for a single edge, say A   B between the
variables A and B. Let the edge be present in graph m + and absent in m . The
fact that the marginal likelihood decomposes into terms pertaining to each of the
variables (cf. Eq. 4) entails that all the terms regarding the remaining variables
cancel out in the Bayes factor p(Djm + )=p(Djm ), which is the standard relative
Bayesian score. With the denition of the Eective Degrees of Freedom (EDF) 1
dEDF = d (m + )
EP d (m )
EP ; (7)
we immediately obtain from Proposition 1 that p(Djm + )=p(Djm )   dEDF in the
1 Note that EDF are not necessarily non-negative.

limit  ! 0, and hence
Proposition 2: Let m + and m be the two network structures as dened above.
Let the prior belief be given according to Eq. 2. Then in the limit  ! 0:
log p(Djm + )
p(Djm ) !

1 if dEDF > 0;
+1 if dEDF < 0: (8)
The result holds independently of a particular choice of strictly positive prior distri-
butions p(X i ;  i ). If the prior over the network structures is strictly positive, this
limiting behavior also holds for the posterior ratio.
A positive value of the log Bayes factor indicates that the presence of the edge
A   B is favored, given the parents A ; conversely, a negative relative score
suggests that the absence of this edge is preferred. The divergence of this relative
Bayesian score implies that there exists a (small) positive threshold value O > 0
such that, for any  < O , the same graph(s) are favored as in the limit.
Since Proposition 2 applies to every edge in the network, it follows immediately
that the empty (complete) graph is assigned the highest relative Bayesian score
when EDF are positive (negative). Regularization of network structure in the case
of positive EDF is therefore extreme, permitting only the empty graph. This is
precisely the opposite of what one may have expected in this limit, namely the
complete graph corresponding to the unregularized maximum likelihood estimate
(MLE). In contrast, when EDF are negative, the complete graph is favored. This
agrees with MLE.
Roughly speaking, positive (negative) EDF correspond to large (small) data sets.
It is thus surprising that a small data set, where one might expect an increased
restriction on model complexity, actually gives rise to the complete graph, while
a large data set yields the { most regularized { empty graph in the limit  ! 0.
Moreover, it is conceivable that a "medium" sized data set may give rise to both
positive and negative EDF. This is because the marginal contingency tables implied
by the data with respect to a sparse (dense) graph may contain a small (large)
number of zero-cell-counts. The relative Bayesian score can hence become rather
unstable in this case, as completely dierent graph structures are optimal in the
limit  ! 0, namely graphs where each variable has either the maximal number of
parents or none.
Note that there are two reasons for the hyper-parameters  x i ; i to take on small
values (cf. Eq. 2): (1) a small equivalent sample size , or (2) a large number of
joint states, i.e. jX i j  j i j  , due to a large number of parents (with a large
number of states). Thus, these hyper-parameters can also vanish in the limit of a
large number of congurations (x; ) even though the scale parameter  remains
xed. This is precisely the limit dening Dirichlet processes [4], which, analogously,
produce discrete samples. With a nite data set and a large number of joint cong-
urations, only the typical limit in Proposition 2 is possible. This follows from the
fact that a large number of zero-cell-counts forces EDF to be negative. The sur-
prising behavior implied by Proposition 2 therefore does not carry over to Dirichlet
processes. As found in [8], however, the use of a product of Dirichlet process priors
in nonparametric inference can also lead to surprising eects.
When dEDF = 0, it is indeed true that the value of the log Bayes factor can converge
to any (possibly nite) value as  ! 0. Its value is determined by the priors
p(X i ;  i ), as well as by the counts implied by the data. The value of the Bayes
factor can be therefore easily set by adjusting the prior weights p(x i ;  i ).
3.2 Large Scale-Parameter
In the other limiting case, where  !1, the Bayes factor approaches a nite value,
which in general depends on the given data and on the prior distributions p(X i ;  i ).

50 100 150 200 250 300
­1
­0.5
0.5
1
1.5
2
a
z=3
z=2
z=0
z=4
lBF
Figure 1: The log Bayes factor (lBF) is depicted as a function of the scale parameter
. It is assumed that the two variables A and B are binary and have no parents;
and that the "data" imply the contingency table: NA=0;B=0 = NA=1;B=1 = 10 + z
and NA=1;B=0 = NA=0;B=1 = 10 z, where z is a free parameter determining the
statistical dependence between A and B. The prior p(X i ;  i ) was chosen to be
uniform.
This can be seen easily by applying the Stirling approximation in the limit  !1
after rewriting Eq. 4 in terms of Gamma functions (cf. also [2, 6]). When the
popular choice of a uniform prior p(X i ;  i ) is used [1], then
log p(Djm + )
p(Djm ) ! 0 as  !1 ; (9)
which is independent of the data. Hence, neither the presence nor the absence of
the edge between A and B is favored in this limit. Given a uniform prior over the
network structures, p(m) =const, the posterior distribution p(mjD) over the graphs
thus becomes increasingly spread out as  grows, permitting more variable network
structures.
The behavior of the Bayes factor between the two limits  ! 0 and  !1 is exem-
plied for positive EDF in Figure 1: there are two qualitatively dierent behaviors,
depending on the degree of statistical dependence between A and B. A suÆciently
weak dependence results in a monotonically increasing Bayes factor which favors the
absence of the edge A   B at any nite value of . In contrast, given a suÆciently
strong dependence between A and B, the log Bayes factor takes on positive values
for all (nite)  exceeding a certain value + of the scale parameter. Moreover, +
grows as the statistical dependence between A and B diminishes. Consequently,
given a domain with a range of degrees of statistical dependences, the number of
edges in the learned graph increases monotonically with growing scale parameter 
when each variable has at most one parent (i.e., in the class of trees or forests). This
is because increasingly weaker statistical dependencies between variables are recov-
ered as  grows; the restriction to forests excludes possible "interactions" among
(several) parents of a variable. As suggested by our experiments, this increase in
the number of edges can also be expected to hold for general Bayesian network
structures (although not necessarily in a monotonic way).
This reveals that regularization of network structure tends to diminish with a grow-
ing scale parameter. Note that this is in the opposite direction to the regularization
of parameters (cf. Section 2). Hence, the scale parameter  of the Dirichlet prior
determines the trade-o between regularizing the parameters vs. the structure of the
Bayesian network model.
If a uniform prior over the network structures is chosen, p(m) =const, the above
discussion also holds for the posterior ratio (instead of the Bayes factor). The
behavior is more complicated, however, when a non-uniform prior is assumed. For
instance, when a prior is chosen that penalizes the presence of edges, the posterior

favours the absence of an edge not only when the scale parameter is suÆciently
small, but also when it is suÆciently large. This is apparent from Fig. 1, when the
log Bayes factor is compared to a positive threshold value (instead of zero).
4 Example
This section exemplies that the entire model (parameters and structure) has to
be considered when learning from data. This is because regularization of model
structure diminishes, while regularization of parameters increases with a growing
scale parameter  of the Dirichlet prior, as discussed in the previous sections.
When the entire model is taken into account, one can use a sensitivity analysis in
order to determine the dependence of the learned model on the scale parameter
, given the prior p(X i ;  i ) (cf. Eq. 2). The inuence of the scale parameter 
on predictive accuracy of the model can be assessed by cross-validation or, in a
Bayesian approach, prequential validation [11, 3]. Another possibility is to treat
the scale parameter  as an additional parameter of the model to be learned from
data. Hence, prior belief regarding the parameters  can then enter only through
the (normalized) distributions p(X i ;  i ). However, note that this is suÆcient to
determine the (average) prior parameter estimate   (cf. Eq. 3), i.e., when N = 0.
Assuming an (improper) uniform prior distribution over , its posterior distribution
is p(jD) / p(Dj), given data D. Then D = argmax  p(Dj), where p(Dj) =
P
m p(Dj; m) p(m) 2 can be calculated exactly if the summation is feasible (like in
the example below). Alternatively, assuming that the posterior over  is strongly
peaked, the likelihood may also be approximated by summing over the k most likely
graphs m only (k = 1 in the most extreme case; empirical Bayes). Subsequently,
model structure m and parameters 
 can be learned with respect to the Bayesian
score employing D .
In the following, the eect of various values assigned to the scale parameter  is
exemplied concerning the data set gathered from Wisconsin high-school students
by Sewell and Shah [10]. This domain comprises 5 discrete variables, each with
2 or 4 states; the sample size is 10,318. In this small domain, exhaustive search
in the space of Bayesian network structures is feasible (29; 281 graphs). Both the
prior distributions p(m) for all m and p(X i ;  i ) are chosen to be uniform. Figure 2
shows that the number of edges in the graph with the highest posterior probability
grows with an increasing value of the scale parameter, as expected (cf. Section 3).
In addition, cross-validation indicates best predictive accuracy of the learned model
at   100; :::; 300, while the likelihood p(Dj) takes on its maximum at D  69.
Both approaches agree on the same network structure, which is depicted in Fig. 3.
This graph can easily be interpreted in a causal manner, as outlined in [5]. 3 We
note that this graph was also obtained in [5] due, however, to additional constraints
concerning network structure, as a rather small prior strength of  = 5 was used. For
comparison, Fig. 3 also shows the highest-scoring unconstraint graph due to  = 5,
which does not permit a causal interpretation, cf. also [5]. This illustrates that
the "right" choice of the scale parameter  of the Dirichlet prior, when accounting
for both model structure and parameters, can have a crucial impact on the learned
network structure and the resulting insight in the ("true") dependencies among the
variables in the domain.
2 We assume that m and  are independent a priori, p(mj) = p(m).
3 Since we did not impose any constraints on the network structure, unlike to [5],
Markov-equivalence leaves the orientation of the edge between the variables IQ and CP
unspecied.

 a. XV 5 p(Dj)
p(DjD )
5 6 0:045 10 10
50 7 0:044 0:13
100 7 0:040 0:05
200 7 0:040 10 14
300 7 0:040 10 30
500 7 0:042 10 65
1; 000 8 0:047 10 151
Figure 2: As a function of : number of
arcs (a.) in the highest-scoring graph;
average KL divergence in 5-fold cross-
validation (XV 5), std= 0:006; likelihood
of  when treated as an additional model
parameter (D = 69).
PE
CP
IQ
SEX
SES
PE
CP
IQ
SEX
SES
SEX: gender of student
CP: college plans
PE: parental encouragement
SES: socioeconomic status
IQ: intelligence quotient
Figure 3: Highest-scoring (unconstraint)
graphs when  = 5 (left), and when  =
46; :::; 522 (right). Note that the latter
graph can also be obtained at  = 5 when
additional constraints are imposed on the
structure, cf. [5].
Acknowledgments
We would like to thank Chen-Hsiang Yeang and the anonymous referees for valuable
comments. Harald Steck acknowledges support from the German Research Foun-
dation (DFG) under grant STE 1045/1-1. Tommi Jaakkola acknowledges support
from Nippon Telegraph and Telephone Corporation, NSF ITR grant IIS-0085836,
and from the Sloan Foundation in the form of the Sloan Research Fellowship.
References
[1] W. Buntine. Theory renement on Bayesian networks. Conference on Uncer-
tainty in Articial Intelligence, pages 52{60. Morgan Kaufmann, 1991.
[2] G. Cooper and E. Herskovits. A Bayesian method for the induction of proba-
bilistic networks from data. Machine Learning, 9:309{47, 1992.
[3] A. P. Dawid. Statistical theory. The prequential approach. Journal of the Royal
Statistical Society, Series A, 147:277{305, 1984.
[4] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals
of Statistics, 1:209{30, 1973.
[5] D. Heckerman. A tutorial on learning with Bayesian networks. In M. I. Jordan
(Ed.), Learning in Graphical Models, pages 301{54. Kluwer, 1996.
[6] D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks:
the combination of knowledge and statistical data. Machine Learning, 20:197{
243, 1995.
[7] R. E. Kass and L. Wasserman. Formal rules for selecting prior distributions:
a review and annotated bibliography. Technical Report 583, CMU, 1993.
[8] S. Petrone and A. E. Raftery. A note on the Dirichlet process prior in Bayesian
nonparametric inference with partial exchangeability. Technical Report 297,
University of Washington, Seattle, 1995.
[9] J. Sethuraman and R. C. Tiwari. Convergence of Dirichlet measures and the
interpretation of their parameter. In S. S. Gupta and J. O. Berger (Eds.),
Statistical Decision Theory and Related Topics III, pages 305{15, 1982.
[10] W. Sewell and V. Shah. Social class, parental encouragement, and educational
aspirations. American Journal of Sociology, 73:559{72, 1968.
[11] M. Stone. Cross-validatory choice and assessment of statistical predictions.
Journal of the Royal Statistical Society, Series B, 36:111{47, 1974.
[12] S. G. Walker and B. K. Mallick. A note on the scale parameter of the Dirichlet
process. The Canadian Journal of Statistics, 25:473{9, 1997.

