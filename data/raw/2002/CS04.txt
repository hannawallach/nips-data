A Minimal Intervention Principle for Coordinated Movement

Emanuel Todorov Department of Cognitive Science University of California, San Diego todorov@cogsci.ucsd.edu Michael I. Jordan Computer Science and Statistics University of California, Berkeley jordan@cs.berkeley.edu

Abstract Behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail. Here we offer an explanation: we show that not only are variability and goal achievement compatible, but indeed that allowing variability in redundant dimensions is the optimal control strategy in the face of uncertainty. The optimal feedback control laws for typical motor tasks obey a "minimal intervention" principle: deviations from the average trajectory are only corrected when they interfere with the task goals. The resulting behavior exhibits task-constrained variability, as well as synergetic coupling among actuators--which is another unexplained empirical phenomenon.

1 Introduction Both the difficulty and the fascination of the motor coordination problem lie in the apparent conflict between two fundamental properties of the motor system: the ability to accomplish its goal reliably and repeatedly, and the fact that it does so with variable movements [1]. More precisely, trial-to-trial fluctuations in individual degrees of freedom are on average larger than fluctuations in task-relevant movement parameters--motor variability is constrained to a redundant or "uncontrolled" manifold [16] rather than being suppressed altogether. This pattern has now been observed in a long list of behaviors [1, 6, 16, 14]. In concordance with such naturally occurring variability, experimentally induced perturbations [1, 3, 12] are compensated in a way that maintains task performance rather than a specific stereotypical movement pattern. This body of evidence is fundamentally incompatible with standard models of motor coordination that enforce a strict separation between trajectory planning and trajectory execution [2, 8, 17, 10]. In such serial planning/execution models, the role of the planning stage is to resolve the redundancy inherent in the musculo-skeletal system, by replacing the behavioral goal (achievable via infinitely many movement trajectories) with a specific "desired trajectory." Accurate execution of the desired trajectory guarantees achievement of the goal, and can be implemented with relatively simple trajectory-tracking algorithms. While this approach is computationally viable (and often used in engineering), the numerous observations of task-constrained variability and goal-directed corrections indicate that the online execution mechanisms are able to distinguish, and selectively enforce, the details that are crucial for the achievement of the goal. This would be impossible if the behavioral


goal were replaced with a specific trajectory. Instead, these observations imply a very different control scheme, one which pursues the behavioral goal more directly. Efforts to delineate such a control scheme have led to the idea of motor synergies, or high-level "control knobs," that have invariant and predictable effects on the task-relevant movement parameters despite variability in individual degrees of freedom [9, 11]. But the computational underpinnings of such an approach--how the synergies appropriate for a given task and plant can be constructed, what control scheme is capable of utilizing them, and why the motor system should prefer such a control scheme in the first place--remain unclear. This general form of hierarchical control implies correlations among the control signals sent to multiple actuators (i.e., synergetic coupling) and a corresponding reduction in control space dimesionality. Such phenonema have indeed been observed [4, 18], but the relationship to the hypothetical functional synergies remains to be established. In this paper we aim to resolve the apparent conflict at the heart of the motor coordination problem, and clarify the relationship between variability, task goals, and motor synergies. We treat motor coordination within the framework of stochastic optimal control, and postulate that the motor system approximates the best possible control scheme for a given task. Such a control scheme will generally take the form of a feedback control law. Whenever the task allows redundant solutions, the initial state of the plant is uncertain, the consequences of the control signals are uncertain, and the movement duration exceeds the shortest sensory-motor delay, optimal performance is achieved by a feedback control law that resolves redundancy moment-by-moment--using all available information to choose the most advantageous course of action under the present circumstances. By postponing all decisions regarding movement details until the last possible moment, this control law takes advantage of the opportunities for more successful task completion that are constantly being created by unpredictable fluctuations away from the average trajectory. Such exploitation of redundancy not only results in higher performance, but also gives rise to task-constrained variability and motor synergies--the phenomena we seek to explain. The present paper is related to a recent publication targeted at a neuroscience audience [14]. Here we provide a number of technical results missing from [14], and emphasize the aspects of our work that are most likely to be of interest to the computational modeling community.

2 The Minimal Intervention principle Our general explanation of the above phenomena follows from an intuitive property of optimal feedback controllers which we call the "minimal intervention" principle: deviations from the average trajectory are corrected only when they interfere with task performance. If this principle holds, and the noise perturbs the system in all directions, the interplay of the noise and control processes will result in variability which is larger in task-irrelevant directions. At the same time, the fact that certain deviations are not being corrected implies that the corresponding control subspace is not being used--which is the phenomenon typically interpreted as evidence for motor synergies [4, 18]. Why should the minimum intervention principle hold? An optimal feedback controller has nothing to gain from correcting task-irrelevant deviations, because its only concern is task performance and by definition such deviations do not interfere with performance. On the other hand, generating a corrective control signal can be detrimental, because: 1) the noise in the motor system is known to be multiplicative [13] and therefore could increase; 2) the cost being minimized most likely includes a control-dependent effort penalty which could also increase.


We now formalize the notions of "redundancy" and "correction," and show that for a surprisingly general class of systems they are indeed related--as our intuition suggests. 2.1 Local analysis of a general class of optimal control problems Redundancy is not easy to define. Consider the task of reaching, which requires the fingertip to be at a specified target at some point in time . At time , all arm configurations for which the fingertip is at the target are redundant. But at times different from this geometric approach is insufficient to define redundancy. Therefore we follow a more general approach.

     

Consider a system with state

¢¤!"¡#¢¥¤§¦$"¢¤§¦%¦'&)(

E



¡£¢¥¤§¦©¨

, control

¢¤§¦©¨

, instantaneous scalar cost

, and dynamics

0

¡21435¢¥¤!"¡6"7¦ ¤98A@B¢C¤!§¡6%D¦

0 0FE

where

¢¤§¦)¨GH

is multidimensional standard Brownian motion. Control signals are

generated by a feedback control law, which can be any mapping of the form

¢¤!"¡#¢¥¤§¦"¦

Q I¢¥¤§¦P1

. The analysis below heavily relies on properties of the optimal cost-to-go func-

tion, defined as

RTS ¢¥¤!§¡U¦V1XW#Ya` c¥ehtsvuw ¢¥xy§¡£¢xp¦$ ¢Cxy§¡£¢xp¦§¦§¦ x bdcegfgehpirq  Q

where the minimum is achieved by the optimal control law

Q S

0 ¢¥¤!§¡£¢¤§¦%¦

.

Suppose that in a given task the system of interest (driven by the optimal control law)

generates an average trajectory

¡©¢¤§¦R

. On a given trial, let

Sbe¢¡©8)¡U¦VR R S

¡

be the deviation form the

average trajectory at time . Let

the deviation

¡

; i.e.,

redundancy: the deviation

 ¢¡U¦1

R S¡

¤

SR

the change in the optimal cost-to-go

¢¡d¦

R S

due to

is redundant iff

 ¢¡U¦#1G(

S

. Now we are ready to define

. Note that our definition

reduces to the intuitive geometric definition at the end of the movement, where the cost function and optimal cost-to-go are identical. To define the notion of "correction," we need to separate the passive and active dynamics:

35¢¤!§¡6%D¦51¢¥¤!§¡U¦U8v¢¤!"¡U¦T

The (infinitesimal) expected change in

be identified:



signal is naturally defined as

¡d41¢C¤! ¡©8v¡U¦ ¢¤!due

¡

to the control

B1

y"9¢¡U¦V1"¡  d¡Ue.

Q S ¡I8v¡U¦

The corrective action of the control .

Q S

¢¤! ¡I8¡U¦

 R S

can now

In order to relate the quantities

something about the optimal control law

optimal control law

f

where

tion

%g6W#Ya`R¢¤!"¡U¦¢¤!§¡U¦.

R S qS



¢¥¤!§¡£¢¤§¦"¦

Q S

 ¢¡U¦Q R S

S

and

y"9¢d¡U¦

, we obviously need to know

. For problems in the above general form, the

is given [7] by the minimum

¢h¤!§¡6%D¦d8A35¢¥¤!"¡6"7¦ji ¢¤!§¡U¦d8lkmon  prqs@B¢C¤!§¡6"7¦ji ¢¤!"¡U¦T@B¢¤!"¡6"7¦§u

and

R S qtq ¢¥¤!§¡U¦

are the gradient and Hessian of the optimal cost-to-go func-

To be able to minimize this expression explicitly, we will restrict the class of

@)¢C¤!§¡6%D¦v1 wyxrz ¢¤!"¡U¦T {s{|{ x ¢¥¤!"¡U¦}~ 

¢¤!§¡6%D¦v1 ¢¤!"¡U¦U8lkm  i ¢C¤!§¡U¦}H

RTSq

f RTSqtq

problems to

The matrix notation means that the

¥

column of

@ xy ¢C¤!§¡U¦}

is . Note that the latter

formulation is still very general, and can represent realistic musculo-skeletal dynamics and motor tasks.


Using the fact1 that

@@ i ¡  H x D x 1

inating terms that do not depend on becomes

£¢ z i i nf |p¢¥¤§¦¦1 nf |p¢¨¦©¤ ¦,

and



and elim-

, the expression that has to be minimized w.r.t



 i ¢¤!§¡U¦ i ¢¤!§¡U¦d8lkm  i  ¢¥¤!§¡U¦U8 R S q 

Therefore the optimal control law is

Q S



¢H z x' ¢C¤!§¡U¦ i ¢¤!"¡U¦ x' ¢C¤!§¡U¦  ! cwfq h  R S qtq

¢C¤!§¡U¦1 #" ¢¤!"¡U¦$ ¢¤!"¡U¦ i ¢¤!"¡U¦ R S q z

We now return to the relationship between "redundancy" and "correction." The time in-

dex

¤

will be suppressed for clarity. We expand the optimal cost-to-go to second order:

¢¡d¦ d¡

, also expand its gradient

R S q R S qtq

¡9 '% y¢¡D¦("qS ¢¡U¦ $ ¢¡d¦ i ¢R ¢¡U¦d8

to first order:

R S

¢¡£8¡U¦&%R ¢¡I8¡U¦V8¡ i ¢¡D¦8¡d¡i,

R¢¡I8d¡U¦ S

R S q

R S

as being constant in a small neighborhood of . The effect of the control signal becomes

Substituting in the above defini-

tions yields

| ¢C¡U¦)% C¡6R}Sq ¢¡U¦U8 %9¢C¡U¦)% C¡6R}Sq ¢¡U¦U8 R}S

z

§% ¢¡U¦68 ¢¡d¦¡¢¡U¦T¡U¦.

qS

qtqR

and approximate all other quantities

qtqS

sian

corrective action. Furthermore,

¢¡d¦8qtq

R S

¢¡U¦R

qtqS R¢¡U¦T¡

S ¢C¡U¦

¡687de@9qtq

RTSRTSqtq ¢¡U¦¡Ue ¢¡U¦¡Ue0 cq h cq 2143 0 cq 65h

stands for

¡ BAi 7

.

! h

where the weighted dot-product notation

Thus both

R S q

and

1 (

y"U¢¡U¦

are dot-products of the same two vectors. When

--which can happen for infinitely many

¡

when the Hes-

is singular--the deviation is redundant and the optimal controller takes no

¢¡d¦(" ¢¡U¦ $ ¢¡D¦ i

z

 ¢¡U¦  %9¢d¡U¦

and

R S

are positively correlated because

is a positive semi-definite matrix2. Thus the optimal controller re-

sists single-trial deviations that take the system to more costly states, and magnifies deviations to less costly states. This analysis confirms the minimal intervention principle to be a very general property of optimal feedback controllers, explaining why variability patterns elongated in taskirrelevant dimensions (as well as synergetic actuator coupling) have been observed in such a wide range of experiments involving different actuators and behavioral goals. 2.2 Linear-Quadratic-Gaussian (LQG) simulations The local analysis above is very general, but it leaves a few questions open: i) what happens redundancy) relate to the cost function (which defines the task); iii) what is the distribution of states resulting from the sequence of optimal control signals? To address such questions (and also build models of specific motor control experiments) we need to focus on a class of control problems for which the optimal control law can actually be found. To that end, we have modified [15] the extensively studied LQG framework to include the multiplicative control noise characteristic of the motor system. The control problems studied here and in

when the deviation ¡ is not small; ii) how does the optimal cost-to-go (which defines

IQPSRQT

prqsutDUWVYX

1 Defining the unit vector

. Then

D6`BC@aD

IbICEDa PQR Rdc

as having a in position and

PfRH

in all other positions, we can write , since .

Pihc

2

D XF D6`BC@aD Cc`ea XGca DgX D6`e`Ba Xv Dapxw C@aD Cc D

has to be positive semi-definite--or else we could find a control signal that makes the

instantaneous cost negative, and that is impossible by definition. Therefore

semi-definite.

Vvy

is also positive


1

dv : dq

0.5

dv : corr

0

-0.5 0

dcov : dq 25 Time Step

50

Figure 1:  

%© x 

¢¡

gular values less than

state is

and £

w k

k

¤£ were generated randomly, with the restiction that   has sin(i.e. the passive dynamics is stable); the last component of the

(for similarity with motor control tasks),

1¥£

¦

. For each problem (§

1©¨ (

¢  8 ¡d¦ i |$# ¢¥¡U¦9¢  8 ¡d¦ ¡ i $# ¢¡U¦ ¡0  %  i &%etc.  8 ¡U¦ , R   and the 0

dq" stands for the correlation between the

the next section are in the form Dynamics

Feedback Cost Note that the system state

7¡¡ 1z ¡ 8w

wi w w w w £

is now partially observable, through noisy sensor readings

)¡

1) y¡w 8A 8wyxz  {|{s{ x  ~ ¡ 8A10i   H w

w

w('w w

, w

 ¢  8 ¡U¦ i ¢  8 ¡U¦r ¡ i ¡   ¢  8 ¡U¦ i B¢  8 ¡U¦r ¡ i ¡

w 

and £ are positive semi-definite,

) and each point in time , we gen-

¡

¤

erated 100 random unit vector  and scaled them by mean(sqrt(svd(cov( ))))."! Then

R  R , £ 

¢

£ ,



. The notation "dv :

0 0 0

E w w

¡ 7

w.

When the noise is additive instead of being multiplicative, the optimal control problem has

the well-known solution [5]

QrSw w w w54 w(' w w w w w

¡ w

¢2¡ ¦13% ¡2 ¡2 z 16 72¡ 8vd 898 67¢ @¡A2¡ ¦

where 2 is an internal estimate of the system state, updated recursively by a Kalman filter. The sequences of matrices % and 8 are computed from the associated discrete-time Ricatti equations [5]. Multiplicative noise complicates matters, but we have found [15] that for systems with stable passive dynamics a similar control strategy is very close to optimal. The modified equations for % and 8 are given in [15]. The optimal cost-to-go function is

R S w w

 CB

w w(' w ¦ ¦

w

¢2¡ ¦v11 ¡2£ ¡2 8vy`z n



89  

wuw w wu

¢ B &% ¦ 4  1D£

The Hessian  of the optimal cost-to-go is closely related to the task cost £ , but also

includes future task costs weighted by the passive   and closed-loop  

ics.

¢ ¦ ¢ B E% ¦

dynam-

ww

Specific motor control tasks are considered below. Here we generate 100 random problems in the above form, compute the optimal control law in each case, and correlate the quantities all times. We also show in Figure 1 that the Hessian of the optimal cost-to-go has similar shape to the task cost ("dv : dq" curve), and that the state covariance is smaller along dimensions where the task cost is larger; i.e., the correlation "dcov : dq" is negative. See the figure legend for details.

 R S and corr. As the "dv : corr" curve in Figure 1 shows, they are positively correlated at


b dipwydi¥srdwvx"v acb dfehgib prqtsudwvyx"v

B C ¡¥¤§

@ 8A

89



r  R R TrRV

h

14347 564 112)3 ()0

¡¥¤¦

SUT VXW`Y Y  ¢¡ ¨© ¥!"$#%'& DFEHGPIRQQ

¡   £¢ 

Figure 2: Simulations of motor control tasks ­ see text.

3 Applications to motor coordination We have used the modified LQG framework to model a wide range of specific motor control tasks [14, 15], and always found that optimal feedback controllers generate variability that is elongated in redundant dimensions. Here we illustrate two such models. The first model (Figure 2, Bimanual Tasks) includes two 1D point masses with positions X1 and X2, each driven with a force actuator whose output is a noisy second-order low-pass filtered version of the corresponding control signal. The feedback contains noisy position, velocity, and force information--delayed by 50 msec (by augmenting the system state with a sequence of recent sensor readings). The " Difference" task requires the two points to start moving 20cm apart, and stop at identical but unspecified locations. The covariance of the final state is elongated in the task-irrelevant dimension: the two points always stop close to each other, but the final location can vary substantially from trial to trial. A related phenomenon has been observed in the more complex bimanual task of inserting a pointer in a cup [6]. We now modify the task: in "Sum," the two points start at the same location and have to stop so that the midpoint between them is at zero. Note that the state covariance is reoriented accordingly. We also illustrate a Via Point task, where a 2D point mass has to pass through a sequence of two intermediate targets and stop at a final target (tracing an S-shaped curve). Variability is minimal at the via points. Furthermore, when one via point is made smaller (i.e., the weight of the corresponding positional constraint is increased), the variability decreases at that point. Due to space limitations, we refer the reader to [14] for details of the models. In [14] we also report a via point experiment that closely matches the predicted effect. 4 Multi-attribute costs and desired trajectory tracking As we stated earlier, replacing the task goal with a desired trajectory (which achieves the goal if executed precisely) is generally suboptimal. A number of examples of such suboptimality are provided in [14]. Here we present a more general view of desired trajectory tracking which clarifies its relationship to optimal control. Desired trajectory tracking can be incorporated in the present framework by using a modified cost, one that specifies a desired state at each point in time, and penalizes the deviations from that state. Such a modified cost would normally include the original task cost (e.g., the terms that specify the desired terminal state), but also a large number of additional terms that do not need to be minimized in order to accomplish the actual task. This raises the question: what happens to the expected values of the terms in the original cost, when we attempt to minimize other costs simultaneously? Intuitively, one would expect the orig-


inal costs to increase (relative to the costs obtained by the task-optimal controller). The geometric argument below formalizes these ideas, and confirms our intuition.

£ § ¥

¦ © £ ¨ £

¨ ¦

¦ §  ¢¡ £ ¤

  "! ¢¤!"¡6"7¦X1  '&¢   ¢¥¤!§¡6%D¦ (z

Consider a family of optimal control problems parameterized by the vector

cost functions . Here

costs, and

( 



%$

are the corresponding non-negative weights. Without loss of generality

 #

, with

are different component

we can assume that

  )("0 1

 

positive quadrant of the unit sphere. Let

5#¢I¦4¨7683  &

4

k

, i.e., the weight vector

¢¤!§¡U¦

Q

$

# 21 3  & ¨

lies in the

be an optimal control law3, and

be the vector of expected component costs achieved by

Q

$

; i.e.,

9  5#¢I¦r1 A@c h s u  ¢C¤!§¡£¢¥¤§¦$ ¢¤!"¡#¢C¤§¦§¦"¦ ¤

iyq

4 4

responding

1 ¢#I¦

B 4

, such that the mapping



Q $ 0

1

#¢4 ¦

. Consider a weight vector

A#¢I¦

# and its cor-

is locally smooth and invertible. Then

we can define the inverse mapping

the weight manifold

from the expected component cost manifold

6

to

, as illustrated in Figure 3.

9

and

,

the total expected cost achieved by From the definitions of

Since

Q $

 $ Q

$

is

contains

4G4 P6 ¨

#¢QA¢4 ¦,¦



R¢4 ¦

5#¢4 ¦$ DC2EF#¢QA¢4 ¦$6

e

#5#

for all . Therefore, if we construct the

and is orthogonal to

not containing the origin. Thus

4

the entire manifold

k

dimensional hyperplane

6

4¢4 4HG"I

, no other

¦

that

4 4 ¦$ e

.

is an optimal control law for the problem defined by the weight vector

control law can achieve a smaller total expected cost, and so

0

4

has to lie in the half-space

is tangent to the manifold

6

at point

non-negative curvature, and the unit vector

Let

R5 e 1(

4T)1¢C(y¦14 4

4 Y

(

4 RI¢4 ¦51UT¢S#7¦¨¢V6¦.,T

. Define

we obtain the tangent

which is normal to

6

at

4

, has

satisfies 4

R¢¨5T7¦'1beWRa¢parametricand#¢45T6¦1UT¢ X#at4¢. UT¢7¦"¦.RByis

4`Y4

curve that passes through the point of interest :

differentiating

to the curve

D¦

Since to

R `Ce )(

, we have

.

away

4

5T¢7¦"¦ 4

4

R

6 R5 er&(

R5normal R6 e 1l( 4 YYYe68cannotYturnY

Y4 4 Y

UT¢6¦4at

The non-negative curvature of

from the normal

R a# 1

3 4

, we have

implies ; i.e., the tangent

Q

. Therefore

. Differentiating the latter equality once again yields

crossing the hyperplane .

c de

4 YY

If we assume that the optimal control law is unique, all inequalities below become strict.

For a general 2D manifold

qrq t qrq t

embedded in , the mapping on the unit sphere

that satisfies

p

Y 4bY #without`Ce B(4 Pgs

, and since

cgfih h

is known as the Gauss map, and plays an important role in surface

differential geometry.


The above result means that whenever we change the weight vector

vector

5#¢I¦

4 #

, the corresponding

of expected component costs achieved by the (new) optimal control law will

change in an "opposite" direction. More precisely, suppose we vary

that passes through one of the corners of

increase. Then the component cost

( ¢£ z 91

z 5#¢I¦ , say

¢ "( %(y¦

will increase.

k

¡ ¢ ¡ 

#

along a great circle

( z

decreases and all , so that

References [1] Bernstein, N.I. The Coordination and Regulation of Movements. Pergamon Press, (1967). [2] Bizzi, E., Accornero, N., Chapple, W. & Hogan, N. Posture control and trajectory formation during arm movement. J Neurosci 4, 2738-44 (1984). [3] Cole, K.J. & Abbs, J.H. Kinematic and electromyographic responses to perturbation of a rapid grasp. J Neurophysiol 57, 1498-510 (1987). [4] D'Avella, A. & Bizzi, E. Low dimensionality of supraspinally induced force fields. PNAS 95, 7711-7714 (1998). [5] Davis, M.H.A. & Vinter, R. Stochastic Modelling and Control. Chapman and Hall, (1985). [6] Domkin D., Laczko, J., Jaric, S., Johansson, H., & Latash, M. Structure of joint variability in bimanual pointing tasks. Exp Brain Res 143, 11-23 (2002). [7] Fleming, W. and Soner, H. (1993). Controlled Markov Processes and Viscosity Solutions. Applications of Mathematics, Springer-Verlag, Berlin. [8] Flash, T. & Hogan, N. The coordination of arm movements: an experimentally confirmed mathematical model. J Neuroscience 5, 1688-1703 (1985). [9] Gelfand, I., Gurfinkel, V., Tsetlin, M. & Shik, M. In Models of the structuralfunctional organization of certain biological systems. Gelfand, I., Gurfinkel, V., Fomin, S. & Tsetlin, M. (eds.) MIT Press, 1971. [10] Harris, C.M. & Wolpert, D.M. Signal-dependent noise determines motor planning. Nature 394, 780-784 (1998). [11] Hinton, G.E. Parallel computations for controlling an arm. Journal of Motor Behavior 16, 171-194 (1984). [12] Robertson, E.M. & Miall, R.C. Multi-joint limbs permit a flexible response to unpredictable events. Exp Brain Res 117, 148-52 (1997). [13] Sutton, G.G. & Sykes, K. The variation of hand tremor with force in healthy subjects. Journal of Physiology 191(3), 699-711 (1967). [14] Todorov, E. & Jordan, M. Optimal feedback control as a theory of motor coordination. Nature Neuroscience, 5(11), 1226-1235 (2002). [15] Todorov, E. Optimal feedback control under signal-dependent noise: Methodology for modeling biological movement. Neural Computation, under review. Available at http://cogsci.ucsd.edu/~todorov. (2002). [16] Scholz, J.P. & Schoner, G. The uncontrolled manifold concept: Identifying control variables for a functional task. Exp Brain Res 126, 289-306 (1999). [17] Uno, Y., Kawato, M. & Suzuki, R. Formation and control of optimal trajectory in human multijoint arm movement: Minimum torque-change model. Biological Cybernetics 61, 89-101 (1989). [18] Santello, M. & Soechting, J.F. Force synergies for multifingered grasping. Exp Brain Res 133, 457-67 (2000).


