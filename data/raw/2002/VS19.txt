Learning about Multiple Objects in Images: Factorial Learning without Factorial Search

  Christopher K. I. Williams and Michalis K. Titsias

School of Informatics, University of Edinburgh, Edinburgh EH1 2QL, UK c.k.i.williams@ed.ac.uk M.Titsias@sms.ed.ac.uk

Abstract We consider data which are images containing views of multiple objects. Our task is to learn about each of the objects present in the images. This task can be approached as a factorial learning problem, where each image must be explained by instantiating a model for each of the objects present with the correct instantiation parameters. A major problem with learning a factorial model is that as the number of objects increases, there is a combinatorial explosion of the number of configurations that need to be considered. We develop a method to extract object models sequentially from the data by making use of a robust statistical method, thus avoiding the combinatorial explosion, and present results showing successful extraction of objects from real images.

1 Introduction In this paper we consider data which are images containing views of multiple objects. Our task is to learn about each of the objects present in the images. Previous approaches (discussed in more detail below) have approached this as a factorial learning problem, where each image must be explained by instantiating a model for each of the objects present with the correct instantiation parameters. A serious concern with the factorial learning problem is that as the number of objects increases, there is a combinatorial explosion of the number of configurations that need to be considered. Suppose there are possible objects,

¡

and that there are

¢

possible values that the instantiation parameters of any one object can

take on; we will need to consider

£¥¤¦¢¨§¨©

combinations to explain any image. In contrast,

in our approach we find one object at a time, thus avoiding the combinatorial explosion. In unsupervised learning we aim to identify regularities in data such as images. One fairly simple unsupervised learning model is clustering, which can be viewed as a mixture model where there are a finite number of types of object, and data is produced by choosing one of these objects and then generating the data conditional on this choice. As a model of objects in images standard clustering approaches are limited as they do not take into account the variability that can arise due to the transformations that can take place, described by instantiation parameters such as translation, rotation etc of the object. Suppose that there are manifold in the image space. Learning about objects taking this regularity into account has

  different instantiation parameters, then a single object will sweep out a -dimensional



http://anc.ed.ac.uk


been called transformation-invariant clustering by Frey and Jojic (1999, 2002). However, this work is still limited to finding a single object in each image. A more general model for data is that where the observations are explained by multiple causes; in our example this will be that in each image there are objects. The approach of Frey and Jojic (1999, 2002) can be extended to this case by explicitly considering the simultaneous instantiation of all objects (Jojic and Frey, 2001). However, this gives rise to a large search problem over the instantiation parameters of all objects simultaneously, and approximations such as variational methods are needed to carry out the inference. In our method, by contrast, we discover the objects one at a time using a robust statistical method. Sequential object discovery is possible because multiple objects combine by occluding each other. The general problem of factorial learning has longer history, see, for example, Barlow (1989), Hinton and Zemel (1994), and Ghahramani (1995). However, Frey and Jojic made the important step for image analysis problems of using explicit transformations of object models, which allows the incorporation of prior knowledge about these transformations and leads to good interpretability of the results. A related line of research is that concerned with discovering part decompositions of objects. Lee and Seung (1999) described a non-negative matrix factorization method addressing this problem, although their work does not deal with parts undergoing transformations. There is also work on learning parts by Shams and von der Malsburg (1999), which is compared and contrasted with our work in section 4. The structure of the remainder of this paper is as follows. In section 2 we describe the taining multiple objects (  2.2). In section 3 we present experimental results for up to five model, first for images containing only a single object (  2.1) and then for images conobjects appearing against stationary and non-stationary backgrounds. We conclude with a discussion in section 4. 2 Theory 2.1 Learning one object In this section we consider the problem of learning about one object which can appear at various locations in an image. The object is in the foreground, with a background behind it. This background can either be fixed for all training images, or vary from image to image. The two key issues that we must deal with are (i) the notion of a pixel being modelled as foreground or background, and (ii) the problem of transformations of the object. We consider first the foreground/background issue. Consider an image ¡ of size ¢¤£¦¥§¢©¨ containing ¢ ¢©£¢©¨ pixels, arranged as a length ¢

¡

¡



background  . As the object will be smaller than ¢ £ ¥ ¢ ¨ pixels, we will need to specify vector. Our aim is to learn appearance-based representations of the foreground  and the which pixels belong to the background and which to the foreground; this is achieved by a independently from the corresponding entry in a vector of probabilities " . For pixel # , if vector of binary latent variables ! , one for each pixel. Each binary variable in ! is drawn

$&%('0) $&%¦'21 , then the pixel will be ascribed to the background with high probability, and if , it will be ascribed to the foreground with high probability. We sometimes refer to

"3 asisamodelled

mask.

% by a mixture distribution:

3 %465 ##WV ¤ ¤3 ©© 3 %87@9A% CB 3 %&7D9A%&EGFIH % 7@X % YB 3 % 7@X % EGFIHV ¤¤ ©© PRQTS PRQTS %%  1UE )`E

(1)


where FIH and



FIHV

are respectively the foreground and background variances. Thus, ignoring

transformations, we obtain #

¤¨©¡  ¡£¢¥¤§¦©¨ % $

% 3 % 7@9 % # ¤ 

© ¤ 1 $ % % 7@X % ©#WV ¤3 ©

The second issue that we must deal with is that of transformations. Below we consider only translations, although the ideas can be extended to deal with other transformations such as scaling and rotation (see e.g. Jojic and Frey (2001)). Each possible transformation (e.g. translations in units of one pixel) is represented by a corresponding transformation matrix, a permutation matrix. The semantics of foreground and background mean that the mask " must also be transformed, so that we obtain

so that matrix

 

corresponds to transformation

!  

and

 is the transformed foreground

model. In our implementation the translations use wrap-around, so that each

" is in fact

# ¤ ¡ © $#  

%¢ ¥¤§¦

1 % ¨&¤  " © # ¤ ¤  © © ¤ % 3 %87  '   %

&¤  ©% ©# ¤3 ((© " V

)

%87@XG% (2)

Notice that the foreground  and mask " are transformed by

not. In order for equation 2 to make sense, each element of

(lying in

1

).

This is certainly true for the case when

 

¨)`E   " mustbeavalidprobability

, but the background  is

is a permutation matrix (and can

be true more generally).

To complete the model we place a prior probability # on each transformation

taken to be uniform over all possibilities so that #

set ¡ 6 "798 @

,

 1UE E B

¥A¡



the log likelihood

0 ¤¨©¡

 1DC7 ¤§¦ EGFIH # ¤ ¡ © . ThiscanbeachievedthroughusingtheEM

5# ¤$#0©©by

  ; this is

we can adapt the parameters

"7#B

B ¤ 2143"¤§¦

¡  E E E@F# H E@FIHV



. Given a data

maximizing

algorithm to handle the missing data which is the transformation and ! . The model developed in this section is similar to Jojic and Frey (2001), except that our mask

" has probabilistic semantics, which means that an exact M-step can be used as opposed

to the generalized M-step used by Jojic and Frey. 2.2 Coping with multiple objects If there are foreground objects, one natural approach is to consider models with

¡

tent variables, each taking on the

¢

&B¤ ©

¡ la-

values of the possible transformations. We also need

to account for object occlusions. By assuming that the

¡

objects can arbitrarily occlude

one another (and this occlusion ordering can change in different images), there are

QP¡

pos-

sible arrangements. A model that accounts for multiple objects is described in Jojic and Frey (2001) where the occlusion ordering of the objects is taken as being fixed since they assume that each object is ascribed to a global layer. A full search over the parameters which scales exponentially with . An alternative is to consider approximations; Ghahramani (1995) suggests mean field and Gibbs sampling approximations and Jojic and Frey (2001) use approximate variational inference. Our goal is to find one object at a time in the images. We describe two methods for doing this. The first uses random initializations, and on different runs can find different objects; we denote this RANDOM STARTS. The second method (denoted GREEDY) removes objects found in earlier iterations and looks for as-yet-undiscovered objects in what remains. For both methods we need to adapt the model presented in section 2.1. The problem is that occlusion can occur of both the foreground and the background. For a foreground pixel, a different object to the one being modelled may be interposed between the camera and our object, thus perturbing the pixel value. This can be modelled with a mixture distribution

RP¡ (assuming unknown occlusion ordering for each image) must consider ¡ ¢ § possibilities,

as # TS B 3 % 7@9 % EGFIH ¤



V1 S % S



% 7D9 %  ¤3 ©



U© ¤



XW© ¥¤3 ©,

where

 is the fraction of times


a foreground pixel is not occluded and the robustifying component

W¥¤3 ©

% is a uniform

distribution common for all image pixels. Such robust models have been used for image matching tasks by a number of authors, notably Black and colleagues (Black and Jepson, 1996). Similarly for the background, a different object from the one being modelled may be interposed between the background and the camera, so that we again have a mixture model

# VV ¤.

S

3 %87@XG% 

© V ¤

S

B 3 %87DXG% E@FIHV ©  ¤

1 S

V XW© ¤3 ©,

%

with similar semantics for the parameter

(If the background has high variability then this robustness may not be required, but it

will be in the case that the background is fixed while the objects move.) 2.2.1 Finding the first object With this robust model we can now apply the RANDOM STARTS algorithm by maximizing the likelihood of a set of images with respect to the model using the EM algorithm. The expected complete data log likelihood is given by

¡  ¢  ¤¡ 7    © H  EGFIH



1

8U¦ @

© C¤§¦ 3 ¢



¤§¦

¢ ¤ ¡ © ¤ ! ©¤£ ¤ &  # 7 6 7 EGFIH

  ¦¥ 7¨§© ¤

"  

7



1 F H

£

EGFIH 

 

0 © 7V ¤ " ¦¥

©





1 F H

V

  EGFIH ¤¡ 7  © H 



F H 

 ¤ 7 ! © ¤ ¤

1 F H V ©



S (3) E

where  H

defines the element-wise product between two vectors,   is written

as  for© compactness and



denotes the ¢ -dimensional vector containing ones. The ©

%"! %$#&%('

13246587

expected values of several latent variables are as follows: ¢



binary variables ! with each element storing the probability #

# !@9A)CB ED$# BEFG# !0H¤)CB)

# !PI ) B D # B FQ# ! H¤) B )SRT# VUW#£ !PI ) B ) EX@# B F B )

£

£ £ £

bilities for the foregroundFG# on image

D

C

# £edB ! H¤) B D )

is equal to D

C

#

£

Bd

c

FG# ! H¤) B

D£

%

@%

£

£

¦ % £ V



, ¥

78§

7

&9¤ #¨©¡

 !0)

% %$#&%('

4

£

4

)

is the transformation responsibility, ! is a ¢ -dimensional vector associated with the

¤S %  1 E #"7¡   © 

is the vector containing the robust responsi-

£

responsibilities of the background.c Note that the latter responsibilities do not depend on All of the above expected values of the missing variables are estimated in the r -step using

the transformation

0

since the background is not transformed.

¡

the current parameter values. In the s -step we maximise the

the model parameters " ,  ,  but for example

ut ¢

E F H 

and

FIHV

function with respect to

. We do not have space to show all of the updates

c § f

¦

§7¡

using transformation

)

 

, so that its #`Yba element

)SRh# iU§ f$g D)qpW#

£ g

£ Bd

and similarly the vector

©7¥

V

defines the robust

C¤ 3 ¦ ¤ ¦ ¢

 ¢ ¤ ¡ © '0 # 7  £ 7 7¨§ ¥

©

¨!  ¡

©

7 (Gv ¢ C¤§¦ 3 ¢

 ¤ ¦ ¢ ¤ ¡ © '0 # 7  £ 7 78§ E ¥

©

¨!  (4)

7 7

where Gv stands for the element-wise division between two vectors. This update is quite

intuitive. Consider the case when ¢  &¤  #¨©¡  1

for

! !%  

)

otherwise. For pix-

w

els which are ascribed to the foreground (i.e.

transformed by

 Vw£

(which is

 "¦Uw

¤! Vw7 7¨§ ©

¥

'and1

), the values in

7¡

are

$

as the transformations are permutation matrices). This

 

©

removes the effect of the transformation and thus allows the foreground pixels found in each training image to be averaged to produce  . On different runs we hope to discover objects. However, this is rather inefficient as the basins of attraction for the different objects may be very different in size given the initialization. Thus we describe the GREEDY algorithm next.


2.2.2 The GREEDY algorithm We assume that we have run the RANDOM STARTS algorithm and have learned a foreground model  and mask " . We wish to remove from consideration the pixels of the

¦ ¦

learned object (in each training image) in order to find a new object by applying the same

algorithm. For each example image ¡ we can use the responsibilities #

most likely transformation   .1 Now note that the transformed mask

 

¦

¢¡w7

&¤  #¨©¡

to find the

" obtainsvalues

¦

close to 1 for all object pixels, however some of these pixels might be occluded by other not-yet-discovered objects and we do not wish to remove them from consideration. Thus

¦

we consider the vector £



'¤

¦

¡ " ©

¡ w7

foreground responsibilities ¥ 7 , £ will roughly give close to foreground model  H and mask " H , then for each transformation

occluded object pixels. To further explain all pixels having

 ¤¤£ © ¦

# ¤ ¡ ¥¡ $#   © w7 E 

%¢ ¥¤§¦

¦

%

¨¤¤£ ¦ © ¤ '¤¥¡w7  ©

% B 3 % 7 7

© 

¡

¥ 7 . According to the semantics of the robust



1%

values only for the non-

 '

)

we introduce a new

of model 2, we obtain

w7

¦w7 ©

 ¦

% EGF H 

¤ £ © ¤ '0 " H © # ¤ &¤ A H © © ¤ 0 " H © # V ¤ % ¤ % % 3 % 7@X %



% 3 % 7 

Note that we have dropped the robustifying component

 W ¤3 ©

%

© ©  (5)

from model 1, since the

parameters of this object have been learned. By summing out over the possible transforma-

tions we can maximize the likelihood with respect to  H , " H , ,  and

The above expression says that each image pixel

mixture distribution; the pixel

3 %

3 % g

FIH 

FIHV

.

is modelled by a three-component

can belong to the first object with probability

¤¦£¤ ©

¦

does not belong to the first object and belongs to the second one with probability

£ © ¤ " H © , while with the remaining probability it is background. Thus, the search for a new object involves only the pixels that are not accounted for by model 1 (i.e. those for This process can be continued, so that after finding a second model, the remaining back-

which ¤£

¤ © ¦ % ' ) ).

¡

ground is searched for a third model, and so on. The formula for



# ¤ ¡ ¡

§%

 ¤§¦

$#)¦U ¥¥    ©

w7

E E ¡ §©¨w 7 E 

%¢ §¢ % ¥¤§¦ ¤§¦ ¤§¦

% 

¨



)¦U )¦U

objects becomes

¤ £ © ¤¦£ © ¤ '¤ ¡w  ©  X© 

)¦U ¤§¦

%  % B 3 %&7 %8EGF H





¦

% %

%

,

' 

¤ £ © &0 " © # ¤ '0A © ©  % ¤

¡

§

% 3 % 7  ¤ § % 

§%



This is a

If 

 1

the first

¤ £ © ¤ 0 " © # V ¤  %



¡ 

§  

 % 3 % 7DX %

© (6)



1

component mixture at each pixel, where the

¡then1thecomponents  term

¡  "¦U¤§¦

¤  ¡

w

©

%

 1

Yba object is the background.

is defined to be equal to . Note that all parameters of

1

are kept fixed (learned in previous stages). We always deal with

only one object at a time and thus with one transformation latent variable. This approach can be viewed as approximating the full factorial model by sequentially learning each factor (object). A crucial point is that the algorithm is not assumed to extract layers in images, ordered from the nearest layer to the furthest one. In fact in next section we show a twoobject example of a video sequence where we learn first the occluded object. Space limitations do not permit us to show the function and updates for the parameters, but these are very similar to the RANDOM STARTS, since we also learn only the parameters of one object plus the background while keeping fixed all the parameters of previously discovered objects.

¡

1 It would be possible to make a "softer" version of this, where the transformations are weighted

the best-fitting transformation and  otherwise after learning  and ! . by their posterior probabilities, but in practice we have found that these probabilities are usually  for


Mask Foreground * Mask

Mask Foreground * Mask Background

(a) (b)

Figure 1: Learning two objects against a stationary background. Panel (a) displays some frames of the training images, and (b) shows the two objects and background found by the GREEDY algorithm.

3 Experiments We describe three experiments extracting objects from images including up to five movable objects, using stationary as well as non-stationary backgrounds. In these experiments the

uniform distribution

W¥¤3 ©

%

is based on the maximum and minimum pixel values of all

training image pixels. In all the experiments reported below

)

 

. Also we assume that the total number of objects

¡

S S

and V were chosen to be



that appear in the images is known,

thus the GREEDY algorithm terminates when we discover the

appearances  , the mask " and the parameters

¡

Yba object.

The learning algorithm also requires the initialization of the foreground  and background initialised to 0.5, the background appearance  to the mean of the training images and the all image pixels). For the foreground appearance  we compute the pixelwise mean of the training images and add independent Gaussian noise with the equal variances at each pixel, where the variance is set to be large enough so that the range of pixel values found in the training images can be explored. In the GREEDY algorithm each time we add a new object  the parameters  ,  , " , the mean of the training images; this is done to avoid local maxima since the background found by considering only some of the objects in the images can be very different than the true background. Figure 1 illustrates the¤£detection of two objects against a stationary background2. Some ex1(a) and results are shown in Figure 1(b). For both objects we show both the learned mask and the elementwise product of the learned foreground and mask. In most runs the person with the lighter shirt (Jojic) is discovered first, even though he is occluded and the person with the striped shirt (Frey) is not. Video sequences of the raw data and the extracted objects can be viewed at http://www.dai.ed.ac.uk/homes/s0129556/lmo.html .

F H and F H V . Each element of the mask " is

variances

F H and F H V



are initialized to equal large values (larger than the overall variance of



FIH EGFIH  

  V are initialized as described above. This means that the background  is reset to

amples of the 44 11¢¡ ¥ ¡ training images (excluding the black border) are shown in Figure

In Figure 2 five objects are learned against a stationary background, using a dataset of

images of size ¥¦¥ ¥

¡§¡

U)¡

. Notice the large amount of occlusion in some of the training images

shown in Figure 2(a). Results are shown in Figure 2(b) for the GREEDY algorithm.

2 These data are used in Jojic and Frey (2001). We thank N. Jojic and B. Frey for making available

these data via http://www.psi.toronto.edu/layers.html.


Mask Mask Mask Mask Mask

Foregr. * Mask Foregr. * Mask Foregr. * Mask Foregr. * Mask Foregr. * Mask

(a) (b)

Figure 2: Learning five objects against a stationary background. Panel (a) displays some of the training images and (b) shows the objects learned by the GREEDY algorithm.

Mask Foreground * Mask

Mask Foreground * Mask Background

(a) (b)

Figure 3: Two objects are learned from a set of images with non-stationary background. Panel (a) displays some examples of the training images, and (b) shows the objects found by the GREEDY algorithm.

In Figure 3 we consider learning objects against a non-stationary background. Actually three different backgrounds were used, as can be seen in the example images shown in STARTS algorithm the CD was found in 9 out of 10 runs. The results with the GREEDY algorithm are shown in Figure 3(b). The background found is approximately the average of the three backgrounds. Overall we conclude that the RANDOM STARTS algorithm is not very effective at finding multiple objects in images; it needs many runs from different initial conditions, and sometimes fails entirely to find all objects. In contrast the GREEDY algorithm is very effective.

Figure 3(a). There were ¡  ¥ ¥¦¥ ¥ ¡¦¡ images in the training set. Using the RANDOM

4 Discussion Shams and von der Malsburg (1999) obtained candidate parts by matching images in a pairwise fashion, trying to identify corresponding regions in the two images. These candidate image patches were then clustered to compensate for the effect of occlusions. We make four observations: (i) instead of directly learning the models, they match each image our method; (ii) in their method the background must be removed otherwise it would give rise to large match regions; (iii) they do not define a probabilistic model for the images (with all its attendant benefits); (iv) their data (although based on realistic CAD-type models) is synthetic, and designed to focus learning on shape related features by eliminating complicating factors such as background, surface markings etc. In our work the model for each pixel is a mixture of Gaussians. There is some previous

against all others (with complexity £¥¤B © H ), as compared to the linear scaling with B in


work on pixelwise mixtures of Gaussians (see, e.g. Rowe and Blake 1995) which can, for example, be used to achieve background subtraction and highlight moving objects against a stationary background. Our work extends beyond this by gathering the foreground pixels into objects, and also allows us to learn objects in the more difficult non-stationary background case. For the stationary background case, pixelwise mixture of Gaussians might be useful ways to create candidate objects. The GREEDY algorithm has shown itself to be an effective factorial learning algorithm for image data. We are currently investigating issues such as dealing with richer classes of transformations, detecting automatically, and allowing objects not to appear in all images. Furthermore, although we have described this work in relation to image modelling, it can be applied to other domains. For example, one can make a model for sequence data by having Hidden Markov models (HMMs) for a "foreground" pattern and the "background". Faced with sequences containing multiple foreground patterns, one could extract these patterns sequentially using a similar algorithm to that described above. It is true that components simultaneously, but there may be severe local minima problems in the search space so that the sequential approach might be preferable. Acknowledgements: CW thanks Geoff Hinton for helpful discussions concerning the idea of learning one object at a time. References Barlow, H. (1989). Unsupervised Learning. Neural Computation, 1:295­311. Black, M. J. and Jepson, A. (1996). EigenTracking: Robust matching and tracking of articulated objects using a view-based representation. In Buxton, B. and Cipolla, R., editors, Proceedings of the Fourth European Conference on Computer Vision, ECCV'96, pages 329­342. Springer-Verlag. Frey, B. J. and Jojic, N. (1999). Estimating mixture models of images and inferring spatial transformations using the EM algorithm. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 1999. IEEE Computer Society Press. Ft. Collins, CO. Frey, B. J. and Jojic, N. (2002). Transformation Invariant Clustering and Linear Component Analysis Using the EM Algorithm. Revised manuscript under review for IEEE PAMI. Ghahramani, Z. (1995). Factorial Learning and the EM Algorithm. In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, Advances in Neural Information Processing Systems 7, pages 617­624. Morgan Kaufmann, San Mateo, CA. Hinton, G. E. and Zemel, R. S. (1994). Autoencoders, minimum description length, and Helmholtz free energy. In Cowan, J., Tesauro, G., and Alspector, J., editors, Advances in Neural Information Processing Systems 6. Morgan Kaufmann. Jojic, N. and Frey, B. J. (2001). Learning Flexible Sprites in Video Layers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2001. IEEE Computer Society Press. Kauai, Hawaii. Lee, D. D. and Seung, H. S. (1999). Learning the parts of objects by non-negative matrix factorization. Nature, 401:788­791. Rowe, S. and Blake, A. (1995). Statistical Background Modelling For Tracking With A Virtual Camera. In Pycock, D., editor, Proceedings of the 6th British Machine Vision Conference, volume volume 2, pages 423­432. BMVA Press. Shams, L. and von der Malsburg, C. (1999). Are object shape primitives learnable? Neurocomputing, 26-27:855­863.

¡

¡ for sequence data it would be possible to train a compound HMM consisting of  1 HMM


