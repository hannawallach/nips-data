Critical Lines in Symmetry of Mixture Models and its Application to Component Splitting

Kenji Fukumizu Institute of Statistical Mathematics Tokyo 106-8569 Japan fukumizu@ism.ac.jp Shotaro Akaho AIST Tsukuba 305-8568 Japan s.akaho@aist.go.jp Shun-ichi Amari RIKEN Wako 351-0198 Japan amari@brain.riken.go.jp

Abstract We show the existence of critical points as lines for the likelihood function of mixture-type models. They are given by embedding of a critical point for models with less components. A sufficient condition that the critical line gives local maxima or saddle points is also derived. Based on this fact, a component-split method is proposed for a mixture of Gaussian components, and its effectiveness is verified through experiments. 1 Introduction The likelihood function of a mixture model often has a complex shape so that calculation of an estimator can be difficult, whether the maximum likelihood or Bayesian approach is used. In the maximum likelihood estimation, convergence of the EM algorithm to the global maximum is not guaranteed, while it is a standard method. Investigation of the likelihood function for mixture models is important to develop effective methods for learning. This paper discusses the critical points of the likelihood function for mixture-type models by analyzing their hierarchical symmetric structure. As generalization of [1], we show that, given a critical point of the likelihood for the model with (H - 1) components, duplication of any of the components gives critical points as lines for the model with H components. We call them critical lines of mixture models. We derive also a sufficient condition that the critical lines give maxima or saddle points of the larger model, and show that given a maximum of the likelihood for a mixture of Gaussian components, an appropriate split of any component always gives an ascending direction of the likelihood. Based on this theory, we propose a stable method of splitting a component, which works effectively with the EM optimization for avoiding the dependency on the initial condition and improving the optimization. The usefulness of the algorithm is verified through experiments. 2 Hierarchical Symmetry and Critical Lines of Mixture Models 2.1 Symmetry of Mixture models Suppose fH(x | (H ) is a mixture model with H components, defined by

)

fH(x | (H ) = ) H j=1 cj p(x | j), cj = j/(1 + ··· + H), (1)


where p(x | ) is a probability density function with a parameter . We write, for simplicity, (H = (1, . . . , H ), (H = (1, . . . , H ), and (H = ((H ; (H ).

) ) ) ) )

The key of our discussion is the following two symmetric properties, which are satisfied by mixture models;

(S-1) fH(x | (H ; (H ) -2) , H -1 , H -1 ) = fH -1 (x | (H -2) , H -1 + H; (H -1) ).

(S-2) There exists a function A() such that for j = H - 1 and H,

fH j (x | (H ; (H ) -2)

j fH

, H -2) -1 , H -1 ) = (x | (H

A() H

-1 -1 , H -1 + H; (H -1) ).

In mixture models, the function A() is simply given by A() = 1 + ··· + H. Hereafter, we discuss in general a model with the assumptions (S-1) and (S-2). The results in Sections 2.1 and 2.2 depend only on these assumptions . While in mixture models

1

similar conditions are satisfied with any choices of two components, we describe only the case of H - 1 and H just for simplicity. We write H for the space of the parameter (H . )

Another example which satisfies (S-1) and (S-2) is Latent Dirichlet Allocation (LDA, [2]), which models data of a group structure (e.g. document as a set of words). For x = (x1, . . . , xM ), LDA with H components is defined by

fH(x | (H ) =

where DH(u (H) |(H ) = )

) DH(u

j) j

(H) |(H ) ) M =1 H j=1 ujp(x|j) du (H) , (2)

H -1

(

j (j)

H j=1 uj- is the Dirichlet distribution over the (H-

j

1

1)-dimensional simplex H -1 . It is easy to see (S-1) and (S-2) hold for LDA by using

Lemma 6 in Appendix. LDA includes mixture models eq.(1) as the special case of M = 1. model with (H - 1) components and a scalar , the parameter   H defined by

It is straightforward from (S-1) that, given a parameter (H -1) = ((H -1) ; (H -1) ) of the

j = j, j = j (1  j  H - 2)

, H = (1 - )H -1 , H -1 = H = H -1

(3) H -1 = H -1

gives the same function as fH- (x | (H 1 -1) ). In mixture models/LDA, this corresponds to

duplication of the (H - 1)-th component with partitioning the mixing/Dirichlet parameter in the ratio  : (1 - ). Since  is arbitrary, a point in the smaller model is embedded into the larger model as a line in the parameter space H. This implies that the parameter to causes various interesting phenomena in estimation, learning, and generalization ([3]). 2.2 Critical Lines ­ Embedding of a Critical Point Given a sample {X(1), . . . , X(N }, we define an objective function for learning by

realize fH -1 (x | (H -1) ) lacks identifiability in H. Such singular structure of a model

) )

LH((H ) = N n=1 n(fH(X(n | (H )), ) ) (4)

where n(f) are differentiable functions, which may depend on n. The objective of learning is to maximize LH. If n(f) = log f for all n, maximization of LH((H ) is equal to the maximum likelihood estimation.

)

Suppose 

LH

(H

that is,

1

(H-1)

-1

= (1 , . . . , H

(H-1) (

  ((H -1) ), -1 ; 1, . . . , H   -1 ) is a critical point of LH -1

-1)

) = 0. Embedding of this point into H gives a critical line;

The results do not require that p(x | ) is a density function. Thus, they can be easily extended

to function fitting in regression, which gives the results on multilayer neural networks in [1].


Theorem 1 (Critical Line). Suppose that a model satisfies (S-1) and (S-2). Let 

be a critical point of LH

 (H-1)

-1 with H



-1 = 0, and  be a parameter given by eq.(3) for

. Then,  is a critical point of LH((H ) for all . )

(H-1)

Proof. Although this is essentially the same as Theorem 1 in [1], the following proof gives better intuition. Let (s, t; , ) be reparametrization of (H- , H; H- , H), defined by

1 1

=  + H, H =  - H s = H -1 + H, t = H -1 - H, H -1 -1 . (5)

This is a one-to-one correspondence, if H

alent to the condition H

new coordinate, H

-1 = H.

-1 + H = 0. Note that  = 0 is equiv-

Let  = ((H -2) , s, t; (H -2) , , ) be the

() be the objective function eq.(4) under this parametrization, and

 be the parameter corresponding to . Since we have, by definition,

LH((H -2) , s+t 2

H

, s-t 2 ; (H -2) ,  + s-t 2 ,  - s+t 2 ), the condition (S-1) means

((H -2) , s, t; (H -2) , , 0) = LH

H

() = (6)

((H -2) , s; (H -2) -1 , ).

Then, it is clear that the first derivatives of

and  are equal to those of LH

 H

-1 ((H -1)

at  with respect to (H

(H-1) ) at 

-2) , s, (H -2) H ,

, and they are zero. The derivative

()/t vanishes from eq.(6), and  H ()/ = 0 from following Lemma 2.

Lemma 2. Let H be a hyperplane given by { |  = 0}. Then, for all o  H, we have

fH  (x | o) = 0. (7)

Proof. Straightforward from the assumption (S-2) and   = H

-1

 H - H

-1 -1

 H .

Given that a maximum of LH is larger than that of LH , Theorem 1 implies that the

function LH always has critical points which are not global maximum. Those points lie on lines in the parameter space. Further embedding of the critical lines into larger models gives high-dimensional critical planes in the parameter space. This property is very general, and in LDA and mixture models we do not need any assumptions on p(x | ). In these models, by the permutation symmetry of components, there are many choices for embedding, which induces many critical lines and planes for LH. 2.3 Embedding of a Maximum Point in LDA and Mixture Models

The next question is whether or not the critical lines from a maximum of LH -1 gives

maxima of LH. The answer requires information on the second derivatives, and depends on models. We show a general result on LDA, and that on mixture models as its corollary.

Theorem 3. Suppose that the model is LDA defined by eq.(2). Let 

maximum point of LH -1

(H-1) be an isolated

, and  be its embedding given by eq.(3). Define a symmetric

matrix R of the size dim by

R = N n=1

n(fH +

(X(n | 

1

H-1 j=1

) (H-1)

M

) 2p(Xµ (n)

-1

µ=1 µ

I(n

|H  -1 )

))



|H  -1

j +1 

M µ=1 M =1 =µ Jµ, (n) p(Xµ

(n) ) p(X (n) |H 

-1 )

,



H-1 j=1



where  (f) denotes the derivative of (f) w.r.t. f, and

Iµ (n) = DH    , H

 , H

-1 (u | 1 , . . . , H

  (u | 1 , . . . , H

-2 -1 + 1) ujp(X (n) |j) du (H-1) ,

H -2 =µ

Jµ, = (n) DH -1 -2 -1 + 2) H-1 j=1 ujp(X (n) |j) du (H-1) .

H -2 =µ,


Then, we have (i) If R is negative definite, the parameter  is a maximum of LH for all   (0, 1). (ii) If R has a positive eigenvalue, the parameter  is a saddle point for all   (0, 1).

Remark: The conditions on R depend only on the parameter  (H-1) .

Proof. We use the parametrization  defined by eq.(5). For each t, let Ht be a hyperplane with t fixed, and LH,t be the function LH restricted on Ht. The hyperplane Ht is a slice ~ transversal to the critical line, along which LH has the same value. Therefore, if the Hessian matrix of LH,t on Ht is negative definite at the intersection  ( = (t + 1)/2), the point ~ is a maximum of LH, and if the Hessian has a positive eigenvalue,  is a saddle point.

Since in  coordinate we have LH,t((H ~

(H -1)

-1) , s; (H -1) , , 0) = LH -1 ((H -1) , s;

, ), the Hessian of LH,t at  is given by ~

HessLH -1 (

HessLH,t() = ~

(H-1) ) O

~ 2LH,t() 

~ 2LH,t() a

O

. (8)

The off-diagonal blocks are zero, because we have

Lemma 2. By assumption, HessLH (H-1) ( -1

= 0 for a =  from

) is negative definite. Noting that the terms

including fH(X(n ; )/ vanish from Lemma 2, it is easy to obtain

(1 - )(H- )3/(  1 H-1 j=1

) 2LH,t() ~  =

j ) × R by using Lemma 6 and the definition of . 

By setting M = 1 in LDA model, we have the sufficient conditions for mixture models. Corollary 4. For a mixture model, the same assertions as Theorem 3 hold for

) 

 j . The assertion is obvious.



R = ~ N n=1 n(fH ) (H-1)

1

-1 (X(n |  2p(X(n | H- ) .

)) (9)

Proof. For M = 1, Jµ, = 0 and I(n = H  (n) ) -1 / H-1 j=1

2.4 Critical Lines in Various Models We further investigate the critical lines for specific models. Hereafter, we consider the maximum likelihood estimation, setting n(f) = log f for all n. Gaussian Mixture, Mixture of Factor Analyzers, and Mixture of PCA Assume that each component is the D-dimensional Gaussian density with mean µ and variance-covariance matrix V as parameters, which is denoted by (x; µ, V ). The matrix derivatives with respect to (µ, µ), (µ, V ), and (V, V ), respectively. It is well known that the second derivative 2/µµ of a Gaussian density is equal to the first derivative /V . Then, S2 is equal to zero by the condition of a critical point. If the data is randomly generated, S3 and S4 are of full rank almost surely. This type of matrix necessarily has a positive eigenvalue. It is not difficult to extend this discussion to models with scalar or diagonal variance-covariance matrices as variable parameters. Similar arguments hold for mixture of factor analyzers (MFA, [4]) and mixture of probabilistic PCA (MPCA, [5]). In factor analyzers or probabilistic PCA, the variancecovariance matrix is restricted to the form V = F F T + S,

R in eq.(9) has a form R = ~ ~ S2 S3 S3 S4 T , where S2, S3, and S4 correspond to the second


where F is a factor loading of rank k and S is a diagonal or scalar matrix. Because the R corresponding to the second derivatives on µ is not of full rank. In a similar manner to ~ Gaussian mixtures, R has a positive eigenvalue. In summary, we have the following ~ Theorem 5. Suppose that a model is Gaussian mixture, MFA, or MPCA. If R is of full ~ rank, every point  on the critical line is a saddle point of LH. This theorem means that if we have the maximum likelihood estimator for H - 1 components, we can find an ascending direction of likelihood by splitting a component and modifying their means and variance-covariance matrices in the direction of the positive eigenvector. This leads a component splitting method, which will be shown in Section 3.1. Latent Dirichlet Allocation We consider LDA with multinomial components. Using the D-dimensional random vector x = (xa)  {(1, 0, . . . , 0)T , . . . , (0, . . . , 0, 1)T }, which indicates a chosen element, the multinomial distribution over D elements is expressed as an exponential family by

first derivative of (x; µ, FF T + S) with respect to F is (x;µ,F F T +S) V F , the block in

p(x | ) = D a=1 (pa)xa = exp{ D-1 a=1 axa - log(1 + D-1 a=1 ea)},

where pa is the expectation of xa, and   RD- is a natural parameter given by a = log(pa/pD). It is easy to obtain

R = N n=1  (fH (n) (n)  |H (n) )p(X



- p(H -1) )T ,

 |H -1 (X(n |  ) (H-1) )) M µ=1 )

=µ Jµ, p(Xµ -1 -1

1

× (Xµ ~ (n ) - p(H )(X ~ (n ) -1)

where X ~ (n ) is the truncated (D - 1)-dimensional vector, and p(H -1)  (0,1)D -1

(10) is the

expectation parameter for (H - 1)-th component of  (H-1) .

In general, Jµ, are intractable in large problems. We explain a simple case of H = 2 and M = D. Let p be the frequency vector of the D elements, which is the maximum likelihood estimator for the one multinomial model. In this case, we have Jµ, = 1 and

(n)

R = N n=1 M µ,=1 (Xµ ~ (n ) - p)(X ~ (n ) - p)T - M µ=1 (Xµ ~ (n ) - p)(Xµ ~ (n ) - p)T .

(n)

First, suppose we have a data set with X (n) = e for all n and 1    D = M , where

ej is the D-dimensional vector with the j-th component 1 and others zero. Then, we have gives maxima for LDA with H = 2. Next, suppose the data consists of D groups, and every

p = (1/D, . . . , 1/D) and D µ=1 (Xµ ~ (n ) - p) = 0, which means R < 0. The critical line

data in the j-th group is given by X

the matrix R is D j=1

(n) = ej. While we have again p = (1/D, . . . , 1/D),

(N/D) × D(D - 1)(ej - p)(ej - p)T > 0. Thus, all the points

on the critical lines are saddle points. These examples explain two extreme cases; in the former we have no advantage in using two components because all the data X(n are the )

same, while in the latter the multiple components fits better to the variety of X(n . 3 Component Splitting Method in Mixture of Gaussian Components 3.1 EM with Component Splitting It is well known that the EM algorithm suffers from strong dependency on initialization. In addition, because the likelihood of a mixture of Gaussian components is not upper bounded )


Algorithm 1 : EM with component splitting for Gaussian mixture 1. Initialization: calculate the sample mean µ1 and variance-covariance matrix V1. 2. H := 1.

3. For all 1  h  H, diagonalize Vh according to eq.(12) in Appendix.  as Vh  = UhhUh , and calculate Rh T ~

4. For 1  h  H, calculate the eigenvector (rh, Wh) of Rh corresponding to the ~ largest eigenvalue. 5. For 1  h  H, optimize  by line search to maximize the likelihood for

ch =

cH +1

1 2 ch , 

1 2 ch , 

µh = µh - rh, 

= µh + rh,  µH +1

Vh = Uhe-

VH +1

Wh he- Wh T Uh ,

(11)

= = UheWhheWhUh . T

Let h be the optimizer and Lh be the likelihood. 6. For h := arg maxh Lh, split h-th component according to eq.(11) with h.

o

7. Optimize the parameter (H +1) using EM algorithm. Let  (H+1) be the result.

o

8. If H + 1 = MAX H, then END. Otherwise, H := H + 1 and go to 3.

5

4

3 3

3 6

2

3

4 3 2 1 0 4 3 2 1 0

0 0

1

0

0

-3 -3 -3

-1

6 3 3 3 -6

0 0 0 -3

-6 -3

(b) Success

-3

(c) Failure (a) Data

Figure 1: Spiral data. In (b) and (c), the lines represent the factor loading vectors Fh and -Fh at the mean values, and the radius of a sphere is the scalar part of the variance.

for small variances, we should use an optimization technique to give an appropriate maximum. Sequential split of components can give a solution to these problems. From Theorem 5, a stable and effective way of splitting a Gaussian component is derived to increase the likelihood. We propose EM with component splitting, which adds a component one by one after maximizing the likelihood at each size. Ueda et al ([6]) proposes Split and Merge EM, in which the components repeat split and merge in a triplet, keeping the total number fixed. While their method works well, it requires a large number of trials of EM for candidate triplets, and the splitting method is heuristic. Our splitting method is well based on theory, and EM with splitting gives a series of estimators for all model sizes in a single run. Algorithm 1 is the procedure of learning. We show only the case of mixture of Gaussian. The exact algorithm for the mixture of PCA/FA will be shown in a forthcoming paper. It is noteworthy that in splitting a component, not only the means but also the variancecovariance matrices must be modified. The simple additive rule Vnew = Vold + V tends to fail, because it may make the matrix non-positive definite. To solve this problem, we use Lie algebra expression to add a vector of ascending direction. Let V = UUT be the diagonalization of V , and consider V (W) = UeW eW UT for a symmetric matrix


W . This gives a local coordinate of the positive definite matrices around V = V (0). Modification of V through W gives a stable way of updating variance-covariance matrices. 3.2 Experimental results We show through experiments how the proposed EM with component splitting effectively maximizing the likelihood. In the first experiment, the mixture of PCA with 8 components of rank 1 is employed to fit the synthesized 150 data generated along a piecewise linear spiral (Fig.1). Table 1-(a) shows the results over 30 trials with different random numbers. We use the on-line EM algorithm ([7]), presenting data one-by-one in a random order. The EM with random initialization reaches the best state (Fig.1-(b)) only 6 times, while EM with component splitting achieves it 26 times. Fig.1-(c) shows an example of failure. The next experiment is an image compression problem, in which the image "Lenna" of 160×160 pixels

(Fig.2) is used. The image is partitioned into 20×20 blocks of 8×8 pixels, which are regarded as 400 data in R64. We use the mixture of PCA with 10 components of rank 4, and obtain a compressed image by X = Fh(Fh Fh)- Fh X, where X is a 64 dimensional block and h indicates the component of the shortest Euclidean distance X - µh . Table 1-(b) shows the shows the quality of the compression. In both experiments, we can see the better optimization performance of the proposed algorithm. (a) Likelihood for spiral data (30 runs)

^ T 1 T

residual square error (RSE), 400 j=1 ^ Xj - Xj 2 , which

Best Worst Av.

EM -534.9 (6 times) -648.1 -583.9 EMCS -534.9 (26 times) -587.9 -541.3

20

40

60

80

100

120

140

160 20 40 60 80 100 120 140 160

Figure 2: "Lenna". (b) RSE for "Lenna" (10 runs)

×104 Best Worst Av. EM 5.94 6.40 6.15 EMCS 5.38 6.12 5.78

Table 1: Experimental results. EM is the conventional EM with random initialization, and EMCS is the proposed EM with component splitting.

4 Discussions In EM with component splitting, we obtain the estimators up to the specified number of components. We need a model selection technique to choose the best one, which is another important problem. We do not discuss it in this paper, because our method can be combined with many techniques, which select a model after obtaining the estimators. However, we should note that some famous methods such as AIC and MDL, which are based on statistical asymptotic theory, cannot be applied to mixture models because of the unidentifiability of the parameter. Further studies are necessary on model selection for mixture models. Although the computation to calculate the matrix R is not cheap in a mixture of Gaussian components, the full variance-covariance matrices are not always necessary in practical problems. It can save the computation drastically. Also, some methods to reduce the computational cost should be more investigated. In selecting a component to split, we try line search for all the components and choose the one giving the largest likelihood. While this works well in our experiments, the proposed method of component splitting can be combined with other criterions to select a component.


One of them is to select the component giving the largest eigenvalue of Rh. In Gaussian ~ mixture models, this is very natural; the block of the second derivatives w.r.t. V in R is ~ equal to the weighted fourth cummulant, and a component with a large cummulant should be split. However, in mixture of FA and PCA, this does not necessarily work well, because the decomposition V = FF T + S does not give a natural parametrization. Although we have discussed only local properties, a method incorporating global information might be more preferable. These are left as a future work. Appendix

Lemma 6. Suppose H(u

IH((H ; (H ) = ) )

fies (S-1); IH((H ; (H

)

(H) ; (H ) ) satisfies the assumption (S-1). Define

H- 1

-2)

(u (H) ; (H )DH(u ) (H) |(H )du ) (H) . Then, IH also satis-

, H , H ) = IH ((H -2) , H + H; (H -1) -1 -1 -1 -1 ).

Proof. Direct calculation. Matrix Rh for Gaussian mixture ~ We omit the index h for simplicity, and use Einstein's convention. Let U = (u1, . . . , uD) and  = diag(1, . . . , D). For V (W) = UeW eW UT , we have V (O)/Wab = (a +(1-ab)b)(uauTb +ubuTa ), where ab is Kronecker's delta. Let T (3) and T (4) be the weighted third and fourth sample moments, respectively, with weights .

(x(n ;µ,V) f(H (x(n ;

-1) )

V

(H-1)

)

)

T~(3) and T~(4) are defined by T~(3) = V abc

respectively, where V

matrix R = ~ O B BT C

ap

ap V bq V cr Tpqr and T~(4) abcd (3) = V ap V bq V cr ds Tpqrs, (4)

is the (ap)-component of V -1 . Direct calculation leads that the

, where the decomposition corresponds to  = (µ, W), is given by

Bµa,Wbc = (b + (1 - bc)c)uTb T~(3)uc ··a

CWabWcd = (aubuTa + (1 - ab)buauTb )pq(cuduTc + (1 - cd)ducuTd )rs

× T~(4) pqrs - (V pq V rs + V pr V qs + V ps V qr ) .

(12)

In the above equation, T~(3) is the D × D matrix with fixed a for T~(3) . References [1] K. Fukumizu and S. Amari. Local minima and plateaus in hierarchical structures of multilayer perceptrons. Neural Networks, 13(3):317­327, 2000. [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. Advances in Neural Information Processing Systems, 14, 2002. MIT Press. [3] S. Amari, H. Park, and T. Ozeki. Geometrical singularities in the neuromanifold of multilayer perceptrons. Advances in Neural Information Processing Systems, 14, 2002. MIT Press. [4] Z. Ghahramani and G. Hinton. The EM algorithm for mixtures of factor analyzers. Technical Report CRG-TR-96-1, University of Toronto, Department of Computer Science, 1997. [5] M. Tipping and C. Bishop. Mixtures of probabilistic principal component analysers. Neural Computation, 11:443­482, 1999. [6] N. Ueda, R. Nakano, Z. Ghahramani, and G. Hinton. SMEM algorithm for mixture models. Neural Computation, 12(9):2109­2128, 2000. [7] M. Sato and S. Ishii. On-line EM algorithm for the normalized Gaussian network. Neural Computation, 12(2):2209­2225, 2000. ··a bca


