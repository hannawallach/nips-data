Information Diffusion Kernels

John Lafferty School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA lafferty@cs.cmu.edu Guy Lebanon School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA lebanon@cs.cmu.edu

Abstract A new family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models. Based on the heat equation on the Riemannian manifold defined by the Fisher information metric, information diffusion kernels generalize the Gaussian kernel of Euclidean space, and provide a natural way of combining generative statistical modeling with non-parametric discriminative learning. As a special case, the kernels give a new approach to applying kernel-based learning algorithms to discrete data. Bounds on covering numbers for the new kernels are proved using spectral theory in differential geometry, and experimental results are presented for text classification. 1 Introduction The use of kernels is of increasing importance in machine learning. When "kernelized," simple learning algorithms can become sophisticated tools for tackling nonlinear data analysis problems. Research in this area continues to progress rapidly, with most of the activity focused on the underlying learning algorithms rather than on the kernels themselves. Kernel methods have largely been a tool for data represented as points in Euclidean space, with the collection of kernels employed limited to a few simple families such as polynomial or Gaussian RBF kernels. However, recent work by Kondor and Lafferty [7], motivated by the need for kernel methods that can be applied to discrete data such as graphs, has proposed the use of diffusion kernels based on the tools of spectral graph theory. One limitation of this approach is the difficulty of analyzing the associated learning algorithms in the discrete setting. For example, there is no obvious way to bound covering numbers and generalization error for this class of diffusion kernels, since the natural function spaces are over discrete sets. In this paper, we propose a related construction of kernels based on the heat equation. The key idea in our approach is to begin with a statistical model of the data being analyzed, and to consider the heat equation on the Riemannian manifold defined by the Fisher information metric of the model. The result is a family of kernels that naturally generalizes the familiar Gaussian kernel for Euclidean space, and that includes new kernels for discrete data by beginning with statistical families such as the multinomial. Since the kernels are intimately based on the geometry of the Fisher information metric and the heat or diffusion equation on the associated Riemannian manifold, we refer to them as information diffusion kernels.


Unlike the diffusion kernels of [7], the kernels we investigate here are over continuous parameter spaces even in the case where the underlying data is discrete. As a consequence, some of the machinery that has been developed for analyzing the generalization performance of kernel machines can be applied in our setting. In particular, the spectral approach of Guo et al. [3] is applicable to information diffusion kernels, and in applying this approach it is possible to draw on the considerable body of research in differential geometry that studies the eigenvalues of the geometric Laplacian. In the following section we review the relevant concepts that are required from information geometry and classical differential geometry, define the family of information diffusion kernels, and present two concrete examples, where the underlying statistical models are the multinomial and spherical normal families. Section 3 derives bounds on the covering numbers for support vector machines using the new kernels, adopting the approach of [3]. Section 4 describes experiments on text classification, and Section 5 discusses the results of the paper. 2 Information Geometry and Diffusion Kernels

Let

4AB ¡ C

 ¢¡¤£¦¥¨§©!#"%$'&)(10#567¥8§42

3CEDFassumeG

and

V

D

be a -dimensional statistical model on a set

§the4 ¡IHQPSRT¥¨§4 . mapping

is



9!@

3

. For each

"

at each point in the interior of

The Fisher information matrix

UV

BXW §Ya`   bc".

of at

Let is

given by

BXW §Y¡ed AfB AW `g¡ihqp AB HrPfRs¥¨§4 f ASW HrPfRs¥8§4 t¥8§4  D U G D G D 2 4 (1)

or equivalently as

 B,V BXW §YV

BXW §Y¡Iu hqp ABwv ¥¨§4  ASWxv ¥8§4 

"

defines a Riemannian metric on

2 4y

In coordinates , giving

 

2

(2) the structure of a

-dimensional Riemannian manifold. One of the motivating properties of the Fisher infor-

mation metric is that, unlike the Euclidean distance, it is invariant under reparameterization.

For detailed treatments of information geometry we refer to [1, 6]. For many statistical models there is a natural way to associate to each data point

q§4 

rameter vector

4 a pa-

in the statistical model. For example, in the case of text, under the

multinomial model a document is naturally associated with the relative frequencies of the

word counts. This amounts to the mapping which sends a document

likelihood model

space,



4 41

§4 41§ §Y§Givenwq§such¦a. 4 .

to its maximum

mapping, we propose to apply a kernel on parameter

4

¥8§Y84 

4

More generally, we may associate a data point with a posterior distribution under

a suitable prior. In the case of text, this is one way of "smoothing" the maximum likelihood model, using, for example, a Dirichlet prior. Given a kernel on parameter space, we then

§4 4 ¡ihqih §Y E¥¨§Y8 E¥¨§Y     average over the posteriors to obtain a kernel on data: b  b  4  4  2 2  y (3)

It remains to define the kernel on parameter space. There is a fundamental choice: the kernel associated with heat diffusion on the parameter manifold under the Fisher information

metric. For a manifold coordinates by 

BW § 6 § 

g) 

det

V)V

BXW ) 

ASW

with metric

V the Laplacian

det

VgfBXW  e

¡ d ABwv

is given in local (4)


UV UV

BW

where

`s¡ BW ` ¡  ¢ , generalizing the classical operator div



compact the Laplacian has discrete$#%

eigenfunctions



! satisfying "!

B



B ¡ B B.

eigenvalues

!



¡ ¤

£

¡ B C

© © © ¦¥ C tF

¨§ ©

¢ 



When the manifold has a boundary, appropri-



§ . When



is

with corresponding

ate boundary conditions must be imposed in order that

conditions set

)

! 

B C ¡



is self-adjoint. Dirichlet boundary

C

¨&

and Neumann boundary conditions require

CC F  ¡

§ C  ¡ ¨'"(((

 where

is the outer normal direction. The# following theorem summarizes the basic properties for 10

C 

the kernel of the heat equation

Theorem 1.32 Let

kernel

(3) ¥

B@

A

RQ TS

#  U

 C4 

CB !

C§F

q



 on

 .

be a geodesically complete Riemannian6567manifold. Then@9 heat

x §(2)q 42 HG

F

IG P2

qc¡ 42 ©

P2

32  32

exists and satisfies (1)ED

B§4  B§q,. 42 (4)

!

¡ b §4 qc¡

32

 

§4 ¡ § 4 , H



 F

§4 2

G

 

¡8

§4 q¡§4 §q,

the

, and (5)

§4 q

We refer to [9] for a proof. Properties 2 and 3 imply that



equation in

that D

S 4W

V

V A S 4W

§4   §4 2 ¡ 

§4 4 ¡

V

B

4b 

§4 q §q 2

D 42

YX

V

S 4W

2

2

 

V . Therefore,

V`"ab

S 4W

since

 

§4

is a positive operator; thus

q §4  §q§ ¡

32

solves42 the heat

, starting from32 . Integrating propertyD 3 against a32function42V D

V V

perature

B B Bb§§44B¡4 B

 

q2g4

shows

2

32



2§4

is positive definite. Together, these properties show that

defines a Mercer2 kernel.

Note that when using such a kernel for classification, the discriminant function

¥

4

dc 2

2e c

B B

on labeled data point

B,

and

f§4 ¡



§4 !¡

can2 be interpreted as the solution to the heat equation with initial tem2e

 on unlabeled points.

The following two basic examples illustrate the geometry of the Fisher information metric and its associated diffusion kernel: the multinomial corresponds to a Riemannian manifold of constant positive curvature, and the spherical normal family to a space of constant negative curvature. 2.1 The Multinomial The multinomial is an important example of how information diffusion kernels can be

£¦¥¨§©Ew0 

applied naturally to discrete data. For the multinomial family

¥ ¢ ih

the -simplex,

the -sphere of radius 2.

22

RQgf

¢

d

. The transformation

B(  B ¡  B 56 e  B ¡ B

pG

, is an element of

maps the -simplex to

2

The representation of the Fisher information metric given in equation (2) suggests the geometry underlying the multinomial. In particular, the information metric is given by sponds to the inner product of tangent vectors to the sphere, and information geometry for the multinomial is the geometry of the positive orthant of the sphere. The geodesic distance between two points is given by

V BW §Yf ¡ (  AB HQPSR  AW HrPfR  ¡ AB ASW ¥ qfQ r ¢ ¢ r r r sX G G

` so that the Fisher information corre-

 w 

2 §Y ¡ 

(

y gf RQ fB ¢ thvuxwPyCy  P v  B  B I (5)

¢

This metric places greater emphasis on points near the boundary, which is expected to be important for text problems, which have sparse statistics. In general for the heat kernel on a Riemannian manifold, there is an asymptotic expansion in terms of the parametrices; see for example [9]. This expands the kernel as

 §4 q¡ §u ¦ P2   §g # 2 

§u4 

32

e

B §4 q 32 1

B

fehg

§  4



RQ

fB d (6)

Using the first order approximation and the explicit distance for the geodesic distance gives


1 1

0.9 0.9

0.8 0.8

0.7 0.7

0.6 0.6

0.5 0.5

 

0.4

0.3

 

0.4

0.3

0.2 0.2

0.1 0.1

0

¡ 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 ¡ 0.5 0.6 0.7 0.8 0.9 1

Figure 1: Example decision boundaries using support vector machines with information diffusion kernels for trinomial geometry on the 2-simplex (top right) and spherical normal

¡ th

geometry,

2

(bottom right), compared with the standard Gaussian kernel (left).

a simple formula for the approximate information diffusion kernel for the multinomial as



§Y £¢ §u ¦ ¥¤

 e   § g gf RQ

fB ¢  #

d P

uxwPyCy e



 

v  B  B

¡  (7)

(

¢

In Figure 1 this kernel is comparedthwith the standard Euclidean space Gaussian kernel for

the case of the trinomial model, 2.2 Spherical Normal .

2 ¡

Now¢ consider the statistical family given by  

9

BW

is the mean and

&  (

§

¨

¥8§©Eg¡ §§¦©¨T¦¨¡%§§¦¨ 

(

  ¢ where

is the scale of the variance. A calculation shows that

  ¢

. Thus, the Fisher information metric gives

" ¡ & (  &

V BXW §Y!¡¢¦

f

the structure of the

¡ 

h

d e it is given

upper half plane in hyperbolic space.

The heat kernel on hyperbolic space

by

2

e th

h

! !



e u $#&%('d AA'



(

has a closed form [2]. For

! ¥  #

2

b

§4 4 ¡ d"! d # h P5 

e C

 u  #

'

e



and for

¡   ¡ §4 41 2

(8)

the kernel is given by

where

§4 4 ¡ d)! # h ! ! 

e u 10 $#&%('d AA'

h e 

 P5

e



! h @ 2 3

is the geodesic distance between the two points in

kernel is identical to the Gaussian kernel on .

If only the mean

&

e54P76% P %(' 2¡

A@

¢ §

g

# y e

3! 9

f # F §

9

# y e



( . For

18 

'

3

2 d

(9) the

b¡B¦

is unspecified, then the associated kernel is the standard Gaussian

RBF kernel. In Figure 1 the kernel for hyperbolic space is compared with the Euclidean


space Gaussian kernel for the case of ah 1-dimensional normal model with unknown mean and variance, corresponding to . Note that the curved decision boundary for the diffusion kernel makes intuitive sense, since as the variance decreases the mean is known with increasing certainty. 3 Spectral Bounds on Covering Numbers In this section we prove bounds on the entropy and covering numbers for support vector machines that use information diffusion kernels; these bounds in turn yield bounds on the expected risk of the learning algorithms. We adopt the approach of Guo et al. [3], and make use of bounds on the spectrum of the Laplacian on a Riemannian manifold, rather than on VC dimension techniques. Our calculations give an indication of how the underlying geometry influences the entropy numbers, which are inverse to the covering numbers. We begin by recalling the main result of [3], modifying their notation slightly to conform

with ours. Let

suppose that

eigenvalues of



   

$ & ( 2

# be a compact subset of -dimensional Euclidean space, and

is a MercerD kernel.P2 Denote by

42 2

56

¢ 

a a a¦

6 &

  W §©©denote © ©

2 ¡

, i.e., of the mapping

B

V

9¡  def

corresponding eigenfunctions. We assume that

 4

Given points

bounded by

 §©¡

©



§©¡ q£¢ §q

V

 



W¥¤ W ¤,@ §¦

2

and let



¡ £ B0 4 . with weight vector



the the



, the SVM hypothesis class for

¨

where

X

`

©r © ¤ ©¤isdenote

§¨ ¡¤£§4   567§  §4    §4   ¤ ¤ 0

is defined as the collection of functions

¢

y y y 4

!

HX ¢

y y y

`

X  ©

the mapping from



!

` (10)

to feature space defined by the Mercer kernel, and

and the corresponding Hilbert space inner product and norm. It is of

interest to obtain uniform bounds on the covering numbers

size of the smallest -cover of

7 u



Q

¢

!$##$#$!!

V

B  §4 B .

   §  ¨



%§  §¨ ¦,

defined as the

in the metric induced by the norm

¤ ¤

V

The following is the main result of Guo et al. [3].

'&

@"!¨

¡

Given an integer



£¢

W

f ¢



IH4 1 @ U  32544$476U

62

%

Theorem 2.

DC tF© FEG

1) ¡ 8 , let (0)1

denote the smallest integer for which

A

99 @© ( 1) 4 1 @

U

§ § and define



%

.

2 4$44U 6£B 62B

(P)1

e

¥ RQ

B@1W B y

B   %§1) ¡ §¨ ¦ Then

To apply this result, we will obtain bounds on the indices using spectral theory in Rie-

mannian geometry. The following bounds on the eigenvalues of the Laplacian are due to Li and Yau [8].

Theorem 3. Let



be a compact Riemannian manifold of dimension with non-negative





¦ ¦ © © © ¢  



2

Ricci curvature, and assume that the boundary of is convex. Let 

denote the eigenvalues of the Laplacian with Dirichlet boundary conditions. Then

Q

¤

§

¦ W

e

¤

§

¢

§2  (

 R

and

Q    

§2  ( d

 R



where

R

(11)

is the volume of

 Q ¢ and Q 

are constants depending only on the dimension.

Note that the manifold of the multinomial model satisfies the conditions of this theorem. Using these results we can establish the following bounds on covering numbers for information diffusion kernels. We assume Dirichlet boundary conditions; a similar result can diffusion coefficient in order to indicate how the bounds depend on the geometry.

R ¡

be proven for Neumann boundary conditions. We include the constant vol 

§  and


Theorem 4. Let



be a compact Riemannian manifold, with volume

R , satisfying the

conditions of Theorem 3. Then the covering numbers for the Dirichlet heat kernel

 HQPSR %§ ¡ §¨ ¦¡ satisfy g

R

qf

 

( HrPfR  d 

 

on



¤  §     (12)

Proof.



#

P2

d HrPfR

§4 qBy

the lower bound in Theorem 3, the Dirichlet eigenvalues of the heat kernel

    #

§

, which are given by



© © © W



%  ¢  Q

W ¡ ¡ 6

S

W

¡

, satisfy

(

e h

HQPSR

¡£¢

HrPfR W

% a



Q

¢

§2  ¤

A   B

W

. Thus,

e h

HQPSR ¢ B

¢

R   a ( fRQ  

where the second inequality comes from upper bound of Theorem 3, the inequality

HrPfR W  e

(

  Q ( h

WB ( ( 1) (

RQ ¥ ¢



¢ a



(

Q ¢

2 e

2 (

h  R

¡

(



( % (13)

D 

W

a

4 4 ¢

2 W 2

¥¤§¦ ¢ f

¢

h

. Now using the

will hold if

2 ( h  R

 a

¢

2

qf

e  R #

 a f  Q



or equivalently  Q

R ¤ ( (

§ 

§  e h ( # QQ

qf

¢

2



e

2 ( (

h

( 



h

( HrPfR e HrPfR

( % (14)

 % (15)

The above inequality will hold in case

(

¨©©©©

h

R §  ¤ HrPfR

(

qf

(  % 

 Q a

a 

§Q # Q ¢ 

( (  ¨©©©©

R § fh

¤

 ¢

e §



2Q HrPfR

¢

  §

HrPfR  ¤ %

a

% 

( qf ( 

  (16)

since we may assume that

constant

Q

B@1W ¤ ¡

¢

§2 .

Q

¤ ¤ HQPSRB ¥ RQ

¢ ; thus, ( 1)  Q

¤ ¦ ¤

§



for a new in Theorem 2

HrPfR ¢ ¡

Plugging this bound on

4 g

S  

@ §

S  

§



B W B

and using

 

4   § @

¤ 

¤

( 1)

into the expression for

 1)

¦

, we have after some algebra that

HQPSR

4

§

¤

¦§ § %

 . Inverting the above equation in

 

@

% gives equation (12).

We note that Theorem 4 of [3] can be used to show that this bound does not, in fact,

depend on

4

g ¦



§

§

HrPfR ¤

and

@

¨ . Thus, for fixed



the covering numbersg scale as

4

   ¢

HQPSR %§  g¡  ¤

A  B , and for fixed they scale as §

@

in the diffusion

 HrPfR %§   ¡ 

time .

4 Experiments We compared the information diffusion kernel to linear and Gaussian kernels in the context of text classification using the WebKB dataset. The WebKB collection contains some 4000 university web pages that belong to five categories: course, faculty, student, project and staff. A "bag of words" representation was used for all three kernels, using only the word frequencies. For simplicity, all hypertext information was ignored. The information diffusion kernel is based on the multinomial model, which is the correct model under the


0.18

linear rbf diffusion linear rbf diffusion

0.16 0.3

0.14

0.25

0.12

rate rate

error 0.1 error 0.2

set set

Test 0.08 Test

0.15

0.06

0.04 0.1

0.02

50 100 150 200 250 50 100 150 200 250

Number of training examples Number of training examples

Figure 2: Experimental results on the WebKB corpus, using SVMs for linear (dot-dashed) and Gaussian (dotted) kernels, compared with the information diffusion kernel for the multinomial (solid). Results for two classification tasks are shown, faculty vs. course (left) and faculty vs. student (right). The curves shown are the error rates averaged over 20-fold cross validation.

(incorrect) assumption that the word occurrences are independent. The maximum likelihood mapping was used to map a document to a multinomial model, simply normalizing the counts to sum to one. Figure 2 shows test set error rates obtained using support vector machines for linear, Gaussian, and information diffusion kernels for two binary classification tasks: faculty vs. course and faculty vs. student. The curves shown are the mean error rates over 20-fold cross validation and the error bars represent twice the standard deviation. For the Gaussian and

¨ e

information diffusion kernels we tested values of the kernels' free parameter ( or

h h

the set   

£      £¢¤  0

yd y¡  y d

) in

. The plots in Figure 2 use the best parameter value in the

above range.

56 q§  2  2

Our results are consistent with previous experiments on this dataset [5], which have observed that the linear and Gaussian kernels result in very similar performance. However the information diffusion kernel significantly outperforms both of them, almost always obtaining lower error rate than the average error rate of the other kernels. For the faculty vs. course task, the error rate is halved. This result is striking because the kernels use identical representations of the documents, vectors of word counts (in contrast to, for example, string kernels). We attribute this improvement to the fact that the information metric places more emphasis on points near the boundary of the simplex. 5 Discussion Kernel-based methods generally are "model free," and do not make distributional assumptions about the data that the learning algorithm is applied to. Yet statistical models offer many advantages, and thus it is attractive to explore methods that combine data models and purely discriminative methods for classification and regression. Our approach brings a new perspective to combining parametric statistical modeling with non-parametric discriminative learning. In this aspect it is related to the methods proposed by Jaakkola and Haussler [4]. However, the kernels we investigate here differ significantly from the Fisher

¤

kernel proposed in [4]. In particular, the latter is based on the Fisher score

at a single point



D

HrPfRs¥¨§¦¥ f

in parameter space,¥ and in# the case of an exponential family model it is #

4 41

given by a covariance

¨§

§4 41¡ B B d©D B` B d©D B`

A

U¥

B

A

U¥

B

. In contrast, infor-


mation diffusion kernels are based on the full geometry of the statistical family, and yet are also invariant under reparameterization of the family. Bounds on the covering numbers for information diffusion kernels were derived for the case of positive curvature, which apply to the special case of the multinomial. We note that the resulting bounds are essentially the same as those that would be obtained for the Gaussian kernel on the flat -dimensional torus, which is the standard way of "compactifying" Euclidean space to get a Laplacian having only discrete spectrum; the results of [3] are manifolds with curvature bounded below by a negative constant should also be attainable. While information diffusion kernels are very general, they may be difficult to compute in particular cases; explicit formulas such as equations (8­9) for hyperbolic space are rare. To approximate an information diffusion kernel it may be attractive to use the parametrices cases where the distance itself is difficult to compute exactly, a compromise may be to approximate the distance between nearby points in terms of the Kullback-Leibler divergence, The primary "degree of freedom" in the use of information diffusion kernels lies in the

2

¢

formulated for the case

¡ 2 d , corresponding to the circle   . Similar bounds for general

§Yw   and geodesic distance

2

between points, as we have done for the multinomial. In

§Y  ¢¡ §X¥¨§© ¤ ¥8§©E ¦.  ¢  using the relation 2 

4 §4 .

puxw 7 u

specification of the mapping of data to model parameters,

we have used the maximum likelihood mapping

4

56 q§  ¡56 ¦R

4

For the multinomial,



D

which is

¥§4 ,

simple and well motivated. As indicated in Section 2, there are other possibilities. This remains an interesting area to explore, particularly for latent variable models. Acknowledgements This work was supported in part by NSF grant CCR-0122581. References [1] S. Amari and H. Nagaoka. Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs. American Mathematical Society, 2000. [2] A. Grigor'yan and M. Noguchi. The heat kernel on hyperbolic space. Bulletin of the London Mathematical Society, 30:643­650, 1998. [3] Y. Guo, P. L. Bartlett, J. Shawe-Taylor, and R. C. Williamson. Covering numbers for support vector machines. IEEE Trans. Information Theory, 48(1), January 2002. [4] T. S. Jaakkola and D. Haussler. Exploiting generative models in discriminative classifiers. In Advances in Neural Information Processing Systems, volume 11, 1998. [5] T. Joachims, N. Cristianini, and J. Shawe-Taylor. Composite kernels for hypertext categorisation. In Proceedings of the International Conference on Machine Learning (ICML), 2001. [6] R. E. Kass and P. W. Vos. Geometrical Foundations of Asymptotic Inference. Wiley Series in Probability and Statistics. John Wiley & Sons, 1997. [7] R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In Proceedings of the International Conference on Machine Learning (ICML), 2002. [8] P. Li and S.-T. Yau. Estimates of eigenvalues of a compact Riemannian manifold. In Geometry of the Laplace Operator, volume 36 of Proceedings of Symposia in Pure Mathematics, pages 205­239, 1980. [9] R. Schoen and S.-T. Yau. Lectures on Differential Geometry, volume 1 of Conference Proceedings and Lecture Notes in Geometry and Topology. International Press, 1994.


