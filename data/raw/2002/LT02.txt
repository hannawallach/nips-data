A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages

 ¢¡¤£  ¦¥§£

 ¢¡¤£ Dorthe Malzahn ¨ Manfred Opper

 ¨¥©£

Informatics and Mathematical Modelling, Technical University of Denmark, R.-Petersens-Plads Building 321, DK-2800 Lyngby, Denmark Neural Computing Research Group, School of Engineering and Applied Science, Aston University, Birmingham B4 7ET, United Kingdom dm@imm.dtu.dk opperm@aston.ac.uk

Abstract We apply the replica method of Statistical Physics combined with a variational method to the approximate analytical computation of bootstrap averages for estimating the generalization error. We demonstrate our approach on regression with Gaussian processes and compare our results with averages obtained by Monte-Carlo sampling. 1 Introduction The application of tools from Statistical Mechanics to analyzing the average case performance of learning algorithms has a long tradition in the Neural Computing and Machine Learning community [1, 2]. When data are generated from a highly symmetric distribution and the dimension of the data space is large, methods of statistical mechanics of disordered systems allow for the computation of learning curves for a variety of interesting and nontrivial models ranging from simple perceptrons to Support-vector Machines. Unfortunately, the specific power of this approach, which is able to give explicit distribution dependent results represents also a major drawback for practical applications. In general, data distributions are unknown and their replacement by simple model distributions might only reveal some qualitative behavior of the true learning performance. In this paper we suggest a novel application of the Statistical Mechanics techniques to a topic within Machine Learning for which the distribution over data is well known and controlled by the experimenter. It is given by the resampling of an existing dataset in the so called bootstrap approach [3]. Creating bootstrap samples of the original dataset by random resampling with replacement and retraining the statistical model on the bootstrap sample is a widely applicable statistical technique. By replacing averages over the true unknown distribution of data with suitable averages over the bootstrap samples one can estimate various properties such as the bias, the variance and the generalization error of a statistical model. While in general bootstrap averages can be approximated by Monte-Carlo sampling, it is useful to have also analytical approximations which avoid the time consuming retraining of the model for each sample. Existing analytical approximations (based on asymptotic techniques) such as the delta method and the saddle point method (see e.g.[5]) require


usually explicit analytical formulas for the estimators of the parameters for a trained model. These may not be easily obtained for more complex models in Machine Learning. In this paper, we discuss an application of the replica method of Statistical Physics [4] which combined with a variational method [6] can produce approximate averages over the random drawings of bootstrap samples. Explicit formulas for parameter estimates are avoided and replaced by the implicit condition that such estimates are expectations with respect to a certain Gibbs distribution to which the methods of Statistical Physics can be well applied. We demonstrate the method for the case of regression with Gaussian processes (GP) (which is a kernel method that has gained high popularity in the Machine Learning community in recent years [7]) and compare our analytical results with results obtained by Monte-Carlo sampling. 2 Basic setup and Gibbs distribution We will keep the notation in this section fairly general, indicating that most of the theory

can be developed for a broader class of models. We assume that a fixed set of data

¡ ¥

is modeled by a likelihood of the type

 ¥  #"%$'&)( ! ¥! ¦ A@B ¨ 5

0132  5764 98

¡

¥§¦ ¦ ©¨ ¨¨ ¦  ¢¡¤£

where the "training error"

infinite dimensional object) which must be estimated from the data. We will later specialize

to supervised learning problems where each data point

£

of an input

(usually a finite dimensional vector) and a real label . In this case,

!

¥DC9

E

!consists

stands for a function

8

is parametrized by a parameter

!

(1) (which can be a finite or even

¦ ¥DC  ¨FE C

which models the outputs, or for the parameters (like the weights of a neural network)

which parameterize such functions. We will later apply our approach to the mean square

error given by

8

¥! ¦  ¨ 5 £ HIG ! ¥ ¥ ¥§C   2 5 E 5  ¥ (2)

The first basic ingredient of our approach is the assumption that the estimator for the unbution over all possible 's. This avoids the problem of writing down explicit, complicated

known "true" function

!!

formulas for estimators. To be precise, we assume that the statistical estimator

is based on the training set

the measure

USV

 T¡

) can be represented as the expectation of

V

!

can be represented as the mean with respect to a posterior distri-

!QSRP

(which

with respect to

!3  ¡XW £ Ya`G ! W $'&)(

01#2  5764 b8V

¡

¥! ¦ A@B ¨ 5 (3)

which is constructed from a suitable prior distribution

Y $'&)(

V £dcfeg` ! W 01 2  5764 8

¡

` ! W

and the likelihood (1).

¥! ¦  @B ¨ 5 (4)

denotes a normalizing partition function. Our choice of (3) does not mean that we restrict ourselves to Bayesian estimators. By introducing specific ("temperature" like) parameters in the prior and the likelihood, the measure (3) can be strongly concentrated at its mean such that maximum likelihood/MAP estimators can be included in our framework. 3 Bootstrap averages We will explain our analytical approximation to resampling averages for the case of supervised learning problems. If we are interested in, say, estimating the expected error on test


points 1 which are not contained in the training set

  ¡

 

 a¡

of size   and if we have no hold

out data, we can create artificial data sets

the original set , where each data point

¦£¢'s

¦£¢¥¤

by resampling (with replacement) ¡ data from

  ¡

¦

is taken with equal probability

G

  .

Hence, some of the will appear several times in the bootstrap sample and others not at

all. A proxy for the true average test error can be obtained by retraining the model on each

bootstrap training set

tained in

 

 

, calculating the test error only on those points which are not con-

and finally averaging over many sets

 

. In practice, the case ¡

£§ 

maybe of

main importance, but we will also allow for estimating a lager part of the "learning curve" by allowing for ¡©¨  and ¡©§  . We will not discuss the statistical properties of such bootstrap estimates and their refinements (such as Efron's .632 estimate) in this paper, but refer the reader to the standard literature [3, 5].

For any given set

numbers 

¦¢ £

appears in the set

¥¡

 ¨¨ ¡

¡

, we represent a bootstrap sample¢ by the vector of "occupation" . Denoting the expectation over random bootstrap samples by  ,

¡ ¢ with  ¡ ¡ £¡ . ¡

is the number of times example

Q

 

  ¥§C)¢A

 

¢6

Efron's estimator for the bootstrap generalization error is

 "!$#&%

Q £ G

 

46

¢



¥¡   

¡ 

¡(' !VPQ Q

2

! # %

E ¢10 "2¥

¡ W (5)

where we specialized to the square error for testing. Eq.(5) computes the average bootstrap  %

test error at each data point 3 from ¢

3 54 and 6 else, guarantees that only realizations of bootstrap training sets which do not contain the test point. Introducing the abbreviation

7

¥! ¦  ¥§C  ¨ ¢ £d! ¢

2 E

¢

£

 T¡

. The Kronecker symbol, defined by

 

£

contribute

G

for

5

(which is a linear function of ), and using the definition of the estimator

! !

!©QP

(6) as an average

of 's over the Gibbs distribution (3), the bootstrap estimate (5) can be rewritten as



¥¡  £ G

 

46

¢

VG

! # %  "!$#&%

 Q ¡ Y G c e ` ! Weg` ! W ! 7 ¥ ¡ ¥ ¥ ¦8¢ ¥! ¦¢£9

¡

¨ 7 ¥ ¨



¡ 

Q

V V

¡ W

T$'&)(

copies (or replicas)

!

¡

9

01 2  5764

¡

5 ¥8 ¥! ¦ A@ ¥! ¦ F @BCBD 8 ¡ ¨ 5 ¥ ¨ 5

¡

 (7)

which involves

H

and

!

¥ of the variable . More complicated types

!

of test errors which are polynomials or can be approximated by polynomials in rewritten in a similar way, involving more replicas of the variable . !

!©QP

can be

4 Analytical averages using the "replica trick" For fixed ¡ , the distribution of ¡ 's is multinomial. It is simpler (and does not make a big ¢ difference when ¡ is sufficiently large) when we work with a Poisson distribution for the

size of the set

 

with ¡ as the mean number of data points in the sample. In this case we

get the simpler, factorizing joint distribution

 ¥ 

for the occupation numbers ¡ where ¢

1

£ E ¢

 !F#HGPIRQ

` 6

¡ ¢HS

¡

¦

V ! # %

` £§¡   . With Eq. (8) follows  Q ¡ W £

(8) GTIUQ .

The average is over the unknown distribution of training data sets.


To enable the analytical average over the vector  (which is the "quenched disorder" in the language of Statistical Physics) it is necessary to introduce the auxiliary quantity

V V

¡  

¥¡ 

£ 

G IU!

I

G¢  Q ¡ Y   ¥ ¡ ¥ 7 ¥ ¦¢ ¥! ¦8¢ ¡ ¨ 7 ¥ ¨ 9

46

¢

 "!$#&%

  

¡

c01 e ` ! We `5 ! W !

2  5764

¡

¥¡ 

¡

$'&)( 9 ¥8 ¥! ¦  ¥! ¦ F @B 8 ¡ ¨ 5 @ ¥ ¨ 5 DB (9)

for £ real, which allows to¡  write is that for integers £  ,

variable

!

H ¥¡ 

¥¡  £¥¤§¦©¨ ¡

    

. The advantage of this definition

can be represented in terms of £ replicas of the original ¢

for which an explicit average over ¡ 's is possible. At the end of all calculations

an analytical continuation to arbitrary real £ and the limit £ 6 must be performed. Using

the definition of the partition function (4), we get for integer £ 



  

¥¡ 

G IR!

£ G¢  Q ¡ c

9

E

! W !  7

5764 ¡ 5

¡

¥ ¦  ¥! ¦ 

¨

 

6 8

¡

¡

¢ 7 ¨

¥

¢ 9

4 ¥! ¦  @B  ¨ 5



46

¢



 ! # %  

  

¡ 

$'&)(

6 ¡

V 01e 2` 

H

(10)

DB 

Exchanging the expectation over datasets with the expectation over 's and using the ex-

plicit form of the distribution (8) we obtain

  

¥¡    G IR! £ G¢  ¨

46

¢



$'&)(



2

  G ! I

¡ E

% & # #"%$ ('£

  7

   ¡    6

¡

! ¥! ¦  ¥! ¦ 0)

¡

¢ 7 ¥ ¨ ¢

(11)

where the brackets 1 which is given by

U32

denote an average with respect to a Gibbs measure for replicas

¥!  ¡ ¨¨

  £

 

! £ G  ` ! W  

6   ¡

2   G

¡

5 I

E ¡ ¡

E $'&)(

©"  $

  W

V V24

% &76

 

 

 5764 6

(12)

where 4

£

(13) ¡ 

and where the partition function  has been introduced for convenience to normalize the   measure for £98 6 . In most nontrivial cases, averages with respect to the measure (12) can not be calculated exactly. Hence, we have to apply a sensible approximation. Our idea is to use techniques which have been frequently applied to probabilistic models [10] such as the variational approximation, the mean field approximation and the TAP approach. In this paper, we restrict ourself to a variational Gaussian approximation. More advanced approximations will be given elsewhere. 5 Variational approximation A method, frequently used in Statistical Physics which has also attracted considerable interest in the Machine Learning community, is the variational approximation [8]. Its goal is to replace an intractable distribution like (12) by a different, sufficiently close distribution from a tractable class which we will write in the form £

U V24

¡  "  

E



$'&)(

 6

¡

V ` ! W

 

¡  W

(14)


U U

 

U U 4 ¡ 

will be used in (11) instead of to approximate the average.

e.g. [10]) to minimize the relative entropy between

of the variational free energy

   

£

¤¢¡

2

 

V24

c E

V eg` ! W

¡ 

  ¡

 

will be chosen (see

and

@

resulting in a minimization

2

4

 6

¡



$'&)( 2 4   ¡ W

1     2 ¡ ¡

(15)

32 ¡

being an upper bound to the true free energy ¤¢¡  for any integer £ . The brackets 1  

denote averages with respect to the variational distribution (14).

4

V

For our application to Gaussian process models, we will now specialize to Gaussian priors

` ! W.

For

 

, we choose the quadratic expression

£ G HG  

¡

4%

 

¥DC  ¥§C  ¥DC A@ 5 !  5 ! £ 5 ¤ £

4

¡

5764 01

 £ 6

¡

P 4

 

  ¡

¥

 ¡

P ¥§C  ¥DC A@B 5 !  5 6 (16)

as a suitable trial Hamiltonian, leading to a Gaussian distribution (14). The functions variational solutions to arbitrary real £ , we assume that the optimal parameters should ¤ P £

and ¥ P

are the variational parameters to be optimized. To continue the

¥DC 

be replica symmetric, i.e. we set

¦

¥  ¥

¥DC  5

as well as

¤ £

¥DC  5

¤

¥DC  5

for



¥§C  5

¥DC  5 ¥DC  5

1

!! 

£8¥DC¥DC

¤

¨§ and

1  5



each of the £

! ¥§C 92

¡

, 

P¥DC

0

5

¨2C 5

£¥§C !! !

£ ¡

¤

P





¥DC . 5

P

¡ ! 1



1



 ¥DC 

£

5

¥§C  ¥DC 

25

£ P P £ P

The variational free energy can then be expressed by

the local2 moments ("order parameters" in the language£ of Statistical§Physics)  £ 2

¡ !

1  2

¡ !

1 2

¡

¦ for

£8

and

2

¡



5

¥DC  ¥DC 

5

¥§C C55 

©

¥§C¨ 

££

which have the same replica symmetric structure. Since

£ matrices (such as

¤

is possible to obtain variational equations which contain the number £ of replicas as a simple parameter for which the limit £  6 can be explicitly performed (see appendix). In

this limit, the limiting order parameters

¥§C , ¥DC C  5 5 ¨ 5



5

© are found to have simple inter-

¥DC 

pretations as the (approximate) mean and variance of the predictor



the average over bootstrap data sets while averaged posterior covariance. 

¥§C C 

¨ 5

!©QP

with respect to

P

£

) are assumed to have only two types of entries, it

becomes the (approximate) bootstrap

6 Explicit results for regression with Gaussian processes

V

We consider a GP model for regression with training energy given by Eq. (2). In this

case, the prior measure distribution for the vector



¥§C C , ¢ ¨ 5 ¥§C  ¨FE where

` !¥!W

¥§Ccan

¡

¨¨

be simply represented by an   dimensional Gaussian

!

having zero mean and covariance matrix

¥§C  F

is the covariance kernel of the GP.

U U

  by Using the limiting (for £  6 ) values of order parameters, and by approximating

¡ 

in Eq.(11), the explicit result for the bootstrap mean square generalization error is found to

be



¥¡   £ G G IRQ W ¥ ¥DC G¢ C  I  ¨ ¢ ¦ ¥ ¥  46

¢

 V¥

¡

©

2 ¥§C  

¢



¥DC C  ¨ ¢ ¢

2

4 E ¢ @ ¥

2 ¥ 

` 

 

The entire analysis can be repeated for testing (keeping the training energy fixed) with a

general loss function of the type

  #" £ ¥¡  G IRQ G Q

   

G IRQ

G   46

¢



£

46

¢



!

!PQ

 !F# % GTI

)

The result is

E ¢

¥ ¥DCU¢ . E

¡ ! PQ !



6 ¡ S @

G



(17)

¡

2 ¥ ¥DCU¢2 '

4



¥  `

S



6 ¡

e %$

&(' 10

¡

H £ c  © !

¥§C)¢  G 2

(18)

 ¢§¥ 2 @ E ¢)@



¥DC C  I ¨ $43 ¢ ¢ ¦

¥DCU¢ C)¢ 

¨

¥ 5


8

Simulation Theory }N=1000

7

Error

Test 6

m=N

Bootstrap 5

40 500 1000 1500 2000

2.0 1.9 Error 1.8 1.7 1.6 1.5 1.40

Simulation Theory }N=1000

Test

m=N Bootstrap

500 1000 1500 2000

Size m of Bootstrap Sample Size m of Bootstrap Sample

Figure 1: Average bootstrapped generalization error on Abalone data using square error loss (left) and epsilon insensitive loss (right). Simulation (circles) and theory (lines) based

on the same data set 

set ¥

£

6 .

G

¥§CI C¡ £  $&)( ¥ C  C¡ with¦ H£¢  ¨

 ¥ ¥

2 2¡  £

6T6P6 data points. The GP model uses an RBF kernel

¢ ¥

with

G

£¥¤

on whitened inputs. For the data noise we

We have applied our theory to the Abalone data set [11] where we have computed the approximate bootstrapped generalization errors for the square error loss and the so-called

¦

-insensitive loss which is defined by

¨ ©

§ 0I

    ¢¡

2¡

£ £ '



   ¤

if if if 6

¨  W 

!

$# 

with

£ !©QP

¥  ¥DC 

¢

£2

I

 

  ¦ 

VVV6¥¥ 2



£G

G

¥2G ¥¦G

¦ @

¦ "!

¨¨ ¤

E ¢

. We have set

£

@  ¦

¤

W W (19)

%

and

¦

6 . The bootstrap average from our

theory is obtained from Eq.(18). Figure 1 shows the generalization error measured by the square error loss (Eq.(17), left panel) as well as the one measured by the -insensitive loss ¦ (right panel). Our theory (line) is compared with simulations (circles) which were based

on Monte-Carlo sampling averages that were computed using the same data set

 

  ¡

£

 ¢¡

having

G

G

 

6P6T6 . The Monte-Carlo training sets of size ¡ are obtained by sampling from

with replacement. We find a good agreement between theory and simulations in the

region were ¡ ¨   . When we oversample the data set ¡    , however, the agreement is not so good and corrections to our variational Gaussian approximation would be required.

 ¡

Figure 2 shows the bootstrap average of the posterior variance

whole data set

  ¡ £

,  



¢6

¡

¥DCU¢ C)¢  ¨

 over the

G

6P6P6 , and compares our theory (line) with simulations (circles)

which were based on Monte-Carlo sampling averages. The overall approximation looks better than for the bootstrap generalization error. Finally, it is important to note that all displayed theoretical learning curves have been obtained computationally much faster than their respective simulated learning curves. 7 Outlook The replica approach to bootstrap averages can be extended in a variety of different directions. Besides the average generalization error, one can compute its bootstrap sample fluctuations by introducing more complicated replica expressions. It is also straightforward to apply the approach to more complex problems in supervised learning which are related to Gaussian processes, such as GP classifiers or Support-vector Machines. Since


-1

Simulation Theory }N=1000

10

Variance

Posterior -2

10

0 500 1000 1500 2000

Size m of Bootstrap Sample Figure 2: Bootstrap averaged posterior variance for Abalone data. Simulation (circles) and

theory (line) based on the same data set

 T¡ with   £ G 6P6T6 data points.

our method requires the solution of a set of variational equations of the size of the original training set, we can expect that its computational complexity should be similar to the one needed for making the actual predictions with the basic model. This will also apply to the problem of very large datasets, where one may use a variety of well known sparse approximations (see e.g. [9] and references therein). It will also be important to assess the quality of the approximation introduced by the variational method and compare it to alternative approximation techniques in the computation of the replica average (11), such as the mean field method and its more complex generalizations (see e.g. [10]). Acknowledgement We would like to thank Lars Kai Hansen for stimulating discussions. DM thanks the Copenhagen Image and Signal Processing Graduate School for financial support. Appendix: Variational equations For reference, we will give the explicit form of the equations for variational and order parameters in the limit £ 6 . The derivations will be given elsewhere. We obtain

¥DC)¢ 

2  2 

G  

5764 5764



G

 

¥DCU¢ C  ¥DC  ¨ 5 5 ¥DC C  ¥DC C  ¥§C 

¥

P

¢ ¨ 5

I

¨ 5 5 ¤ P  

£¢¥¤ @

¢  ¥ ¥§C  ¥DC F. ¢ 5 £  ¡ ¢ 5 ¤ P¡ ¢ ¤ P ¢ ¡

©

£  (20)

¡



¥§C C  ¢ ¨ 

¥DCU¢ C  ¨ 5

£ 

¡

(21)

where the matrix  is given by



¡ 

£

I

 ¢

5 £  where ¥DC C  ¢ ¨ 5

¡

is the kernel matrix. Finally

2 (22)

The order parameter equations Eqs.(20-22) must be solved together with the variational equations which are given by

¦ ¥§C 

¤

P

¢

£ ¥I

¡ ¥ @

¥DC)¢ C)¢ F ¨

(23) 


¥

P

¥§C 

¢

¦ ¥§C)¢ ¥P ¥DCPU¢ ¥§C)¢F. ¤

¤

P £ ¡ ¤ ¤ P

¥§C)¢A2

2 £ 2 V¥ E

¢

¦ ¥DC  ¥§CP)¢   ¤

E

¢

2

£ © ¢ @ ¥  ¥DC)¢ CU¢ ¨ W

¥¦ ¥DC 7

¤

P ¢

¡

¥

(24) (25)

with

Combining Eqs.(22) and (23), a self consistent matrix equation

¢ ¢

obtained where depends on the¢ diagonal elements

on a good initial guess for

parameters (24,25). © and 

¥DCU¢  ¥§C)¢ C)¢   ¨

¢

¥§C C  ¨



¢ ¥§C C . ¨



£

¥ ¡  @  ¢9I ¡



is

Its iterative solution (based

) requires usually only a few iterations. The order

can then be solved subsequently using Eq.(20,21) with

References [1] A. Engel and C. Van den Broeck, Statistical Mechanics of Learning (Cambridge University Press, 2001). [2] H. Nishimori, Statistical Physics of Spin Glasses and Information Processing (Oxford Science Publications, 2001). [3] B. Efron and R. J. Tibshirani, An Introduction to the Bootstrap, Monographs on Statistics and Applied Probability 57 (Chapman¢ Hall, 1993). [4] M. M´ezard, G. Parisi, and M. A. Virasoro, Spin Glass Theory and Beyond, Lecture Notes in Physics 9 (World Scientific, 1987). [5] J. Shao and D. Tu, The Jackknife and Bootstrap, Springer Series in Statistics (Springer Verlag, 1995). [6] D. Malzahn and M. Opper, A variational approach to learning curves, NIPS 14, Editors: T.G. Dietterich, S. Becker, Z. Ghahramani, (MIT Press, 2002). [7] R. Neal, Bayesian Learning for Neural Networks, Lecture Notes in Statistics 118 (Springer, 1996). [8] R. P. Feynman and A. R. Hibbs, Quantum mechanics and path integrals (Mc GrawHill Inc., 1965). [9] L. Csat´o and M. Opper, Sparse Gaussian Processes, Neural Computation 14, No 3, 641 - 668 (2002). [10] M. Opper and D. Saad (editors), Advanced Mean Field Methods: Theory and Practice, (MIT Press, 2001). [11] From http://www1.ics.uci.edu/£ mlearn/MLSummary.html. The data set contains 4177 examples. We used a representative fraction (the forth block (a 1000 data) from the list).


