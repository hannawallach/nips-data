The Decision List Machine

Marina Sokolova SITE, University of Ottawa Ottawa, Ont. Canada,K1N-6N5 sokolova@site.uottawa.ca Nathalie Japkowicz SITE, University of Ottawa Ottawa, Ont. Canada,K1N-6N5 nat@site.uottawa.ca Mario Marchand SITE, University of Ottawa Ottawa, Ont. Canada,K1N-6N5 marchand@site.uottawa.ca John Shawe-Taylor Royal Holloway, University of London Egham, UK, TW20-0EX jst@cs.rhul.ac.uk

Abstract We introduce a new learning algorithm for decision lists to allow features that are constructed from the data and to allow a tradeoff between accuracy and complexity. We bound its generalization error in terms of the number of errors and the size of the classifier it finds on the training data. We also compare its performance on some natural data sets with the set covering machine and the support vector machine.

1 Introduction

The set covering machine (SCM) has recently been proposed by Marchand and Shawe-Taylor (2001, 2002) as an alternative to the support vector machine (SVM) when the objective is to obtain a sparse classifier with good generalization. Given a feature space, the SCM tries to find the smallest conjunction (or disjunction) of features that gives a small training error. In contrast, the SVM tries to find the maximum soft-margin separating hyperplane on all the features. Hence, the two learning machines are fundamentally different in what they are trying to achieve on the training data. To investigate if it is worthwhile to consider larger classes of functions than just the conjunctions and disjunctions that are used in the SCM, we focus here on the class of decision lists introduced by Rivest (1987) because this class strictly includes both conjunctions and disjunctions and is strictly included in the class of linear threshold functions (Marchand and Golea, 1993). Hence, we denote by decision list machine (DLM) any classifier which computes a decision list of Boolean-valued features, including features that are possibly constructed from the data. In this paper, we use the set of features introduced by Marchand and Shawe-Taylor (2001, 2002) known as data-dependent balls. By extending the sample compression technique of Littlestone and Warmuth (1986), we bound the generalization error of the DLM with data-dependent balls in terms of the number of errors and the number of balls it achieves on the training data. We also show that the DLM with balls can provide


better generalization than the SCM with this same set of features on some natural data sets. 2 The Decision List Machine Let x denote an arbitrary n-dimensional vector of the input space X which could be training set S = P N consists of a set P of positive training examples and a set N of negative training examples. We define a feature as an arbitrary Boolean-valued function that maps X onto {0,1}. Given any set H = {hi(x)}|H| of features hi(x)

arbitrary subsets of n . We consider binary classification problems for which the

i=1

and any training set S, the learning algorithm returns a small subset R  H of features. Given that subset R, and an arbitrary input vector x, the output f(x) of the Decision List Machine (DLM) is defined to be: If (h1(x)) then b1 Else If (h2(x)) then b2 ... Else If (hr(x)) then br

Else br +1

where each bi  0,1 defines the output of f(x) if and only if hi is the first feature (where r = |R|) is known as the default value. Note that f computes a disjunction of his, we simply place in f the negation of each hi with bi = 0 for i = 1...r and

to be satisfied on x (i.e. the smallest i for which hi(x) = 1). The constant br +1

of the his whenever bi = 1 for i = 1...r and br +1 = 0. To compute a conjunction

br +1 = 1. Note, however, that a DLM f that contains one or many alternations

(i.e. a pair (bi,bi ) for which bi = bi +1 +1 for i < r) cannot be represented as a (pure)

conjunction or disjunction of his (and their negations). Hence, the class of decision lists strictly includes conjunctions and disjunctions. From this definition, it seems natural to use the following greedy algorithm for building a DLM from a training set. For a given set S = P  N of examples (where P  P and N  N) and a given set H of features, consider only the features hi  H which make no errors on either P or N . If hi makes no error with P , let Qi be the subset of examples of N on which hi makes no errors. Otherwise, if hi makes no error with N , let Qi be the subset of examples of P on which hi makes no errors. In both cases we say that hi is covering Qi. The greedy algorithm starts with S = S and an empty DLM. Then it finds the hi with the largest |Qi| and appends this hi to the DLM. It then removes Qi from S and repeat to find

the hk with the largest |Qk| until either P or N is empty. It finally assigns br to the class label of the remaining non-empty set. +1

Following Rivest (1987), this greedy algorithm is assured to build a DLM that makes no training errors whenever there exists a DLM on a set E  H of features that makes zero training errors. However, this constraint is not really required in practice since we do want to permit the user of a learning algorithm to control the tradeoff between the accuracy achieved on the training data and the complexity (here the size) of the classifier. Indeed, a small DLM which makes a few errors on the training set might give better generalization than a larger DLM (with more features) which makes zero training errors. One way to include this flexibility is to early-stop the greedy algorithm when there remains a few more training examples to be covered. But a further reduction in the size of the DLM can be accomplished


Algorithm BuildDLM(P, N, pp, pn, s, H) Input: A set P of positive examples, a set N of negative examples, the penalty values pp and pn , a stopping point s, and a set H = {hi(x)}|H| of Boolean-valued features.

i=1

Output: A decision list f consisting of a set R = {(hi,bi)}ri =1

of features hi with

their corresponding output values bi, and a default value br+1. Initialization: R = ,P = P,N = N 1. For each hi  H, let Pi and Ni be respectively the subsets of P and N correctly classified by hi. For each hi compute Ui, where: Ui = max {|Pi|- pn ·|N - Ni|, |Ni|- pp ·|P - Pi|} 2. Let hk be a feature with the largest value of Uk. 3. If (|Pk| - pn · |N - Nk|  |Nk| - pp · |P - Pk|) then R = R  {(hk,1)}, P = P - Pk, N = Nk. 4. If (|Pk| - pn · |N - Nk| < |Nk| - pp · |P - Pk|) then R = R  {(¬hk,0)}, N = N - Nk, P = Pk. 5. Let r = |R|. If (r < s and P =  and N = ) then go to step 1

def

6. Set br +1 = ¬br. Return f.

Figure 1: The learning algorithm for the Decision List Machine

by considering features hi that do make a few errors on P (or N ) if many more examples Qi  N (or Qi  P ) can be covered. Hence, to include this flexibility in choosing the proper tradeoff between complexity and accuracy, we propose the following modification of the greedy algorithm. For every feature hi, let us denote by Pi the subset of P on which hi makes no errors and by Ni the subset of N on which hi makes no error. The above greedy algorithm is considering only features for which we have either Pi = P or Ni = N , but to allow small deviation from these choices, we define the usefullness Ui of feature hi by Ui = max {|Pi|- pn ·|N - Ni|, |Ni|- pp ·|P - Pi|}

def

where pn denotes the penalty of making an error on a negative example whereas pp denotes the penalty of making an error on a positive example. Hence, each greedy step will be modified as follows. For a given set S = P  N , we will select the feature hi with the largest value of Ui and append this hi in the DLM. If |Pi| - pn · |N - Ni|  |Ni| - pp · |P - Pi|, we will then remove from S every example in Pi (since they are correctly classified by the current DLM) and we will also remove from S every example in N - Ni (since a DLM with this feature is already misclassifying N - Ni, and, consequently, the training error of the DLM will not increase if later features err on examples in N -Ni). Otherwise if |Pi|-pn·|N -Ni| < |Ni|-pp·|P -Pi|, we will then remove from S examples in Ni  (P - Pi). Hence, we recover the simple greedy algorithm when pp = pn = . The formal description of our learning algorithm is presented in Figure 1. The penalty parameters pp and pn and the early stopping point s are the model-selection parameters that give the user the ability to control the proper tradeoff between the training accuracy and the size of the DLM. Their values could be determined either


by using k-fold cross-validation, or by computing our bound (see section 4) on the generalization error. It therefore generalizes the learning algorithm of Rivest (1987) by providing this complexity-accuracy tradeoff and by permitting the use of any kind of Boolean-valued features, including those that are constructed from the data. Finally let us mention that Dhagat and Hellerstein (1994) did propose an algorithm for learning decision lists of few relevant attributes but this algorithm is not practical in the sense that it provides no tolerance to noise and does not easily accommodate parameters to provide a complexity-accuracy tradeoff. 3 Data-Dependent Balls For each training example xi with label yi  {0,1} and (real-valued) radius , we define feature hi, to be the following data-dependent ball centered on xi:

hi,(x) = h(x, xi) = def yi if d(x, xi)   yi otherwise

where yi denotes the Boolean complement of yi and d(x,x ) denotes the distance between x and x . Note that any metric can be used for d. So far, we have used only the L1,L2 and L metrics but it is certainly worthwhile to try to use metrics that actually incorporate some knowledge about the learning task. Moreover, we could use metrics that are obtained from the definition of an inner product k(x,x ). Given a set S of m training examples, our initial set of features consists, in principle, only to consider the set of m - 1 distances {d(xi,xj)}j . This reduces our initial

of H = iS [0,[ hi,. But obviously, for each training example xi, we need

=i

set H to O(m2) features. In fact, from the description of the DLM in the previous section, it follows that the ball with the largest usefulness belongs to one of the following following types of balls: type Pi, Po, Ni, and No. Balls of type Pi (positive inside) are balls having a positive example x for its center and a radius given by  = d(x,x )- for some negative example x (that we call a border point) and very small positive number . Balls of type Po (positive outside) have a negative example center x and a radius  = d(x,x )+ given by a negative border x . Balls of type Ni (negative inside) have a negative center x and a radius  = d(x, x ) - given by a positive border x . Balls of type No (negative outside) have a positive center x and a radius  = d(x,x )+ given by a positive border x . This proposed set of features, constructed from the training data, provides to the user full control for choosing the proper tradeoff between training accuracy and function size. 4 Bound on the Generalization Error Note that we cannot use the "standard" VC theory to bound the expected loss of DLMs with data-dependent features because the VC dimension is a property of a function class defined on some input domain without reference to the data. Hence, we propose another approach. Since our learning algorithm tries to build a DLM with the smallest number of datadependent balls, we seek a bound that depends on this number and, consequently, on the number of examples that are used in the final classifier (the hypothesis). We can thus think of our learning algorithm as compressing the training set into a small subset of examples that we call the compression set. It was shown by Littlestone and Warmuth (1986) and Floyd and Warmuth (1995) that we can bound


the generalization error of the hypothesis f if we can always reconstruct f from the compression set. Hence, the only requirement is the existence of such a reconstruction function and its only purpose is to permit the exact identification of the hypothesis from the compression set and, possibly, additional bits of information. Not surprisingly, the bound on the generalization error increases rapidly in terms of these additional bits of information. So we must make minimal usage of them. We now describe our reconstruction function and the additional information that it needs to assure, in all cases, the proper reconstruction of the hypothesis from a compression set. Our proposed scheme works in all cases provided that the learning algorithm returns a hypothesis that always correctly classifies the compression set (but not necessarily all of the training set). Hence, we need to add this constraint in BuildDLM for our bound to be valid but, in practice, we have not seen any significant performance variation introduced by this constraint. We first describe the simpler case where only balls of types Pi and Ni are permitted and, later, describe the additional requirements that are introduced when we also permit balls of types Po and No. Given a compression set  (returned by the learning algorithm), we first partition it into four disjoint subsets Cp,Cn,Bp, and Bn consisting of positive ball centers, negative ball centers, positive borders, and negative borders respectively. Each example in  is specified only once. When only balls of type Pi and Ni are permitted, the center of a ball cannot be the center of another ball since the center is removed from the remaining examples to be covered when a ball is added to the DLM. But a center can be the border of a previous ball in the DLM and a border can be the border of more than one ball. Hence, points in BpBn are examples that are borders without being the center of another ball. Because of the crucial importance of the ordering of the features in a decision list, these sets do not provide enough information by themselves to be able to reconstruct the hypothesis. To specify the ordering of each ball center it is sufficient to provide log2(r) bits of additional information where the number r of balls is given by r = cp+cn for cp = |Cp| and cn = |Cn|. To find the radius i for each center xi we start with Cp = Cp,Cn = Cn,Bp = Bp,Bn = Bn, and do the following, sequentially from the first center to the last. If center xi  Cp, then the radius is given by i = minxjCnBn d(xi,xj)- and we remove center xi from Cp and any other point from Bp covered by this ball (to find the radius of the other balls). If center xi  Cn, then the radius is given by i = minxjCpBp d(xi,xj)- and we remove center xi from Cn and any other point from Bn covered by this ball. The output bi for each ball hi is 1 if the center xi  Cp and 0 otherwise. This reconstructed decision list of balls will be the same as the hypothesis if and only if the compression set is always correctly classified by the learning algorithm. Once we can identify the hypothesis from the compression set, we can bound its generalization error. Theorem 1 Let S = P  N be a training set of positive and negative examples of size m = mp + mn. Let A be the learning algorithm BuildDLM that uses data-dependent balls of type Pi and Ni for its set of features with the constraint that the returned function A(S) always correctly classifies every example in the compression set. Suppose that A(S) contains r balls, and makes kp training errors on P, kn training errors on N (with k = kp + kn), and has a compression set  = Cp  Cn  Bp  Bn (as defined above) of size  = cp + cn + bp + bn . With probability 1 -  over all random training sets S of size m, the generalization error er(A(S)) of A(S) is bounded by

er(A(S))  1 - exp -1 m -  - k lnB + ln(r!) + ln 1 


def where  =

where B

-6

2 6 · ((cp +1)(cn +1)(bp +1)(bn +1)(kp +1)(kn +1))- ·  and

mp - cp bp mn cn mn - cn bn mp - cp - bp kp mn - cn - bn kn

2

def =

mp cp

Proof Let X be the set of training sets of size m. Let us first bound the probability Pm = P{S  X : er(A(S))  | m(S) = m} given that m(S) is fixed to some value

def

m where mdef m, mp, mn, cp, cn, bp, bn, kp, kn). For this, denote by Ep the subset =( of P on which A(S) makes an error and similarly for En. Let I be the message of log2(r!) bits needed to specify the ordering of the balls (as described above). Now define Pm to be

Pm def

= P{S  X : er(A(S))  | Cp = S1,Cn = S2,Bp = S3,Bn = S4 Ep = S5,En = S6,I = I0,m(S) = m}

for some fixed set of disjoint subsets {Si}6i =1

of S and some fixed information mes-

sage I0. Since B is the number of different ways of choosing the different compression subsets and set of error points in a training set of fixed m, we have: Pm  (r!) · B · Pm where the first factor comes from the additional information that is needed to specify the ordering of r balls. Note that the hypothesis fdefA(S) is fixed in Pm (because = the compression set is fixed and the required information bits are given). To bound Pm, we make the standard assumption that each example x is independently and identically generated according to some fixed but unknown distribution. Let p be the probability of obtaining a positive example, let  be the probability that the fixed hypothesis f makes an error on a positive example, and let  be the probability that f makes an error on a negative example. Let tp=cp +bp +kp and let tn=cn + bn + kn. We then have:

def

def

Pm = (1 - )mp-tp(1 - )m

m-tn

 (1 - )m

m =tp

= [(1 - )p + (1 - )(1 - p)]m  (1- )m

-tn-tp

Consequently:

-tp (1 - )m

-tn-mp

m - tn - tp mp - tp pmp-tp(1 - p)m -tn-mp

-tn-m

m - tn - tp m - tp pm -tp (1 - p)m -tn-m

-tn-tp = (1 - er(f))m -tn-tp

Pm  (r!) · B · (1 - )m -tn-tp .

The theorem is obtained by bounding this last expression by the proposed value for (m) and solving for since, in that case, we satisfy the requirement that

P S  X:er(A(S))  =

m

 (m)P S  X: m(S) = m 

PmP S  X: m(S) = m (m) = 

m m

where the sums are over all possible realizations of m for a fixed mp and mn. With the proposed value for (m), the last equality follows from the fact that


 i=1 (1/i2) = 2/6.

The use of balls of type Po and No introduces a few more difficulties that are taken into account by sending more bits to the reconstruction function. First, the center of a ball of type Po and No can be used for more than one ball since the covered examples are outside the ball. Hence, the number r of balls can now exceed cp + cn = c. So, to specify r, we can send log2() bits. Then, for each ball, we can send log2c bits to specify which center this ball is using and another bit to specify if the examples covered are inside or outside the ball. Using the same notation as before, the radius i of a center xi of a ball of type Po is given by i = maxxjCnBn d(xi, xj)+ , and for a center xi of a ball of type No, its radius is given by i = maxxjCpBp d(xi,xj)+ . With these modifications, the same proof of Theorem 1 can be used to obtain the next theorem. Theorem 2 Let A be the learning algorithm BuildDLM that uses data-dependent balls of type Pi, Ni, Po, and No for its set of features. Consider all the definitions used for Theorem 1 with cdef cp+cn. With probability 1- over all random training = sets S of size m, we have

er(A(S))  1 - exp -1 m -  - k lnB + ln + rln(2c) + ln 1 

Basically, our bound states that good generalization is expected when we can find a small DLM that makes few training errors. In principle, we could use it as a guide for choosing the model selection parameters s, pp, and pn since it depends only on what the hypothesis has achieved on the training data. 5 Empirical Results on Natural data We have compared the practical performance of the DLM with the support vector machine (SVM) equipped with a Radial Basis Function kernel of variance 1/. The data sets used and the results obtained are reported in Table 1. All these data sets where obtained from the machine learning repository at UCI. For each data set, we have removed all examples that contained attributes with unknown values (this has reduced substantially the "votes" data set) and we have removed examples with contradictory labels (this occurred only for a few examples in the Haberman data set). The remaining number of examples for each data set is reported in Table 1. No other preprocessing of the data (such as scaling) was performed. For all these data sets, we have used the 10-fold cross validation error as an estimate of the generalization error. The values reported are expressed as the total number of errors (i.e. the sum of errors over all testing sets). We have ensured that each training set and each testing set, used in the 10-fold cross validation process, was the same for each learning machine (i.e. each machine was trained on the same training sets and tested on the same testing sets). The results reported for the SVM are only those obtained for the best values of the kernel parameter  and the soft margin parameter C found among an exhaustive list of many values. The values of these parameters are reported in Marchand and Shawe-Taylor (2002). The "size" column refers to the average number of support vectors contained in SVM machines obtained from the 10 different training sets of 10-fold cross-validation. We have reported the results for the SCM (Marchand and Shawe-Taylor, 2002) and the DLM when both machines are equipped with data-dependent balls under the L2 metric. For the SCM, the T column refers to type of the best machine found


Data Set

Name

BreastW Votes Pima Haberman Bupa Glass Credit

#exs

683 52 768 294 345 214 653

SVM

size 58 18

errors

19 3

SCM with balls 1.8 2 15

T c p s errors

DLM with balls

pp pn 2.1 1 s 2

d 0.9 1 6

c c 1.1 3 189 1.4 1 71

526 203 146 71 266 107 125 34 423 190

d 2.8 9 106 d  2 36 d 1.2 4 194

T c s c s c c c

errors

14 3 189 65 108

0.1 0.3 1 1.5 1.5 6

2 2 3 2 7 4

4.8  12 28 1  11 197

Table 1: Data sets and results for SVMs, SCMs, and DLMs.

(c for conjunction, and d for disjunction), the p column refers the best value found for the penalty parameter, and the s column refers the the best stopping point in terms of the number of balls. The same definitions applies also for DLMs except that two different penalty values (pp and pn) are used. In the T column of the DLM results, we have specified by s (simple) when the DLM was trained by using only balls of type Pi and Ni and by c (complex) when the four possible types of balls where used (see section 3). Again, only the values that gave the smallest 10-fold cross-validation error are reported. The most striking feature in Table 1 is the level of sparsity achieved by the SCM and the DLM in comparison with the SVM. This difference is huge. The other important feature is that DLMs often provide slightly better generalization than SCMs and SVMs. Hence, DLMs can provide a good alternative to SCMs and SVMs. Acknowledgments Work supported by NSERC grant OGP0122405 and, in part, by the EU under the NeuroCOLT2 Working Group, No EP 27150. References Aditi Dhagat and Lisa Hellerstein. PAC learning with irrelevant attributes. In Proc. of the 35rd Annual Symposium on Foundations of Computer Science, pages 64­74. IEEE Computer Society Press, Los Alamitos, CA, 1994. Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the Vapnik-Chervonenkis dimension. Machine Learning, 21(3):269­304, 1995. N. Littlestone and M. Warmuth. Relating data compression and learnability. Technical report, University of California Santa Cruz, 1986. Mario Marchand and Mostefa Golea. On learning simple neural concepts: from halfspace intersections to neural decision lists. Network: Computation in Neural Systems, 4:67­85, 1993. Mario Marchand and John Shawe-Taylor. Learning with the set covering machine. Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001), pages 345­352, 2001. Mario Marchand and John Shawe-Taylor. The set covering machine. Journal of Machine Learning Reasearch (to appear), 2002. Ronald L. Rivest. Learning decision lists. Machine Learning, 2:229­246, 1987.


