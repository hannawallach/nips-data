Conditional Models on the Ranking Poset

Guy Lebanon School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 lebanon@cs.cmu.edu John Lafferty School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 lafferty@cs.cmu.edu

Abstract A distance-based conditional model on the ranking poset is presented for use in classification and ranking. The model is an extension of the Mallows model, and generalizes the classifier combination methods used by several ensemble learning algorithms, including error correcting output codes, discrete AdaBoost, logistic regression and cranking. The algebraic structure of the ranking poset leads to a simple Bayesian interpretation of the conditional model and its special cases. In addition to a unifying view, the framework suggests a probabilistic interpretation for error correcting output codes and an extension beyond the binary coding scheme.

 

1 Introduction Classification is the task of associating a single label

¤

¡£¢¥¤

with a covariate . A gener-

alization of this problem is conditional ranking, the task of assigning to

ranking of the items in

¦

¦

a full or partial

. This paper studies the algebraic structure of this problem, and

proposes a combinatorial structure called the ranking poset for building probability models for conditional ranking. In ensemble approaches to classification and ranking, several base models are combined to produce a single ranker or classifier. An important distinction between different ensemble methods is whether they use discrete inputs, ranked inputs, or confidence-rated predictions. In the case of discrete inputs, the base models provide a single item in , and no preference for a second or third choice is given. In the case of ranked input, the base classifiers output a full or partial ranking over . Of course, discrete input is a special case of ranked input, where the partial ranking consists of the single topmost item. In the case of confidence-rated predictions, the base models again output full or partial rankings, but in addition provide a confidence score, indicating how much one class should be preferred to another. While confidence-rated predictions are sometimes preferable as input to an ensemble method, such confidence scores are often not available (as is typically the case in metasearch), and even when they are available, the scores may not be well calibrated. This paper investigates a unifying algebraic framework for ensemble methods for classification and conditional ranking, focusing on the cases of discrete and ranked inputs. Our approach is based on the ranking poset on items, denoted , which consists of the collection of all full and partial rankings equipped with the partial order given by re-

¤

¤

§ ¨©


finement of rankings. The structure of the poset of partial ranking over

¤

gives rise to

natural invariant distance functions that generalize Kendall's Tau and the Hamming distance. Using these distance functions we define a conditional model  ¢¡¤£¦¥¨§©©!#" for classification and ranking, and includes as a special case the Mallows model [11]. In addition, the model represents algebraically the way in which input classifiers are combined in certain ensemble methods, including error correcting output codes [4], several versions of AdaBoost [7, 1], and cranking [10]. In Section 2 we review some basic algebraic concepts and in Section 3 we define the ranking poset. The new model and its Bayesian interpretation are described in Section 4. A derivation of some special cases is given in Section 5, and we conclude with a summary in Section 6. 2 Permutations and Cosets We begin by reviewing some basic concepts from algebra, with some of the notation and definitions borrowed from Critchlow [2]. permutation of )0(# , then ¥3£¦4" denotes the rank given to item 4 and ¥65 £¦4" denotes the item assigned to rank 4 . The collection of all permutations of -items forms the nonabelian symmetric group of order , denoted 7 . The multiplicative notation ¥98©A@B¥¢©

where ¥$%©&©! ¢ ¨ ©

. This conditional model generalizes several existing models  

Identifying the items to be ranked  ' ¡ ¡ © with the numbers (#' , if ¥ denotes a §

§21  §

§ ©

is used to denote function composition.

The subgroup of 7

7 ©

5

; thus,

©

consisting of all permutations that fix the top C positions is denoted

7 © 5 D@E)¥ F7 §¥3£¦4"3@G4H242@I(#'%CP10

7

© 5

Q¥R@S)Q©¥T§U© 97

¢ © 5

V1

¢ © (1)

The right coset

(2)

is equivalent to a partial ranking, where there is a full ordering of the C top-ranked items.

The set of all such partial rankings forms the quotient space 7 XW#7

§

An ordered partition of

§

is a sequence Y`@

§2¢ §¢a

© ©

5

.

§

of positive integers that sum

to . Such an ordered partition corresponds to a partial ranking of type Y with

in the first position,

§b

is conveyed about orderings within each position. A partial ranking of the top C items is

a special case with cd@eCgfB(# v

§hi@p2@ §d@p(q §srhi@ §FtuC

§ §21

. More formally, let

 @B)q(q  1q bw@x)  fy(q  f

§

v

§

v

©q888P7§ ©

§¢b1q888'

v

a @B)  fy888f a  fy(# .

§

contains all permutations ¥

¢75 ©

 items

items in the second position and so on. No further information

Then the subgroup 7y@ 7

the set equality ¥3£ "@ v

within each

v

holds for each 4 ; that is, all permutations that only permute

for which

. A partial ranking of type Y is equivalent to a coset 7  ¥ and the set of such

partial rankings forms the quotient space 7

©XW#7

.

We now describe a convenient notation for permutations and cosets. In the following, we list items separated by vertical lines, indicating that the items on the left side of the line are preferred to (ranked higher than) the items on the right side of the line. For example, the permutation ¥3£U(Q"w@B¤U¥3£q"@(#¥3£0"d@B is denoted by X§e(0§ . A partial ranking 7gf ¥ denoted by !§e(#%¤Uj!%k . A partial ranking 7  where Yl@mX% with items (#%¤%k ranked in the first position is denoted by (qX%kX§nj . properties: oP£¦¥$U¥"g@wv , oP£¦¥$©¢"¨xyv when ¥z@e© , oP£¦¥$%©¢"g@woP£©¥" , and the triangle

where the top 3 items are ¤%¤( is denoted by P§!§i(V§j!%k . A classification ¡ 5h may thus be

h

A distance function o on 7 © is a function oxpq7 ©rl7 ©xsut

that satisfies the usual


¢ ¤ ¦  § ¤ ¤ 

 ¢¡ £¤¡ ¥  ¢¡ ¥¦¡ £ £¦¡  ¢¡ ¥ ¥¤¡  ¢¡ £ £¦¡ ¥¤¡   ¥¦¡ £¤¡  

© ¤ ¦  ¢ ¤ ¢©  ¢ ¢© ¤  © ¤ ¤  § ¤ ¢© 

PSfrag replacements PSfrag replacements

¥¤¡  §¨ £ £¢¨ ¥¤¡    §¡ £¦¨ ¥  §¨ £¦¡ ¥  §¨ ¥¦¡ £ £¦¡  §¨ ¥

© ¤ ¢©  §© ¦© ¦  § ¦© ¦©  © ¤ ¢© 

 §¨ £¢¨ ¥ §© ¢© ¦© 

Figure 1: The Hasse diagram of

¨

of the lines are dotted for easier visualization.

inequality o!£¥$©¢"

of the items



¡V' ¡oP£¦¥$is ©

h

(left) and a partial Hasse diagram of  (right). Some

¨

Formally, this amounts to right invariance oP£¦¥$%©¢" @GoP£¦¥ X© X" , for all ¥$%©¦

 !

is Kendall's Tau £¦¥$%©¢" , given by "

£¦¥¢© 5 £4"2t¥¢© 5 £21"U" #&(' 0)  

A popular right invariant distance on 7

©

#

£¥$©¢" @

¦Dxev and £

" %$

©

¦

X"'f oP£¤%©¢" forall ¥$%©¦ F7 . Inaddition,sincetheindexing arbitrary, it is appropriate to require invariance to relabeling of .

¢97 © ¤

¢ ©

5 

(3)



where £

interpreted as the number of discordant pairs of items between ¥ and © , or the minimum number of adjacent transpositions needed to bring ¥65 to ©h5 . An adjacent transposition flips a pair of items that have adjacent ranks. Critchlow [2] derives extensions of Kendall's

 

©

Tau and other distances on 7 3 The Ranking Poset

to distances on partial rankings.

)

¦P"i@ ( for ) ¦P"i@Rv

otherwise [8]. Kendall's Tau £¦¥$%©¢" can be "

We first define partially ordered sets and then proceed to define the ranking poset. Some of the definitions below are taken from [12], where a thorough introduction to posets can be found. A partially ordered set or poset is a pair £43d65q" , where 3 is a set and 5 is a binary relation

that satisfies (1) 75

¦ ¦

, (2) if 75 and 85 , and (3) if 75

and

¦ ¡ ¡

and 85@9

¡

then A5B9 for all  C9 D3 . We write DE

covers

¦

and write GF when HE

¦ ¡

¦

¦¦ ¡

¡P ¢ ¦ ¡ ¡¦ ¦ ¡

then when D5

¦A@¦ ¡

. We say that

¡

and there is no 9 I3 such that HEA9 and 9PE

¢

¡ ¦Dz@¦ ¡

. A

finite poset is completely described by the covering relation. The planar Hasse diagram of

£43dQ5q" is the graph for which the elements of

3 are the nodes and the edges are given by

the covering relation. In addition, we require that if RF

§ ¢ © 7 . The partial order of

The ranking poset

¨ ©

¦ ¡ ¡

then is drawn higher than .

¦

is the poset in which the elements are all possible cosets 7TS#¥ ,

where U is an ordered partition of

refinement; that is, ¥8El© if we can get from ¥ to © by adding vertical lines. Note that

and ¥ ¨ ©

is defined by

is different from the poset of all set partitions of )q(#P 21 ordered by partition refinement

since in

of

¨

h

¨ ©

the order of the partition elements matters. Figure 1 shows the Hasse diagram

and a portion of the Hasse diagram of  .

¨

§

¨©

A subposet £2Vi65XW3" of £43dQ5`Y6" is defined by Vbac3 and 85XW

¦ ¡

if and only if d5XY .

¦ ¡

A chain is a poset in which every two elements are comparable. A saturated chain e of


length C is a sequence of elements 0' ¦¡  ¦ ¢

3 that satisfy

¦¢  ¦

F

 m888XF

F

¦

.

A chain of 3 is a maximal chain if there is no other saturated chain of 3 that contains it. A poset, there is a rank or grade function Ap03 is a

¦ ¦

graded poset of rank

§

is a poset in which every maximal chain has length . In a graded

§

minimal element and £

It is easy to see that

£ ¤"3@¦£P£ P"f ( if

¨ ©

¡ ¦

£ es )QvX¥¤01 such that £ " @mv if

¦ ¡ RF

£

.

is a graded poset of rank

consisting of )

¦ ¢¨ © ¦" ¢ §££

1 . In particular, the elements in the C th grade,

§tD(¨

and the rank of every element

©¨§©

to denote the subposet of is the number of vertical lines in its denotation. We use

¨ ©

all of which are incomparable, are denoted by

grade

¨ ©¨§©

¨ ©¨§

multilabel classifications l§i)q(#P  5

 . Classifications 4 §)0(# 4 residein

§21

§21

. Full orderings occupy the topmost

¨ ©¢§

. Other elements of

¨ ©¨§

are

where  a )q(q' .

§21

4 Conditional Models on the Ranking Poset We now present a family of conditional models defined in terms of the ranking poset. To begin, suppose that o is a right invariant function on . That is, oP£¦¥$%©¢" @ o!£¥ ¤© X" for

¨ ©  !

all ¥$© and  action of 7 on

¢ ¨© ©

¨¡ ©

7 . Here right invariance is defined with respect to the natural , given by ¢ ©

) Q1§i) 1  0§s888§i) 1 86y@S)Q¢£ "H1&§) ¢£ "H1#§ 888Q§i)Q¢£ "s1    

¡

 

¡

  

¡



¡



¡

! 

(4)

The function o may or may not be a metric; its interpretation as a measure of dissimilarity, however, remains. and up moves on the Hasse diagram will be denoted by " and # respectively. A distance o defined in terms of " and # moves is easily shown to be right invariant because the is, the group action of 7 commutes with " and # moves:

We will examine several distances that are based on the covering relation F of

¨© . Down

group action of 7 ©

© ¨ ©

does not change the covering relation between any two elements; that

¨ ©

')(0 (

¨ ©

on tt¤t¤tHs $&% ¨ ©

0((

¨ ©

¨ ©

1 ( 0(

¨ ©

ttXt¤tHs $&%

¨ ©

0((

¨ ©

' 1 (5)

tt¤t¤tHs% $ ttXt¤tHs% $

While the metric properties of o are not required in our model, the right invariance property

is essential since we want to treat all

¡  in the same manner.

We are now ready to give the general form of a conditional model on

¨ © . Let o be an

invariant function, as above. The model takes as input C rankings ©  ©!b#'© contained or default model. Then o and ¤4  specify an exponential model   ¡ £¦¥¨§65w" given by GH

in some subset 2 a

¨ ©¢§© ¨ ©

of the ranking poset. For example, each ©¨3 could be an element of

 . Let ¤4  beaprobabilitymassfunctionon ¨ © , which will be the "carrier density"

5

  ¡ £¥T§65w" @ 7 where 8 TSVU t , ¥ XW a

¢ ¢

normalizing constant 7

£98h@5d" @

( £98 @5w"

¨ ©

`Facb

#

3 PI

¤A V£¦¥"CB4DFE $ ¢oP£¦¥$%©C3"6QR

¨ © . The term

3

(6) £98 ¥5w" is the

, and ©F3 Y2 a GH

#

3  I

¢ 7

#

¤   £¦¥"CB4DFE $ 3 o!£¥$© 3 "QR  (7)


Thus, conditional on 5

¡ ¥¤ ¢

2 ,  ¡ £U8Q§ w" formsaprobabilitydistributionover¥

§¤

5

¢ W@a ¨ © .

Given a data set £¥£¢ ¥5¦¢ ©¨" , the parameters

mizing the conditional loglikelihood #£ " @ 8  maximum. 4.1 A Bayesian interpretation I

§ 

¢

S will typically be selected by maxi P¡¤£¦¥ ¢ §65 ¢ " , a marginal likelihood

¥¤ ¥¤

or posterior. Under mild regularity conditions, #£ " will be convex and have a unique global

I

We now derive a Bayesian interpretation for the model given by (6). Our result parallels the interpretation of multistage ranking models given by Fligner and Verducci [6]. The key fact is that, under appropriate assumptions, the normalizing term does not depend on the partial ordering in the one-dimensional case. Proposition 4.1. Suppose that o is right invariant and that 2 is invariant under the action

of 7 . If 7 © ©

acts transitively on W then

#

 a "!

Proof. First, note that since 2 a

2 @ 2 for each   ¢ ©

since for ©l@`) 1V§ 888Q§)

that 41% © ¨@G© . Now, since 7

©

¡ 

$#¡

¢

&% `

¤

#

 a '!

¤

@ $#¡ ¢ )(0% `

(8)

for all ¥$U¥21 W and ¢

I

¢

t . 7 . Indeed, 2 U 2 by the invariance assumption, and 2XU2 2

¨ ©

is invariant under the action of 7 , it follows that

©

¡ ¢

1

¥3 

2 we have 413@`)Q5 £ "H1n§s888§i)Q5 £ "H1 ©



¡ 



¡ ¢ 03



such

acts transitively on W , for all ¥$¥21

¢ W there is 5 F7 such that 45i@ 61 . ¥

¥ (9) (10) (11)

¢ ©

We thus have that

7

#

 a  !

#

 a  !

#

 a  !

#

 a  !

@

@

7

Thus, we can write fact depend on ¥ .

@ £ U¥"w@ I

$#¡ $#¡ $#¡ $#¡

&% `

¤

£ ¥" @ I ¢

¢

¢

)7%7 )(9%7

)(9% ¤

` ¤

(by right invariance of ) 8

` ¤

¢ ` (by invariance of @ )

7

£ " since the normalizingconstant for ¥ I

¢ (12) W does not in

The underlying generative model is given as follows. Assume that ¥

the prior ¤4 0£¥" and that ©  ¢© are independently drawn from generalized Mallows

models

  BA¡ £©F3 § ¥" @ 7 ( £ Q"3 ¡ A # ¤

¢ W is drawn from

¢ &% ` A

that the posterior distribution over ¥ is given by ¤A V£¦¥"DC   BA¡ £©C3 § ¥" 3

)`Cacb ¤A V£¦¥"DC   BA¡ £©C3 § ¥" 3 @ C

¢)2 I

! (13)

where ©F3

. Then under the conditions of Proposition 4.1, we have from Bayes' rule

E A ¡ A # ¡ A ¢ &%` ¤ 7

£ Q"H5

AI ¡3  ¤

3

¤A 7 V£¦¥" ! C 3

£ 3 " 5  `Cacb ¤   £¥"

 E FAG# BA¡ ¢ H%`

! (14)

@   ¡ £¦¥¨§65w"I

(15)


We thus have the following characterization of   ¡ £U8Q§65w" . Proposition 4.2. If o is right invariant, 2 is invariant under the action of 7 , and 7

transitively on W , then the model   ¡ £8§65w" defined in equation (6) is the posterior under

independent sampling of generalized Mallows models, ©P3    BA¡ £8§ ¥" , withprior ¥ 

The conditions of this proposition are satisfied, for example, when I@B7   

7 XWq7 ¢¡S as is assumedinthespecialcases ofthenextsection. © W

©XWq7

S and

¤4  . 2 @

© © acts

5 Special Cases This section derives several special cases of model (6), corresponding to existing ensemble methods. The special cases correspond to different choices of  2  S and o in the W definition of the model. In each case ¤   £¥" is taken to be uniform, though the extension to non-uniform ¤   £¦¥" is immediate. Following [9], the unnormalized versions of all the models may be easily derived, corresponding to the exponential loss used in boosting.

 

5.1 Cranking and Mallows

Let x@ t @W @ G@ S 2 ¨ ©¨§©

model  @x7 , and let o!£¥$©¢" be the minimumnumberof down©

up (" # ) moves on the Hasse diagram of 5

¨ © needed to bring ¥ to © . Since adjacent

transpositions of permutations may be identified with a down move followed by an up move over the Hasse diagram, o!£¥$©¢" is equal to Kendall's Tau £¥$©¢" . For example,

"

£U(0§X§¤P§!§i(" @  andthe correspondingpathinFigure1is "

(0§X§ " (#%!§ # !§i(V§ " !§i(q # X§!§e( " n%!§e( # !§X§e(# In this case model (6) becomes the cranking model [10]

¤

  ¡ £¦¥T§¥8P" @ 7 (

E  A¤£  ¡ A¦¥

£  P"8 5 ! ¢

&%

` A  8 t ¢ ¥$©C3 97  ¢ © (16)

The Bayesian interpretation in this case is well known, and is derived in [6]. The generative model is independent sampling of © 3 from a Mallows model whose location parameter is ¥ and whose scale parameter is 3 . Other special cases that fall into this category are the models of Feigin [5] and CritchlowI and Verducci [3]. 5.2 Logistic models

Let @ t , W @T2B@7 W#7 S © ©

 , and let o!£¥$©¢" be the minimum number of up-down

(# " ) moves in the Hasse diagram. Since W @)2 @ 7

§

oP£¦¥$%©¢" @GoP£ 7 © 5  XH7  © 5  5¤"3@

5 ©XW#7 ©

 

5



v if 5 £("3@ 5P5 £("  otherwise

 

(17)

In this case model (6) becomes equivalent to the multiclass generalization of logistic regression. If the normalization constraints in the corresponding convex primal problem are

removed, the model becomes discrete AdaBoost.M2; that is, oP£¦¥$%©¢3V£

(discrete) multiclass weak learner ¨ £ 



¦ ¡¤" ¢

¦P""W

becomes the

)QvX(#1 in the usual boostingnotation. See [9]

for details on the correspondence between exponential models and the unnormalized models that correspond to AdaBoost. 5.3 Error correcting output codes A more interesting special case of the algebraic structure described in Sections 3 and 4 is where the ensemble method is error correcting output coding (ECOC) [4]. Here we set


W @D7 XWq7 © © 5  , l@ 2 ¨ ©¢§ W , andtake the parameterspace to be

S@E)8 yt §  @ bq@B888q@ S

¢

 and

¡   v¤1q (18)

As before, oP£¦¥$%©¢" is the minimal number of up-down (# " ) moves in the Hasse diagram needed to bring ¥ to © .

Since ¥ @ 7

©

  , the model computes probabilities of classifications

¡£¢¥¤  ¤  . On in-

put , the base rankers output ©¨3q£ P" ¦

5

¦ ¢ ¨ ©¨§

cW , which corresponds to one of the binary ¢

I I I I

classifiers in ECOC for the appropriate column of the binary coding matrix. For example, consider a binary classifier trained on the coding column £(#%v¤(#vXvXvq" . On an input , the classifier outputs 0 or 1, corresponding to the partial rankings © @ nUj!%k¤¨§!§e(#% and ©d@B(q!§njXHkn¨§ ,respectively. ¦

Since ¥

¢d7 © ©

Wq7 5

 and ©

¢ ¨ ©¢§

W

oP£¦¥$©¢" @ oP£§ 7

@

©

   

¦

( if 5 £("

 ¤) 1  §i) 1 ©a  "

 a

¢

¡ ¡ 5

 otherwise.

)

(19) (20)

For example, if ¥ @BX§e(#%¤jX%k¤¨§ and ©@xnjX%k¤¨§P§i(q , then oP£¦¥$©¢"@ ( , as can be seen from the sequence of moves X§e(#%¤jX%k¤¨§ # X§jXHkn¨§P§i(q " njX%k¤¨§P§i(q (21) If ¥9@B(V§¤XUjXHkn§ and ©9@ nUj!%kn§!§e(#% , then oP£¦¥$%©¢" @  , with the sequence of moves (0§n%¤Uj!%k¤¨§ # (0§nUj!%k¤¨§!§ " (#%¤Uj!%kn§!§ # ¤UjXHkn§!§e(0§ " ¤UjXHkn§!§e(#w (22)



Since 3

thus selects the label corresponding to the partial ranking ¡ since is strictly negative,  ¡ £¦¥¨§ w" is a monotonically decreasing function in 

5

Equivalence with the ECOC decision rule thus follows from the fact that  I

3 

$

I @ 3 , the exponentof the model becomes  oP£¦¥$%©C3" . At test` time, the.model I I

¥ @ argD  ¡n£¦¥T§65w"oP£¦¥$%©Now,

3 C3" .

o!£¥$©C3&"nt¨C

is the Hamming distance between the appropriate row of the coding matrix and the concatenation of the bits returned from the binary classifiers. Thus, with the appropriate definitions of  and o , the conditional model on the ranking W 2 poset is a probabilistic formulation of ECOC that yields the same classification decisions. This suggests ways in which ECOC might be naturally extended. First, relaxing the constraint @ b @B888V@ results in a more general model that corresponds to ECOC with a weighted Hamming distance, or index sensitive "channel," where the learned weights may adapt to the precision of the various base classifiers. Another simple generalization results from using a nonuniform carrier density ¤   £¦¥" . A further generalization is achieved by considering that for a given coding matrix, the trained classifier for a given column outputs either ) 1  §i) 1 © or ) 1 © §) 1  depending on the input . Allowing the output of the classifier instead to belong to other grades of results in a model that corresponds to error correcting output codes with nonbinary codes. While this is somewhat antithetic to the original spirit of ECOC--reducing multiclass to binary--the base classifiers in ECOC are often multiclass classifiers such as decision trees in [4]. For such classifiers, the task instead can be viewed as reducing multiclass to partial ranking. Moreover, there need not be an explicit coding matrix. Instead, the input rankers may output different partial rankings for different inputs, which are then combined according to model (6). In this way, a different coding matrix is built for each example in a dynamic manner. Such a scheme may be attractive in bypassing the problem of designing the coding matrix.

I I I

       

¨ © ¦ ¡ a ¡ a ¡ a ¡ a


6 Summary An algebraic framework has been presented for classification and ranking, leading to conditional models on the ranking poset that are defined in terms of an invariant distance or dissimilarity function. Using the invariance properties of the distances, we derived a generative interpretation of the probabilistic model, which may prove to be useful in model selection and validation. Through different choices of the components ¤ 0¥W 2 and o , the family of models was shown to include as special cases the Mallows model, and the classifier combination methods used by logistic models, boosting, cranking, and error correcting output codes. In the case of ECOC, the poset framework shows how probabilities may be assigned to partial rankings in a way that is consistent with the usual definitions of ECOC, and suggests several natural extensions. Acknowledgments We thank D. Critchlow, G. Hulten and J. Verducci for helpful input on the paper. This work was supported in part by NSF grant CCR-0122581. References [1] M. Collins, R. E. Schapire, and Y. Singer. Logistic regression, AdaBoost and Bregman distances. Machine Learning, 48, 2002. [2] D. E. Critchlow. Metric Methods for Analyzing Partially Ranked Data. Lecture Notes in Statistics, volume 34, Springer, 1985. [3] D. E. Critchlow and J. S. Verducci. Detecting a trend in paired rankings. Journal of the Royal Statistical Society C, 41(1):17­29, 1992. [4] T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via errorcorrecting codes. Journal of Artificial Intelligence Research, 2:263­286, 1995. [5] P. D. Feigin. Modeling and analyzing paired ranking data. In M. A. Fligner and J. S. Verducci, editors, Probability Models and Statistical Analyses for Ranking Data. Springer, 1992. [6] M. A. Fligner and J. S. Verducci. Posterior probabilities for a concensus ordering. Psychometrika, 55:53­63, 1990. [7] Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. In International Conference on Machine Learning, 1996. [8] M. G. Kendall. A new measure of rank correlation. Biometrika, 30, 1938. [9] G. Lebanon and J. Lafferty. Boosting and maximum likelihood for exponential models. In Advances in Neural Information Processing Systems, 15, 2001. [10] G. Lebanon and J. Lafferty. Cranking: Combining rankings using conditional probability models on permutations. In International Conference on Machine Learning, 2002. [11] C. L. Mallows. Non-null ranking models. Biometrika, 44:114­130, 1957. [12] R. P. Stanley. Enumerative Combinatorics, volume 1. Wadsworth & Brooks/Cole Mathematics Series, 1986.

 


