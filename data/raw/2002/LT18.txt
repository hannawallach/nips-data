Effective Dimension and Generalization of
Kernel Learning
Tong Zhang
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
tzhang@watson.ibm.com
Abstract
We investigate the generalization performance of some learning prob­
lems in Hilbert function Spaces. We introduce a concept of scale­
sensitive effective data dimension, and show that it characterizes the con­
vergence rate of the underlying learning problem. Using this concept, we
can naturally extend results for parametric estimation problems in finite
dimensional spaces to non­parametric kernel learning methods. We de­
rive upper bounds on the generalization performance and show that the
resulting convergent rates are optimal under various circumstances.
1 Introduction
The goal of supervised learning is to predict an unobserved output value y based on an
observed input vector x. This requires us to estimate a functional relationship y  p(x)
from a set of training examples. Usually the quality of the predictor p(x) can be measured
by a loss function f(p(x); y). In machine learning, we assume that the data (x; y) are drawn
from an unknown underlying distribution. Our goal is to find p(x) so that the expected true
loss of p given below is as small as possible:
L(p()) = E x;y f(p(x); y);
where we use E x;y to denote the expectation with respect to the true (but unknown) under­
lying distribution.
In this paper we focus on smooth convex loss functions f that are second order differen­
tiable with respect to the first component. In addition we assume that the second derivative
is bounded both above and below (away from zero). 1 For example, our analysis applies to
important methods such as least squares regression (aka, Gaussian processes) and logistic
regression in Hilbert spaces.
In order to obtain a good predictor ^
p(x) from training data, it is necessary to start with a
model of the functional relationship. In this paper, we consider models that are subsets in
some Hilbert function space H . Denote by k  kH the norm in H . In particular, we consider
models in a bounded convex
subset
 of H . We would like to find the best model
in
 1 This boundedness assumption is not essential. However in this paper, in order to emphasize the
main idea, we shall avoid using a more complex derivation that handles more general situations.

defined as:
p
 () = arg min
p2

L(p) = arg min
p2

E x;y f(p(x); y): (1)
In supervised learning, we construct an estimator ^
p of
p
 () from a set of n training ex­
amples S = f(x 1 ; y 1 ); : : : ; (x n ; yn )g. Throughout the paper, we use symbol^to denote
empirical quantities based on the n observed training data S. Specifically, we use ^
E x;y to
denote the empirical expectation with respect to the training samples, and
^
L(p) = ^
E x;y f(p(x); y) =
1
n
n
X
i=1
f(p(x i ); y i ):
Assume that input x belongs to a set X . We make the reasonable assumption that p is
point­wise continuous under the k  kH topology: 8x 2 X , lim p!p0 p(x) = p 0 (x) where
p ! p 0 is in the sense that kp p 0 kH ! 0. This assumption is equivalent to the condition
sup kpkH 1 p(x) < +1 (8x 2 X), implying that each data point x can be regarded as
a bounded linear functional  x on H such that 8p 2 H :  x (p) = p(x). Since a Hilbert
space H is self­dual, we can represent  x by an element in H . Therefore 8x we can define
 x 2 H as  x  p = p(x) for all p 2 H , where  denotes the inner product of H .
It is clear that  x can be regarded as a representing feature vector of x in H . In the literature,
the inner product K(x 1 ; x 2 ) =  x1  x2 is often referred to as the kernel of H , and H as the
reproducing kernel Hilbert space which is determined by the kernel function K(x 1 ; x 2 ).
The purpose of this paper is to develop bounds on the true risk L(^p) of any empirical esti­
mator ^ p compared to the optimal risk
L(p
 ) based on its observed risk ^
L(^p). Specifically
we seek a bound of the following form:
L(^p) 
L(p
 ) + c( ^
L(^p) ^
L(p
 )) + (^p; ; n);
where c is a positive constant that only depends on the loss function f , and  is a parameter
that characterizes the effective data dimensionality for the learning problem.
If ^ p is the empirical estimator that minimizes ^
L
in
 , then the second term on the right
hand side is non­positive. We are thus mainly interested in the third term. It will be shown
that if H is a finite dimensional space, then the third term is O(d=n) where d = dim(H)
is the dimension of H . If H is an infinite dimensional space (or when d is large compared
to n), one can adjust  appropriately based on the sample size n to get a bound O(dn=n)
where the effective dimension dn at the optimal scale  becomes sample­size dependent.
However the dimension will never grow faster than dn = O( p
n) and hence even in the
worse case, (^p; ; n) converges to zero at a rate no worse than O(1= p
n).
A consequence of our analysis is to obtain convergence rates better than O(1= p
n). For
empirical estimators with least squares loss, this issue has been considered in [1, 2, 4]
among others. The approach in [1] won't lead to the optimal rate of convergence for non­
parametric classes. The L 2 ­covering number based analysis in [2, 4] use the chaining
argument [4] and ratio large deviation inequalities. However, it is known that chaining does
not always lead to the optimal convergence rate, and for many problems covering numbers
can be rather difficult to estimate. The effective dimension based analysis presented here,
while restricted to learning problems in Hilbert spaces (kernel methods), addresses these
issues.
2 Decomposition of loss function
Consider a convex
subset
  H , which is closed under the uniform norm topology. Let
p
 be the optimal predictor
p
 in
 defined in (1). By differentiating (1) at the optimal

solution, and using the convexity
of
 with respect to p, we obtain the following first order
condition:
E x;y f 0
1
(p
 (x); y)(p(x)
p
 (x))  0 (8p 2 
 ; (2)
where f 0
1 (p; y) is the derivative of f(p; y) with respect to p. This inequality will be very
important in our analysis.
Definition 2.1 The Bregman distance of f (with respect to its first variable) is defined as:
d f (p; q; y) = f(q; y) f(p; y) f 0
1 (p; y)(q p):
It is well known (and easy to check) that for a convex function, its Bregman divergence is
always non­negative. As mentioned in the introduction, we assume for simplicity that there
exist positive constants c l and c u such that 0 < c l  f 00
1 (p; y)=2  c u , where f 00
1 is the
second order derivative of f with respect to the first variable. Using Taylor expansion of f ,
it is easy to see that we have the following inequality for d f :
c l (p q) 2  d f (p; q; y)  c u (p q) 2 : (3)
Now, 8f
2
 , we consider the following decomposition:
L(p)
L(p
 ) = E x;y d f
(p
 (x); p(x); y) +E x;y f 0
1
(p
 (x); y)(p(x)
p
 (x)):
Clearly by the non­negativeness of Bregman divergence and (2), the two terms on the right
hand side of the above equality are all non­negative. This fact is very important in our ap­
proach. The above decomposition of L gives the following decomposition of loss function:
f(p(x); y)
f(p
 (x); y) = d f
(p
 (x); p(x); y) + f 0
1
(p
 (x); y)(p(x)
p
 (x)):
We thus obtain from (3):
c l (p(x)
p
 (x)) 2 + f 0
1
(p
 (x); y)(p(x)
p
 (x))
f(p(x); y)
f(p
 (x); y) (4)
c u (p(x)
p
 (x)) 2 + f 0
1
(p
 (x); y)(p(x)
p
 (x)):
3 Empirical ratio inequality and generalization bounds
Given a positive definite self­adjoint operator Q : H ! H , we define an inner product
structure on H as:
hu; vi Q = u  Qv = u T Qv:
The corresponding norm is kukQ = hu; ui 1=2
Q .
Given a positive number , and let I be the identity operator, we define the following
self­adjoint operator on H :
Q  = (E x  x  T
x + I) 1 ;
where we have used the matrix notation  x  T
x to denote the self­adjoint operator H ! H
defined as: ( x  T
x )h =  x ( x  h) = h(x) x .
In addition, we consider the inner product space T  on the set of self­adjoint operators on
H , with the inner product defined as
hA; Bi T = tr(Q  AQ  B);
where tr(S) is the trace of a linear operator S (sum of eigenvalues). The corresponding
norm is denoted as k  k T .
We start our analysis with the following simple lemma:

Lemma 3.1 For any function a(x; y), the following bounds are valid:
sup
p2H
j ^
E x;y a(x; y)p(x) E x;y a(x; y)p(x)j
p
E x p(x) 2 + kpk 2
H
 k ^
E x;y a(x; y) x E x;y a(x; y) x kQ ;
sup
p2H
j ^
E x p(x) 2 E x p(x) 2 j
E x p(x) 2 + kpk 2
H
 k ^
E x  x  T
x E x  x  T
x k T :
Proof Note that E x p(x) 2 + kpk 2
H = p T Q  1 p. Therefore let v = ^
E x;y a(x; y) x
E x;y a(x; y) x , we obtain from Cauchy­Schwartz inequality
j ^
E x;y a(x; y)p(x) E x;y a(x; y)p(x)j = jp  vj  (p T Q 
1 p) 1=2 (v T Q  v) 1=2 :
This proves the first inequality.
To show the second inequality, we simply observe that the left hand side is the largest
absolute eigenvalue of the operator A = Q  ( ^
E x  x  T
x E x  x  T
x ), which is upper bounded
by
p
tr(A 2 ). Therefore the second inequality follows immediately from the definition of
T  ­norm. 2
The importance of Lemma 3.1 is that it bounds the behavior of any estimator p 2 H
(which can be sample dependent) in terms of the norm of the empirical mean of n zero­
mean Hilbert­space valued random vectors. The convergence rate of the latter can be easily
estimated from the variance of the random vectors, and therefore we have significantly
simplified the problem.
In order to estimate the variance of the random vectors on the right hand sides of
Lemma 3.1, and hence characterize the behavior of the learning problem, we shall in­
troduce the following notion of effective data dimensionality at a scale :
D  = E x  T
x Q   x = E x k x k 2
Q :
Some properties of D  are listed in Appendix A, which can be used to estimate the quantity.
In particular for a finite dimensional space H , D  is upper bounded by the dimensionality
dim(H) of the space. Moreover the equality can be achieved by letting  ! 0 as long as
E x  x  T
x is full rank. Thus this quantity behaves like (scale­sensitive) data dimension.
We also define the following quantities to measure the boundedness of the input data:
MH  sup
x
k x kH ; M  = sup
x
k x kQ : (5)
It is easy to see that M   MH =
p
.
Lemma 3.2 Let c = sup x;y a(x; y), then we have
E x;y ka(x; y) x E x 0 ;y 0 a(x 0 ; y 0 ) x 0 k 2
Q  c 2 D  ;
E x k x  T
x k T = D  ; E x k x  T
x E x 0  x 0  T
x 0 k 2
T  D  M  2 :
Proof Let  = E x 0 ;y 0 a(x 0 ; y 0 ) x 0 , then we have
E x;y ka(x; y) x k 2
Q = E x;y ka(x; y) x k 2
Q kk 2
Q  c 2 D  ;
which gives the first inequality.
Note that 8 2 H : k T k T = kk 2
Q . Therefore
E x k x  T
x k T = E x k x k 2
Q = D  ;

leading to the second equality. Since k x  T
x k T = k x k 2
Q  M  2 , we have
E x k x  T
x k 2
T  E x k x  T
x kM  2  D  M  2 :
Similar to the proof of the first inequality, it is easy to check that this implies the third
inequality. 2
Next we need to use the following version of Bernstein inequality in Hilbert spaces.
Proposition 3.1 ([5]) Let  i be zero­mean independent random vectors in a Hilbert space.
If there exist B; M > 0 such that for all natural numbers l  2: 1
n
P n
i=1 Ek i k l
H 
B 2
2 l!M l 2 . Then for all Æ > 0: P (k 1
n
P
i  i kH  Æ)  2 exp( n
2 Æ 2 =(B 2 + ÆM)).
In this paper, we shall use the following variant of the above bound for convenience.
P
     
1
n
X
i
 i
     H
 2M t
n +
r
2t
n B
!
 2 exp( t): (6)
Lemma 3.3 Under the assumptions of Lemma 3.2, let   (t) =
q
2tD
n + 4tM
n . Then with
probability of at least 1 2 exp( t):
sup
p2H
j ^
E x;y a(x; y)p(x) E x;y a(x; y)p(x)j
p
E x p(x) 2 + kpk 2
H
   (t)c:
Similarly, with probability of at least 1 2 exp( t), we have:
sup
p2H
j ^
E x p(x) 2 E x p(x) 2 j
E x p(x) 2 + kpk 2
H
   (t)M  :
Proof The bounds are straight forward applications of (6) and the previous two lemmas.
Due to the limitation of space, we skip the details. 2
We are now ready to derive the following main result of the paper:
Theorem 3.1 Assume sup x;y jf 0
1
(p
 (x); y)j 
c
 . Let  = c u =c l where c l and c u satisfy
(3). Consider any sample dependent estimator ^
p such that ^
p
2
 . That is, ^
p
2
 is a
function of the training sample S. Let   (t) =
q
2tD
n + 4tM
n . If we choose  such that
  (t)M   0:5, then with probability of at least 1 4 exp( t), the generalization error
is bounded as:
L(^p) 
L(p
 ) + 4[ ^
L(^p) ^
L(p
 )] + 3c l k^p
p
 k 2
H +
4 2 c
2
   (t) 2
c l
:
Proof We introduce the following notations for convenience:
^
A(p) = ^
E x;y f 0
1
(p
 (x); y)(p(x)
p
 (x)); A(p) = E x;y f 0
1
(p
 (x); y)(p(x)
p
 (x));
^
B(p) = ^
E x (p(x)
p
 (x)) 2 ; B(p) = E x (p(x)
p
 (x)) 2 ;
C(p) = B(p) + kp
p
 k 2
H :
We obtain from Lemma 3.3 that with probability of at least 1 4 exp( t):
j ^
A(^p) A(^p)j   
(t)c
 C(^p) 1=2 ; j ^
B(^p) B(^p)j    (t)M  C(^p):

Combining the above two inequalities, we obtain:
j ^
A(^p) A(^p)j + c l j ^
B(^p) B(^p)j   
(t)[c
 C(^p) 1=2 + c l M  C(^p)]:
Using (4) and recalling (2), we obtain
c l
c u
[L(^p)
L(p
 )]  [ ^
L(^p) ^
L(p
 )] +  
(t)[c
 C(^p) 1=2 + c l M  C(^p)]: (7)
Let
K 1 (p) = [L(p)
L(p
 )] + c l kp
p
 k 2
H ; ^
K 2 (p) = [ ^
L(p) ^
L(p
 )] +
c 2
l
c u
kp
p
 k 2
H ;
then (2) and (4) imply that c l C(p)  K 1 (p). We can derive from (7)
c l
c u
K 1 (^p)  ^
K 2 (^p) +   (t)
2
4
c

s
K 1 (^p)
c l
+M  K 1 (^p)
3
5 :
Using the assumption that   (t)M   0:5, we obtain
c l
c u
K 1 (^p)  2 ^
K 2 (^p) + 2 
(t)c

s
K 1 (^p)
c l
;
which can be regarded as a quadratic inequality of K 1=2
1 (^p). Solving the inequality using
elementary algebra, we obtain:
K 1 (^p)  4 ^
K 2 (^p) + 4 2 c
2
   (t) 2
c l
;
which immediately implies the theorem. 2
Note that both D  and M  go to zero as  !1, therefore the assumption   (t)M   0:5
can be satisfied as long as we pick  that is larger than a critical value  0 . Using the bound
M   MH =
p
, we easily obtain the following result.
Corollary 3.1 Under the assumptions of Theorem 3.1. Assume also that the diameter
of
 is bounded by A: A = sup
p;q2
 kp qkH . Then for all  and an upper bound ~
D  of D  . If
2 ~
D   1 and = ~
D   32 2 tM 2
H =n, we have with probability of at least 1 4 exp( t),
L(^p) 
L(p
 ) + 4[ ^
L(^p) ^
L(p
 )] + (3A 2 c l +
c
2
 c l M 2
H
):
4 Examples
We will only consider empirical estimator ^
p that minimizes ^
L(p)
in
 . In this case, [ ^
L(^p)
^
L(p
 )]  0 in Corollary 3.1. We shall thus only focus on the third term.
Worst case effective dimensionality and generalization
In the worst case, we have D   M 2
H =. Therefore if 8t  n, we can always let  =
4
p
2M 2
H
p
t=n in Corollary 3.1 and obtain with probability at least 1 4 exp( t):
L(^p) 
L(p
 ) + 4(3c l A 2 M 2
H +
c
2
 c l
)
r
2t
n :

Finite dimensional problems
We can use the bound D   dim(H). Therefore we can let  = 32 dim(H) 2 tM 2
H =n in
Corollary 3.1 and obtain:
L(^p) 
L(p
 ) + 32 dim(H) 2 M 2
H (3c l A 2 M 2
H +
c
2
 c l
)
t
n
:
It is well known that the rate of the order O(dim(H)=n) is optimal in this case.
Smoothing splines
For simplicity, we only consider 1­dimensional problems. For smoothing splines, the cor­
responding Hilbert space consists of functions p satisfying the smoothness condition that
R
[p (s) (x)] 2 dx is bounded (p (s) is the s­th derivative of p and s > 1=2). We may con­
sider periodic functions (or their restrictions in an interval) and the condition corresponds
to a decaying Fourier coefficients condition. Specifically, the space can be regarded as the
reproducing kernel Hilbert space with kernel
K(x 1 ; x 2 ) =
X
k0
(k + 1) 2s (sin(kx 1 ) sin(kx 2 ) + cos(kx 1 ) cos(kx 2 )):
Now, using Proposition A.3, we have D   inf k1
h
2k + 2=
(2s 1)k 2s 1
i
. Therefore
D k 2s  4sk
2s 1 . Note that we may take M 2
H = 2=(2s 1). Therefore assuming
(2s 1) 2 n  2 2s+10 s 2 t we can let  = k 2s in Corollary 3.1 where k is the largest
integer such that k 2s+1  (2s 1) 2 n
2 9 s 2 t . This gives the following bound (with probability at
least 1 4 exp( t)).
L(^p) 
L(p
 ) + 2 2s ( 6c l A 2
2s 1
+
c
2
 c l
)

2 9 s 2 t
(2s 1) 2 n
 2s=(2s+1)
:
This rate matches the best possible convergence rate for any data­dependent estimator. 2
Exponential kernel
Exponential kernel has recently been popularized by Vapnik. Again for simplicity we con­
sider 1­dimensional problems where x 2 [ 1; 1]. The kernel function is given by
K(x 1 ; x 2 ) = exp(x 1 x 2 ) =
n
X
i=0
1
i! x i
1 x i
2 :
Therefore D   inf k0 [k + 2 + 1
k! ]. We obtain an upper bound L(^p) 
L(p
 ) +
O( ln n
n ln ln n ), implying that the effective dimension is at most O(ln n= ln ln n) for exponen­
tial kernels.
5 Conclusion
In this paper, we introduced a concept of scale­sensitive effective data dimension, and
used it to derive generalization bounds for some kernel learning problems. The resulting
convergence rates are optimal for various learning problems. We have also shown that the
2 The lower bound is well­known in the non­parametric statistical literature (for example, see [3]).

effective dimension at the appropriate chosen optimal scale can be sample­size dependent
and behaves like p
n in the worst case.
This shows that despite the claim that a kernel method learns a predictor from an infinite
dimensional Hilbert space, for a fixed sample size, the effective dimension is rather small.
This in fact indicates that they are not any more powerful than learning in an appropriately
chosen finite dimensional space. This observation also raises the following computational
question: given n­samples, kernel methods use n parameters in the computation but as
we have shown, the effective number of parameters (effective dimension) is not more than
O( p
n). Therefore it could be possible to significantly reduce the computational cost of
kernel methods by explicitly parameterizing the effective dimensions.
A Properties of scale­sensitive effective data dimension
We list some properties of the scale­sensitive data dimension D  . Due to the limitation of
space, we shall skip the proofs. The following lemma implies that the quantity D  behaves
like dimension if the underlying space H is finite dimensional.
Proposition A.1 If H is a finite dimensional space, then D   dim(H). Moreover, for all
Hilbert spaces H , we have the following bound D   M 2
H =, where MH is defined in (5).
Proposition A.2 Consider the complete set of ortho­normal eigen­pairs f( i ; u i ) : i  1g
of the operator E x  x  T
x , where u i  u j = 0 if i 6= j and u i  u i = 1. This gives the
decomposition: E x  x  T
x =
P
i  i u i u T
i , where  i = E x u i (x) 2 . We have the identity:
D  =
P
i
 i
 i + .
In many cases, we can find a so­called feature representation of the kernel function
K(x 1 ; x 2 ) =  x1   x2 . In such cases the eigenvalues  k can be easily bounded.
Proposition A.3 Consider the following feature space decomposition of kernel:  x1 
 x2 =
P
i   i (x 1 )  i (x 2 ), where each   i is a real valued function. If  1   2    , then we
have the following bound:
P
ik  i  E x
P
ik   i (x) 2 . This implies
D   inf
k0
"
k + sup
x
X
i>k
  i (x) 2 =
#
:
References
[1] W.S. Lee, P.L. Bartlett, and R.C. Williamson. The importance of convexity in learning
with squared loss. IEEE Trans. Inform. Theory, 44(5):1974--1980, 1998.
[2] Shahar Mendelson. Learning relatively small classes. In COLT 01, pages 273--288,
2001.
[3] Charles J. Stone. Optimal global rates of convergence for nonparametric regression.
Annals of Statistics, 10:1040--1053, 1982.
[4] S.A. van de Geer. Empirical Processes in M­estimation. Cambridge University Press,
2000.
[5] Vadim Yurinsky. Sums and Gaussian vectors. Springer­Verlag, Berlin, 1995.

