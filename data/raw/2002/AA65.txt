FloatBoost Learning for Classification

¡

 

Stan Z. Li Microsoft Research Asia Beijing, China Heung-Yeung Shum Microsoft Research Asia Beijing, China ZhenQiu Zhang Institute of Automation CAS, Beijing, China HongJiang Zhang Microsoft Research Asia Beijing, China

Abstract AdaBoost [3] minimizes an upper error bound which is an exponential function of the margin on the training set [14]. However, the ultimate goal in applications of pattern classification is always minimum error rate. On the other hand, AdaBoost needs an effective procedure for learning weak classifiers, which by itself is difficult especially for high dimensional data. In this paper, we present a novel procedure, called FloatBoost, for learning a better boosted classifier. FloatBoost uses a backtrack mechanism after each iteration of AdaBoost to remove weak classifiers which cause higher error rates. The resulting float-boosted classifier consists of fewer weak classifiers yet achieves lower error rates than AdaBoost in both training and test. We also propose a statistical model for learning weak classifiers, based on a stagewise approximation of the posterior using an overcomplete set of scalar features. Experimental comparisons of FloatBoost and AdaBoost are provided through a difficult classification problem, face detection, where the goal is to learn from training examples a highly nonlinear classifier to differentiate between face and nonface patterns in a high dimensional space. The results clearly demonstrate the promises made by FloatBoost over AdaBoost.

1 Introduction Nonlinear classification of high dimensional data is a challenging problem. While designing such a classifier is difficult, AdaBoost learning methods, introduced by Freund and Schapire [3], provides an effective stagewise approach: It learns a sequence of more easily learnable "weak classifiers", and boosts them into a single strong classifier by a linear combination of them. It is shown that the AdaBoost learning minimizes an upper error bound which is an exponential function of the margin on the training set [14]. Boosting learning originated from the PAC (probably approximately correct) learning theory [17, 6]. Given that weak classifiers can perform slightly better than random guessing

¢

http://research.microsoft.com/ szli The work presented in this paper was carried out at Microsoft Research Asia. £ ¤


on every distribution over the training set, AdaBoost can provably achieve arbitrarily good bounds on its training and generalization errors [3, 15]. It is shown that such simple weak classifiers, when boosted, can capture complex decision boundaries [1]. Relationships of AdaBoost [3, 15] to functional optimization and statistical estimation are established recently. A number of gradient boosting algorithms are proposed [4, 8, 21]. A significant advance is made by Friedman et al. [5] who show that the AdaBoost algorithms minimize an exponential loss function which is closely related to Bernoulli likelihood. In this paper, we address the following problems associated with AdaBoost: 1. AdaBoost minimizes an exponential (some another form of ) function of the margin over the training set. This is for convenience of theoretical and numerical analysis. However, the ultimate goal in applications is always minimum error rate. A strong classifier learned by AdaBoost may not necessarily be best in this criterion. This problem has been noted, eg by [2], but no solutions have been found in literature. 2. An effective and tractable algorithm for learning weak classifiers is needed. Learning the optimal weak classifier, such as the log posterior ratio given in [15, 5], requires estimation of densities in the input data space. When the dimensionality is high, this is a difficult problem by itself. We propose a method, called FloatBoost (Section 3), to overcome the first problem. FloatBoost incorporates into AdaBoost the idea of Floating Search originally proposed in [11] for feature selection. A backtrack mechanism therein allows deletion of those weak classifiers that are non-effective or unfavorable in terms of the error rate. This leads to a strong classifier consisting of fewer weak classifiers. Because deletions in backtrack is performed according to the error rate, an improvement in classification error is also obtained. To solve the second problem above, we provide a statistical model (Section 4) for learning weak classifiers and effective feature selection in high dimensional feature space. A base set of weak classifiers, defined as the log posterior ratio, are derived based on an overcomplete set of scalar features. Experimental results are presented in (Section 5) using a difficult classification problem, face detection. Comparisons are made between FloatBoost and AdaBoost in terms of the error rate and complexity of boosted classifier. Results clear show that FloatBoost yields a strong classifier consisting of fewer weak classifiers yet achieves lower error rates. 2 AdaBoost Learning In this section, we give a brief description of AdaBoost algorithm, in the notion of RealBoost [15, 5], as opposed to the original discrete AdaBoost [3]. For two class problems, a¨#"%$'&§(0)#¦1%)32labelled training examples is given as

set of ¡£¢¥¤§¦©¨¤¦¦¡£¢¦©¨ !

¢4"5$7698

 

, where is the class label associated with example

. A stronger classifier is a linear combination of

B

ACB ¡£¢DFE

G HPI ¡S¢

@ weak classifiers

¤RQ H (1)

¡£¢DC$T6

In this real version of AdaBoost, the weak classifiers can take a real value,

Q H

¡£¢DU$V1%) ¦W(0)

AgB ¡£¢D

and have absorbed the coefficients needed in the discrete¡£¢ih ¢ A ¡£¢DXE`Y©acb dfe AgB

The class label for is obtained as

version (there,

Q

H

, ).

p while the magnitude p

indicates the confidence. Every training example is associated with a weight. During the learning process, the weights are updated dynamically in such a way that more emphasis is


placed on hard examples which are erroneously classified previously. It is important for the original AdaBoost. However, recent studies [4, 8, 21] show that the artificial operation of explicit re-weighting is unnecessary and can be incorporated into a functional optimization procedure of boosting.

0. (Input) (1) Training examples

where

2

'(¡0)%132;

 ¢¡¤£¦¥¨§©©   ¥¨§"!#$!%&

of which

 4 ¡98#6)

examples have

54¡¤176,

and examples have

(2) The maximum number 1. (Initialization)

@Y¡a`¡

GIHQPR GIHQPR44 ¡ SUTSFX©©

;

@BADCFE;of

weak classifiers to be combined;

for those examples with for those examples with

54V¡W176. 4 ¡98#6 or

2. (Forward Inclusion)

while (1)

@cbd@ ADCFE @Yef@g1d6;

(2) Choose (3) Update 3. (Output)

 ¥¨§D¡0v ieqpsrutwvx8y¥¨§".

according to Eq.4;

© h

A



¡S¢ E

BFigure, ¨

or An error occurs when

¡£¢DU$76

achieved by

Q

GIHh"i4i R 4 i¥¨§ 4,

and normalize to

GIH R  4 4i ¡6;

1: RealBoost Algorithm.

¨ AgB ¡£¢D

ed

. The "margin" of an example

¨ ¡£¢D Q

¡£¢ ¦ ¨R

on the training set examples is defined as . This can be consid-

ered as a measure of the confidence of the 's prediction. The upper bound on classification error achieved by can be derived as the following exponential loss function [14] ACB Q

f

¡ A B 5E

g¦hi jkl j mon p G "

AdaBoost construct

A B A B

hh

¤ ¡£¢ ¤ ¡£¢ ( E

B

B ¡£¢ HUI h

Q

rq ¤ ¤ Q

Q

¡S¢ H

by stage-wise minimization of Eq.(2).

¡£¢D B ¡S¢

, the best

Q

(2) Given the current

A B ¡£¢D E

for the new strong classifier

is the one which leads to the minimum cost

B E b a d ¡ A B ¤ ¡S¢ (

h ¡

¡S¢©

Q

w5x

Q

ts¦u v f (3)

It is shown in [15, 5] that the minimizer is

)

y{zo| B ¡S¢ E

 m p B ¤

h

Q

~}}

b

where

and letting

are the weights given at time

 )

yz|

)

PB ¡S¢ E b



¡£¢Q

@ . Using

¨CE

p p ¡£¨CE

 mm pp B

¤

B

}

hh 

¤ 

¡S¨ ¢ ¦

p

(0)#¦

¡£¨CE (0)

p

¡£¨CE'1%)

¢ ¦ ¢ ¦

(4)

¡£¨R

p

 E

} ¡£¢ ¨D¦ p   }

E

 ¡£¢ ¡£¢

~}}



B

 

¨CE'1%)#¦

(0) 



y zo|

E

b

¡£¨CE'1%) 

$

(5) (6)

we arrive

B ¡£¢ ¡£¢D (



The half log likelihood ratio

and the threshold





(7)

is learned from the training examples of the two classes,

is determined by the log ratio of prior probabilities.



can be adjusted


to balance between detection rate and false alarm (ROC curve). The algorithm is shown in Fig.1 (Note: Re-weight formula in this description is equivalent to the multiplicative rule in the original form ofB AdaBoost [3, 15]). In Section 4, we will present an model for

} ¡S¢ ¨D¦  m p ¤ h 

approximating p .

3 FloatBoost Learning FloatBoost backtracks after the newest weak classifier

weak classifiers

Q

H

B

Q

is added and delete unfavorable

from the ensemble (1), following the idea of Floating Search [11].

Floating Search [11] is originally aimed to deal with non-monotonicity of straight sequential feature selection, non-monotonicitymeaning that adding an additional feature may lead to drop in performance. When a new feature is added, backtracks are performed to delete those features that cause performance drops. Limitations of sequential feature selection is thus amended, improvement gained with the cost of increased computation due to the extended search. 0. (Input)

(1) Training examples

where

$4V¡W176'(¡0)#12

2;

 ¢¡¤£¦¥¨§©©   ¥¨§"!#$!%&

of which

)

,

¡

GG HQPR4HQPR ¡ SUT©

¥ i ,

@BADCFE

$4¡98#6; examples have of weak classifiers;

and examples have

(2) The maximum number

(3) The error rate 1. (Initialization)

(1) (2)

¡

¡ ¢

and the acceptance threshold .

@Y¡a`¡ ¡¤£ &

A£¢¤4

¡ SFX©

for those examples with for those examples with

max-value (for ¥ , .

¦

P

¡96 s  @$4V¡ ADCFE8#6), 4 ¡W176 or

;

2. (Forward Inclusion)

(1)

(2) Choose (3) Update

¦

(4)

i ¡ i¨§©©eqpsrutwvx8y& i ¥ i~,

according to Eq.4;

£ h"i

A£¢¤

; If

¡ ¦

Gh"i4Hi R

@Yef@g1d6;

4 i¥¨§ 4,¡



and normalize to

then

¡ A£¢¤i

G H R ¡ 4¡ 4i ¡6; ¥ i ;

3. (Conditional Exclusion)

¥¦A£¢¤i$§i¨§i 8Bh¡ ¦"!"#bi i¨§V©;



¡ i ¡W

(1)

(2) If (a) (b)

h"¡¡ "!"# ¥ i 8Bh";

¡

©V© ¡ ¥ il 8Bhh ;@Y¡¤@ 8 6;

, then

¡

 ;

l¡8BhA£¢¤

(c) goto 3.(1);

(3) else (a) if

(b) 4. (Output)

G @Y¡W@BADCFE ¥¦ i~b&%

R 4Hi e prtwvx8yor4% i¥¨§ 4;

¢

, then goto 4; goto 2.(1);

 ¥¨§D¡0v H(' R!"# l h¥¨§".

Figure 2: FloatBoost Algorithm.

The FloatBoost procedure is shown in Fig.2 Let )

of @

weak classifiers; 0

¡ A B 

B E & ¤¦¦

Q

B 2

be the so-far-best set

B HPI

¤ ¡£¢D

Q

H (or a

A B ¡S¢FE Q

be the error rate achieved by

q

weighted sum of missing rate and false alarm rate which is usually the criterion in one-class detection problem); 021435 be the minimum error rate achieved so far with an ensemble of 6

H

weak classifiers. In Step 2 (forward inclusion), given already selected, the best weak classifier is added one


at a time, which is the same as in AdaBoost. In Step 3B (conditional exclusion), FloatBoost removes the least significant weak classifier from ) , subject to the condition that the removal leads to a lower error rate 0 1435 . These are repeated until no more removals can

B ¤

f  

be done. The procedure terminates when the risk on the training set is below

maximum number @ ¡ 1

¢

is reached.

h or the

Incorporating the conditional exclusion, FloatBoost renders both effective feature selection and classifier learning. It usually needs fewer weak classifiers than AdaBoost to achieve the same error rate 0 . 4 Learning Weak Classifiers The section presents a method for computing the log likelihood in Eq.(5) required in learning optimal weak classifiers. Since deriving a weak classifier in high dimensional space is a non-trivial task, here we provide a statistical model for¢stagewise learning of weak classi-

£ fiers based on some scalar features. A scaler feature of

the ¤ -dimensional data space to the real line, ¥§¦

of, say, a wavelet¡£¢D

the transform, ¥ ¦

¡S¢X$V6

is computed by a transform from . A feature can be the coefficient

transform in signal and image processing. If projection pursuit is used as ¢

¥

¤ ¡£¢D ¦¦

¥

¡£¢ 2

m p

H

¢

£

features can be created ©

is simply& the -th coordinate of . A dictionary of ¨ candidate scalar

E

the feature selected in the 6 -th stage, while ¥§¦ £

-th transform.

¡£¢D

. In the following, we use ¥

is the feature computed from

to denote using the

Assuming that © is an over-complete basis, a set of candidate weak classifiers for the

optimal weak classifier (7) can be designed in the following way: First, at stage

¥ m p

1%) ¤ ¦ ¦¦

¨D¦

B ¤



@

features ¥ ¥

¤

m p msp

¡S¢

m p

B

hh

 ¡ ¥

 ¡ ¥

@

 mwherep, B ¤

h

(8)

have been selected and the weight is given as by using the distributions of features

 m p



we can approximate



B ¡£¢ ¨D¦

p

 m p p ¤

h 

E

mmm p mppm

¤ ¦

p ¥

¤ ¨D¦ ¤

p

h

¨¦

B

p

¥

¦¦ B

m pm h

¥¤ B





¨D¦

¥

hm

¤ ¦

¥

p

p p

pm

¥

¥

m ¡ ¥ ¦

¤ ¦¦ B

B ¤  ¦

spm m pm

p

¨¦ ¤ ¦

p

¥

B

h m p B ¤

h

sp

 ¦



¥ ¦

¨D¦

@

B

¤

¡

¥

¤ ¦¦

hm h  B ¤

p  h p 

¡



¤

Because © is an over-complete basis set, the approximation is good enough for large Note that is actually because con-

enough @ and when the

¡

¥

m p m p

¨D¦ ¤ ¦¦ H

p

¥

@ features are chosen appropriately.

¥

m p

¤  H





¡

¥

m p

H ¨D¦

p

 m p ¤ H

h



 m p H

tains the information about entire history of

¥ ¤ ¦¦ ¥ ¤

H



h

Therefore, we have

¨D¦

 m p

B ¤

h 



¡£¢ ¡ ¤ ¨D¦

p



m p

m

p.

p

h

and accounts for the dependencies on

h

(9)

¥

mm p

B

p

¤ ¨D¦

p

m"!p 

¡

¥

m sp ¡ ¥ B

h

mp

p



¥#¦p  ¡

m pm ¤ 

¨D¦

¨D¦



¥#¦CpB

m pfor¥

m p

h

 B ¤ h p  (10) (11)

On the right-hand side of¤ the above equation, all conditional densities are fixed except the last one is to choose the

f

. Learning the best weak classifier at stage

¡ ¨D¦ B 

best feature ¥ ¦ suchthat isminimizedaccordingtoEq.(3).

¡ ¨D¦

 m p B ¤

h



The conditional probability1%)densities ¨ E

and the negative class



¥ ¦

p

h @

¨ E (0)



can be estimated using the histograms computed from the B ¤

for the positive class

 m p.

weighted voting of the training examples using the weights

 m p

B )

y{z| ¡

B

¦ ¡£¢DFE

I

b p

¡

¥#¦ ¥#¦ ¨ E ¨ E (0) ¦ 1%) ¦ B

p

 mm pp ¤

hh ¤ 

h

Let

 (12)


¦

m p B ¡£¢DFE

and Q

¡   ¦ m p B ¡£¢D 1 £¢

)

m. p m p B

B

E &

Q

¦

We can derive the set of candidate weaker classifiers as

¡£¢

¥¤ £ 2 p (13)

¡S¢%E B ¡£¢D

A B ¤ ¡£¢ ( B ¡S¢ Q

among all in )

m p B ¤

h

Recall that the best

h

Q

is given by Eq.(3) among all

for the new strong classifier B

)

¡

$

Q

m p

A B

, for which the optimal

weak classifier has been derived as (7). According the theory of gradient based boosting

[4, 8, 21],1 we can choose the optimal weak classifier by finding the ¡ A B ¤

gradient

h

where

¦ f

¡ ACB

f

¢D¤

f

¢4

¤ FE

h

©¨¨

¡ ¦

 ¥¦

¨¨



§¦ f B ¡£¢

Q that best fits the

(14)

¡£¢DU$

)

m p

that best fits

§¦ f

B

In our stagewise approximation formulation, this can be done by first finding the B

1

Q

in direction and then scaling it so that the two has the same

(re-weighted) norm. An alternative selection scheme is simply to¨ choose ¡

error

¡

¥#¦

E (0)#¦

rate (or someB risk), computed from the two histograms ¤

¨ E'1%)#¦

h , is minimized.

 m p ¥ ¦ p

p



£ msoh p

B that the ¤

and

5 Experimental Results Face Detection The face detection problem here is to classifier an image of standard size (eg 20x20 pixels) into either face or nonface (imposter). This is essentially a one-class problem in that everything not a face is a nonface. It is a very hard problem. Learning based methods have been the main approach for solving the problem , eg [13, 16, 9, 12]. Experiments here follow the framework of Viola and Jones [19, 18]. There, AdaBoost is used for learning face detection; it performs two important tasks: feature selection from a large collection features; and constructing classifiers using selected features. Data Sets A set of 5000 face images are collected from various sources. The faces are cropped and re-scaled to the size of 20x20. Another set of 5000 nonface examples of the same size are collected from images containing no faces. The 5000 examples in each set is divided into a training set of 4000 examples and a test set of 1000 examples. See Fig.3 for a random sample of 10 face and 10 nonface examples.

Figure 3: Face (top) and nonface (bottom) examples. Scalar Features Three basic types of scalar features ¥ ¦ are derived from each example, as shown in Fig.4, for constructing weak classifiers. These block differences are an extended set of steerable filters used in [10, 20]. There are hundreds of thousands of different ¥ ¦ for admissible

¢ ¦©¨D¦ #¢ ¦ #¨

 



values. Each candidate weak classifier is constructed as the log likelihood ratio

¡ ¨D¦ B ¤ 

 m p

h

(12) computed from the two¨ histograms ¨ E (0) E 1%)

face ( ) and nonface (

¥ ¦

p

of a scalar feature ¥ ¦ for the

) examples (cf. the last part of the previous section).


. The rectangles are of size

and) are at distances of ¦

 m

¢

Figure 4: The three types of simple Harr wavelet like features

y

¢ ¢£¢X¨

¡ 

¡ ¢ ¦ #¨ 

p

a value calculated by the weighted (¤

defined on a sub-window apart. Each feature takes

) sum of the pixels in the rectangles.

Performance Comparison The same data sets are used for evaluating FloatBoost and AdaBoost. The performance is measured by false alarm error rate given the detection rate fixed at 99.5%. While a cascade of stronger classifiers are needed to achiever very low false alarm [19, 7], here we present the learning curves for the first strong classifier composed of up to one thousand weak classifiers. This is because what we aim to evaluate here is to contrast between FloatBoost and AdaBoost learning algorithms, rather than the system work. Interested reader is referred to [7] for a complete system which achieved a tection system, the first real-time system of the kind in the world, is being submitted to the conference).

d ) ¥ false alarm of h with the detection rate of 95%. (A live demo of multi-view face de-

0.8

0.75

AdaBoost-train AdaBoost-test FloatBoost-train FloatBoost-test

0.7

0.65

0.6

Rates

Error 0.55

0.5

0.45

0.4

0.35 100 200 300 400 500 600 700 800 900 1000

# Weak Classifiers

Figure 5: Error Rates of FloatBoost vs AdaBoost for frontal face detection. The training and testing error curves for FloatBoost and AdaBoost are shown in Fig.5, with the detection rate fixed at 99.5%. The following conclusions can be made from these curves: (1) Given the same number of learned features or weak classifiers, FloatBoost always achieves lower training error and lower test error than AdaBoost. For example, on the test set, by combining 1000 weak classifiers, the false alarm of FloatBoost is 0.427 versus 0.485 of AdaBoost. (2) FloatBoost needs many fewer weak classifiers than AdaBoost in order to achieve the same false alarms. For example, the lowest test error for AdaBoost is 0.481 with 800 weak classifiers, whereas FloatBoost needs only 230 weak classifiers to achieve the same performance. This clearly demonstrates the strength of FloatBoost in


learning to achieve lower error rate. 6 Conclusion and Future Work By incorporating the idea of Floating Search [11] into AdaBoost [3, 15], FloatBoost effectively improves the learning results. It needs fewer weaker classifiers than AdaBoost to achieve a similar error rate, or achieves lower a error rate with the same number of weak classifiers. Such a performance improvement is achieved with the cost of longer training time, about 5 times longer for the experiments reported in this paper. The Boosting algorithm may need substantial computation for training. Several methods can be used to make the training more efficient with little drop in the training performance. Noticing that only examples with large weigh values are influential, Friedman et al. [5] propose to select examples with large weights, i.e. those which in the past have been wrongly classified by the learned weak classifiers, for the training weak classifier in t+- he

¡ 

next round. Top examples within a fraction of where . References V$Ve  ) ¦ Rc)h

  d d sd )%1

of the total weight mass are used,

[1] L. Breiman. "Arcing classifiers". The Annals of Statistics, 26(3):801­849, 1998. [2] P. Buhlmann and B. Yu. "Invited discussion on `Additive logistic regression: a statistical view of boosting (friedman, hastie and tibshirani)' ". The Annals of Statistics, 28(2):377­386, April 2000. [3] Y. Freund and R. Schapire. "A decision-theoretic generalization of on-line learning and an application to boosting". Journal of Computer and System Sciences, 55(1):119­139, Aug 1997. [4] J. Friedman. "Greedy function approximation: A gradient boosting machine". The Annals of Statistics, 29(5), October 2001. [5] J. Friedman, T. Hastie, and R. Tibshirani. "Additive logistic regression: a statistical view of boosting". The Annals of Statistics, 28(2):337­374, April 2000. [6] M. J. Kearns and U. Vazirani. An Introduction to Computational Learning Theory. MIT Press, Cambridge, MA, 1994. [7] S. Z. Li, L. Zhu, Z. Q. Zhang, A. Blake, H. Zhang, and H. Shum. "Statistical learning of multi-view face detection". In Proceedings of the European Conference on Computer Vision, page ???, Copenhagen, Denmark, May 28 - June 2 2002. [8] L. Mason, J. Baxter, P. Bartlett, and M. Frean. Functional gradient techniques for combining hypotheses. In A. Smola, P. Bartlett, B. Sch¨olkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 221­247. MIT Press, Cambridge, MA, 1999. [9] E. Osuna, R. Freund, and F. Girosi. "Training support vector machines: An application to face detection". In CVPR, pages 130­136, 1997. [10] C. P. Papageorgiou, M. Oren, and T. Poggio. "A general framework for object detection". In Proceedings of IEEE International Conference on Computer Vision, pages 555­562, Bombay, India, 1998. [11] P. Pudil, J. Novovicova, and J. Kittler. "Floating search methods in feature selection". Pattern Recognition Letters, (11):1119­1125, 1994. [12] D. Roth, M. Yang, and N. Ahuja. "A snow-based face detector". In Proceedings of Neural Information Processing Systems, 2000. [13] H. A. Rowley, S. Baluja, and T. Kanade. "Neural network-based face detection". IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(1):23­28, 1998. [14] R. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. "Boosting the margin: A new explanation for the effectiveness of voting methods". The Annals of Statistics, 26(5):1651­1686, October 1998. [15] R. E. Schapire and Y. Singer. "Improved boosting algorithms using confidence-rated predictions". In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pages 80­91, 1998. [16] K.-K. Sung and T. Poggio. "Example-based learning for view-based human face detection". IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(1):39­51, 1998. [17] L. Valiant. "A theory of the learnable". Communications of ACM, 27(11):1134­1142, 1984. [18] P. Viola and M. Jones. "Asymmetric AdaBoost and a detector cascade". In Proceedings of Neural Information Processing Systems, Vancouver, Canada, December 2001. [19] P. Viola and M. Jones. "Rapid object detection using a boosted cascade of simple features". In Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Kauai, Hawaii, December 12-14 2001. [20] P. Viola and M. Jones. "Robust real time object detection". In IEEE ICCV Workshop on Statistical and Computational Theories of Vision, Vancouver, Canada, July 13 2001. [21] R. Zemel and T. Pitassi. "A gradient-based boosting algorithm for regression problems". In Advances in Neural Information Processing Systems, volume 13, Cambridge, MA, 2001. MIT Press.


