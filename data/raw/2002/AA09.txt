Adaptive Scaling for Feature Selection in SVMs

Yves Grandvalet Heudiasyc, UMR CNRS 6599, Universit´e de Technologie de Compi`egne, Compi`egne, France Yves.Grandvalet@utc.fr Stephane Canu ´ PSI INSA de Rouen, St Etienne du Rouvray, France Stephane.Canu@insa-rouen.fr

Abstract This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors defining the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters defining the classifier. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.

1 Introduction In pattern recognition, the problem of selecting relevant variables is difficult. Optimal subset selection is attractive as it yields simple and interpretable models, but it is a combinatorial and acknowledged unstable procedure [2]. In some problems, it may be better to resort to stable procedures penalizing irrelevant variables. This paper introduces such a procedure applied to Support Vector Machines (SVM). The relevance of input features may be measured by continuous weights or scale factors, which define a diagonal metric in input space. Feature selection consists then in determining a sparse diagonal metric, and sparsity can be encouraged by constraining an appropriate norm on scale factors. Our approach can be summarized by the setting of a global optimization problem pertaining to 1) the parameters of the SVM classifier, and 2) the parameters of the feature space mapping defining the metric in input space. As in standard SVMs, only two tunable hyper-parameters are to be set: the penalization of training errors, and the magnitude of kernel bandwiths. In this formalism we derive an efficient algorithm to monitor slack variables when optimizing the metric. The resulting algorithm is fast and stable. After presenting previous approaches to hard and soft feature selection procedures in the context of SVMs, we present our algorithm. This exposure is followed by an experimental section illustrating its performances and conclusive remarks.


2 Feature Selection via adaptive scaling Scaling is a usual preprocessing step, which has important outcomes in many classification methods including SVM classifiers [9, 3]. It is defined by a linear transformation within

¡£¢¥¤¦¡   ¤§¢ ¤¢ "!#$%

the input space: factors. , where diag

¨©

is a diagonal matrix of scale

Adaptive scaling consists in letting

©

to be adapted during the estimation process with the

explicit aim of achieving a better recognition rate. For kernel classifiers,

©

is a set of hyper-

parameters of the learning process. According to the structural risk minimization principle

[8],

©

can be tuned in two ways: 1. estimate the parameters of classifier

eral values of

'

  (30%142 '

 )("0%12 &

by empirical risk minimization for sev-

to produce a structure of classifiers

& ©

. Select one element of the structure by finding the set

'

5multi-indexed

("0%12

by

minimiz-

ing some estimate of generalization error.

2. estimate the parameters of classifier

&

and the hyper-parameters

'

pirical risk minimization, while a second level hyper-parameter, say

'

  (30%142

6 (307 %12

by em-

, constrains

in order to avoid overfitting. This procedure produces a structure of clas-

sifiers indexed by

 7

, whose value is computed by minimizing some estimate of

generalization error. The usual paradigm consists in computing the estimate of generalization error for regularly spaced hyper-parameter values and picking the best solution among all trials. Hence, the first approach requires intensive computation, since the trials should be completed over a Several authors suggested to address this problem by optimizing an estimate of generalization error with respect to the hyper-parameters. For SVM classifiers, Cristianini et al. [4] first proposed to apply an iterative optimization scheme to estimate a single kernel width hyper-parameter. Weston et al. [9] and Chapelle et al. [3] generalized this approach to multiple hyper-parameters in order to perform adaptive scaling and variable selection. The experimental results in [9, 3] show the benefits of this optimization. However, relying on the optimization of generalization error estimates over many hyper-parameters is hazardous. Once optimized, the unbiased estimates become down-biased, and the bounds provided by VC-theory usually hold for kernels defined a priori (see the proviso on the radius/margin bound in [8]). Optimizing these criteria may thus result in overfitting. In the second solution considered here, the estimate of generalization error is minimized The role of this constraint is twofold: control the complexity of the classifier, and encourage variable selection in input space. This approach is related to some successful soft-selection procedures, such as lasso and bridge [5] in the frequentist framework and Automatic Relevance Determination (ARD) [7] in the Bayesian framework. Note that this type of optimization procedure has been proposed for linear SVM in both frequentist [1] and Bayesian frameworks [6]. Our method generalizes this approach to nonlinear SVM. 3 Algorithm 3.1 Support Vector Machines

8 9 -dimensional grid over values.

 7 with respect to , a single (second level) hyper-parameter, which constrains ' 5 (30%12 .

The decision function provided by SVM is

& © ¨¡  ¢PIRQTS ¡

©

¨ 4UWV

@BADCFE6¨G& © ¨¡ H,©

¢YXa`Pb `Gc`ed `gf where function ¨¡ UWV ¡ f & © is defined as: (1)


where the parameters

 ¡¡

¢¡¡£

f ¨I V

are obtained by solving the following optimization problem:

IRQ I

b ` I Q S ¡

U`X`12

`

¥§¦¨©¦  `

¤

subject to

S ¤¦¡

AE 

with

© ¨ 

defined as

¨0) ¨  UWV$  ©

"! 

`$## f&%'%&%f©( ¢¢ f&%'%&%f©(1% (2)

S ¡

¨ . 



and the parameters In this problem setting,

©

of the

feature space mapping (typically a kernel bandwidth) are tunable hyper-parameters which need to be determined by the user.

3.2 A global optimization problem In [9, 3], adaptive scaling is performed by iteratively finding the parameters mate of generalization error with respect to hyper-parameters

SVM classifier

& © for a fixed value of © ' ¢   (3012  )( 012 f

f ¨I V

of the

and minimizing a bound on the esti-

¨'  

. The algorithm

minimizes 1) the SVM empirical criterion with respect to parameters and 2) an estimate of generalization error with respect to hyper-parameters. In the present approach, we avoid the enlargement of the set of hyper-parameters by letting

'

 )(3012

to be standard parameters of the classifier. Complexity is controlled by

by constraining the magnitude of

©



and

. The latter defines the single hyper-parameter of the

learning process related to scaling variables. The learning criterion is defined as follows:

¡ ¡¡¡¡¡¡¡ U34`X`142

`

¢¡¡¡¡¡¡¡¡¡£ ¤ ¤

subject to

Q I ©AE ¥§¦¨2¦AE `I¨6)0

b` I Q S ¡

© ¨  U V 



8 %1420) X 87 ¢877



"! 

`$## f'%&%'%f5( ¢¢ f'%&%'%f5(



9 ¢ f&%'%'%$f

(3)

8



Like in standard SVM classification, the minimization of an estimate of generalization error is postponed to a later step, which consists in picking the best solution among all trials on

7

the two dimensional grid of hyper-parameters

In (3), the constraint on

©

¨  .

 

should favor sparse solutions. To allow

5

to go to zero, should

@ 8A,

f

be positive. To encourage sparsity, zeroing a small

9

, hence

@

should be small. In the limit of

BDC ¢

should allow a high increase of

@EF)

, the constraint counts the number

of non-zero scale parameters, resulting in a hard selection procedure. This choice might seem appropriate for our purpose, but it amounts to attempt to solve a highly non-convex optimization problem, where the number of local minima grows exponentially with the

8 

input dimension . To avoid this problem, we suggest to use

@ S¢ ¡which¤¦¡is.

© ¨, 

¢

the smallest

Indeed, for value for which the problem is convex with the linear mapping

linear kernels, the constraint on

the penalization on the

setting

@

¢ 

GIH

©

amounts to minimize the standard SVM criterion where

norm is replaced by the penalization of the

G

2

G

2

provides the solution of the SVM classifier described in [1]. For non-linear

GQPSRRUTVP

norm. Hence,

kernels however, the two solutions differ notably since the present algorithm modifies the metric in input space, while the SVM classifier modifies the metric in feature space.

¢ 

Finally, note that unicity can be guaranteed for

bandwidths (

EW)

).

 7 @

and Gaussian kernels with large


3.3 An alternated optimization scheme Problem (3) is complex; we propose to solve iteratively a series of simplier problems.

The function

©

S &

is first optimized with respect to parameters

(standard SVM problem). Then, the parameters

¡£¢¥¤

¨I ¨G©  V"¨© H&

¡£¢¥¤

f

©

f ¨I V

for a fixed mapping

of the feature space mapping are

optimized while some characteristics of

© ¡¨¢¥¤

value, the optimal §¦ ¦

descent algorithm.

In this scheme,

¨I ¨G©  V3¨© H

¡£¢¥¤ ¡£¢¥¤ ¦

§¦

f

are kept fixed: At step ¡£¢¥©  , starting from a given

are computed. Then

© ¤

is determined by a

2

are computed by solving the standard quadratic opti-

mization problem (2). Our implementation, based on an interior point method, will not be detailed here. Several SVM retraining are necessary, but they are faster than the usual training since the algorithm is initialized appropriately with the solutions of the preceding round. For solving the minimization problem with respect to , we use a reduced conjugate gradient technique. The optimization problem was simplified by assuming that some of the other 3) set of support vectors fixed. For the three versions, the optimal value of , or at least the optimum is computed directly (in a single iteration). We do not detail our first version here, since the two last ones performed much better. The main steps of the two last versions are sketched below.

©

I

variables are fixed. We tried several versions: 1) fixed; 2) Lagrange multipliers  fixed;

optimal value of the slack variables 

V

can be obtained by solving a linear program, whose

3.4 Sclaling parameters update Starting from an initial solution f

I ¢ 12 b S ¡

¨© ¨©` V"¨G©definingI

I f

H,

our goal is to update

©

by solving a

simple intermediate problem providing an improved solution to the global problem (3). We

first assume that the Lagrange multipliers

that



I

is defined as   



©

I

¨ . 

are not affected by

I

©

updates, so

c

Regarding problem (3), is sub-optimal when

©

varies; nevertheless is guaranteed to

be an admissible solution. Hence, we minimize an upper bound of the original primal cost which guarantees that any admissible update (providing a decrease of the cost) of the intermediate problem will provide a decrease of the cost of the original problem.

The intermediate optimization problem is stated as follows:

 ¡¡¡¡¡¡¡¡¡¡¡¡¡

`

X`¦

c `c ` d ` f



b b 

© ¨¡ 6U3  

b X` b



` ¦ 0) 8 %123)





c d `gf



© ¨¡ UWV    ¡



¡ X`12

`

¢¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡£

©¤

¦¨©¦  AE  

subject to



`$# f&%'%'%$f©(

¢

# ¢ 



 "!

X 87 ¢877 0

9 ¢

f&%'%'%$f©( f'%'%&%f % 8 (4)



Solving this problem is still difficult since the cost is a complex non-linear function of

scale factors. Hence, as stated above,

©

will be updated by a descent algorithm. The latter

requires the evaluation of the cost and its gradient with respect"! to

means that we should be able to compute



 and





`12 `  `12 ` 

9©©

. In particular, this for any value of . ©


For given values of

¢¡¡¡¡¡¡¡£

 ¡¡¡¡¡

© and  , 

X`12

` is the solution of the following problem:

¤ ¨©¦

AE  

b X12 b



` 



3)



` c d `Bf

 subject to

© ¨¡ UWV    ¡

`$# f&%'%'%$f©(

¢

# ¢ 

f&%'%'%$f©( f



 (5)

"!

whose dual formulation is

¢¡¡¡¡¡¡¡£

 ¡¡¡¡¡

¡ £

¢ X` ¤ £ b X 

12 

`

¤

X`12 ¤ b` ¢

  ¤ 3)

¢  ` ` ` c d ` f



b  ¥¦

142 "! © ¨¡   ¡  §

(6)

subject to ) #

¢ f&%'%&%f©(1%

`©¨

142 b ¡ ¡

c d `Hf 

This linear problem is solved directly by the following algorithm:



`

  

¨ 



b

1) sort

"! 

side and for all negative examples on the other side; 2) compute the pairwise sum of sorted

values; 3) set

£

With

©

,





for all positive and negative examples whose sum is positive.

and its derivative with respect to

©

are easily computed. Parameters



¤ ¢ `12 `

in descending order for all positive examples on the one

are then updated by a conjugate reduced gradient technique, i.e. a conjugate gradient



algorithm ensuring that the set of constraints on 3.5 Updating Lagrange multipliers

©

are always verified.

Assume now that only the support vectors remain fixed while optimizing . This assumption is used to derive a rule to update at reasonable computing cost the Lagrange multipliers !

 together with

©

by computing  . At  the following holds [3]:

¢Pb

f f 9© ¨ © V`,

©

1. for support vectors of the first category

X b 12 

c d `Bf



& © ¨¡ 



© ¨¡  U V



¡

` ¢ b

` ` c4`

)

)

(7)

2. for support vectors of the second category (such that 

¢  .

From these equations, and the assumption that support vectors remain support vectors (and that their category do not change) one derives a system of linear equations defining the

derivatives of  and with respect to

V ©

[3]:

1. for support vectors of the first category

X

d ` f



b  © ¨¡ 6U  



12 ¡ X b   



c 12 © c d ` f

c`©

© ¨¡  U )  ¡ 5V© ¢ ) (8)

2. for support vectors of the second category

9©

¢


3. Finally, the system is completed by stating that the Lagrange multipliers should

X b ¢ c

12   obey the constraint ) : 

X



c 12 ©



b ¢  ) (9)

The value of

 )

¥c `   

is updated from these equations, and the step size is limited to ensure that for support vectors of the first category. Hence, in this version, is also an I

admissible sub-optimal solution regarding problem (3).

4 Experiments In the experiments reported below, we used

@

¢



¨  

f

for the constraint on

©

(3). The scale pa-

rameters were optimized with the last version, where the set of support vectors is assumed to be fixed. Finally, the hyper-parameters were chosen using the span bound [3]. Although the value of the bound itself was not a faithful estimate of test error, the average loss induced by using the minimizer of these bounds was quite small. 4.1 Toy experiment In [9], Weston et al. compared two versions of their feature selection algorithm, to standard SVMs and filter methods (i.e. preprocessing methods selecting features either based on Pearson correlation coefficients, Fisher criterion score, or the Kolmogorov-Smirnov statistic). Their artificial data benchmarks provide a basis for comparing our approach with their, which is based on the minimization of error bounds. Two types of distributions are provided, whose detailed characteristics are not given here. In the linear problem, 6 dimensions out of 202 are relevant. In the nonlinear problem, two features out of 52 are relevant. For each distribution, 30 experiments are conducted, and the average test recognition rate measures the performance of each method. For both problems, standard SVM achieve a 50% error rate in the considered range of training set sizes. Our results are shown in Figure 1.

0.5 0.5

7

0.4 0.4

0.3 0.3

0.2 0.2

0.1 0.1

0 10 20 30 40 50 75 100 0 10 20 30 40 50 75 100

Figure 1: Results obtained on the benchmarks of [9]. Left: linear problem; right nonlinear problem. The number of training examples is represented on the -axis, and the average test error rate on the -axis. Our test performances are qualitatively similar to the ones obtained by gradient descent on the radius/margin bound in [9], which are only improved by the forward selection algorithm b

 


minimizing the span bound. Note however that Weston et al. results are obtained after a correct number of features was specified by the user, whereas the present results were obtained fully automatically. Knowing the number of features that should be selected by

97

the algorithm is somewhat similar to select the optimal value of parameter

¢

In the non-linear problem, for

selected; for

)V)

(

¢ )

( @

for each .

training examples, an average of 26.5 features are

, an average of 6.6 features are selected. These figures show that

( @





although our feature selection scheme is effective, it should be more stringent: a smaller value of would be more appropriate for this type of problem. The two relevant variables

 ¢¡¤£

( ¢ £ £ ( ¢ ¡§¦

are selected in

) )

¢

of cases for

)

, in ¥

)

for n=50, and in

) )

for and

. For these two sample sizes, they are even always ranked first and second.



Regarding training times, the optimization of

 

©

required an average of over 100 times

more computing time than standard SVM fitting for the linear problem and 40 times for the nonlinear problem. These increases scale less than linearly with the number of variables, and are certainly yet to be improved.

4.2 Expression recognition We also tested our algorithm on a more demanding task to test its ability to handle a large number of features. The considered problem consists in recognizing the happiness expression among the five other facial expressions corresponding to universal emotions (disgust, frontal faces, with standardized positions of eyes, nose and mouth. The training set com-

¡ sadness, fear, anger, and surprise). The data sets are made of

)©¨V)

gray level images of

prises and

V)) 

positive images, and negative ones.

)  

negative ones. The test set is made of

)

positive images

 

We used the raw pixel representation of images, resulting in 4200 highly correlated features. For this task, the accuracy of standard SVMs is 92.6% (11 test errors). The recognition rate is not significantly affected by our feature selection scheme (10 errors), but more than 1300 pixels are considered to be completely irrelevant at the end of the iterative proThis selection brings some important clues for building relevant attributes for the facial recognition expression task. highest value. We see that, according to the classifier, the relevant areas for recognizing the happiness expression are mainly in the mouth area, especially on the mouth wrinkles, and to a lesser extent in the white of the eyes (which detects open eyes) and the outer eyebrows. On the right hand side of this figure, we displayed masked support faces, i.e. support faces scaled by the expression mask. Although we lost many important features regarding the identity of people, the expression is still visible on these faces. Areas irrelevant for the recognition task (forehead, nose, and upper cheeks) have been erased or softened by the expression mask.

cedure (estimating

© required about 80 times more computing time than standard SVM).

Figure 2 represents the scaling factors

© , where black is zero and white represents the



5 Conclusion We have introduced a method to perform automatic relevance determination and feature selection in nonlinear SVMs. Our approach considers that the metric in input space defines a set of parameters of the SVM classifier. The update of the scale factors is performed by iteratively minimizing an approximation of the SVM cost. The latter is efficiently minimized with respect to slack variables when the metric varies. The approximation of the cost function is tight enough to allow large update of the metric when necessary. Furthermore, because at each step our algorithm guaranties the global cost to decrease, it is stable.


Figure 2: Left: expression mask of happiness provided by the scaling factors

©

; Right,

top row: the two positive masked support face; Right, bottom row: four negative masked support faces.

Preliminary experimental results show that the method provides sensible results in a reasonable time, even in very high dimensional spaces, as illustrated on a facial expression recognition task. In terms of test recognition rates, our method is comparable with [9, 3]. Further comparisons are still needed to demonstrate the practical merits of each paradigm. Finally, it may also be beneficial to mix the two approaches: the method of Cristianini et al. since the relative relevance of each feature (as measured by ) would be estimated by empirical risk minimization, instead of being driven by an estimate of generalization error. References [1] P. S. Bradley and O. L. Mangasarian. Feature selection via concave minimization and support vector machines. In Proc. 15th International Conf. on Machine Learning, pages 82­90. Morgan Kaufmann, San Francisco, CA, 1998. [2] L. Breiman. Heuristics of instability and stabilization in model selection. The Annals of Statistics, 24(6):2350­2383, 1996. [3] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector machines. Machine Learning, 46(1):131­159, 2002. [4] N. Cristianini, C. Campbell, and J. Shawe-Taylor. Dynamically adapting kernels in support vector machines. In M. S. Kearns, S. A. Solla, and D. A. Cohn, editors, Advances in Neural Information Processing Systems 11. MIT Press, 1999. [5] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: data mining , inference, and prediction. Springer series in statistics. Springer, 2001. [6] T. Jebara and T. Jaakkola. Feature selection and dualities in maximum entropy discrimination. In Uncertainity In Artificial Intellegence, 2000. [7] R. M. Neal. Bayesian Learning for Neural Networks, volume 118 of Lecture Notes in Statistics. Springer, 1996. [8] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer Series in Statistics. Springer, 1995. [9] J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Poggio, and V. Vapnik. Feature selection for SVMs. In Advances in Neural Information Processing Systems 13. MIT Press, 2000.

 7 [4] could be used to determine and  . The resulting algorithm would differ from [9, 3],  ¡   7


