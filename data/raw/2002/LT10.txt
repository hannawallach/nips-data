The Effect of Singularities in a Learning Machine when the True Parameters Do Not Lie on Such Singularities

Sumio Watanabe Precision and Intelligence Laboratory Tokyo Institute of Technology 4259 Nagatsuta, Midori-ku, Yokohama, 226-8503 Japan E-mail: swatanab@pi.titech.ac.jp Shun-ichi Amari Laboratory for Mathematical Neuroscience RIKEN Brain Science Institute Hirosawa, 2-1, Wako-shi, Saitama, 351-0198, Japan E-mail: amari@brain.riken.go.jp

Abstract A lot of learning machines with hidden variables used in information science have singularities in their parameter spaces.

At

singularities, the Fisher information matrix becomes degenerate, resulting that the learning theory of regular statistical models does not hold. Recently, it was proven that, if the true parameter is contained in singularities, then the coefficient of the Bayes generalization error is equal to the pole of the zeta function of the Kullback information. In this paper, under the condition that the true parameter is almost but not contained in singularities, we show two results. (1) If the dimension of the parameter from inputs to hidden units is not larger than three, then there exits a region of true parameters where the generalization error is larger than those of regular models, however, if otherwise, then for any true parameter, the generalization error is smaller than those of regular models. (2) The symmetry of the generalization error and the training error does not hold in singular models in general.

1 Introduction

A lot of learning machines with hidden parts such as multi-layer perceptrons [8], gaussian mixtures[2], Boltzman machines, and Bayesian networks with latent variables [4] are nonidentifiable statistical models. In such learning machines, the mapping from the parameter to the probability distribution is not one-to-one. Moreover, they have complex singularities. In this paper, a parameter w of a parametric probability density function p(x|w) is called to be a singularity if and only if det I(w) = 0,


where I(w) is the Fisher information matrix at w. If a learning machine has singularities, then neither the maximum likelihood estimator nor the Bayes a posteriori distribution converges to the normal distribution in general [1][5]. Recently, despite of the mathematical difficulty of such learning machines, the asymptotic Bayes generalization error has been clarified using algebraic geometrical method [5][6]. The Bayes generalization error G(n), which is defined as the average Kullback distance from the true distribution to the Bayes predictive distribution,

is equal to

G(n) =  n 1 + o(n )

where n is the number of training samples and (-) is the rational number that is equal to the largest pole of the zeta function of the Kullback information and the prior [6][7]. If the true parameter is not a singular point, then  = d/2, where d is the dimension of the parameter space, whereas, if the set of the true parameters consists of singularities, then  is different from d/2 [6][8]. In almost learning machines, singularities of the parameter space correspond to smaller models contained in the parametric model. However, in practical applications, the true distribution is seldom contained completely in a finite model, and it often happens that the true parameter is almost but not completely contained in singularities. In this paper, in order to clarify the effect of singularities when the true parameter lies in the neighborhood of singularities, we propose a new scaling method by which the Kullback distance from the singularities to the true distribution is equal to c/n, where n is the number of training samples and c is a controlling parameter. This scaling method, which is often used in comparing the powers of statistical hypothesis testing algorithms, enables us to clarify the effect of singularities. We show two results. (1) If the number of the parameters from inputs to hidden units is not larger than three, then there exists c > 0 such that the generalization error is larger than those of the corresponding regular model. However, if otherwise, then for an arbitrary c  0, the generalization error is made to be smaller by the singularities. (2) The symmetry of the generalization error and the training error does not hold in nonidentifiable learning machines in general. 2 A Singular Model Since singularities in learning machines with hidden variables have quite complex geometrical structures in general, it needs the advanced method in modern algebraic geometry to treat them in a general manner [6]. In this paper, we study a simple hierarchical model. Even in this simple model, a universal phenomenon caused by singularities can be found. Let us consider a learning problem:

Learner : True : p(y|x, a, b) q(y|x) = 12  = 12  1 exp(-2 (y - af(b, x))2), 1

exp(-2 (y - n f(b0, x))2), a0

(1) (2)

where y  R1 is an output, x  RM is an input with the probability distribution q(x). The parameter space is defined by {(a, b)  R1ОRN }. The Kullback distance expectation value over x. If f(0, x)  0, then an arbitrary point in {a = 0}{b = 0} from q(y|x) to p(y|x, a, b) is equal to (1/2n) a20Ex[f(b0, x)2], where Ex denotes the is a singularity. We assume that the a priori distribution (a, b) is a C1-class function and (b)  (0, b) has a compact support.


Let Dn = {(xi, yi); i = 1, 2, иии, n} be a set of training samples independently taken from q(x)q(y|x). Both the Bayes a posteriori distribution p(a, b|Dn) and the Bayes predictive distribution p(y|x, Dn) are respectively defined by

n

p(a, b|Dn) p(y|x, Dn)

= 1 Cn (a, b) p(yi|xi, a, b),

i=1

= p(y|x, a, b) p(a, b|Dn) da db,

where Cn is a normalizing constant. The generalization error G(n) and the training error T (n) are respectively defined by

Generalization Error: Training Error: G(n) T (n) = =

E log

E 1 n

q(yn+1|xn+1) p(yn+1|xn+1, Dn)

n

log

,

k=1

q(yk|xk) p(yk|xk, Dn) ,

where E shows the expectation value over all sets of training samples Dn and the testing samples (xn+1, yn+1). If the learning machine is a regular statistical model, then both G(n) = d/(2n) + o(1/n) and T (n) = -d/(2n) + o(1/n) hold, where d is the dimension of the parameter space, hence the coefficient d does not depend on the true parameter. In this paper, we show that this property does not hold in a singular learning machine. We assume that the learning machine satisfies the condition f(b, x) = fj(b)ej(x) (3)

J

j=1

where {ej(x)2}is a set of orthonormal functions, Ex[ei(x)ej(x)] = ij. Then it follows Theorem 1 The Bayes generalization and training errors can be asymptotically expanded as

that f(b) j=1 fj(b)2 = Ex[f(b, x)2]. Then we have the following theorem.

G(n) T (n) = = (a0, b0) 2n х(a0, b0) 2n 1 + o(n ), 1 + o(n ).

Here (a0, b0) and х(a0, b0) are constant functions of n defined by

J

(a0, b0) = 1 + a20 f(b0) 2

1 Z

- Eg

J

a0fj(b0) Z

(g) gj j=1

1 Z

х(a0, b0) = (a0, b0) - Eg 2gj

j=1

Z(g) gj

where g = (gj) is the J dimensional gaussian distribution whose average and the covariance matrix are respectively zero and the identity, and Eg shows the expectation value over g, and

J

{j=1(gj + a0fj(b0))fj(b)}2 Z(g) = exp

1 2 f(b) db.

2

(b) f(b)


Proof of Theorem 1. We use the rescaling parameter  = average < S(, b) > of a function of S(, b) by

< S(, b) >=

( exp(-L(, b)) S(, b) /n, b) d db exp(-L(, b)) (/

n a and define the

n, b) d db

where, we use notations d(, b, x) = f(b, x) - a0f(b0, x) and

n

Li(, b) d(, b, xi)2 - n

i=1

L(, b) Li(, b) = =

1 n 1 2



i d(, b, xi).

Here i

The Bayes generalization and training errors are respectively equal to  yi - a0f(b0,xi)/n is a sample from the standard normal distribution.

G(n) T (n) = =

E - log < exp{-Ln+1n (, b)}

n

1 E - n

>

log < exp{-Lk(n , b)} > .

k=1

When n  , the central limiting theorem ensures the convergences in probability and in law respectively,

n n

1 n ej(xi) ek(xi)  jk, 1n

i ej(xi)  gj,

i=1 i=1

where g = (gj) is subject to the normal distribution whose average and covariance matrix are respectively equal to zero and the identity. Then by using log(1 - t) = -t + t2/2 + o(t2) for small t, it follows that

J

1 Z

Eg { Z

gj

lim 2nG(n) = n - a0fj(b0)}2 ,

J 1 Z

gj Z

gj

j=1

lim 2nT (n) n = lim 2nG(n) - 2Eg n ,

j=1

where Eg shows the expectation value over the random variable g and

J J

Z(g) = 1 exp -2 2fj(b)2 + fj(b)(gj + a0fj(b0)) (b) d db.

j=1 j=1

By using the identity

1 Z

{Z

gj

}2 = 1 2Z Z gj

2

 1 Z

- gj { Z gj },

and Eg[(/gj)f(g)] = Eg[gjf(g)] for an arbitrary function f(g), we obtain Theorem 1. (End of Proof: Theorem 1). Theorem 1 shows that, if a0 = 0, then (a0, b0) = 1, which coincides with the general theory for the case when the true parameter is contained in the singularities [6]. In fact, if a0 = 0, the zeta function of the Kullback information

(z) = a2z b 2z (a, b) da db,

coefficient (a0, b0) for a0 = 0, b0 = 0 is obtained. Unfortunately it can not be has the largest pole at z = -1/2. The new point of this paper is that the learning represented by any simple function.


3 The Effect of Singularities

In order to study the effect of singularities, we adopt the simple learning machine, af(b, x) = abjej(x) (4)

N

j=1

where a  Rb ,,bthatRN, x  RM (N > 1). Also we assume that (b) depends only machine, if the true regression function is y = 0, then the set of true parameters is {(a,b);a = 0 or b = 0}. Remark. By using the re-parameterization wi = abi, the learning machine eq.(4)

the norm



is to say, (b) can be rewritten as ( b ). In this learning

results in N

p(y|x, w) = 12  1 exp(-2 (y - wjej(x)))2).

1

j=1

This learner is a regular statistical model, hence both G(n) = N/(2n) + o(1/n) and with N, let us clarify the effect of singularities. T (n) = -N/(2n) + o(1/n) hold. Therefore, by comparing (a0, b0) and -х(a0, b0) Theorem 2 Let us consider the learning machine and the true distribution given by eq.(1) and eq.(2), which are restricted as eq.(4). generalization and training errors are respectively given by If N  2, then the Bayes

(a0, b0) х(a0, b0) = =

1 + Eg (a20 b0 2

YN (g)

+ a0b0 и g)YN-

2 (g)

1 - 2N + Eg (a20 b0

/2

2 2 YN (g)

)YN-

2 (g)

+ 3a0b0 и g + 2 g

(5) (6)

where

YN (g) =

0

1 d sinN  exp(-2 a0b0 + g 2 sin2 ).

Proof of Theorem 2. We introduce the general polar coordinate b = (r, ). The function Z(g) in Theorem 1 is given by

Z(g) = dr d exp{ ((g + a0b0) и )2 2 } (r) rN- . 2

Therefore Z(g) is independent of the direction of g+a0b0, we can assume g+a0b0 = g + a0b0 О (1, 0, иии, 0) without loss of generality. By representing  = b/r as

bi/r bN /r = =

sin 1 иии sin i- cos i

1

sin 1 иии sin N- ,

1

/2

(1  i  N - 1),

we obtain

Z(g) = const. sinN- 1 exp( 2

0

a0b0 + g 2 2 cos2 1) d1.

which completes the proof. (End of Proof: Theorem 2). Unfortunately, the function (a0, b0) in eq.(5) can not be represented by any classically analytic function. Figure 1 shows the value (a0, b0) given by eq.(5) by numerical calculations, for the cases N = 2, 3, .., 6. The horizontal and longitudinal lines respectively show |a0| b0 and (a0, b0)/N. The generalization error


1.2

1

0.8

0.6

lambda/N

0.4

"Gener:N=2" "Gener:N=3" "Gener:N=4" "Gener:N=5" "Gener:N=6" "lambda=1"

0.2

0 0 2 4 6 a0||b0|| 8 10 12

Figure 1: Coefficients of Generalization Errors (a0, b0)/N for a0 b0

0

-0.2

-0.4

"Train:N=2" "Train:N=3" "Train:N=4" "Train:N=5" "Train:N=6"

mu/N

-0.6

-0.8

-1 0 2 4 6 a0||b0|| 8 10 12

Figure 2: Coefficients of Training Errors х(a0, b0)/N for a0 b0

is smaller than that of the corresponding regular statistical model if and only if (a0, b0)/N < 1. . If N = 2 and N = 3, (a0,b0) becomes larger than N, if the true parameter mismatches the singularities. When N = 2, in the region |a0| b0

For all cases 2  N  6, (a0, b0) converges to the dimension N when |a0| b0 

N. When N = 3, only in the interval 3.8 < |a0| b0

> 2.8, (a0, b0) > < 6.8, (a0, b0) > N.

N, even if the true parameter is not contained in singularities. If the dimension of On the other hand, if N  4, the learning coefficient (a0, b0) is always smaller than the parameter is large, then singularities make the Bayes generalization error smaller than regular statistical models, independently of the place of the true parameter. This result can be analyzed more precisely by the asymptotic expansion.


Theorem 3 The coefficients can be asymptotically expanded when |a0| b0

(a0, b0) х(a0, b0)

In this theorem, a20 b0

2

= =

N -

(N - 1)(N 2- 3) a20 b0

), 2

 .

(N - 1)2 a20 b0 2

+ o(a20 1

-N + + o(a20

2

1 b0 ).

b0

/2 is equal to the Kullback distance from the singularities

to the true distribution. It should be emphasized that the symmetrical relation (a0, b0) + х(a0, b0) = 0 does not hold near the singularities. In the generalization N  4. When N = 3, then the coefficient is equal to zero. Proof of Theorem 3 The function YN(g) in Theorem 2 is rewritten as

error, the coefficient of 1/a20 b0 2 is positive if N = 2, whereas it is negative if

1 xN

x2 a0b0+g

2

exp(-x2 )dx YN (g) = 1 a0b0 + g

2 0

1 -  = 1 + 2

Then by using

1

x2 a0b0+g 1 -

x2 2 a0b0 + g

2 ,

2

we have an asymptotic expansion,

(a0, b0) = 1 + Eg (a20 b0 2 + a0b0 и g)

CN a0b0 + g CN-

2

a0b0 + g

M +1

M -1

+ + CN+2 2 a0b0 + g CN 2 a0b0 + g

M +3

,

M +1

where CN = 2( N-1)/2

(N2 ). The training error can be obtained by the same way.

+1

(End of Proof: Theorem 3). 4 Discussion Let us shortly discuss three points. Firstly, in this paper, we compared a simple layered model with a regular statistical model. If we employ a linear learner y = bjej(x),

N

j=1

then we can expect the more precise statistical estimation by making it to be the hierarchical model, y = abjej(x),

N

j=1

if N  4 and Bayesian estimation is applied. Secondly, the Bayesian model selection is usually carried out by minimizing the stochastic complexities, F (Dn) = - log p(yi|xi, a, b)(a, b) dab. .

n

i=1


Let us consider the model selection problem, the model y = 0 or the model in eq.(1). If the Kullback distance from the singularities to the true paramater is equal to c/n and if n is sufficiently large, then for an arbitrary c, y = 0 is selected with the probability one. Theoretically speaking, this fact shows that the minimum stochastic complexity criterion is not equivalent to the minimum generalization error criterion. And lastly, we have shown that, if the true parameter is at the neighborhood of singularities, then the symmetry of the generalization error and the training error does not hold. Therefore the generalization error can not be estimated based on the training error using the conventional method. These three points are the important problems for future study. 5 Conclusion Effect of singularities when the true parameter mismatches them is clarified. Singularities make the Bayes generalization error to be small if the dimension of the inputs to hidden units is large. We expect that this research will be a base to clarify the reason why neural information processing systems need hierarchical structures. This work was supported by the Ministry of Education, Science, Sports, and Culture in Japan, Grant-in-aid for scientific research 12680370. References [1] Amari,S., Park,H., and Ozeki,T. (2002) Geometrical singularities in the neuromanifold of multilayer perceptrons. Advances in Neural Information Processing Systems, Vol.14. [2] Hartigan, J.A. (1985) A Failure of likelihood asymptotics for normal mixtures. Proceedings of the Berkeley Conference in Honor of J.Neyman and J.Kiefer, Vol.2, pp.807-810. [3] Hironaka, H. (1964). Resolution of singularities of an algebraic variety over a field of characteristic zero. Annals of Mathematics, 79, 109-326. [4] Rusakov, D, Geiger,D.(2002) Asymptotic model selection for naive Bayesian networks. Proc. of UAI02. [5] Watanabe, S. (1999). Algebraic analysis for singular statistical estimation. Lecture Notes in Computer Science, 1720, 39-50. [6] Watanabe, S.,(2001) Algebraic analysis for nonidentifiable learning machines. Neural Computation, 13,(4), pp.899-933. [7] Watanabe, S. (2001) Algebraic information geometry for learning machines with singularities. Advances in Neural Information Processing Systems, Vol.13, 329-336. [8] Watanabe, S. (2001) Algebraic geometrical methods for hierarchical learning machines. International Journal of Neural Networks, Vol.14, No.8, 1049-1060. [9] Watanabe,S., & Amari,S.-I.(2003) Learning coefficients of layered models when the true distriburion mismatches the singularities.Neural Computation, to appear.


