Extracting Relevant Structures with Side Information

Gal Chechik and Naftali Tishby ggal,tishby @cs.huji.ac.il School of Computer Science and Engineering and The Interdisciplinary Center for Neural Computation The Hebrew University of Jerusalem, 91904, Israel  

¡

Abstract The problem of extracting the relevant aspects of data, in face of multiple conflicting structures, is inherent to modeling of complex data. Extracting structure in one random variable that is relevant for another variable has been principally addressed recently via the information bottleneck method [15]. However, such auxiliary variables often contain more information than is actually required due to structures that are irrelevant for the task. In many other cases it is in fact easier to specify what is irrelevant than what is, for the task at hand. Identifying the relevant structures, however, can thus be considerably improved by also minimizing the information about another, irrelevant, variable. In this paper we give a general formulation of this problem and derive its formal, as well as algorithmic, solution. Its operation is demonstrated in a synthetic example and in two real world problems in the context of text categorization and face images. While the original information bottleneck problem is related to rate distortion theory, with the distortion measure replaced by the relevant information, extracting relevant features while removing irrelevant ones is related to rate distortion with side information.

1 Introduction A fundamental goal of machine learning is to find regular structures in a given empirical data, and use it to construct predictive or comprehensible models. This general goal, unfortunately, is very ill defined, as many data sets contain alternative, often conflicting, underlying structures. For example, documents may be classified either by subject or by writing style; spoken words can be labeled by their meaning or by the identity of the speaker; proteins can be classified by their structure or function - all are valid alternatives. Which of these alternative structures is "relevant" is often implicit in the problem formulation. The problem of identifying "the" relevant structures is commonly addressed in supervised learning tasks, by providing a "relevant" label to the data, and selecting features that are discriminative with respect to this label. An information theoretic generalization of this supervised approach has been proposed in [9, 15] through the information bottleneck method (IB). In this approach, relevance is introduced through another random variable (as is the label in supervised learning) and the goal is to compress one (the source) variable, while maintaining as much information about the auxiliary (relevance) variable. This framework


has proven powerful for numerous applications, such as clustering the objects of sentences with respect to the verbs [9], documents with respect to their terms [1, 6, 14], genes with respect to tissues [8, 11], and stimuli with respect to spike patterns [10]. An important condition for this approach to work is that the auxiliary variable indeed corresponds to the task. In many situations, however, such "pure" variable is not available. The variable may in fact contain alternative and even conflicting structures. In this paper we show that this general and common problem can be alleviated by providing "negative information", i.e. information about "unimportant", or irrelevant, aspects of the data that can interfere with the desired structure during the learning.

As an illustration, consider a simple nonlinear regression problem. Two variables

are related through a functional form

class and

¦ ¡©



¡£¢¥¤§¦¨ ©

, where

¤§¦ © 

 

and

¡

is in some known function

is noise with some distribution that depends on . When given a sample of

pairs with the goal of extracting the relevant dependence

 

¡ ¢!¤§¦¨ ©

, the noise

¦ "#$©

¡



-

which may contain information on and thus interfere with extracting - is an irrelevant

variable. Knowing the joint distribution of result. can of course improve the regression

A more "real life" example can be found in the analysis of gene expression data. Such data, as generated by the DNA-chips technology, can be considered as an empirical joint distribution of gene expression levels and different tissues, where the tissues are taken from different biological conditions and pathologies. The search for expressed genes that testify for the existence of a pathology may be obscured by genetic correlations that exist also in other conditions. Here again a sample of irrelevant expression data, taken for instance from a healthy population, can enable clustering analysis to focus on the pathological features only, and ignore spurious structures. These two examples, and numerous others, are all instantiations of a common problem: in order to better extract the relevant structures information about the irrelevant components of the data should be incorporated. Naturally, various solutions have been suggested to this basic problem in many different contexts (e.g. spectral subtraction, weighted regression analysis). The current paper presents a general unified information theoretic framework for such problems, extending the original information bottleneck variational problem to deal with discriminative tasks of that nature, by observing its analogy with rate distortion theory with side information.

2 Information Theoretic Formulation To formalize the problem of extracting relevant structures consider first three categorical

variables

% &('

, and

cover structures in

1£¦%2&('3©

&0)

whose co-occurrence distributions are known. Our goal is to un-

, that do not exist in

1£¦%4#&5)§©

. The distribution

1£¦%762&8'3©

may contain several conflicting underlying structures, some of which may also exist in

1£¦%4#&0)9©

& '

. These variables stand for example for a set of terms

whose structure we seek, and an additional set of documents

%&

, a set of documents

)

, or a set of genes

and two sets of tissues with different biological conditions. In all these examples

&£)

are conditionally independent given

distribution factorizes as:

@9¦ ¡ ' ¡ ) ©A¢B@¦¨ ©C@9¦¡ 'ED ©@¦¡ )FD ©

%

&£'

and

. We thus make the assumption that the joint

.

The relationship between the variables can be expressed by a Venn diagram (Figure 1A), where the area of each circle corresponds to the entropy of a variable (see e.g. [2] p.20 and [3] p.50 for discussion of this type of diagrams) and the intersection of two circles corresponds to their mutual information. The mutual information of two ran-

dom variables is the familiar symmetric functional of their joint distribution,

.

P¥QSRT

@¦¨ "¡©VUXW$Y0`badcQheQ$RTfeTieqp agc adc GH¦¨%76#&(©I¢


A. B.

Figure 1: A. A Venn diagram illustrating the relations between the entropy and mutual ina variable, while the intersection of two circles corresponds to their mutual information. As

formation of the variables % & ' & ) , , . The area of each circle corresponds to the entropy of

&8'

and

&£)

are independent given

%

, their mutual information vanishes when

thus all their overlap is included in the circle of

%

 

is known,

. B. A graphical model representation of

IB with side information. Given the three variables

tic representation

about

 & )

  of

%

% &8' &£)

, ,

& ' & )

&, '

we seek a compact stochas-

but removes information which preserves information about

and . In this graph are indeed conditionally independent given

%

.

To identify the relevant structures in the joint distribution

compact representation of the variable

the relevant variable

& '

the irrelevance variable

&, )

%

@9¦ ¡ '3©

, we aim to extract a

with minimal loss of mutual information about

and at the same time with maximal loss of information about

. The goal of information bottleneck with side information

(IBSI) is therefor to find a stochastic map of

maximizes its mutual information with

&£)

& '

%

to a new variable   ,

@9¦ D ©

¢¡ , in a way that

and minimizes the mutual information about

. In general one can achieve this goal perfectly only asymptotically and the finite case

leads to a sub optimal compression, an example of which is depicted in the blue region in figure 1. These constrains can be cast into a single variational functional,

£

¢¥G¦%76 ©

¤  ¦¥¨§© ¢ 

G¦I62& ' © G¦86#& ) ©

¥¨ ¢   (1)

where the Lagrange parameter § determines the tradeoff between compression and infor-

mation extraction while the parameter

information about the relevant

& )

&8'

 determines the tradeoff between preservation of

variable and loss of information about the irrelevant one

. In some applications, such as in communication, the value of  may be determined by

the relative cost of transmitting the information about

& )

by other means.

The information bottleneck variational problem, introduced in [15], is a special case of

our current variational problem with 

¢



available. In that case only the distributions 3 Solution Characterization

@9¦, D © @¦© @¦¨¡ ' D©

¢¡ , ¢¡ and ¡

namely, no side or irrelevant information is

are determined.

The complete Lagrangian of this constrained optimization problem is given by

£

@¦ D © "¢ G¦%76 © ¡ ! ¤  "¥#§© ¢  G¦86#& ' © GH¦I6#& ) © Q ¦ © @¦ D © ¥¨   !$¥&% (' )%10 ¢¡ (2)

where '

¦ ©

, are the normalization Lagrange multipliers. Here, the minimization is per-

@¦tic D ©q6@¦©f6¨@9¦¡@¦' D©q6¨@9¦¡) D©

relations to

¢¡

¡ , ¡

¡ ¡ ¡

consistent equations.

formed with respect to the stochastic mapping

© @¦¨¡ ' D© @9¦¡)9©

and

@¦ D ©

¡ , taking into account its probabilis-

. Interestingly, performing the minimization over

as independent variables leads to the same solution of self-


£

Proposition 1 The extrema of

@9¦ D © ¢

¢¡

@¦©

¡

obey the following self consistent equations

 ¢¡ ¤£)

¦ ¦

¦¥¨§©



adc

T e T Q

agc

0

e

¤) ¥¨§©



adcT adcT Qe eC©2©

0

(3)

@9¦© ¢ Q @9¦ D ©@¦ ©

!

¢¡ % ¢¡

@¦¨¡ ' D© ¢ ¡

@¦¨¡ ) D© ¢ ¡

¢ @¦© ¢¡

P

@¦©

¡

!

#"$&%@¦

 

¡

©

¥ §

%

Q @¦¡ ' D ©C@9¦ D ©@¦ © ¢¡

Q @¦¡ ) D ©@¦ D ©@¦¨ © ¢¡

'P D ©Q DXD@9¦¡ ' D© Q e @9¦ ©VUW Y adcQhe 4c ¡ !)¥¨

where

  % 0

'5¦(10¦')(10@ 3@¦¨¡§¢





DD

a normalization factor and gence [2], Proof:

'2(10



@¦¡) D © DD@¦¨¡) D© C© ¡  ¡ is

is the Kullback-Leibler diver-

P Q

Following the Markovian relation

¢ @9¦¡ Dq# ©@¦ D ©@¦¨ © ¢

5@¦

5

¢¡

¡ ¢¡

D © G¦I62& ' © ¢ ¢ 

5@9¦

5

¢¡

P Q

P Q D ©

@¦¨¡ D  ©3¢7@¦¡ D © @¦¡ D ©@¦ D ©@¦¨ © ¡

¢¡

T Q % % 0 %



, we write

@¦¨¡ ©3¢

¤¡

@¦¨¡ D ©C@9¦ ©

¤¡

and obtain for the second term of Eq. 3

@¦¡ ' D ©@¦ D ©@¦¨ ©VUXW$Y ¢¡

@¦¡ ' D© @¦¨¡ ' D © ¡

@¦¨¡ ' D © @¦¨¡ ' ©

6

6

@9¦¡@9¦¡'' D©

¡

87© (4)

¢ @9¦ © @¦¡ ' D ©VUXW$Y

)%

¢ @9¦ © ¥

T

Similar differentiation for the other terms yield

5@¦

where

DCE')(10©@9¦¡7 £

¢ @¦¨ ©VUXW$Y

¥

@¦¨ © §

'

¡

69')(10

@¦@¦ D ©

¡

©

©



¡

D ©

@9¦¡ ' D © DXD@¦¡ ' D© A@¦ ©

¡ 

@')(107

©

©

@¦¡ ' D © DD@¦¨¡ ' © ! BA

(5)

5

¦ ¡ ' #¡)9©3¢ cQ  ¦

&§

 @¦¨ ©Q ¦ ¡ ' #¡ ) ©

IHadc ¦' (10

ee

' D © DD@¦¨¡ ' D©  @¦¨¡ ' D © DXD@9¦¡ '3©

¡  &

F')(10 @¦¡ ) D © DD@¦¨¡ ) D© ¡ 



G

'



' (10 !)¥  @¦¨¡H) D © DXD@9¦¡)9© C©  ,

holds all terms independent of . Equating the derivative to zero then yields the first equa¡

tion of proposition 1. The formal solutions of the above variational problem have an exponential form which is a natural generalization of the solution of the original IB problem. As in the original IB, when

§ goes to infinity the Lagrangian reduces to

collapse to a hard clustering solution, where

probabilities.

G¦@¦I62&('3©becomeI62&0)9©

¢  ¥

¢¡

D ©  ¢ 

G¦

, and the exponents binary cluster membership

Further intuition about the operation of IBSI can be obtained by rewriting the second

0

¢ UW Y `IadcadcT

Q

 

term in Eq. 2,

P P¥T P¥T

0

¢

! 

 

@9¦q¡ ' #¡H)© UW Y `adcadcT

¢¡

G¦86#&8'3© G¦86#&T )9©ee ¢

¢  ¥  ¢  0

p



Q 0 ¢¡

¥

For

0

eAp

UT



P P T P¥T

GH¦¨%76 ©  

tion   that maximizes the mean log likelihood ratio

@¦¨¡ ' D© @¦¨¡) D© ¡

and ¡

UW Y ` adcTT adc

 SR

PT e R @¦ee q#¡adcTT' ¡)©VUXW$YRT ` adcRTagcTe.

0

PT  0 ee p

and a fixed level of , IBSI thus operates to extract a compact representa-

0

0

RT RT e,

adc measuring

adceeqpVT

0

the discriminability between the distribution of .

agc 


The above setup can be extended to the case of multiple variables on which multi-

information should be preserved

  ¡   should be removed

@¦@¦D © ¢¡

¢¡

© ¢

¡ )  #¡ )

¦ ) £

¡ ¥ AAAP

£

  ¡

 

¡ '  X¡ '

and variables on which multi-information ¡

, as discussed in [8]. This yields

§© 

0 

adcT agcT '

£ £

P

£ £

¥ §©



adcT adcT

£

AAA



¡

Qhe9  e   Qhe  e©

0

£ (6)

which can be solved together with the other self-consistent conditions, similarly to Eq. 4. 4 Relation to Rate Distortion Theory with Side Information The problem formulated above is related to the theory of rate distortion with side informa-

tion ([17],[2] p. 439). In rate distortion theory (RDT) a source variable

encoded into a variable  

%

is stochastically

, which is decoded at the other side of the channel with some

¤

distortion. The achievable code rate, at a given distortion level

¤

@¦ D ©

¡

¦ ©

'

'

, is bounded by the

optimal rate, also known as the rate distortion function,

determined by the stochastic map

. The optimal encoding is

, where the representation quantization is found by

GH¦¨%76 ©§¢ ¦ © ¤

  minimizing the average distortion. For the optimal code

'

.

This rate can be improved by utilizing side information in the form of another variable, ¥ , that is known at both ends of the channel. In this case, an improved rate can be achieved by the rate distortion function with this side information has a lower lower-bound, given by the distortion constraint (see [17] for details). In the information bottleneck framework the average distortion is replaced by the mutual information about the relevant variable, while the rate-distortion function is turned into a convex curve that characterizes the complexity of the relation between the variables, (see [15, 13]).

avoiding sending information about % that can be extracted from ¥ . Indeed, in this case

¤

¦'¦

©3¢¥G¦%76 © GH¦I6¦¥ © ¤  ¥   , where   is the optimal quantization of % in this case, under

Similarly, IBSI avoids differentiating instances of

contain information also about

variable ¥ , while

&('

& )

. The variable

is just the "informative"

&& )of

%

that are informative about

&0'

if they

is analogous to the side information

the original IB. While the formal

analogy between these problems helps in their mathematical formulation, it is important to emphasize that these are very different problems both in motivation and scope. Whereas RDT with side information is a specific communication problem with some given (often arbitrary) distortion function, our problem is a general statistical non-parametric analysis ent pattern recognition and discriminative learning problems can be cast into this general information theoretic framework - far beyond the original setting of RDT with side information. 5 Algorithms The set of self-consistent equations (Eq. 4), can be solved by iterating the equations, given initial distributions, similar to the algorithm presented for the IB [15, 8], with similar convergence proofs. Unlike the original IB equations, convergence of the algorithm is no longer allways guaranteed, simply because the problem is not guaranteed to have feasible algorithm is guaranteed to converge. As in the case of IB, various heuristics can be applied, such as deterministic annealing hard clustering [13]; or a sequential K-means like algorithm [12]. The latter provides a good compromise between top-down annealing and agglomerative greedy approaches and

technique that depends solely by the choice of the variables % &£' , and &5) . Many differ-

solutions for all  values. However, there exist a non empty set of  values for which this

in which increasing the parameter § is used to obtain finer clusters; greedy agglomerative


achieves excellent performance. This is the algorithm we adopted  in this paper, modifying

the algorithm detailed in [12], by using a target function



GH¦I6#& ) ©

  .

¢ ) GH¦I6% © G¦I62&I'3©" §   "¥ ¢ 

 

6 Applications We describe two applications of our method: a simple synthetic example, and a "real world" problem of hierarchical text categorization. We also used IBSI to extract relevant features in face images, but these results will be published elsewhere due spavce considerations.

6.1 A synthetic example To demonstrate the ability of our approach to uncover weak but interesting hidden structures in data, we designed a co-occurrences matrix contains two competing sub-structures (see figure 2A). For demonstration purposes, the matrix was created such that the stronger

structure can be observed on the left and the weaker structure on the right. Compressing

into two clusters while preserving information on

%

 

& ' ¢

using IB (  ), yields the cluster-

ing of figure 2B, in which the upper half of 's are all clustered together. This clustering follows from the strong structure on the left of 2A. We now created a second co-occurrencematrix, to be used for identifying the relevant struc-

ture, in which each half of

%

yield similar distributions

IBSI now successfully ignores the strong but irrelevant structure in

1£¦¨¡ ) D ©1£¦¨&£'

. Applying sequential-

% ©

and retrieves

the weak structure. Importantly, this is done in an unsupervised manner, without explicitly pointing to the strong but irrelevant structure. This example was designed for demonstration purposes, thus the irrelevant structures is for real data, in which structures are much more covert.

strongly manifested in 1£¦¨%76#&£)§© . The next example shows that our approach is also useful

A. B. C. D.

P(X,Y+) P(X,T) P(X,Y-) P(X,T)

X

Y+ T Y-

T into two clusters using the 1£¦%4#& ' ©

Figure 2: Demonstration of IBSI operation. A. A joint distribution information bottleneck method separates upper and lower values of

tains two distinct and conflicting structure. B. Clustering

%

stronger structure. C. A joint distribution IBSI successfully extract the weaker structure in

ilar in nature to the stronger structure

1£¦%4#&1£¦¨%#&'3©

) ©

that con-

%

, according to the

that contains a single structure, sim-

. D. Clustering

1£¦¨%#& '3©

.

%

into two clusters using


0.65

0.6

accuracy

0.55

0.5 0 10 20 30 40 50 60

n chosen clusters

Figure 3: A. An illustration of the 20 newsgroups hierarchical data we used. B. Catego-

rization accuracy vs. no of word clusters .  

¢

  .IB dashed line. IBSI solid line.

¡

!

6.2 Hierarchical text categorization Text categorization is a fundamental task in information retrieval. Typically, one has to group a large set of texts into groups of homogeneous subjects. Recently, Slonim and colleagues showed that the IB method achieves categorization that predicts manually predefined categories with great accuracy, and largely outperforms competing methods [12]. Clearly, this unsupervised task becomes more difficult when the texts have similar subjects, because alternative categories are extracted instead of the "correct" one. This problem can be alleviated by using side information in the form of additional documents from other categories. This is specifically useful in hierarchical document categorization, in which known categories are refined by grouping documents into sub-categories. [4, 16]. IBSI can be applied to this problem by operating on the terms-documents cooccurrence matrix while using the other top-level groups for focusing on the relevant structures. To this end, IBSI is used to identify clusters of terms that will be later used to cluster a group of documents into its subgroups, While IBSI is targeted at learning structures in unsupervised manner, we have chosen to apply it to a labelled dataset of documents in order to be able to measure how its results agree with manual classification. Labels are not used by our algorithms during learning and serve only to quantify the performance. We used the 20 Newsgroups database collected by [7] preprocessed as described in [12]. This database consists of 20 equal sized groups of documents, hierarchically organized into groups according to their content (figure 3A). We aimed to cluster documents that belong to two newsgroups from the supergroup of computer documents and have very similar subjects comp.sys.ibm.pc.hardware and comp.sys.mac.hardware. As side information we used all documents from the super group of science ( sci.crypt, sci.electronics, sci.med, sci.space). To demonstrate the power of IBSI we used double clustering to separate documents into two groups. The goal of the first clustering phase is to use IBSI to identify clusters of terms that extract the relevant structures of the data. The goal of the second clustering phase is simply to provide a quantitative measure for the quality of the features extracted in the first phase. We therefor performed the following procedure: First, the most frequent 2000 words in

these documents were clustered into



by a single-cluster score

' (10

@¦¨¡V' D© DXDclusters @¦¡ '3©

¡

¡  ¥ 

using IBSI. Then, word clusters were sorted



@¦¡) D© DXD@¦¡)9©

¡  , and the clusters

 

'2(10

with the highest score were chosen. These word-clusters were then used for clustering documents. The performance of this process is evaluated by measuring the overlap of the resulting clusters with the manualy classified groups. Figure 3, plots document-clustering

! !

accuracy for

(i.e. 

¢



¢

  , as a function of . IBSI (

 

b¢

) is compared with the IB method ¡

). Using IBSI successfully improves mean clustering accuracy from about 55

percent to about 63 percents.


7 Discussion and Further Research We have presented an information theoretic approach for extracting relevant structures from data, by utilizing additional data known to share irrelevant structures with the relevant data. Naturally, the choice of side data may considerably influence the solutions obtained with IBSI, simply because using different irrelevant variables, is equivalent to asking different questions about the data analysed. In practice, side data can be naturally defined in numerous applications, in particular in exploratory analysis of scientific experiments, e.g. when searching for features that characterize a disease but not healthy subjects. While the current work is based on clustering to compress the source, the notion of extracting relevance through side information can be extended to other forms of dimentionality reduction, such as non-linear embedding on low dimensional manifolds. In particular side information can be naturally combined with information theoretic modeling approaches such as SDR [5]. Our preliminary results with this approach were found very promissing. Acknowledgements We thank Amir Globerson, Noam Slonim, Israel Nelken and Nir Friedman for helpful discussions. G.C. is supported by a grant from the ministry of Science, Israel. References

[1] L.D. Baker and A. K. McCallum. Distributional clustering of words for text classification. In Proc. of SIGIR, 1998. [2] T.M. Cover and J.A. Thomas. The elements of information theory. Plenum Press, NY, 1991. [3] I. Csiszar and J.Korner. Information theory: Coding Theorems for Discrete Memoryless Systems. Academic Press New York, 1997. 2nd edition. [4] S. Dumais and H. Chen. Hierarchical classification of web content. In Proc. of SIGIR, pages 256­263, 2000. [5] A. Globerson and N. Tishby. Sufficient dimentionality reduction. J. Mach. Learn. Res., 2003. [6] T. Hoffman. Probabilistic latent semantic indexing. In Proc. of SIGIR, pages 50­57, 1999. [7] K. Lang. Learning to filter netnews. In Proc. of 12th Int Conf. on machine Learning, 1995. [8] N. Friedman O. Mosenzon, N. Slonim, and N. Tishby. Multivariate information bottleneck. In Proc of UAI, pages 152­161, 2001. [9] F.C. Pereira, N. Tishby, and L. Lee. Distributional clustering of english words. In Meeting of the Association for Computational Linguistics, pages 183­190, 1993. [10] E. Schneidman, N. Slonim, N. Tishby, R. deRuyter van Steveninck, and W. Bialek. Analyzing neural codes using the information bottleneck method. Technical report, The Hebrew University, 2002. [11] J. Sinkkonen and S. Kaski. Clustering based on conditional distribution in an auxiliary space. Neural Computation, 14:217­239, 2001. [12] N. Slonim, N. Friedman, and N. Tishby. Unsupervised document classification using sequential information maximization. In Proc. of SIGIR, pages 129­136, 2002. [13] N. Slonim and N. Tishby. Agglomerative information bottleneck. In Advances in Neural Information Processing Systems (NIPS), 1999. [14] N. Slonim and N. Tishby. Document clustering using word clusters via the information bottleneck method. In Proc. of SIGIR, pages 208­215, 2000. [15] N. Tishby, F.C. Pereira, and W. Bialek. The information bottleneck method. In Proc. of 37th Allerton Conference on communication and computation, 1999. [16] A. Vinokourov and M.Girolani. A probabilistic framework for the hierarchic organization and classification of document collections. J. Intell. Info Sys., 18(23):153­172, 2002. [17] A. Wyner and J. Ziv. The rate distortion function for source coding with side information at the decoder. IEEE Trans. Information Theory, 22(1):1­10, 1976.


