Maximum Likelihood and the Information Bottleneck

Noam Slonim Yair Weiss

School of Computer Science & Engineering, Hebrew University, Jerusalem 91904, Israel

  noamm,yweiss¡ @cs.huji.ac.il

Abstract

The information bottleneck (IB) method is an information-theoretic formulation for clustering problems. Given a joint distribution ¢¤£¦¥¨§© , this method constructs about  . Maximum likelihood (ML) of mixture models is a standard statistical a new variable  that defines partitions over the values of  that are informative approach to clustering problems. In this paper, we ask: how are the two methods related ? We define a simple mapping between the IB problem and the ML problem for the multinomial mixture model. We show that under this mapping the problems are strongly related. In fact, for uniform input distribution over  or for large sample size, the problems are mathematically equivalent. Specifically, in these cases, every fixed point of the IB-functional defines a fixed point of the (log) likelihood and vice versa. Moreover, the values of the functionals at the fixed points are equal under simple transformations. As a result, in these cases, every algorithm that solves one of the problems, induces a solution for the other.

1 Introduction Unsupervised clustering is a central paradigm in data analysis. Given a set of objects  , one would like to find a partition  "! which optimizes some score function. Tishby et al. [1] proposed a principled information-theoretic approach to this problem. In this approach, given the joint distribution #$&%('0)1! , one looks for a compact representation of  , which preserves as much information as possible about 2 (see [2] for a detailed discussion). The mutual information, 3¨&54026! , between the random variables  and 2 is given by [3] PfeChg of the representation and the preserved relevant information are naturally measured by mu37 5482!@9BADCFEFGIHPQERS#$&%7!¦#T )VU%7!XW`YFacbQdbid Ppg . In[1]itisarguedthatboththecompactness tual information, hence the above principle can be formulated as a trade-off between these quantities. Specifically, Tishby et al. [1] suggested to introduce a compressed representation  of  , by defining q1 rpU%7! . The compactness of the representation is then determined by 3¨&c4s"! , while the quality of the clusters,  , is measured by the fraction of information they capture about 2 , 37 c482!0tf3¨&54026! . The IB problem can be stated as finding a (stochastic) mapping q1 rpU%7! such that the IB-functional uv9w37 c40"!yx(37 c482! is minimized, where  is a positive Lagrange multiplier that determines the trade-off between compression and precision. It was shown in [1] that this problem has an exact optimal (formal) solution without any assumption about the origin of the joint distribution #$ %$'s)! . The standard statistical approach to clustering is mixture modeling. We assume the mea-


surements ) for each % come from one of U6U possible statistical sources, each with its own posterior probability that the measurements at % were generated by each source. These posterior probabilities define a "soft" clustering of  values. While both approaches try to solve the same problem the viewpoints are quite different. In the information-theoretic approach no assumption is made regarding how the data was generated but we assume that the joint distribution #$&%('0)1! is known exactly. In the maximumlikelihood approach we assume a specific generative model for the data and assume we have samples @ %$'s)! , not the true probability.

parameters

 ¢¡

(e.g.

£¤¡ ¦¥§¡

' in Gaussian mixtures). Clustering corresponds to first finding

the maximum likelihood estimates of

  ¡

and then using these parameters to calculate the

¨

In spite of these conceptual differences we show that under a proper choice of the generative model, these two problems are strongly related. Specifically, we use the multinomial mixture model (a.k.a the one-sided [4] or the asymmetric clustering model [5]), and provide a simple "mapping" between the concepts of one problem to those of the other. Using this mapping we show that in general, searching for a solution of one problem induces a search in the solution space of the other. Furthermore, for uniform input distribution #T %7! or for large sample sizes, we show that the problems are mathematically equivalent. Hence, in these cases, any algorithm which solves one problem, induces a solution for the other. 2 Short review of the IB method In the IB framework, one is given as input a joint distribution #$&%('s)! . Given this distribution, a compressed representation  of  is introduced through the stochastic mapping q1 rpU%7! . The goal is to find q1 rpU%7! such that the IB-functional, u 9B3¨&c4s !yx"$3¨&c402 ! is minimized for a given value of  . The joint distribution over  '02 and  is defined through the IB Markovian independence

relation,  ability q

©&%('0)7'0rs!69 #$&%('0)1!sq1 rpU%7!

  2 . Specifically, every choice of q1 rpU%7! defines a specific joint prob-

. Therefore, the distributions q1 rs! and q1&) Urs! that are

involved in calculating the IB-functional are given by

 q1 rs!@9 A

CFHP  q &%('s)¤'0rs!@9BA C #$&%7! q1&rpU%¤!

q1 )VUrs!@9 ! g A C@q   %$'s)¤'srs! 9 ! g A C #T %$'s)! q1&rpU%¤!#"

d

¡

d

¡ (1)

In principle every choice of q1&rpU%¤! is possible but as shown in [1], if q1 rs! and q1&) Urs! are given, the choice that minimizes u is defined through,

q1 rpU%7! 9 $ d bid

PfeC g e ! Pfe g&g

d ¡ '

(2)

where

$

q1 rs!  @'0%7!&%&')(&02143

&@'s%¤! is the normalization(partition) function and 57698T #TUUqf!I9 A # W Ya b! is the

Kullback-Leibler divergence. Iterating over this equation and the

defines an iterative algorithm that is guaranteed to converge to a (local) fixed point of u [1]. 3 Short review of ML for mixture models In a multinomial mixture model, we assume that 2 takes on discrete values and sample sided clustering model [4] [5] we further assume that there can be multiple observations ) correspondingto a single % but they are all sampled from the same multinomial distribution. This model can be described through the following generative process:

it from a multinomial distribution A &) Ur  %¤!s! , where r &%7! denotes % 's label. In the one-

&@3

-step defined in Eq.(1)


  

For each % choose a unique label r  %¤! by sampling from

For 9

­ choose % by sampling from  %¤! .

­ choose ) by sampling from

Let r 9  r ' or topics for all % #$ %$'s)¤' r

"""`'sr eG e! 

¤£¦¥¨§©©



¢ ¡@ rs! .

A &) Ur  % !s! © and increase ¨@&% 's) &! © © by one.



denotes the random vector that defines the (typically hidden) labels, . The complete likelihood is given by:

¥¡ V!

¦A' '

¨@ % 's) ! ! '

9

!#"!#"eG ¡@&r¡@&r !! %$©"  © ©!! ©

eG e

e  % !0!  % !0!  !&"eG '(" y&% e eR e  % ! A¨ ) Ur  % !s! )

A &) Ur  % !s! d ' ! 9 1032 54C 76g HP

'

(3) (4)

where is a count matrix. The (true) likelihood is defined through summing over

all the possible choices of r ,

8

 ¨@ %$'s)!



9¥¡ V!@9A@CB¡ ¦A' ' #T %('0)¤' r ¥D¡ ' ' A V! " (5)

Given

¡@

rs! ' A &)VUrs! and  %¤! such that the likelihood is (at least locally) maximized. Since it is

¨@&%('0)1!

, the goal of ML estimation is to find an assignment for the parameters



easy to show that the ML estimate for

¨@ %¤! 9 A P ¨@ %$'s)! ), we further focus only on estimating ' .



 %7! is just the empirical counts ¨@ %¤!0t (where

¡

A

E§

A standard algorithm for this purpose is the EM algorithm [6]. Informally, in the

we replace the missing value of r &%7! by its distribution #$&r  %¤!pU)¤&%7!0! which we denote by

q  rs! . In the C

G

-step we use that distribution to reestimate ' . Using standard derivation

it is easy to verify that in our context the

q &rs! 9 C %

F

¡

A

-step is defined through

d

PFeChg

d

PfeC g

d d

2

EQR7SUT

2

XWPIA

F -step

HH¨VH¨V&%7!¡@¡¡@ 22 2

&%7! rs! d

Chg

rs! d

%

&%7! @ rs!

Chg

A C g

I 9

%&' 02143

2

d

EQR(SUTT

d

Pfe g ¡

d Pfe g ¡

9 PfeC g e Pfe g g d ¡

' A I 2 d PfeC g

22dCFHPdC

EQR7S 3Yg 2 d PfeC

g g . The

'

where

H

 %¤! and

H V

&%7! are normalization factors and ¨@ )VU%¤! 9

`

¡@ baBADC

rs! q  rs! C simply given by

G

(6) (7) (8) -step is

A &) Urs! A 2¨C @&%('0)1!sq  rs! " C

ca (9)

Iterating over these EM steps is guaranteed to converge to a local fixed point of the likelihood. Moreover, every fixed point of the likelihood defines a fixed point of this algorithm. An alternative derivation [7] is to define the free energy functional:

d

 ¨@ %('0)! FqX' ¦A' ! 9 9¥ e¡

The

F

-step then involves minimizing

it@¡¡HCd HC

f@x

q  rs! W`YFa @&rs! q  rs!XW`YFa q &rs! " C C C ¦g h¡ piq@

P ¨@&%('0)1!1WYa A  )VUrs! sr

(10) (11)

¡ ¦A'

with respect to q while the

algorithm will converge to a local fixed point of

the likelihood. At these fixed points,

d

d

G

-step minimizes it

with respect to . Since this functional is bounded (under mild conditions), the EM

which corresponds to a fixed point of

will become identical to x W Ya  @ %$'s)!

8

¨

¥¡

' A ! .


4 The ML   IB mapping

As already mentioned, the IB problem and the ML problem stem from different motivations and involve different "settings". Hence, it is not entirely clear what is the purpose of "mapping" between these problems. Here, we define this mapping to achieve two goals. The first is theoretically motivated: using the mapping we show some mathematical equivalence between both problems. The second is practically motivated, where we show that algorithms designed for one problem are (in some cases) suitable for solving the other. A natural mapping would be to identify each distribution with its corresponding one. However, this direct mapping is problematic. Assume that we are mapping from ML to IB. If no guarantee that the IB Markovian independence relation will hold once we complete the mapping. Specifically, using this relation to extract q1&rs! through Eq.(1) will in general rethat once we defined q1 rpU%7! and #$&%('0)1! , the other distributions could be extracted by performing the IB-step defined in Eq.(1). Moreover, as already shown in [1], performing this step can only improve (decrease) the corresponding IB-functional. A similar phenomenon is present once we map from IB to ML. Although in principle there are no "consistency"

we directly map q  rs! @&rs! ' C e¡' A¨ )VUrs! q1 rpU%7! '0q1&rs! '0q1&)VUrs!

to , respectively, obviously there is

sult with a different prior over  then by simply defining q1&rs! 9 ¡@&rs!.

However, we notice

problems by mapping directly, we know that once we defined q  rs! and C

tract

¡

and by a simple

A

G

¨@ %$'s)!

, we can ex-

-step. This step, by definition, will only improve the likelihood,

which is our goal in this setting. The only remaining issue is to define a correspondingcomponent in the ML setting for the trade-off parameter  . As we will show in the next section,

the natural choice for this purpose is the sample size,

Therefore, to summarize, we define the

£

q &rs! C

¡

q1 rpU%7! '

§£

G

@

8¢¡

§ 9wAvCFHP ¨@ %('0)! .

mapping by

¨@ %$'s)! #$&%('0)1! '

¡3 § ¡¤£

where

 ' (12)

is a positive (scaling) constant and the mapping is completed by performing an

IB-step or an

G

-step according to the mapping direction. Notice that under this mapping,

every search in the solution space of the IB problem induces a search in the solution space of the ML problem, and vice versa (see Figure 2).

Observation 4.1 When  is uniformly distributed (i.e.,

G &@3

8¥¡ ¨@ %7! #T %7!

or are constant), the

mapping is equivalent for a direct mapping of each distribution to its corre-

sponding one. This observation is a direct result from the fact that if  is uniformly distributed, then the IB-step defined in Eq.(1) and the

G

-step defined in Eq.(9) are mathematically equivalent.

Observation 4.2 When  is uniformly distributed, the EM algorithm is equivalent to the

IB iterative optimization algorithm under the

G 3 @ mappingwith 9 U U. 8¦¡ £

Again, this observation is a direct result from the equivalence of the IB-step and the for uniform prior over  . Additionally, we notice that in this case @ %¤!I9 eG e 9 ¨

$ $G§

-step 9B ,

hence Eq.(6) and Eq.(2) are also equivalent. It is important to emphasize, though, that this algorithm (and problem) are meaningful for any value of  , there is no such freedom (for equivalence holds only for a specific choice of B9 ¨@ %7!

. While clearly the IB iterative

good or worse) in the ML setting, and the exponential factor in EM must be 5 Comparing ML and IB

Claim 5.1 When  is uniformly distributed and

hood

8

£

¨@&%7!

.

9 U U, all the fixed points of the likeliare mapped to all the fixed points of the IB-functional u with  9 @&%7! . Moreover, ¨


at the fixed points, x W Ya

8 a tiu H H , with constant.

Corollary 5.2 When  is uniformly distributed, every algorithm which finds a fixed point of , induces a fixed point of u with  9 @ %7! , and vice versa. When the algorithm finds several fixed points, the solution that maximizes is mapped to the one that minimizes u . Proof: We prove the direction from ML to IB. the opposite direction is similar. We assume point of the likelihood . As a result, this is also a fixed point of the EM algorithm (where q  rs! is defined through an -step). Using observation 4.2 it follows that this fixed-point

¨ 8

that we are given observations

8 ¨@ %('0)! where ¨@&%7! is constant, and ' that define a fixed ¡ A

C

F

is mapped to a fixed-point of u with  9

Since at the fixed point, x W`YFa

u . Rewriting

d

d

8 d

9

¨@ %¤!

, as required.

d

, it is enough to show the relationship between and

8

from Eq.( 10) we get

 ¨@ %('0)! FqX' ¦A' ! 9

8

9¥ e¡

 3 @

@¡HC 9

@¡HC q &rs!XW Ya C q  rs! C ¡@ rs! x @¡HP W Ya A &) Urs! @ C ¨@&%('s)!sq &rs! " C (13)

Using the

G

mapping and observation 4.1 we get

q1 rpU%7!1W Ya q1 rpU%7! q1&rs!

x 

£

@¡HP W`YFa q1&)VUrs!

d @ #T %('0)! q1 rpU%7! " (14)

C

Multiplying both sides by #T %¤! 9 eG e 9 relation, we find that

£ d '

9 @¡HC #T %7!sq1 rpU%7!XW`YFa

£ '

and using the IB Markovian independence

q1 rpU%7! q1 rs! x" @¡HP q1&rs! q1 )VUrs!XW`YFa q1&)VUrs!#"

(15)

Reducing a (constant) ¡ 

£ d '

 5&2 ! 9wx  A HPTq1 rs!sq1 )VUrs!1W Ya(#$&)! to both sides gives:

¡ x ¡ 5 26! 9v3¨&c4s !Tx"(37 402!@9vu ' 

(16)

¨@ %7! .

as required. We emphasize again that this equivalence is for a specific value of  9

Corollary 5.3 When  is uniformly distributed and

, iff it decreases u with  9 ¨@&%7! .

d

£

9 U U, every algorithm decreases

This corollary is a direct result from the above proof that showed the equivalence of the free energy of the model and the IB-functional (up to linear transformations). The previous claims dealt with the special case of uniform prior over  . The following

claims provide similar results for the general case, when the

Claim 5.4 For

§

£¢ (or  £¢ ), all the fixed points of

§8 (or  ) are large enough. are mapped to all the fixed

8 avuti H

.

8

points of u , and vice versa. Moreover, at the fixed points, x W`YFa

Corollary 5.5 When fixed point of u with 

§

¤¢ ¥¢

every algorithm which finds a fixed point of , induces a

, and vice versa. When the algorithm finds several different

fixed points, the solution that maximizes

¦

8

is mapped to the solution that minimize u .

A similar result was recently obtained independently in [8] for the special case of "hard" cluster-

ing. It is also important to keep in mind that in many clustering applications, a uniform prior over  is "forced" during the pre-process to avoid non-desirable bias. In particular this was done in several previous applications of the IB method (see [2] for details).


Small b (iIB) 4 10 Small N

1.22 x 1.215 1.21 1.205 1.2 1.195 0

4. 2

L IB F/r b

(EM) F r(L +b

IB

Large b (iIB) 5 Large N x 10

43

H(Y)) H(Y)

4. 4

4. 5

4. 6 0

10 20 30 40 50

L IB F/r b

(EM) F r(L +b

IB H(Y)) H(Y)

4. 3 2.829

43.5

2.828

44 2.827

20 40 60 44.5 0 10 20 30 40 50 2.826 0

Figure 1: Progress of u and

d

10 20 30 40

for different  and §

values, while running iIB and EM.

Proof: Again, we prove only the direction from ML to IB as the opposite direction is

similar. We are given @ %$'s)! where

point of . Using the

8

F¨

-step in Eq.(6) we extract q  rs! , ending up with a fixed point of the

¡

§

mapping q  rs! becomes deterministic:

¢

8q

C &rs! 9

 £ ¤£

r@9

C

§

9 A CFHP ¨@C %('0)!  ¢ and ' A that define a fixed

 ¢ £

¦¥¨§©¨

follows

¨@ %7!  ¢ % 

f

. Therefore, the

¡ 5 698y¨@&)

EM algorithm. We notice that from

¡

Performing the

G

U%7!hUUA &)VUr !0! 

(17)

otherwise.

mapping we try to update q1&rpU%¤! through Eq.(2). Since now 

¦¥¨§©¨

£

will remain deterministic. Specifically,

q 2!  rpU%7! 9 

£ "£9

r

¢

q1 )VUrs!I9 A &)VUrs! (but q1 rs! 9 @ rs! if the prior over  is not uniform). After completing the

£¢ it follows that q1 rpU%7!

¡  5 698 #$&)VU%¤!pUUq1&)VUr¦!s!

 3 @

 ¡

mapping (including the IB-step), it is easy to verify that we get

(18)

otherwise,

which is equal to its previous value. Therefore, we are at a fixed point of the IB iterative algorithm, and by that at a fixed point of the IB-functional u , as required.

To show that x W`YFa Eq.(13) we see that

8

8 a iqH

u

$('0)$#&%W

d

d 8

9 x W Ya . From (19)

we notice again that at the fixed point

9 x @¡HP W`YFa A¨ )VUrs! @

C ¨@&%('0)1!sq  rs! " C

Using the

G

 3 @

d

9 W

(

mapping and similar algebra as above, we find that

&#$%')

x (37 4026!

 ¢

Ci

¡  5 2 !@9 W (

&#$%') i

 u

d

  5&2 !s!#"

$1')$#$%W

§

£ £ £ ¤2 (20)

 ¢ .

Corollary 5.6 When

How large must

§

every algorithm decreases

iff it decreases u with 

(or  ) be? We address this question through numeric simulations. Yet,

roughly speaking, we notice that the value of

§

for which the above claims (approximately)

hold is related to the "amount of uniformity" in

above proof assumed that each @ %7! is large enough such that q  rs! becomes deterministic.

Clearly, when

¨@&%7!

is less uniform, achieving this situation requires larger

§

values.

¨

¨@&%7!

. Specifically, a crucial step in the C

6 Simulations We performed several different simulations using different IB and ML algorithms. Due to the lack of space, only one example is reported below; In this example we used the


IBT"ra l " w o r l d

X  Y

e

ML  I B m a p p i n g

M L X "ie a l " w o r l d +

T  Y

d

+

+ Iterative IB

+ + E M

+

IB ~ min DKL(q ¢¡¢£ x ,y ,t ) | | Q (x ,y ,t ) ) M L ~ min DKL(p^(x ,y ) | | L (n(x ,y ) : ,) )

Figure 2: In general, ML (for mixture models) and IB operate in different solution spaces. Nonetheless, a sequence of probabilities that is obtained through some optimization routine (e.g., EM) in the "ML space", can be mapped to a sequence of probabilities in the "IB space", and vice versa. The main result of this paper is that under some conditions these two sequences are completely equivalent.

G £

¥¤

¢r© 



¦

different discussion groups. Denoting the documents by  and the words

subset of the 20-Newsgroups corpus [9], consisted of

chosen from

by 2 , after pre-processing [10] we have U U9

£ 

documents randomly

9©¨¨ ' U6U9

£



.

¦



' U2 U9¥§

 

'

§

Since our main goal was to check the differences between IB and ML for different values of (or  ), we further produced another dataset. In this data we randomly choose only

¦

about of the word occurrences for every document

For both datasets we clustered the documents into iterative IB (iIB) algorithm (where we took #$ %$'s)! 9

8 ¡ each algorithm we used the G

£



f%

 , endingupwith

§ £ £. 9©§



clusters, using both EM and the

£

§

(e.g., for iIB, after each iteration we mapped from 3

calculated

In these §



). We repeated this procedure for

£ d

3 @ mapping to calculate

  @

to

G

$ ¨@ %$'s)! '  9 § ' 9 U U).

8

d

and u during the process

$

For

, including the

G

-step, and

different initializations, for each dataset.

runs we found that usually both algorithms improved both functionals mono-

tonically. Comparing the functionals during the process, we see that for the smaller sample size the differences are indeed more evident (Figure 1). Comparing the final values of the smaller value of u . Thus, occasionally, iIB finds a better ML solution or EM finds a better IB solution. This phenomenon was much more common for the large sample size case. 7 Discussion While we have shown that the ML and IB approaches are equivalent under certain conditions, it is important to keep in mind the different assumptions both approaches make regarding the joint distribution over %('0)7'0r . The mixture model (1) assumes that 2 is independent of  given   ! and (2) assumes that #$ )VU%7! is one of a small number (U6U ) of possible conditional distributions. For this reason, the marginal probability over %('0) (i.e.,

 ¦ functionals (after

of §

 

¦

iterations, which typically yielded convergence), we see that in

d

out

runs iIB converged to a smaller value of than EM. In ¨ runs, EM converged to a

#$&%('0) ¥¡ ¦A' !

) is usually different from #( %$'s)! 9

8 $ ¨@ %('0)!

Indeed, an alternative view of

ML estimation is as minimizing 5 698 #T %$'s)!pUU  ¨@&%('0)1! ' A!0!



¥¡.

.

On the other hand, in the IB framework,  is defined through the IB Markovian indepenfor which this relation holds and the marginal distribution over %$'s) is consistent with the input. Interestingly, it is possible to give an alternative formulation for the IB problem which

dence relation:  ©   2 . Therefore, the solution space is the family of distributions


also involves KL minimization [11]. In this formulation the IB problem is related to mini-

mizing

5 698  q  &%('0)7'0rs!pUU  %$'s)¤'srs!0!

  , where  

 %$'s)¤'srs! denotes the family of distributions

  © 2

.

for which the mixture model assumption holds, 

V

In this sense, we may say that while solving the IB problem, one tries to minimize the KL with respect to the "ideal" world, in which  separates  from 2 . On the other hand, while solving the ML problem, one assumes an "ideal" world, and tries to minimize the KL with respect to the given marginal distribution #(&%('s)! . Our theoretical analysis shows that under Once we are able to map between ML and IB, it should be interesting to try and adopt additional concepts from one approach to the other. In the following we provide two such examples. In the IB framework, for large enough  , the quality of a given solution is

8¦¡



mapping, these two procedures are in some cases equivalent (see Figure 2). the

G &@3

G

measured through

R$g  G R$g 8 ¡

which can be used for purposes of model selection and more. Using the

dd¢¡¤££

¦¥

£

[1]. This measure provides a theoretical upper bound,

3 @

mapping, we can now adopt this measure for the ML estimation problem (for large enough component in the IB framework,  , obviously does not. Nonetheless, in principle it is possible to reformulate the IB problem while defining  9   %¤! (without changing the form of the optimal solution). We leave this issue for future research. We have shown that for the multinomial mixture model, ML and IB are equivalent in some cases. It is worth noting that in principle, by choosing a different generative model, one may find further equivalences. Additionally, the IB method was recently extended into the multivariate case, where a new family of IB-like variational problems was presented and solved [11]. A natural question is to look for further generative models that can be mapped to this multivariate IB problems, and we are working in this direction. Acknowledgments Insightful discussions with Nir Friedman, Naftali Tishby and Gal Elidan are greatly appreciated. References ); In EM, the exponential factor ¨@&%7!

in general depends on % . However, its analogous

[1] N. Tishby, F. Pereira, and W. Bialek. The Information Bottleneck method. In Proc. 37th Allerton Conference on Communication and Computation, 1999. [2] N. Slonim. The Information Bottleneck: theory and applications. Ph.D. thesis, The Hebrew University, 2002. [3] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, New York, 1991. [4] T. Hofmann, J. Puzicha, and M. I. Jordan. Learning from dyadic data. In Proc. of NIPS-11, 1998. [5] J. Puzicha, T. Hofmann, and J. M. Buhmann. Histogram clustering for unsupervised segmentation and image retrieval. In Pattern Recognition Letters 20(9), 899-909, 1999. [6] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, vol. 39, pp. 1-38, 1977. [7] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justifies incremental, sparse, and other variants. In M. I. Jordan (editor), Learning in Graphical Models, pp. 355-368, 1998. [8] L. Hermes, T. z¨oller, and J. M. Buhmann. Parametric distributional clustering for image segmentation. In Proc. of European Conference on Computer Vision (ECCV), 2002 [9] K. Lang. Learning to filter netnews. In Proc. of the 12th Int. Conf. on Machine Learning, 1995. [10] N. Slonim, N. Friedman, and N. Tishby. Unsupervised document classification using sequential information maximization. In Proc. of SIGIR-25, 2002. [11] N. Friedman, O. Mosenzon, N. Slonim, and N. Tishby. Multivariate Information Bottleneck. In Proc. of UAI-17, 2001.

§

§

The KL with respect to ¨ is defined as the minimum over all the members in ¨ . Therefore,

here, both arguments of the KL are changing during the process, and the distributions involved in the minimization are over all the three random variables.


