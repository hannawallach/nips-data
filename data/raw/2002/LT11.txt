On the Complexity of Learning the Kernel Matrix

 

Olivier Bousquet, Daniel J. L. Herrmann MPI for Biological Cybernetics Spemannstr. 38, 72076 T¨ubingen Germany olivier.bousquet, daniel.herrmann @tuebingen.mpg.de

¡

Abstract We investigate data based procedures for selecting the kernel when learning with Support Vector Machines. We provide generalization error bounds by estimating the Rademacher complexities of the corresponding function classes. In particular we obtain a complexity bound for function classes induced by kernels with given eigenvectors, i.e., we allow to vary the spectrum and keep the eigenvectors fix. This bound is only a logarithmic factor bigger than the complexity of the function class induced by a single kernel. However, optimizing the margin over such classes leads to overfitting. We thus propose a suitable way of constraining the class. We use an efficient algorithm to solve the resulting optimization problem, present preliminary experimental results, and compare them to an alignment-based approach. 1 Introduction Ever since the introduction of the Support Vector Machine (SVM) algorithm, the question of choosing the kernel has been considered as crucial. Indeed, the success of SVM can be attributed to the joint use of a robust classification procedure (large margin hyperplane) and of a convenient and versatile way of pre-processing the data (kernels). It turns out that with such a decomposition of the learning process into preprocessing and linear classification, the performance highly depends on the preprocessing and much less on the linear classification algorithm to be used (e.g. the kernel perceptron has been shown to have comparable performance to SVM with the same kernel). It is thus of high importance to have a criterion to choose the suitable kernel for a given problem. Ideally, this choice should be dictated by the data itself and the kernel should be 'learned' from the data. The simplest way of doing so is to choose a parametric family of kernels (such as polynomial or Gaussian) and to choose the values of the parameters by crossvalidation. However this approach is clearly limited to a small number of parameters and requires the use of extra data. Chapelle et al. [1] proposed a different approach. They used a bound on the generalization error and computed the gradient of this bound with respect to the kernel parameters. This allows to perform a gradient descent optimization and thus to effectively handle a large number of parameters.


More recently, the idea of using non-parametric classes of kernels has been proposed by Cristianini et al. [2]. They work in a transduction setting where the test data is known in advance. In that setting, the kernel reduces to a positive definite matrix of fixed size (Gram matrix). They consider the set of kernel matrices with given eigenvectors and to choose the eigenvalues using the 'alignment' between the kernel and the data. This criterion has the advantage of being easily computed and optimized. However it has no direct connection to the generalization error. Lanckriet et al. [5] derived a generalization bound in the transduction setting and proposed to use this bound to choose the kernel. Their parameterization is based on a linear combination of given kernel matrices and their bound has the advantage of leading to a convex criterion. They thus proposed to use semidefinite programming for performing the optimization. Actually, if one wants to have a feasible optimization, one needs the criterion to be nice (e.g. differentiable) and the parameterization to be nice (e.g. the criterion is convex with respect to the parameters). The criterion and parameterization proposed by Lanckriet et al. satisfy these requirements. We shall use their approach and develop it further. In this paper, we try to combine the advantages of previous approaches. In particular we propose several classes of kernels and give bounds on their Rademacher complexity. Instead of using semidefinite programming we propose a simple, fast and efficient gradientdescent algorithm. In section 2 we calculate the complexity of different classes of kernels. This yields a convex optimization problem. In section 3 we propose to restrict the optimization of the spectrum such that the order of the eigenvalues is preserved. This convex constraint is implemented by using polynomials of the kernel matrix with non­negative coefficients only. In section 4 we use gradient descent to implement the optimization algorithm. Experimental results on standard data sets (UCI Machine Learning Repository) show in section 5 that indeed overfitting happens if we do not keep the order of the eigenvalues.

2 Bounding the Rademacher Complexity of Matrix Classes

Let us introduce some notation. Let

2

¡£¢ ¥¤§¦©¨¦

 

be a measurable space (the instance space) and

2 ¢ ¨$#$##"¨¥¦¥¨$#$##"¨¢1

 

 &354076 %(' &384%('16 9@ 384A6 9&' 354'16 

¡

ated as follows. A fixed sample of size is given

 

mutation of

is chosen at random (uniformly). The algorithm is given and , i.e. it has access to

¡

IPQBR!SC¢ 'UT VXW '1Y`@aGb9 Bci !Sdfeg!BDC¥ FEHG¦

all instances but to the labels of the first The algorithm picks some classifier classifier on the test instances. Let us denote by

%('

functions from

V V

is defined as

phq!SC¢srutwvyx)©1

2

instances only.

IPQBR!

and the goal is to minimize the error of this

the error of on the testing instances,

B

. The empirical Rademacher complexity of a set

VXW%(' 2 ¨ VBc V!

to

. We consider here the setting of transduction where the data is gener-

and a per-

¢  ¨$##$#$¨¨¢"!¨$#$##$¨&%(' ¨%')!

h

of

where the expectation is taken with respect to the independent Rademacher random variables ). For a vector , means that all

the components of definite.

   

(

are non-negative. For a matrix

 e

, means that



is positive

V V

 ! 

¢ ¦ ¢ ¢ ¤§¦! ¢ ¦

V  e


2.1 General Bound

We denote by

`@!  ¢ ¦ ¤  

 

the function defined as

otherwise. From the proof of Theorem 1 in [5] we obtain the lemma below.

`@!  

¢ ¦ for

 dFe p  ! e ¢¡

, for

£¤¡se

¢ ¦ and

Lemma 1 Let

, for all

¦ ¤

£

B¦¥' h

h

be a set of real-valued functions. For any

we have

, with probability at least

§

© 

¦£  §  

# IPQBR! d b9  Bc2 !A!¨§ V V

¦ VXW rut v©1xA) %(' ¦ VW

V`p9  VBc2 V!! 

Using the comparison inequality for Rademacher processes in [6] we immediately obtain the following corollary.

Corollary 1 Let

least

¦ ¤

£

h

be a set of real-valued functions. For any

¦ VXW

we have

' 2 i

V V

©  

e

, with probability at

# , for all

IPQBR!B¥dh `b9  Bc !A!¨§ h!¨§

¦£¡£  §  

Now we will apply this bound to several different classes of functions. We will thus compute for each of those classes. For a positive definite kernel one

 

considers usually the RKHS formed by the closure of

to the inner product defined by Since we will vary the kernel

i

phq!

)0!R ! R! 21 !43 !6 51!.

¨$# ¨ ¨$# ¢ ¨

x%$'& ¨$#

R! ! Cg!(¥C  

 #"   EG

¡

with respect

it is convenient to distinguish between the vectors in the RKHS and their geometric rela-

¨£7DC3 ¢ ¨

where   tion. We first define the abstract real vectors space

R! E1!

is ¡

the evaluation functional at point . Then we define for a given kernel

as the closure of respect to the scalar product given by

¢ ¨@I ¨ ¢£878F ¨@I£G7GH¢¨ ¨GI

S6 A!)! 6



the Hilbert space

.

6 0!)! £ 7 R! ! )0£ 7 £ 7GC3 )P!R ! ! .1We!Q3.

 

!with

¢FxA5$'&

In this way we can vary , i.e. the geometry, without changing the vector space structure of any finite dimensional subspace of the form can identify the

V¨2TS VS

¡

RKHS above with via and

Lemma 2 Let

U ¡ e

!

be a kernel on , let

   ¨$#$##$¨&'R¥s   0!R2 !A!

x%$& ¨$#$##"¨

£87wC 9¥)B£!7  

£@7

we have

r t vVW xA) ' ¨ ¢ r tsrPt ¨ #

4X6PYa`V`cbRQdfe VXW 

V )Bg £@hpiq3Q U

)  3fu  

V ¢ t ¨ #

 

and . For all

¦

Proof: We have

`V `cbRQdfe VXW

V

)vg £ h iQ3

x) ' ¨ ¢ x) ¨' ¢ VXW

`V `QbRQdfexw VW g

The second equality holds due to Cauchy­Schwarz inequality which becomes here an

equality because of the supremum. Notice that is always non­negative since positive definite. Taking expectations concludes the proof.

The expression in lemma 2 is up to a factor

of functions in with margin

6 0!)!

U

 is

Rademacher complexity of the subspace of

 

us consider the space Moreover, we have

6 ' C

¢ xA%$& ¨#$##$¨S6

£G78F £G7GHA!

. It is important to notice that this is equal to the

which is spanned by the data. Indeed, let

V£h ¦)

)!

i@y U £ h i  U )  3

¨ 3 

the Rademacher complexity of the class

6 0!)!.

¦ '

¦

xA) ' ¨ ¢ VXW

`V `cb 4dfeYV H VW W 

V

¦ '

¡

then this is a Hilbert subspace of

)vg £ h iQ3 U  £ h i U )  3  WH V  

¢ t ¨ #

¦


This proves that we are actually capturing the right complexity since we are computing the complexity of the set of hyperplanes whose normal vector can be expressed as a linear combination of the data points. Now, let's assume that we allow the kernel to change, that

¢¦¥

is, we have a set of possible kernels

Let be

6 ' pw! 6 '

 

, or equivalently a set of possible kernel matrices.

with the inner product induced by

U

and let

hyperplanes with margin

we have

  $# %  %'& )()0

% ¢ ' %#

H"!  VW

¦ %('

 V

R'6 b ! h¤£ C¦ )vg £ h iq3213 U

, and in the space

¡ £ §¡denote

¨©

h¢¡ch

the class of . Using lemma 2

i ¢ rut x)F  ¨ ¢ rut541xA)¡ t ¨ #

h§£ ! y8 %

98

£

denote the Frobenius norm of

 y8 % T VY  VY

, i.e.

98

)  376  

Let

¦

metric positive definite matrices, the Frobenius norm is equal to the -norm of the spec-

¢

trum, i.e.

@8

Also, recall that the trace of such a matrix is equal to the

Finally, recall that for a pos-

hT¡ !BSCd   .BSC  U

$PI ¨#$##"¨A #

V

is given by

and , we have

-norm of its spectrum, i.e.

BC

 

A

 '

y8 % T VXW $A V

It is easy to see that for a fixed kernel matrix

is useful to keep in mind that for certain kernels like the RBF kernel, the trace of the kernel matrix grows approximately linearly in the number of examples, while even if the problem is linearly separable, the margin decreases in the best case to a fixed strictly positive

  U ¢VU ¦  # &   ! BSC

constant. This means that we have

y8 C¢98V"8 y8 y8C98¢ &¢xA)

T  y8 YV98 T DA98i

%¢ S %S#

(1)

itive definite matrix

H

Recall that for sym-

¢ 'VXW ¢ 'VW #

the operator norm

¡

We will denote

@8RQ

`'E`cbR GF 

. Also, it

2.2 Complexity of -balls of kernel matrices The first class that one may consider is the class of all positive definite matrices with -norm

W

, then

bounded by some constant.

Theorem 1 Let X

fe¡ ¦¥W ph  

and

¥¦¥¨i ¨`Y¡dc!

¡

¢

W

. Define

U eXy ¢  # ¦

 ¢a   feqCb8y@8a d@X

¡

Proof: Using (1) we thus have to compute

¡ £ c

Remark: Observe that

 )  3

can always find some



 ¥f ¤a

r t rx) t ¨ ¡ £ c

x) t ¨ ¢  ¢  #

having an eigenvector

worst case element is contained in all three classes.

Recall that in the case of the RBF kernel we have

@h %ih

Xg8 8

 

eXy

  Q



with eigenvalue

 )  3u  .

X

Since we

 we obtain

which concludes the proof.

 

for the same value of . However they have

X

the same Rademacher complexity. From the proof we see that for the calculation of the

complexity only the contribution of in direction of

  U ¢pU ¦ 

X

  !



matters. Therefore for every



the

we would obtain in this case a Rademacher complexity which does not decrease with . It seems clear that proper learning is not possible in such a class, at least from the view point of this way of measuring the complexity. 2.3 Complexity of the convex hull of kernel matrices

Lanckriet et al. [5] considered positive definite linear combinations of

V

i.e. the class

 

¢ ¢ VXWrsV

 

 CeBC&  se X

¢ ¨ #

¡

q kernel matrices,

which means that



(2)


We rather consider the (smaller) class

   



¢ ¢ VWr

s

V V CPBC& ¢ ¨ # X fe

s ¡ (3)

which has simple linear constraints on the feasible parameter set and allows us to use a

straightforward gradient descent algorithm. Notice that

BC

     

r

where .

 "X 

V V V  

is the convex hull of the matrices

 ¨$#$##"¨

¢ 

We obtain the following bound on the Rademacher complexity of this class.

ir eX $eIYr $8&V8 Theorem 2 Let h ! d U¦

be some fixed kernel matrices and

VWHY V

£¢£¢£¢

BC

¡

£

©   as defined in (3) then

 ¨$##$#"¨

Proof: Applying Jensen inequality to equation (1) we calculate first

V

¡ £

)  3 VWH

 

X Y Yr BSC  V

£¢£¢£¢

d eX@ VWH$eIY

£¢£¢£¢

¨

x) VXWrsV ¨ ¢ $PI )  3

¨$##$#"¨V8e ¨X ¨ ¨$##$#"¨

¨

V "8 # V8 Yr & V

BSC

Indeed, consider the sum as a dot product and identify the domain of . Then one recog-

nizes that the first equality holds since the supremum is obtained for

BC

. The second part is due to the fact

.

pe y8

 e e¥! V

) s 3 d "8  

Vs V8

at one of the vectors

8 8  

% ¢

Remark: For a large class of kernel functions the trace of the induced kernel matrix scales hand the operator norm of the induced kernel matrix grows sublinearly in . If the margin is bounded we can therefore ensure learning. With other words, if the kernels inducing Remark: The bound on the complexity for this class is less then the one obtained by Lanckriet et al. [5] for their class. Furthermore, it contains only easily computable quantities. Recognize that in the proof of the above theorem there appears a quantity similar to the maximal alignment of a kernel to arbitrary labels. It is interesting to notice also that the Rademacher complexity somehow measures the average alignment of a kernel to random labels. 2.4 Complexity of spectral classes of kernels Although the class defined in (3) has smaller complexity than the one in (2), we may want to restrict it further. One way of doing so is to consider a set of matrices which have the same eigenvectors. Generally speaking, the kernel encodes some prior about the data and we may want to retain part of this prior and allow the rest to be tuned from the data. A kernel matrix can be decomposed into two parts: its set of eigenvectors and its spectrum (set of eigenvalues). We will fix the eigenvectors and tune the spectrum from the data.

linearly in the sample size . Therefore we have to scale

 ¨$#$##"¨ '

X linearly with . On the other

are consistent, then the convex hull of the kernels is also consistent.

¢

se¡

VYV

 CdBC 

¢ ¢ ¨

§¦©¨¦©

¦ 



¦

Bcp !SCdBSC Bcp ! B ¥ qpGyX !

For a kernel matrix

 

 

¢

¥¤ and

X we consider the spectral class of

X

 

¥¤ ¥¤ 

¤ , given by

(4) ¡

X

¢¨

 

is diag.

¡

Notice that this class can be considered as the convex hull of the matrices

¦

are the eigenvectors (columns of ).

¤



where

V

Remark: We assume that all eigenvalues are different, otherwise the above sets do not agree. Note that Cristianini et al. proposed to optimize the alignment over this class. We obtain the following bound on the complexity of such a class.


all

U se¡ X se ¦

Theorem 3 Let , let be some fixed unitary matrix and

¡ 

¢

i ph !Sd U¦ eX   & #  

as defined in (4), then for

Proof: As before we start with Equation (1). If we denote

©¨ ¡¢¢

V T W §¦

£ A 1

VV%sd3

X

VXWHY V%

£¢£¢£¢

'

¢¡ S' SS

 r t v© $PIY ¢  r t $PIY # X V



Note that

r t x)£ VW'V

we obtain the result. Remark: As a corollary, we obtain that for any number

¦ 

4VWHY

and obtain

£¢£¢£¢

'¥¤T 'VXW¤6¨¦

so that, using Lemma 2.2 in [3] and the fact that

q

of kernel matrices

which commute, the same bound holds on the complexity of their convex hull. 3 Optimizing the Kernel

 ¨$#$##$¨%VS r ¢ ¦ ,

In order to choose the right kernel, we will now consider the bound of Corollary 1. For consider a class of kernels and pick the one that minimizes this bound. This suggests to keep the trace fixed and to maximize the margin. Using Corollary 1 with the bounds derived in Section 2 we immediately obtain a generalization bound for such a procedure. Theorem 3 suggests that optimizing the whole spectrum of the kernel matrix does not significantly increase the complexity. However experiments (see Section 5) show that overfitting occurs. We present here a possible explanation for this phenomenon. Loosely speaking, the kernel encodes some prior information about how the labels two data points should be coupled. Most often this prior corresponds to the knowledge that two similar data points should have a similar label. Now, when optimizing over the spectrum of a kernel matrix, we replace the prior of the kernel function by information given by the data points. It turns out that this leads to overfitting in practical experiments. In section 2.4 we have shown that the complexity of the spectral class is not significantly bigger than the complexity for a fixed kernel, thus the complexity is not a sufficient explanation for this phenomenon. It is likely that when optimizing the spectrum, some crucial part of the prior knowledge is lost. To verify this assumption, we ran some experiments on the real line. We have to separate two clouds of points in . When the clouds are well separated, a Gaussian kernel

a fixed kernel, the complexity term in this bound is proportional to BSC . We will

G

easily deals with the task while if we optimize the spectrum of this kernel with respect to the margin criterion, the classification has arbitrary jumps in the middle of the clouds. A possible way of retaining more of the spatial information contained in the kernel is to keep the order of the eigenvalues fixed. It turns out that in the same experiments, when the eigenvalues are optimized keeping their original order, no spurious jumps occur. We thus propose to add the extra constraint of keeping the order of the eigenvalues fix. This constrain is fulfilled by restricting the functions in (4) to polynomials of degree

q ¥ g¦©¨$##$#"¨

se

 

with non­negative coefficients, i.e. we consider spectral optimization by

¢ ¢ ©Wr V ¢ ¨ #

¡

convex, non­decreasing functions. For a given kernel matrix

  BSC X   s ¤

 V C & ¤

, we thus define

  U & 

s ¡ (5)


Indeed, recent results shows that the Rademacher complexity is reduced in this way [7]. 4 Implementation Following Lanckriet et al. [5] one can formulate the problem of optimizing the margin error bound optimization as a semidefinite programming problem. Here we considered classes of kernels that can be written as linear combinations of kernel matrices with non-negative coefficients and fixed trace. In that case, one obtains the following problem (the subscript

 ¢¡

indicates that we keep the block corresponding to the training data only)

H¤£

&

 

V

©¨

 ¥ Y Y Y  ¦ § subject to s

VXWr ¢ ¨ ¨ ¨



9sV§T §!A !

BSC

&VXW

¨

r

X



9



se se

§ §

¨

 

A







fe ¨

It turns out that implementing this semidefinite program is computationally quite expensive. We thus propose a different approach based on the work of [1]. Indeed, the goal is to minimize a bound of the form  so that if we fix the trace, we simply have to minimize the squared norm of the solution vector . It has been proven in [1] that the gradient of

e % ¡

8! 8

can be computed as "

8#"%$ 8

% ¢ ¤  #

"

'&

F

9

"%$ &

(6)

The algorithm we suggest can thus be described as follows 1. Train an SVM to find the optimal value of with the current kernel matrix.

&

` ` ¢ ¤

0)¥ (

i

 '&

V

&

#

2. Make a gradient step according to (6). Here, (

9F p ! 9



3. Enforce the constraints on the coefficients (normalization and non-negativity). 4. Return to 1 unless a termination criterion is reached. It turns out that this algorithm is very efficient and much simpler to implement than semidefinite programming. Moreover, the semidefinite programming formulations involve a large amount of (redundant) variables, so that a typical SDP solver will take 10 to 100 times longer to perform the same task since it will not use the specific symmetries of the problem. 5 Experiments In order to compare our results we use the same setting as in [5]: we consider the Breast cancer and Sonar databases from the UCI repository and perform 30 random splits with

¥%©! !  y8! ¦! 

¨A ¢ 5 ¤ ¤w¨A8!%¢

60% of the data for training and 40% for¥ testing.

polynomial kernel ,

21

I

§  !  %

I   

denotes the matrix induced by the

the matrix induced by the Gaussian kernel

¨A ¢ $I

First we compare two classes of kernels, linear combinations defined by (2) and convex combination by (3). Figure 1 shows that optimizing the margin on both classes yields roughly the same performance while optimizing the alignment with the ideal kernel is worse. Furthermore, considering the class defined in (3) yields a large improvement on computational efficiency. Next, we compare the optimization of the margin over the classes (3), (4) and (5) with degree 6 polynomials. Figure 1 indicates that tuning the full spectrum leads to overfitting while keeping the order of the eigenvalues gives reasonable performance (this performance is retained when the degree of the polynomial is increased).

, and 43 the matrix by the linear kernel 53

! © ! 

.


  U &  !   U

Breast cancer

BC ©

test error (%) Sonar

BC

test error (%)

&  !

s25.1

7.1

s9.65

18.8

¢  ¢ %#¨   £¢ £¤ ¦¥ £§ 3

¡

0.54 4.2 1.14 16.4

¢ ¢ #X¦

1.09 10.8 1.34 25.1







49.0 27.4 0.55 3.8 1.22 24.4 0.53 3.3 1.17 18.0 0.42 30.8 0.92 33.0 0.9 10.9 1.23 21.4

Figure 1: Performance of optimized kernels for different kernel classes and optimization

procedures (methods proposed in the present paper are typeset in bold face).

3 indicate fixed kernels, see text.

¢

 

¡ and

¢

given by (2) and maximized margin, cf. [5];



given by (3) and maximized alignment with the ideal kernel cf. [2];

maximized margin;

£§

given by (4), i.e. whole spectral class of

D¨ %

given by (5) with

¥q

6 , i.e. keeping the order of the eigenvalues in the spectral class

 %

and maximized margin;



¤

given by (3) and

¥

and maximized margin. The performance of



§

is much better than of



.

6 Conclusion We have derived new bounds on the Rademacher complexity of classes of kernels. These bounds give guarantees for the generalization error when optimizing the margin over a function class induced by several kernel matrices. We propose a general methodology for implementing the optimization procedure for such classes which is simpler and faster than semidefinite programming while retaining the performance. Although the bound for spectral classes is quite tight, we encountered overfitting in the experiments. We overcome this problem by keeping the order of the eigenvalues fix. The motivation of this additional convex constraint is to maintain more information about the similarity measure. The condition to fix the order of the eigenvalues is a new type of constraint. More work is needed to understand this constrain and its relation to the prior knowledge contained in the corresponding class of similarity measures. The complexity of such classes seems also to be much smaller. Therefore we will investigate the generalization behavior on different natural and artificial data sets in future work. Another direction for further investigation is to refine the bounds we obtained, using for instance local Rademacher complexities. References [1] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector machines. Machine Learning, 46(1):131­159, 2002. [2] N. Cristianini, J. Kandola, A. Elisseeff, and J. Shawe-Taylor. On optimizing kernel alignment. Journal of Machine Learning Research, 2002. To appear. [3] L. Devroye and G. Lugosi. Combinatorial Methods in Density Estimation. SpringerVerlag, New York, 2000. [4] J. Kandola, J. Shawe-Taylor and N. Cristianini. Optimizing Kernel Alignment over Combinations of Kernels. In Int Conf Machine Learning, 2002. In press. [5] G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M.I. Jordan. Learning the kernel matrix with semidefinite programming. In Int Conf Machine Learning, 2002. In press. [6] M. Ledoux and M. Talagrand. Probability in Banach Spaces. Springer-Verlag, 1991. [7] O. Bousquet, and D. J. L. Herrmann. Towards Structered Kernel Maschines. Work in Progress.


